{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9548f487-ef4e-48bc-b034-06d3a5938736",
   "metadata": {},
   "outputs": [],
   "source": [
    "activation_seed=21\n",
    "experiment_dir=\"experiment1.5\"\n",
    "prunint_rate=0.3\n",
    "run_mode='benchmark'\n",
    "load_model_dir='./cifar_models'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f6e80ead-659e-47bc-bf87-d87c13b16229",
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------- Setting up parameters -------\n",
      "dumping parameters at  /storage/coda1/p-jli3175/0/mwu385/OTfusion_pruning_final/otfusion/exp_sample/configurations\n",
      "The parameters are: \n",
      " Namespace(experiment_dir='experiment1.5', prunint_rate=0.3, run_mode='benchmark', load_model_dir='./cifar_models', prune_times=4, finetunetimes_benchmark=0, n_epochs=0, batch_size_train=64, batch_size_test=1000, learning_rate=0.004, momentum=0.5, log_interval=100, to_download=True, disable_bias=True, dataset='Cifar10', num_models=2, model_name='vgg11_nobias', config_file=None, config_dir='/storage/coda1/p-jli3175/0/mwu385/OTfusion_pruning_final/otfusion/exp_sample/configurations', num_hidden_nodes=400, num_hidden_nodes1=400, num_hidden_nodes2=200, num_hidden_nodes3=100, num_hidden_nodes4=50, sweep_id=90, gpu_id=0, skip_last_layer=False, skip_last_layer_type='average', debug=False, cifar_style_data=False, activation_histograms=False, act_num_samples=100, softmax_temperature=1, activation_mode=None, options_type='generic', deprecated=None, save_result_file='sample.csv', sweep_name='exp_sample', reg=0.01, reg_m=0.001, ground_metric='euclidean', ground_metric_normalize='none', not_squared=True, clip_gm=False, clip_min=0, clip_max=5, tmap_stats=False, ensemble_step=0.5, ground_metric_eff=True, retrain=30, retrain_lr_decay=-1, retrain_lr_decay_factor=None, retrain_lr_decay_epochs=None, retrain_avg_only=False, retrain_geometric_only=False, load_models='', ckpt_type='best', recheck_cifar=True, recheck_acc=False, eval_aligned=True, enable_dropout=False, dump_model=False, dump_final_models=False, correction=True, activation_seed=21, weight_stats=True, sinkhorn_type='normal', geom_ensemble_type='wts', act_bug=False, standardize_acts=False, transform_acts=False, center_acts=False, prelu_acts=True, pool_acts=False, pool_relu=False, normalize_acts=False, normalize_wts=False, gromov=False, gromov_loss='square_loss', tensorboard_root='./tensorboard', tensorboard=False, same_model=-1, dist_normalize=False, update_acts=False, past_correction=True, partial_reshape=False, choice='0 2 4 6 8', diff_init=False, partition_type='labels', personal_class_idx=9, partition_dataloader=-1, personal_split_frac=0.1, exact=True, skip_personal_idx=False, prediction_wts=False, width_ratio=1, proper_marginals=False, retrain_seed=-1, no_random_trainloaders=False, reinit_trainloaders=False, second_model_name=None, print_distances=False, deterministic=False, skip_retrain=-1, importance=None, unbalanced=False, temperature=20, alpha=0.7, dist_epochs=60, handle_skips=False, timestamp='2023-09-18_23-39-36_417334', rootdir='/storage/coda1/p-jli3175/0/mwu385/OTfusion_pruning_final/otfusion/exp_sample', baseroot='/storage/coda1/p-jli3175/0/mwu385/OTfusion_pruning_final/otfusion', result_dir='/storage/coda1/p-jli3175/0/mwu385/OTfusion_pruning_final/otfusion/exp_sample/results', exp_name='exp_2023-09-18_23-39-36_417334', csv_dir='/storage/coda1/p-jli3175/0/mwu385/OTfusion_pruning_final/otfusion/exp_sample/csv')\n",
      "refactored get_config\n",
      "------- Obtain dataloaders -------\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "------- Training independent models -------\n",
      "QWERTY: Enter Cifar 1\n",
      "number of epochs would b>>>>>>>e  300\n",
      "in _make_layers [Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), AvgPool2d(kernel_size=1, stride=1, padding=0)]\n",
      "Relu Inplace is  False\n",
      "model parameters are \n",
      " [torch.Size([64, 3, 3, 3]), torch.Size([128, 64, 3, 3]), torch.Size([256, 128, 3, 3]), torch.Size([256, 256, 3, 3]), torch.Size([512, 256, 3, 3]), torch.Size([512, 512, 3, 3]), torch.Size([512, 512, 3, 3]), torch.Size([512, 512, 3, 3]), torch.Size([10, 512])]\n",
      "Loading model at path ./cifar_models/model_0/best.checkpoint which had accuracy 0.9030999821424489 and at epoch 127\n",
      "\n",
      "--------- Testing in global mode ---------\n",
      "size of test_loader dataset:  10000\n",
      "\n",
      "Test set: Avg. loss: 0.0003, Accuracy: 9019/10000 (90%)\n",
      "\n",
      "He says accury is  90.30999821424489 but actually 90.19\n",
      "QWERTY: Enter Cifar 1\n",
      "number of epochs would b>>>>>>>e  300\n",
      "in _make_layers [Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), AvgPool2d(kernel_size=1, stride=1, padding=0)]\n",
      "Relu Inplace is  False\n",
      "model parameters are \n",
      " [torch.Size([64, 3, 3, 3]), torch.Size([128, 64, 3, 3]), torch.Size([256, 128, 3, 3]), torch.Size([256, 256, 3, 3]), torch.Size([512, 256, 3, 3]), torch.Size([512, 512, 3, 3]), torch.Size([512, 512, 3, 3]), torch.Size([512, 512, 3, 3]), torch.Size([10, 512])]\n",
      "Loading model at path ./cifar_models/model_1/best.checkpoint which had accuracy 0.9049999803304669 and at epoch 134\n",
      "\n",
      "--------- Testing in global mode ---------\n",
      "size of test_loader dataset:  10000\n",
      "\n",
      "Test set: Avg. loss: 0.0003, Accuracy: 9046/10000 (90%)\n",
      "\n",
      "He says accury is  90.4999980330467 but actually 90.46\n",
      "========================================\n",
      "========================================\n",
      "TEST:args.learning_rate,args.momentum 0.004 0.5\n",
      "========================================\n",
      "========================================\n",
      "========================================\n",
      "\n",
      "--------- Testing in global mode ---------\n",
      "size of test_loader dataset:  10000\n",
      "\n",
      "Test set: Avg. loss: 0.0003, Accuracy: 9019/10000 (90%)\n",
      "\n",
      "========================================\n",
      "========================================\n",
      "TEST:args.learning_rate,args.momentum 0.004 0.5\n",
      "========================================\n",
      "========================================\n",
      "========================================\n",
      "\n",
      "--------- Testing in global mode ---------\n",
      "size of test_loader dataset:  10000\n",
      "\n",
      "Test set: Avg. loss: 0.0003, Accuracy: 9046/10000 (90%)\n",
      "\n",
      "===============================================\n",
      "===============================================\n",
      "before Ensemble,acc: [90.19, 90.46] loss [0.0003167657434940338, 0.00032148255407810213]\n",
      "===============================================\n",
      "===============================================\n",
      "check zero rate for model 0\n",
      "odict_keys(['features.0.weight', 'features.3.weight', 'features.6.weight', 'features.8.weight', 'features.11.weight', 'features.13.weight', 'features.16.weight', 'features.18.weight', 'classifier.weight'])\n",
      "the ratio of zero for features.0.weight\n",
      "tensor(0., device='cuda:0')\n",
      "the ratio of zero for features.3.weight\n",
      "tensor(0., device='cuda:0')\n",
      "the ratio of zero for features.6.weight\n",
      "tensor(0., device='cuda:0')\n",
      "the ratio of zero for features.8.weight\n",
      "tensor(0., device='cuda:0')\n",
      "the ratio of zero for features.11.weight\n",
      "tensor(0., device='cuda:0')\n",
      "the ratio of zero for features.13.weight\n",
      "tensor(0., device='cuda:0')\n",
      "the ratio of zero for features.16.weight\n",
      "tensor(0., device='cuda:0')\n",
      "the ratio of zero for features.18.weight\n",
      "tensor(0., device='cuda:0')\n",
      "the ratio of zero for classifier.weight\n",
      "tensor(0., device='cuda:0')\n",
      "check zero rate for model 1\n",
      "odict_keys(['features.0.weight', 'features.3.weight', 'features.6.weight', 'features.8.weight', 'features.11.weight', 'features.13.weight', 'features.16.weight', 'features.18.weight', 'classifier.weight'])\n",
      "the ratio of zero for features.0.weight\n",
      "tensor(0., device='cuda:0')\n",
      "the ratio of zero for features.3.weight\n",
      "tensor(0., device='cuda:0')\n",
      "the ratio of zero for features.6.weight\n",
      "tensor(0., device='cuda:0')\n",
      "the ratio of zero for features.8.weight\n",
      "tensor(0., device='cuda:0')\n",
      "the ratio of zero for features.11.weight\n",
      "tensor(0., device='cuda:0')\n",
      "the ratio of zero for features.13.weight\n",
      "tensor(0., device='cuda:0')\n",
      "the ratio of zero for features.16.weight\n",
      "tensor(0., device='cuda:0')\n",
      "the ratio of zero for features.18.weight\n",
      "tensor(0., device='cuda:0')\n",
      "the ratio of zero for classifier.weight\n",
      "tensor(0., device='cuda:0')\n",
      "layer features.0.weight has #params  1728\n",
      "layer features.3.weight has #params  73728\n",
      "layer features.6.weight has #params  294912\n",
      "layer features.8.weight has #params  589824\n",
      "layer features.11.weight has #params  1179648\n",
      "layer features.13.weight has #params  2359296\n",
      "layer features.16.weight has #params  2359296\n",
      "layer features.18.weight has #params  2359296\n",
      "layer classifier.weight has #params  5120\n",
      "Activation Timer start\n",
      "Activation Timer ends\n",
      "------- Geometric Ensembling -------\n",
      "Timer start\n",
      "Previous layer shape is  None\n",
      "Processing the coordinates to form ground_metric\n",
      "dont leave off the squaring of the ground metric\n",
      "returns a uniform measure of cardinality:  64\n",
      "returns a uniform measure of cardinality:  64\n",
      "the transport map is  tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')\n",
      "Ratio of trace to the matrix sum:  tensor(0.0469, device='cuda:0')\n",
      "Here, trace is 2.9999806880950928 and matrix sum is 63.99958801269531 \n",
      "this is past correction for weight mode\n",
      "Shape of aligned wt is  torch.Size([64, 3, 9])\n",
      "Shape of fc_layer0_weight_data is  torch.Size([64, 3, 9])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "in _make_layers [Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), AvgPool2d(kernel_size=1, stride=1, padding=0)]\n",
      "Relu Inplace is  False\n",
      "model parameters are \n",
      " [torch.Size([64, 3, 3, 3]), torch.Size([128, 64, 3, 3]), torch.Size([256, 128, 3, 3]), torch.Size([256, 256, 3, 3]), torch.Size([512, 256, 3, 3]), torch.Size([512, 512, 3, 3]), torch.Size([512, 512, 3, 3]), torch.Size([512, 512, 3, 3]), torch.Size([10, 512])]\n",
      "len of model_state_dict is  9\n",
      "len of new_params is  1\n",
      "updated parameters for layer  features.0.weight\n",
      "\n",
      "--------- Testing in global mode ---------\n",
      "size of test_loader dataset:  10000\n",
      "\n",
      "Test set: Avg. loss: 0.0027, Accuracy: 1250/10000 (12%)\n",
      "\n",
      "accuracy after update is  12.5\n",
      "For layer idx 0, accuracy of the updated model is 12.5\n",
      "Previous layer shape is  torch.Size([64, 3, 3, 3])\n",
      "shape of layer: model 0 torch.Size([128, 64, 9])\n",
      "shape of layer: model 1 torch.Size([128, 64, 9])\n",
      "shape of previous transport map torch.Size([64, 64])\n",
      "Processing the coordinates to form ground_metric\n",
      "dont leave off the squaring of the ground metric\n",
      "returns a uniform measure of cardinality:  128\n",
      "returns a uniform measure of cardinality:  128\n",
      "the transport map is  tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        ...,\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0078],\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],\n",
      "       device='cuda:0')\n",
      "Ratio of trace to the matrix sum:  tensor(0.0078, device='cuda:0')\n",
      "Here, trace is 0.9999872446060181 and matrix sum is 127.99836730957031 \n",
      "this is past correction for weight mode\n",
      "Shape of aligned wt is  torch.Size([128, 64, 9])\n",
      "Shape of fc_layer0_weight_data is  torch.Size([128, 64, 9])\n",
      "in _make_layers [Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), AvgPool2d(kernel_size=1, stride=1, padding=0)]\n",
      "Relu Inplace is  False\n",
      "model parameters are \n",
      " [torch.Size([64, 3, 3, 3]), torch.Size([128, 64, 3, 3]), torch.Size([256, 128, 3, 3]), torch.Size([256, 256, 3, 3]), torch.Size([512, 256, 3, 3]), torch.Size([512, 512, 3, 3]), torch.Size([512, 512, 3, 3]), torch.Size([512, 512, 3, 3]), torch.Size([10, 512])]\n",
      "len of model_state_dict is  9\n",
      "len of new_params is  2\n",
      "updated parameters for layer  features.0.weight\n",
      "updated parameters for layer  features.3.weight\n",
      "\n",
      "--------- Testing in global mode ---------\n",
      "size of test_loader dataset:  10000\n",
      "\n",
      "Test set: Avg. loss: 0.0024, Accuracy: 1036/10000 (10%)\n",
      "\n",
      "accuracy after update is  10.36\n",
      "For layer idx 1, accuracy of the updated model is 10.36\n",
      "Previous layer shape is  torch.Size([128, 64, 3, 3])\n",
      "shape of layer: model 0 torch.Size([256, 128, 9])\n",
      "shape of layer: model 1 torch.Size([256, 128, 9])\n",
      "shape of previous transport map torch.Size([128, 128])\n",
      "Processing the coordinates to form ground_metric\n",
      "dont leave off the squaring of the ground metric\n",
      "returns a uniform measure of cardinality:  256\n",
      "returns a uniform measure of cardinality:  256\n",
      "the transport map is  tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')\n",
      "Ratio of trace to the matrix sum:  tensor(0., device='cuda:0')\n",
      "Here, trace is 0.0 and matrix sum is 255.99343872070312 \n",
      "this is past correction for weight mode\n",
      "Shape of aligned wt is  torch.Size([256, 128, 9])\n",
      "Shape of fc_layer0_weight_data is  torch.Size([256, 128, 9])\n",
      "in _make_layers [Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), AvgPool2d(kernel_size=1, stride=1, padding=0)]\n",
      "Relu Inplace is  False\n",
      "model parameters are \n",
      " [torch.Size([64, 3, 3, 3]), torch.Size([128, 64, 3, 3]), torch.Size([256, 128, 3, 3]), torch.Size([256, 256, 3, 3]), torch.Size([512, 256, 3, 3]), torch.Size([512, 512, 3, 3]), torch.Size([512, 512, 3, 3]), torch.Size([512, 512, 3, 3]), torch.Size([10, 512])]\n",
      "len of model_state_dict is  9\n",
      "len of new_params is  3\n",
      "updated parameters for layer  features.0.weight\n",
      "updated parameters for layer  features.3.weight\n",
      "updated parameters for layer  features.6.weight\n",
      "\n",
      "--------- Testing in global mode ---------\n",
      "size of test_loader dataset:  10000\n",
      "\n",
      "Test set: Avg. loss: 0.0023, Accuracy: 1148/10000 (11%)\n",
      "\n",
      "accuracy after update is  11.48\n",
      "For layer idx 2, accuracy of the updated model is 11.48\n",
      "Previous layer shape is  torch.Size([256, 128, 3, 3])\n",
      "shape of layer: model 0 torch.Size([256, 256, 9])\n",
      "shape of layer: model 1 torch.Size([256, 256, 9])\n",
      "shape of previous transport map torch.Size([256, 256])\n",
      "Processing the coordinates to form ground_metric\n",
      "dont leave off the squaring of the ground metric\n",
      "returns a uniform measure of cardinality:  256\n",
      "returns a uniform measure of cardinality:  256\n",
      "the transport map is  tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        ...,\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.0039, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],\n",
      "       device='cuda:0')\n",
      "Ratio of trace to the matrix sum:  tensor(0.0078, device='cuda:0')\n",
      "Here, trace is 1.9999487400054932 and matrix sum is 255.99343872070312 \n",
      "this is past correction for weight mode\n",
      "Shape of aligned wt is  torch.Size([256, 256, 9])\n",
      "Shape of fc_layer0_weight_data is  torch.Size([256, 256, 9])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "in _make_layers [Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), AvgPool2d(kernel_size=1, stride=1, padding=0)]\n",
      "Relu Inplace is  False\n",
      "model parameters are \n",
      " [torch.Size([64, 3, 3, 3]), torch.Size([128, 64, 3, 3]), torch.Size([256, 128, 3, 3]), torch.Size([256, 256, 3, 3]), torch.Size([512, 256, 3, 3]), torch.Size([512, 512, 3, 3]), torch.Size([512, 512, 3, 3]), torch.Size([512, 512, 3, 3]), torch.Size([10, 512])]\n",
      "len of model_state_dict is  9\n",
      "len of new_params is  4\n",
      "updated parameters for layer  features.0.weight\n",
      "updated parameters for layer  features.3.weight\n",
      "updated parameters for layer  features.6.weight\n",
      "updated parameters for layer  features.8.weight\n",
      "\n",
      "--------- Testing in global mode ---------\n",
      "size of test_loader dataset:  10000\n",
      "\n",
      "Test set: Avg. loss: 0.0024, Accuracy: 1248/10000 (12%)\n",
      "\n",
      "accuracy after update is  12.48\n",
      "For layer idx 3, accuracy of the updated model is 12.48\n",
      "Previous layer shape is  torch.Size([256, 256, 3, 3])\n",
      "shape of layer: model 0 torch.Size([512, 256, 9])\n",
      "shape of layer: model 1 torch.Size([512, 256, 9])\n",
      "shape of previous transport map torch.Size([256, 256])\n",
      "Processing the coordinates to form ground_metric\n",
      "dont leave off the squaring of the ground metric\n",
      "returns a uniform measure of cardinality:  512\n",
      "returns a uniform measure of cardinality:  512\n",
      "the transport map is  tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')\n",
      "Ratio of trace to the matrix sum:  tensor(0.0039, device='cuda:0')\n",
      "Here, trace is 1.9998977184295654 and matrix sum is 511.97381591796875 \n",
      "this is past correction for weight mode\n",
      "Shape of aligned wt is  torch.Size([512, 256, 9])\n",
      "Shape of fc_layer0_weight_data is  torch.Size([512, 256, 9])\n",
      "in _make_layers [Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), AvgPool2d(kernel_size=1, stride=1, padding=0)]\n",
      "Relu Inplace is  False\n",
      "model parameters are \n",
      " [torch.Size([64, 3, 3, 3]), torch.Size([128, 64, 3, 3]), torch.Size([256, 128, 3, 3]), torch.Size([256, 256, 3, 3]), torch.Size([512, 256, 3, 3]), torch.Size([512, 512, 3, 3]), torch.Size([512, 512, 3, 3]), torch.Size([512, 512, 3, 3]), torch.Size([10, 512])]\n",
      "len of model_state_dict is  9\n",
      "len of new_params is  5\n",
      "updated parameters for layer  features.0.weight\n",
      "updated parameters for layer  features.3.weight\n",
      "updated parameters for layer  features.6.weight\n",
      "updated parameters for layer  features.8.weight\n",
      "updated parameters for layer  features.11.weight\n",
      "\n",
      "--------- Testing in global mode ---------\n",
      "size of test_loader dataset:  10000\n",
      "\n",
      "Test set: Avg. loss: 0.0027, Accuracy: 904/10000 (9%)\n",
      "\n",
      "accuracy after update is  9.04\n",
      "For layer idx 4, accuracy of the updated model is 9.04\n",
      "Previous layer shape is  torch.Size([512, 256, 3, 3])\n",
      "shape of layer: model 0 torch.Size([512, 512, 9])\n",
      "shape of layer: model 1 torch.Size([512, 512, 9])\n",
      "shape of previous transport map torch.Size([512, 512])\n",
      "Processing the coordinates to form ground_metric\n",
      "dont leave off the squaring of the ground metric\n",
      "returns a uniform measure of cardinality:  512\n",
      "returns a uniform measure of cardinality:  512\n",
      "the transport map is  tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')\n",
      "Ratio of trace to the matrix sum:  tensor(0.0020, device='cuda:0')\n",
      "Here, trace is 0.9999488592147827 and matrix sum is 511.97381591796875 \n",
      "this is past correction for weight mode\n",
      "Shape of aligned wt is  torch.Size([512, 512, 9])\n",
      "Shape of fc_layer0_weight_data is  torch.Size([512, 512, 9])\n",
      "in _make_layers [Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), AvgPool2d(kernel_size=1, stride=1, padding=0)]\n",
      "Relu Inplace is  False\n",
      "model parameters are \n",
      " [torch.Size([64, 3, 3, 3]), torch.Size([128, 64, 3, 3]), torch.Size([256, 128, 3, 3]), torch.Size([256, 256, 3, 3]), torch.Size([512, 256, 3, 3]), torch.Size([512, 512, 3, 3]), torch.Size([512, 512, 3, 3]), torch.Size([512, 512, 3, 3]), torch.Size([10, 512])]\n",
      "len of model_state_dict is  9\n",
      "len of new_params is  6\n",
      "updated parameters for layer  features.0.weight\n",
      "updated parameters for layer  features.3.weight\n",
      "updated parameters for layer  features.6.weight\n",
      "updated parameters for layer  features.8.weight\n",
      "updated parameters for layer  features.11.weight\n",
      "updated parameters for layer  features.13.weight\n",
      "\n",
      "--------- Testing in global mode ---------\n",
      "size of test_loader dataset:  10000\n",
      "\n",
      "Test set: Avg. loss: 0.0024, Accuracy: 1411/10000 (14%)\n",
      "\n",
      "accuracy after update is  14.11\n",
      "For layer idx 5, accuracy of the updated model is 14.11\n",
      "Previous layer shape is  torch.Size([512, 512, 3, 3])\n",
      "shape of layer: model 0 torch.Size([512, 512, 9])\n",
      "shape of layer: model 1 torch.Size([512, 512, 9])\n",
      "shape of previous transport map torch.Size([512, 512])\n",
      "Processing the coordinates to form ground_metric\n",
      "dont leave off the squaring of the ground metric\n",
      "returns a uniform measure of cardinality:  512\n",
      "returns a uniform measure of cardinality:  512\n",
      "the transport map is  tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ratio of trace to the matrix sum:  tensor(0.0020, device='cuda:0')\n",
      "Here, trace is 0.9999488592147827 and matrix sum is 511.97381591796875 \n",
      "this is past correction for weight mode\n",
      "Shape of aligned wt is  torch.Size([512, 512, 9])\n",
      "Shape of fc_layer0_weight_data is  torch.Size([512, 512, 9])\n",
      "in _make_layers [Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), AvgPool2d(kernel_size=1, stride=1, padding=0)]\n",
      "Relu Inplace is  False\n",
      "model parameters are \n",
      " [torch.Size([64, 3, 3, 3]), torch.Size([128, 64, 3, 3]), torch.Size([256, 128, 3, 3]), torch.Size([256, 256, 3, 3]), torch.Size([512, 256, 3, 3]), torch.Size([512, 512, 3, 3]), torch.Size([512, 512, 3, 3]), torch.Size([512, 512, 3, 3]), torch.Size([10, 512])]\n",
      "len of model_state_dict is  9\n",
      "len of new_params is  7\n",
      "updated parameters for layer  features.0.weight\n",
      "updated parameters for layer  features.3.weight\n",
      "updated parameters for layer  features.6.weight\n",
      "updated parameters for layer  features.8.weight\n",
      "updated parameters for layer  features.11.weight\n",
      "updated parameters for layer  features.13.weight\n",
      "updated parameters for layer  features.16.weight\n",
      "\n",
      "--------- Testing in global mode ---------\n",
      "size of test_loader dataset:  10000\n",
      "\n",
      "Test set: Avg. loss: 0.0022, Accuracy: 2219/10000 (22%)\n",
      "\n",
      "accuracy after update is  22.19\n",
      "For layer idx 6, accuracy of the updated model is 22.19\n",
      "Previous layer shape is  torch.Size([512, 512, 3, 3])\n",
      "shape of layer: model 0 torch.Size([512, 512, 9])\n",
      "shape of layer: model 1 torch.Size([512, 512, 9])\n",
      "shape of previous transport map torch.Size([512, 512])\n",
      "Processing the coordinates to form ground_metric\n",
      "dont leave off the squaring of the ground metric\n",
      "returns a uniform measure of cardinality:  512\n",
      "returns a uniform measure of cardinality:  512\n",
      "the transport map is  tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        [0.0020, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        ...,\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],\n",
      "       device='cuda:0')\n",
      "Ratio of trace to the matrix sum:  tensor(0., device='cuda:0')\n",
      "Here, trace is 0.0 and matrix sum is 511.97381591796875 \n",
      "this is past correction for weight mode\n",
      "Shape of aligned wt is  torch.Size([512, 512, 9])\n",
      "Shape of fc_layer0_weight_data is  torch.Size([512, 512, 9])\n",
      "in _make_layers [Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), AvgPool2d(kernel_size=1, stride=1, padding=0)]\n",
      "Relu Inplace is  False\n",
      "model parameters are \n",
      " [torch.Size([64, 3, 3, 3]), torch.Size([128, 64, 3, 3]), torch.Size([256, 128, 3, 3]), torch.Size([256, 256, 3, 3]), torch.Size([512, 256, 3, 3]), torch.Size([512, 512, 3, 3]), torch.Size([512, 512, 3, 3]), torch.Size([512, 512, 3, 3]), torch.Size([10, 512])]\n",
      "len of model_state_dict is  9\n",
      "len of new_params is  8\n",
      "updated parameters for layer  features.0.weight\n",
      "updated parameters for layer  features.3.weight\n",
      "updated parameters for layer  features.6.weight\n",
      "updated parameters for layer  features.8.weight\n",
      "updated parameters for layer  features.11.weight\n",
      "updated parameters for layer  features.13.weight\n",
      "updated parameters for layer  features.16.weight\n",
      "updated parameters for layer  features.18.weight\n",
      "\n",
      "--------- Testing in global mode ---------\n",
      "size of test_loader dataset:  10000\n",
      "\n",
      "Test set: Avg. loss: 0.0024, Accuracy: 582/10000 (6%)\n",
      "\n",
      "accuracy after update is  5.82\n",
      "For layer idx 7, accuracy of the updated model is 5.82\n",
      "Previous layer shape is  torch.Size([512, 512, 3, 3])\n",
      "shape of layer: model 0 torch.Size([10, 512])\n",
      "shape of layer: model 1 torch.Size([10, 512])\n",
      "shape of previous transport map torch.Size([512, 512])\n",
      "Processing the coordinates to form ground_metric\n",
      "dont leave off the squaring of the ground metric\n",
      "ground metric is  tensor([[1.4240, 2.4871, 2.7803, 2.8718, 2.7374, 2.8798, 2.7275, 2.6341, 2.5641,\n",
      "         2.5921],\n",
      "        [2.4888, 1.3252, 2.6731, 2.5913, 2.7583, 2.5500, 2.4173, 2.5214, 2.3047,\n",
      "         2.4100],\n",
      "        [2.7599, 2.6711, 1.3831, 2.9464, 2.9064, 2.8700, 2.7276, 2.8384, 2.7484,\n",
      "         2.7866],\n",
      "        [2.8351, 2.6790, 2.8918, 1.5430, 2.8604, 2.9158, 2.7573, 2.7979, 2.8163,\n",
      "         2.6353],\n",
      "        [2.7550, 2.5959, 2.8634, 2.8449, 1.5042, 2.7447, 2.7852, 2.6810, 2.7030,\n",
      "         2.7293],\n",
      "        [2.9189, 2.5394, 2.9468, 2.8414, 2.7271, 1.5501, 2.7780, 2.7302, 2.7976,\n",
      "         2.6695],\n",
      "        [2.6778, 2.4456, 2.7189, 2.9110, 2.6487, 2.8525, 1.3930, 2.6890, 2.4980,\n",
      "         2.5559],\n",
      "        [2.7393, 2.5495, 2.7799, 2.7902, 2.7150, 2.6589, 2.7189, 1.3924, 2.5582,\n",
      "         2.5120],\n",
      "        [2.5971, 2.3659, 2.7693, 2.6871, 2.7025, 2.8279, 2.5324, 2.6365, 1.2999,\n",
      "         2.4040],\n",
      "        [2.5918, 2.2730, 2.7815, 2.7576, 2.6696, 2.7272, 2.5482, 2.5289, 2.5207,\n",
      "         1.2878]], device='cuda:0', grad_fn=<PowBackward0>)\n",
      "returns a uniform measure of cardinality:  10\n",
      "returns a uniform measure of cardinality:  10\n",
      "the transport map is  tensor([[0.1000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0000, 0.1000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0000, 0.0000, 0.1000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.1000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.1000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.1000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.1000, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.1000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.1000,\n",
      "         0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.1000]], device='cuda:0')\n",
      "Ratio of trace to the matrix sum:  tensor(1., device='cuda:0')\n",
      "Here, trace is 9.999990463256836 and matrix sum is 9.999990463256836 \n",
      "this is past correction for weight mode\n",
      "Shape of aligned wt is  torch.Size([10, 512])\n",
      "Shape of fc_layer0_weight_data is  torch.Size([10, 512])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "in _make_layers [Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), AvgPool2d(kernel_size=1, stride=1, padding=0)]\n",
      "Relu Inplace is  False\n",
      "model parameters are \n",
      " [torch.Size([64, 3, 3, 3]), torch.Size([128, 64, 3, 3]), torch.Size([256, 128, 3, 3]), torch.Size([256, 256, 3, 3]), torch.Size([512, 256, 3, 3]), torch.Size([512, 512, 3, 3]), torch.Size([512, 512, 3, 3]), torch.Size([512, 512, 3, 3]), torch.Size([10, 512])]\n",
      "len of model_state_dict is  9\n",
      "len of new_params is  9\n",
      "updated parameters for layer  features.0.weight\n",
      "updated parameters for layer  features.3.weight\n",
      "updated parameters for layer  features.6.weight\n",
      "updated parameters for layer  features.8.weight\n",
      "updated parameters for layer  features.11.weight\n",
      "updated parameters for layer  features.13.weight\n",
      "updated parameters for layer  features.16.weight\n",
      "updated parameters for layer  features.18.weight\n",
      "updated parameters for layer  classifier.weight\n",
      "\n",
      "--------- Testing in global mode ---------\n",
      "size of test_loader dataset:  10000\n",
      "\n",
      "Test set: Avg. loss: 0.0003, Accuracy: 9019/10000 (90%)\n",
      "\n",
      "accuracy after update is  90.19\n",
      "For layer idx 8, accuracy of the updated model is 90.19\n",
      "using independent method\n",
      "in _make_layers [Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), AvgPool2d(kernel_size=1, stride=1, padding=0)]\n",
      "Relu Inplace is  False\n",
      "model parameters are \n",
      " [torch.Size([64, 3, 3, 3]), torch.Size([128, 64, 3, 3]), torch.Size([256, 128, 3, 3]), torch.Size([256, 256, 3, 3]), torch.Size([512, 256, 3, 3]), torch.Size([512, 512, 3, 3]), torch.Size([512, 512, 3, 3]), torch.Size([512, 512, 3, 3]), torch.Size([10, 512])]\n",
      "\n",
      "--------- Testing in global mode ---------\n",
      "size of test_loader dataset:  10000\n",
      "\n",
      "Test set: Avg. loss: 0.0023, Accuracy: 1090/10000 (11%)\n",
      "\n",
      "len of model parameters and avg aligned layers is  9 9\n",
      "len of model_state_dict is  9\n",
      "len of param_list is  9\n",
      "\n",
      "--------- Testing in global mode ---------\n",
      "size of test_loader dataset:  10000\n",
      "\n",
      "Test set: Avg. loss: 0.0015, Accuracy: 8544/10000 (85%)\n",
      "\n",
      "===============================================\n",
      "===============================================\n",
      "After Ensemble,acc: 85.44 loss 0.001490088927745819\n",
      "===============================================\n",
      "===============================================\n",
      "Timer ends\n",
      "Time taken for geometric ensembling is 61.25327685684897 seconds\n",
      "------- Prediction based ensembling -------\n",
      "\n",
      "Test set: Avg. loss: 0.0003, Accuracy: 9134/10000 (91%)\n",
      "\n",
      "------- Naive ensembling of weights -------\n",
      "[torch.Size([64, 3, 3, 3]), torch.Size([64, 3, 3, 3])]\n",
      "torch.Size([64, 3, 3, 3])\n",
      "[torch.Size([128, 64, 3, 3]), torch.Size([128, 64, 3, 3])]\n",
      "torch.Size([128, 64, 3, 3])\n",
      "[torch.Size([256, 128, 3, 3]), torch.Size([256, 128, 3, 3])]\n",
      "torch.Size([256, 128, 3, 3])\n",
      "[torch.Size([256, 256, 3, 3]), torch.Size([256, 256, 3, 3])]\n",
      "torch.Size([256, 256, 3, 3])\n",
      "[torch.Size([512, 256, 3, 3]), torch.Size([512, 256, 3, 3])]\n",
      "torch.Size([512, 256, 3, 3])\n",
      "[torch.Size([512, 512, 3, 3]), torch.Size([512, 512, 3, 3])]\n",
      "torch.Size([512, 512, 3, 3])\n",
      "[torch.Size([512, 512, 3, 3]), torch.Size([512, 512, 3, 3])]\n",
      "torch.Size([512, 512, 3, 3])\n",
      "[torch.Size([512, 512, 3, 3]), torch.Size([512, 512, 3, 3])]\n",
      "torch.Size([512, 512, 3, 3])\n",
      "[torch.Size([10, 512]), torch.Size([10, 512])]\n",
      "torch.Size([10, 512])\n",
      "in _make_layers [Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), AvgPool2d(kernel_size=1, stride=1, padding=0)]\n",
      "Relu Inplace is  False\n",
      "model parameters are \n",
      " [torch.Size([64, 3, 3, 3]), torch.Size([128, 64, 3, 3]), torch.Size([256, 128, 3, 3]), torch.Size([256, 256, 3, 3]), torch.Size([512, 256, 3, 3]), torch.Size([512, 512, 3, 3]), torch.Size([512, 512, 3, 3]), torch.Size([512, 512, 3, 3]), torch.Size([10, 512])]\n",
      "\n",
      "--------- Testing in global mode ---------\n",
      "size of test_loader dataset:  10000\n",
      "\n",
      "Test set: Avg. loss: 0.0023, Accuracy: 1003/10000 (10%)\n",
      "\n",
      "\n",
      "--------- Testing in global mode ---------\n",
      "size of test_loader dataset:  10000\n",
      "\n",
      "Test set: Avg. loss: 0.0023, Accuracy: 1689/10000 (17%)\n",
      "\n",
      "-------- Retraining the models ---------\n",
      "Retraining model :  model_0\n",
      "num_epochs---------------+++))))))+++ 300\n",
      "num_epochs---------------+++))))))+++ 300\n",
      "lr is  0.05\n",
      "number of epochs would be  30\n",
      "num_epochs-------------- 30\n",
      "Epoch 000\n",
      "/usr/local/pace-apps/manual/packages/pytorch/1.12.0/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:131: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\"Detected call of `lr_scheduler.step()` before `optimizer.step()`. \"\n",
      "/usr/local/pace-apps/manual/packages/pytorch/1.12.0/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:156: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: {'epoch': 0, 'value': 0.32661999999999997} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 0, 'value': 1.7951463354825983} ({'split': 'train'})\n",
      "accuracy: {'epoch': 0, 'value': 0.4468000173568726} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 0, 'value': 1.6127035140991213} ({'split': 'test'})\n",
      "Epoch 001\n",
      "accuracy: {'epoch': 1, 'value': 0.61872} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 1, 'value': 1.1192380926132195} ({'split': 'train'})\n",
      "accuracy: {'epoch': 1, 'value': 0.6669000327587128} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 1, 'value': 0.9896149754524232} ({'split': 'test'})\n",
      "Epoch 002\n",
      "^C\n",
      "Traceback (most recent call last):\n",
      "  File \"/storage/coda1/p-jli3175/0/mwu385/OTfusion_pruning_final/otfusion/main.py\", line 460, in <module>\n",
      "    _, best_retrain_acc = routines.retrain_models(args, [*original_models, geometric_model, naive_model], retrain_loader, test_loader, config, tensorboard_obj=tensorboard_obj, initial_acc=initial_acc, nicks=nicks)\n",
      "  File \"/storage/coda1/p-jli3175/0/mwu385/OTfusion_pruning_final/otfusion/routines.py\", line 394, in retrain_models\n",
      "    retrained_network, acc = cifar_train.get_retrained_model(args, retrain_loader, test_loader, old_networks[i], config, output_root_dir, tensorboard_obj=tensorboard_obj, nick=nick, start_acc=initial_acc[i])\n",
      "  File \"/storage/coda1/p-jli3175/0/mwu385/OTfusion_pruning_final/otfusion/./cifar/train.py\", line 333, in get_retrained_model\n",
      "    best_acc,model = main(config, output_dir, args.gpu_id, pretrained_model=old_network, pretrained_dataset=(train_loader, test_loader), tensorboard_obj=tensorboard_obj,return_model=True)\n",
      "  File \"/storage/coda1/p-jli3175/0/mwu385/OTfusion_pruning_final/otfusion/./cifar/train.py\", line 75, in main\n",
      "    loss.backward()\n",
      "  File \"/usr/local/pace-apps/manual/packages/pytorch/1.12.0/lib/python3.9/site-packages/torch/_tensor.py\", line 396, in backward\n",
      "    torch.autograd.backward(self, gradient, retain_graph, create_graph, inputs=inputs)\n",
      "  File \"/usr/local/pace-apps/manual/packages/pytorch/1.12.0/lib/python3.9/site-packages/torch/autograd/__init__.py\", line 173, in backward\n",
      "    Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n",
      "KeyboardInterrupt\n"
     ]
    }
   ],
   "source": [
    "!python main.py --gpu-id 0 --model-name vgg11_nobias --n-epochs 0 --save-result-file sample.csv \\\n",
    "--sweep-name exp_sample --correction --ground-metric euclidean --weight-stats \\\n",
    "--geom-ensemble-type wts --ground-metric-normalize none --sweep-id 90 \\\n",
    "--ckpt-type best --dataset Cifar10 --ground-metric-eff --recheck-cifar --activation-seed {activation_seed} \\\n",
    "--prelu-acts --past-correction --not-squared --exact\\\n",
    "--learning-rate 0.004\\\n",
    "--momentum 0.5\\\n",
    "--batch-size-train 64 --experiment-dir {experiment_dir}\\\n",
    "--prunint-rate {prunint_rate}\\\n",
    "--run-mode {run_mode}\\\n",
    "--to-download\\\n",
    "--load-model-dir {load_model_dir}\\\n",
    "--retrain 30\\\n",
    "--eval-aligned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5e30ad5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#实验3: prune many times\n",
    "#剪枝实验3:prune times without finetune 85.9\n",
    "activation_seed=21\n",
    "experiment_dir=\"experiment1.5\"\n",
    "prunint_rate=0.3\n",
    "run_mode='prune_times'\n",
    "load_model_dir='./cifar_models'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d40d867e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------- Setting up parameters -------\n",
      "dumping parameters at  /storage/coda1/p-jli3175/0/mwu385/OTfusion_pruning_final/otfusion/exp_sample/configurations\n",
      "The parameters are: \n",
      " Namespace(experiment_dir='experiment1.5', prunint_rate=0.3, run_mode='prune_times', load_model_dir='./cifar_models', prune_times=3, finetunetimes_benchmark=0, n_epochs=10, batch_size_train=256, batch_size_test=1000, learning_rate=0.0001, momentum=0.5, log_interval=100, to_download=True, disable_bias=True, dataset='Cifar10', num_models=2, model_name='vgg11_nobias', config_file=None, config_dir='/storage/coda1/p-jli3175/0/mwu385/OTfusion_pruning_final/otfusion/exp_sample/configurations', num_hidden_nodes=400, num_hidden_nodes1=400, num_hidden_nodes2=200, num_hidden_nodes3=100, num_hidden_nodes4=50, sweep_id=90, gpu_id=0, skip_last_layer=False, skip_last_layer_type='average', debug=False, cifar_style_data=False, activation_histograms=False, act_num_samples=100, softmax_temperature=1, activation_mode=None, options_type='generic', deprecated=None, save_result_file='sample.csv', sweep_name='exp_sample', reg=0.01, reg_m=0.001, ground_metric='euclidean', ground_metric_normalize='none', not_squared=True, clip_gm=False, clip_min=0, clip_max=5, tmap_stats=False, ensemble_step=0.5, ground_metric_eff=True, retrain=30, retrain_lr_decay=-1, retrain_lr_decay_factor=None, retrain_lr_decay_epochs=None, retrain_avg_only=False, retrain_geometric_only=False, load_models='', ckpt_type='best', recheck_cifar=True, recheck_acc=False, eval_aligned=False, enable_dropout=False, dump_model=False, dump_final_models=False, correction=True, activation_seed=21, weight_stats=True, sinkhorn_type='normal', geom_ensemble_type='wts', act_bug=False, standardize_acts=False, transform_acts=False, center_acts=False, prelu_acts=True, pool_acts=False, pool_relu=False, normalize_acts=False, normalize_wts=False, gromov=False, gromov_loss='square_loss', tensorboard_root='./tensorboard', tensorboard=False, same_model=-1, dist_normalize=False, update_acts=False, past_correction=True, partial_reshape=False, choice='0 2 4 6 8', diff_init=False, partition_type='labels', personal_class_idx=9, partition_dataloader=-1, personal_split_frac=0.1, exact=True, skip_personal_idx=False, prediction_wts=False, width_ratio=1, proper_marginals=False, retrain_seed=-1, no_random_trainloaders=False, reinit_trainloaders=False, second_model_name=None, print_distances=False, deterministic=False, skip_retrain=-1, importance=None, unbalanced=False, temperature=20, alpha=0.7, dist_epochs=60, handle_skips=False, timestamp='2023-09-18_23-56-11_115599', rootdir='/storage/coda1/p-jli3175/0/mwu385/OTfusion_pruning_final/otfusion/exp_sample', baseroot='/storage/coda1/p-jli3175/0/mwu385/OTfusion_pruning_final/otfusion', result_dir='/storage/coda1/p-jli3175/0/mwu385/OTfusion_pruning_final/otfusion/exp_sample/results', exp_name='exp_2023-09-18_23-56-11_115599', csv_dir='/storage/coda1/p-jli3175/0/mwu385/OTfusion_pruning_final/otfusion/exp_sample/csv')\n",
      "refactored get_config\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "in _make_layers [Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), AvgPool2d(kernel_size=1, stride=1, padding=0)]\n",
      "Relu Inplace is  False\n",
      "model parameters are \n",
      " [torch.Size([64, 3, 3, 3]), torch.Size([128, 64, 3, 3]), torch.Size([256, 128, 3, 3]), torch.Size([256, 256, 3, 3]), torch.Size([512, 256, 3, 3]), torch.Size([512, 512, 3, 3]), torch.Size([512, 512, 3, 3]), torch.Size([512, 512, 3, 3]), torch.Size([10, 512])]\n",
      "in _make_layers [Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), AvgPool2d(kernel_size=1, stride=1, padding=0)]\n",
      "Relu Inplace is  False\n",
      "model parameters are \n",
      " [torch.Size([64, 3, 3, 3]), torch.Size([128, 64, 3, 3]), torch.Size([256, 128, 3, 3]), torch.Size([256, 256, 3, 3]), torch.Size([512, 256, 3, 3]), torch.Size([512, 512, 3, 3]), torch.Size([512, 512, 3, 3]), torch.Size([512, 512, 3, 3]), torch.Size([10, 512])]\n",
      "in _make_layers [Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), AvgPool2d(kernel_size=1, stride=1, padding=0)]\n",
      "Relu Inplace is  False\n",
      "model parameters are \n",
      " [torch.Size([64, 3, 3, 3]), torch.Size([128, 64, 3, 3]), torch.Size([256, 128, 3, 3]), torch.Size([256, 256, 3, 3]), torch.Size([512, 256, 3, 3]), torch.Size([512, 512, 3, 3]), torch.Size([512, 512, 3, 3]), torch.Size([512, 512, 3, 3]), torch.Size([10, 512])]\n",
      "in _make_layers [Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), AvgPool2d(kernel_size=1, stride=1, padding=0)]\n",
      "Relu Inplace is  False\n",
      "model parameters are \n",
      " [torch.Size([64, 3, 3, 3]), torch.Size([128, 64, 3, 3]), torch.Size([256, 128, 3, 3]), torch.Size([256, 256, 3, 3]), torch.Size([512, 256, 3, 3]), torch.Size([512, 512, 3, 3]), torch.Size([512, 512, 3, 3]), torch.Size([512, 512, 3, 3]), torch.Size([10, 512])]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "enter prune_times, the times is 3,ratio:0.09999999999999999\n",
      "Train Epoch: 1 [0/50000 (0%)]\tLoss: 0.101038\n",
      "Train Epoch: 1 [25600/50000 (51%)]\tLoss: 0.094316\n",
      "\n",
      "--------- Testing in global mode ---------\n",
      "size of test_loader dataset:  10000\n",
      "\n",
      "Test set: Avg. loss: 0.0003, Accuracy: 9029/10000 (90%)\n",
      "\n",
      "Train Epoch: 2 [0/50000 (0%)]\tLoss: 0.072759\n",
      "Train Epoch: 2 [25600/50000 (51%)]\tLoss: 0.089174\n",
      "\n",
      "--------- Testing in global mode ---------\n",
      "size of test_loader dataset:  10000\n",
      "\n",
      "Test set: Avg. loss: 0.0003, Accuracy: 9044/10000 (90%)\n",
      "\n",
      "Train Epoch: 3 [0/50000 (0%)]\tLoss: 0.065089\n",
      "Train Epoch: 3 [25600/50000 (51%)]\tLoss: 0.059306\n",
      "\n",
      "--------- Testing in global mode ---------\n",
      "size of test_loader dataset:  10000\n",
      "\n",
      "Test set: Avg. loss: 0.0003, Accuracy: 9053/10000 (91%)\n",
      "\n",
      "Train Epoch: 4 [0/50000 (0%)]\tLoss: 0.069789\n",
      "Train Epoch: 4 [25600/50000 (51%)]\tLoss: 0.055862\n",
      "\n",
      "--------- Testing in global mode ---------\n",
      "size of test_loader dataset:  10000\n",
      "\n",
      "Test set: Avg. loss: 0.0003, Accuracy: 9048/10000 (90%)\n",
      "\n",
      "Train Epoch: 5 [0/50000 (0%)]\tLoss: 0.067100\n",
      "Train Epoch: 5 [25600/50000 (51%)]\tLoss: 0.069430\n",
      "\n",
      "--------- Testing in global mode ---------\n",
      "size of test_loader dataset:  10000\n",
      "\n",
      "Test set: Avg. loss: 0.0003, Accuracy: 9045/10000 (90%)\n",
      "\n",
      "Train Epoch: 6 [0/50000 (0%)]\tLoss: 0.062366\n",
      "Train Epoch: 6 [25600/50000 (51%)]\tLoss: 0.061508\n",
      "\n",
      "--------- Testing in global mode ---------\n",
      "size of test_loader dataset:  10000\n",
      "\n",
      "Test set: Avg. loss: 0.0003, Accuracy: 9041/10000 (90%)\n",
      "\n",
      "Train Epoch: 7 [0/50000 (0%)]\tLoss: 0.069848\n",
      "Train Epoch: 7 [25600/50000 (51%)]\tLoss: 0.074728\n",
      "\n",
      "--------- Testing in global mode ---------\n",
      "size of test_loader dataset:  10000\n",
      "\n",
      "Test set: Avg. loss: 0.0003, Accuracy: 9047/10000 (90%)\n",
      "\n",
      "Train Epoch: 8 [0/50000 (0%)]\tLoss: 0.054692\n",
      "Train Epoch: 8 [25600/50000 (51%)]\tLoss: 0.072025\n",
      "\n",
      "--------- Testing in global mode ---------\n",
      "size of test_loader dataset:  10000\n",
      "\n",
      "Test set: Avg. loss: 0.0003, Accuracy: 9047/10000 (90%)\n",
      "\n",
      "Train Epoch: 9 [0/50000 (0%)]\tLoss: 0.057060\n",
      "Train Epoch: 9 [25600/50000 (51%)]\tLoss: 0.065342\n",
      "\n",
      "--------- Testing in global mode ---------\n",
      "size of test_loader dataset:  10000\n",
      "\n",
      "Test set: Avg. loss: 0.0003, Accuracy: 9045/10000 (90%)\n",
      "\n",
      "Train Epoch: 10 [0/50000 (0%)]\tLoss: 0.061192\n",
      "Train Epoch: 10 [25600/50000 (51%)]\tLoss: 0.060971\n",
      "\n",
      "--------- Testing in global mode ---------\n",
      "size of test_loader dataset:  10000\n",
      "\n",
      "Test set: Avg. loss: 0.0003, Accuracy: 9049/10000 (90%)\n",
      "\n",
      "Train Epoch: 1 [0/50000 (0%)]\tLoss: 0.099159\n",
      "Train Epoch: 1 [25600/50000 (51%)]\tLoss: 0.078734\n",
      "\n",
      "--------- Testing in global mode ---------\n",
      "size of test_loader dataset:  10000\n",
      "\n",
      "Test set: Avg. loss: 0.0003, Accuracy: 9060/10000 (91%)\n",
      "\n",
      "Train Epoch: 2 [0/50000 (0%)]\tLoss: 0.069846\n",
      "Train Epoch: 2 [25600/50000 (51%)]\tLoss: 0.094642\n",
      "\n",
      "--------- Testing in global mode ---------\n",
      "size of test_loader dataset:  10000\n",
      "\n",
      "Test set: Avg. loss: 0.0003, Accuracy: 9061/10000 (91%)\n",
      "\n",
      "Train Epoch: 3 [0/50000 (0%)]\tLoss: 0.072969\n",
      "Train Epoch: 3 [25600/50000 (51%)]\tLoss: 0.073596\n",
      "\n",
      "--------- Testing in global mode ---------\n",
      "size of test_loader dataset:  10000\n",
      "\n",
      "Test set: Avg. loss: 0.0003, Accuracy: 9059/10000 (91%)\n",
      "\n",
      "Train Epoch: 4 [0/50000 (0%)]\tLoss: 0.085653\n",
      "Train Epoch: 4 [25600/50000 (51%)]\tLoss: 0.066504\n",
      "\n",
      "--------- Testing in global mode ---------\n",
      "size of test_loader dataset:  10000\n",
      "\n",
      "Test set: Avg. loss: 0.0003, Accuracy: 9064/10000 (91%)\n",
      "\n",
      "Train Epoch: 5 [0/50000 (0%)]\tLoss: 0.065018\n",
      "Train Epoch: 5 [25600/50000 (51%)]\tLoss: 0.089911\n",
      "\n",
      "--------- Testing in global mode ---------\n",
      "size of test_loader dataset:  10000\n",
      "\n",
      "Test set: Avg. loss: 0.0003, Accuracy: 9061/10000 (91%)\n",
      "\n",
      "Train Epoch: 6 [0/50000 (0%)]\tLoss: 0.062845\n",
      "Train Epoch: 6 [25600/50000 (51%)]\tLoss: 0.060329\n",
      "\n",
      "--------- Testing in global mode ---------\n",
      "size of test_loader dataset:  10000\n",
      "\n",
      "Test set: Avg. loss: 0.0003, Accuracy: 9062/10000 (91%)\n",
      "\n",
      "Train Epoch: 7 [0/50000 (0%)]\tLoss: 0.064670\n",
      "Train Epoch: 7 [25600/50000 (51%)]\tLoss: 0.052360\n",
      "\n",
      "--------- Testing in global mode ---------\n",
      "size of test_loader dataset:  10000\n",
      "\n",
      "Test set: Avg. loss: 0.0003, Accuracy: 9064/10000 (91%)\n",
      "\n",
      "Train Epoch: 8 [0/50000 (0%)]\tLoss: 0.052098\n",
      "Train Epoch: 8 [25600/50000 (51%)]\tLoss: 0.063170\n",
      "\n",
      "--------- Testing in global mode ---------\n",
      "size of test_loader dataset:  10000\n",
      "\n",
      "Test set: Avg. loss: 0.0003, Accuracy: 9063/10000 (91%)\n",
      "\n",
      "Train Epoch: 9 [0/50000 (0%)]\tLoss: 0.059350\n",
      "Train Epoch: 9 [25600/50000 (51%)]\tLoss: 0.061314\n",
      "\n",
      "--------- Testing in global mode ---------\n",
      "size of test_loader dataset:  10000\n",
      "\n",
      "Test set: Avg. loss: 0.0003, Accuracy: 9061/10000 (91%)\n",
      "\n",
      "Train Epoch: 10 [0/50000 (0%)]\tLoss: 0.056002\n",
      "Train Epoch: 10 [25600/50000 (51%)]\tLoss: 0.052865\n",
      "\n",
      "--------- Testing in global mode ---------\n",
      "size of test_loader dataset:  10000\n",
      "\n",
      "Test set: Avg. loss: 0.0003, Accuracy: 9060/10000 (91%)\n",
      "\n",
      "in times 0, now cumm_ratio 0.09999999999999999,accuracies:[90.49, 90.6],losses:[0.00029925533831119536, 0.0002961399883031845]\n",
      "Train Epoch: 1 [0/50000 (0%)]\tLoss: 0.041718\n",
      "Train Epoch: 1 [25600/50000 (51%)]\tLoss: 0.059165\n",
      "\n",
      "--------- Testing in global mode ---------\n",
      "size of test_loader dataset:  10000\n",
      "\n",
      "Test set: Avg. loss: 0.0003, Accuracy: 9042/10000 (90%)\n",
      "\n",
      "Train Epoch: 2 [0/50000 (0%)]\tLoss: 0.070132\n",
      "Train Epoch: 2 [25600/50000 (51%)]\tLoss: 0.049580\n",
      "\n",
      "--------- Testing in global mode ---------\n",
      "size of test_loader dataset:  10000\n",
      "\n",
      "Test set: Avg. loss: 0.0003, Accuracy: 9041/10000 (90%)\n",
      "\n",
      "Train Epoch: 3 [0/50000 (0%)]\tLoss: 0.050023\n",
      "Train Epoch: 3 [25600/50000 (51%)]\tLoss: 0.052508\n",
      "\n",
      "--------- Testing in global mode ---------\n",
      "size of test_loader dataset:  10000\n",
      "\n",
      "Test set: Avg. loss: 0.0003, Accuracy: 9043/10000 (90%)\n",
      "\n",
      "Train Epoch: 4 [0/50000 (0%)]\tLoss: 0.040984\n",
      "Train Epoch: 4 [25600/50000 (51%)]\tLoss: 0.051655\n",
      "\n",
      "--------- Testing in global mode ---------\n",
      "size of test_loader dataset:  10000\n",
      "\n",
      "Test set: Avg. loss: 0.0003, Accuracy: 9042/10000 (90%)\n",
      "\n",
      "Train Epoch: 5 [0/50000 (0%)]\tLoss: 0.041657\n",
      "Train Epoch: 5 [25600/50000 (51%)]\tLoss: 0.044471\n",
      "\n",
      "--------- Testing in global mode ---------\n",
      "size of test_loader dataset:  10000\n",
      "\n",
      "Test set: Avg. loss: 0.0003, Accuracy: 9044/10000 (90%)\n",
      "\n",
      "Train Epoch: 6 [0/50000 (0%)]\tLoss: 0.052595\n",
      "Train Epoch: 6 [25600/50000 (51%)]\tLoss: 0.039733\n",
      "\n",
      "--------- Testing in global mode ---------\n",
      "size of test_loader dataset:  10000\n",
      "\n",
      "Test set: Avg. loss: 0.0003, Accuracy: 9042/10000 (90%)\n",
      "\n",
      "Train Epoch: 7 [0/50000 (0%)]\tLoss: 0.035213\n",
      "Train Epoch: 7 [25600/50000 (51%)]\tLoss: 0.054223\n",
      "\n",
      "--------- Testing in global mode ---------\n",
      "size of test_loader dataset:  10000\n",
      "\n",
      "Test set: Avg. loss: 0.0003, Accuracy: 9044/10000 (90%)\n",
      "\n",
      "Train Epoch: 8 [0/50000 (0%)]\tLoss: 0.048247\n",
      "Train Epoch: 8 [25600/50000 (51%)]\tLoss: 0.042700\n",
      "\n",
      "--------- Testing in global mode ---------\n",
      "size of test_loader dataset:  10000\n",
      "\n",
      "Test set: Avg. loss: 0.0003, Accuracy: 9046/10000 (90%)\n",
      "\n",
      "Train Epoch: 9 [0/50000 (0%)]\tLoss: 0.046990\n",
      "Train Epoch: 9 [25600/50000 (51%)]\tLoss: 0.043298\n",
      "\n",
      "--------- Testing in global mode ---------\n",
      "size of test_loader dataset:  10000\n",
      "\n",
      "Test set: Avg. loss: 0.0003, Accuracy: 9047/10000 (90%)\n",
      "\n",
      "Train Epoch: 10 [0/50000 (0%)]\tLoss: 0.055003\n",
      "Train Epoch: 10 [25600/50000 (51%)]\tLoss: 0.043753\n",
      "\n",
      "--------- Testing in global mode ---------\n",
      "size of test_loader dataset:  10000\n",
      "\n",
      "Test set: Avg. loss: 0.0003, Accuracy: 9044/10000 (90%)\n",
      "\n",
      "Train Epoch: 1 [0/50000 (0%)]\tLoss: 0.049892\n",
      "Train Epoch: 1 [25600/50000 (51%)]\tLoss: 0.049852\n",
      "\n",
      "--------- Testing in global mode ---------\n",
      "size of test_loader dataset:  10000\n",
      "\n",
      "Test set: Avg. loss: 0.0003, Accuracy: 9052/10000 (91%)\n",
      "\n",
      "Train Epoch: 2 [0/50000 (0%)]\tLoss: 0.054624\n",
      "Train Epoch: 2 [25600/50000 (51%)]\tLoss: 0.045439\n",
      "\n",
      "--------- Testing in global mode ---------\n",
      "size of test_loader dataset:  10000\n",
      "\n",
      "Test set: Avg. loss: 0.0003, Accuracy: 9046/10000 (90%)\n",
      "\n",
      "Train Epoch: 3 [0/50000 (0%)]\tLoss: 0.054971\n",
      "Train Epoch: 3 [25600/50000 (51%)]\tLoss: 0.049146\n",
      "\n",
      "--------- Testing in global mode ---------\n",
      "size of test_loader dataset:  10000\n",
      "\n",
      "Test set: Avg. loss: 0.0003, Accuracy: 9043/10000 (90%)\n",
      "\n",
      "Train Epoch: 4 [0/50000 (0%)]\tLoss: 0.055418\n",
      "Train Epoch: 4 [25600/50000 (51%)]\tLoss: 0.045964\n",
      "\n",
      "--------- Testing in global mode ---------\n",
      "size of test_loader dataset:  10000\n",
      "\n",
      "Test set: Avg. loss: 0.0003, Accuracy: 9047/10000 (90%)\n",
      "\n",
      "Train Epoch: 5 [0/50000 (0%)]\tLoss: 0.047574\n",
      "Train Epoch: 5 [25600/50000 (51%)]\tLoss: 0.058830\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--------- Testing in global mode ---------\n",
      "size of test_loader dataset:  10000\n",
      "\n",
      "Test set: Avg. loss: 0.0003, Accuracy: 9045/10000 (90%)\n",
      "\n",
      "Train Epoch: 6 [0/50000 (0%)]\tLoss: 0.051465\n",
      "Train Epoch: 6 [25600/50000 (51%)]\tLoss: 0.041189\n",
      "\n",
      "--------- Testing in global mode ---------\n",
      "size of test_loader dataset:  10000\n",
      "\n",
      "Test set: Avg. loss: 0.0003, Accuracy: 9046/10000 (90%)\n",
      "\n",
      "Train Epoch: 7 [0/50000 (0%)]\tLoss: 0.034973\n",
      "Train Epoch: 7 [25600/50000 (51%)]\tLoss: 0.042528\n",
      "\n",
      "--------- Testing in global mode ---------\n",
      "size of test_loader dataset:  10000\n",
      "\n",
      "Test set: Avg. loss: 0.0003, Accuracy: 9043/10000 (90%)\n",
      "\n",
      "Train Epoch: 8 [0/50000 (0%)]\tLoss: 0.049500\n",
      "Train Epoch: 8 [25600/50000 (51%)]\tLoss: 0.051660\n",
      "\n",
      "--------- Testing in global mode ---------\n",
      "size of test_loader dataset:  10000\n",
      "\n",
      "Test set: Avg. loss: 0.0003, Accuracy: 9041/10000 (90%)\n",
      "\n",
      "Train Epoch: 9 [0/50000 (0%)]\tLoss: 0.054951\n",
      "Train Epoch: 9 [25600/50000 (51%)]\tLoss: 0.045180\n",
      "\n",
      "--------- Testing in global mode ---------\n",
      "size of test_loader dataset:  10000\n",
      "\n",
      "Test set: Avg. loss: 0.0003, Accuracy: 9038/10000 (90%)\n",
      "\n",
      "Train Epoch: 10 [0/50000 (0%)]\tLoss: 0.042811\n",
      "Train Epoch: 10 [25600/50000 (51%)]\tLoss: 0.054419\n",
      "\n",
      "--------- Testing in global mode ---------\n",
      "size of test_loader dataset:  10000\n",
      "\n",
      "Test set: Avg. loss: 0.0003, Accuracy: 9036/10000 (90%)\n",
      "\n",
      "in times 1, now cumm_ratio 0.19999999999999998,accuracies:[90.44, 90.36],losses:[0.0003017983168363571, 0.00029735870361328125]\n",
      "Train Epoch: 1 [0/50000 (0%)]\tLoss: 0.047712\n",
      "Train Epoch: 1 [25600/50000 (51%)]\tLoss: 0.058935\n",
      "\n",
      "--------- Testing in global mode ---------\n",
      "size of test_loader dataset:  10000\n",
      "\n",
      "Test set: Avg. loss: 0.0003, Accuracy: 9013/10000 (90%)\n",
      "\n",
      "Train Epoch: 2 [0/50000 (0%)]\tLoss: 0.049080\n",
      "Train Epoch: 2 [25600/50000 (51%)]\tLoss: 0.048226\n",
      "\n",
      "--------- Testing in global mode ---------\n",
      "size of test_loader dataset:  10000\n",
      "\n",
      "Test set: Avg. loss: 0.0003, Accuracy: 9015/10000 (90%)\n",
      "\n",
      "Train Epoch: 3 [0/50000 (0%)]\tLoss: 0.068351\n",
      "Train Epoch: 3 [25600/50000 (51%)]\tLoss: 0.051158\n",
      "\n",
      "--------- Testing in global mode ---------\n",
      "size of test_loader dataset:  10000\n",
      "\n",
      "Test set: Avg. loss: 0.0003, Accuracy: 9020/10000 (90%)\n",
      "\n",
      "Train Epoch: 4 [0/50000 (0%)]\tLoss: 0.052537\n",
      "Train Epoch: 4 [25600/50000 (51%)]\tLoss: 0.048139\n",
      "\n",
      "--------- Testing in global mode ---------\n",
      "size of test_loader dataset:  10000\n",
      "\n",
      "Test set: Avg. loss: 0.0003, Accuracy: 9021/10000 (90%)\n",
      "\n",
      "Train Epoch: 5 [0/50000 (0%)]\tLoss: 0.055440\n",
      "Train Epoch: 5 [25600/50000 (51%)]\tLoss: 0.044312\n",
      "\n",
      "--------- Testing in global mode ---------\n",
      "size of test_loader dataset:  10000\n",
      "\n",
      "Test set: Avg. loss: 0.0003, Accuracy: 9024/10000 (90%)\n",
      "\n",
      "Train Epoch: 6 [0/50000 (0%)]\tLoss: 0.052120\n",
      "Train Epoch: 6 [25600/50000 (51%)]\tLoss: 0.043670\n",
      "\n",
      "--------- Testing in global mode ---------\n",
      "size of test_loader dataset:  10000\n",
      "\n",
      "Test set: Avg. loss: 0.0003, Accuracy: 9025/10000 (90%)\n",
      "\n",
      "Train Epoch: 7 [0/50000 (0%)]\tLoss: 0.029736\n",
      "Train Epoch: 7 [25600/50000 (51%)]\tLoss: 0.050152\n",
      "\n",
      "--------- Testing in global mode ---------\n",
      "size of test_loader dataset:  10000\n",
      "\n",
      "Test set: Avg. loss: 0.0003, Accuracy: 9029/10000 (90%)\n",
      "\n",
      "Train Epoch: 8 [0/50000 (0%)]\tLoss: 0.053635\n",
      "Train Epoch: 8 [25600/50000 (51%)]\tLoss: 0.040078\n",
      "\n",
      "--------- Testing in global mode ---------\n",
      "size of test_loader dataset:  10000\n",
      "\n",
      "Test set: Avg. loss: 0.0003, Accuracy: 9031/10000 (90%)\n",
      "\n",
      "Train Epoch: 9 [0/50000 (0%)]\tLoss: 0.035397\n",
      "Train Epoch: 9 [25600/50000 (51%)]\tLoss: 0.040007\n",
      "\n",
      "--------- Testing in global mode ---------\n",
      "size of test_loader dataset:  10000\n",
      "\n",
      "Test set: Avg. loss: 0.0003, Accuracy: 9030/10000 (90%)\n",
      "\n",
      "Train Epoch: 10 [0/50000 (0%)]\tLoss: 0.042172\n",
      "Train Epoch: 10 [25600/50000 (51%)]\tLoss: 0.050206\n",
      "\n",
      "--------- Testing in global mode ---------\n",
      "size of test_loader dataset:  10000\n",
      "\n",
      "Test set: Avg. loss: 0.0003, Accuracy: 9030/10000 (90%)\n",
      "\n",
      "Train Epoch: 1 [0/50000 (0%)]\tLoss: 0.053552\n",
      "Train Epoch: 1 [25600/50000 (51%)]\tLoss: 0.057550\n",
      "\n",
      "--------- Testing in global mode ---------\n",
      "size of test_loader dataset:  10000\n",
      "\n",
      "Test set: Avg. loss: 0.0003, Accuracy: 9019/10000 (90%)\n",
      "\n",
      "Train Epoch: 2 [0/50000 (0%)]\tLoss: 0.039056\n",
      "Train Epoch: 2 [25600/50000 (51%)]\tLoss: 0.052067\n",
      "\n",
      "--------- Testing in global mode ---------\n",
      "size of test_loader dataset:  10000\n",
      "\n",
      "Test set: Avg. loss: 0.0003, Accuracy: 9025/10000 (90%)\n",
      "\n",
      "Train Epoch: 3 [0/50000 (0%)]\tLoss: 0.063521\n",
      "Train Epoch: 3 [25600/50000 (51%)]\tLoss: 0.044057\n",
      "\n",
      "--------- Testing in global mode ---------\n",
      "size of test_loader dataset:  10000\n",
      "\n",
      "Test set: Avg. loss: 0.0003, Accuracy: 9033/10000 (90%)\n",
      "\n",
      "Train Epoch: 4 [0/50000 (0%)]\tLoss: 0.038787\n",
      "Train Epoch: 4 [25600/50000 (51%)]\tLoss: 0.048677\n",
      "\n",
      "--------- Testing in global mode ---------\n",
      "size of test_loader dataset:  10000\n",
      "\n",
      "Test set: Avg. loss: 0.0003, Accuracy: 9038/10000 (90%)\n",
      "\n",
      "Train Epoch: 5 [0/50000 (0%)]\tLoss: 0.052878\n",
      "Train Epoch: 5 [25600/50000 (51%)]\tLoss: 0.045698\n",
      "\n",
      "--------- Testing in global mode ---------\n",
      "size of test_loader dataset:  10000\n",
      "\n",
      "Test set: Avg. loss: 0.0003, Accuracy: 9038/10000 (90%)\n",
      "\n",
      "Train Epoch: 6 [0/50000 (0%)]\tLoss: 0.042870\n",
      "Train Epoch: 6 [25600/50000 (51%)]\tLoss: 0.043999\n",
      "\n",
      "--------- Testing in global mode ---------\n",
      "size of test_loader dataset:  10000\n",
      "\n",
      "Test set: Avg. loss: 0.0003, Accuracy: 9038/10000 (90%)\n",
      "\n",
      "Train Epoch: 7 [0/50000 (0%)]\tLoss: 0.053553\n",
      "Train Epoch: 7 [25600/50000 (51%)]\tLoss: 0.053212\n",
      "\n",
      "--------- Testing in global mode ---------\n",
      "size of test_loader dataset:  10000\n",
      "\n",
      "Test set: Avg. loss: 0.0003, Accuracy: 9034/10000 (90%)\n",
      "\n",
      "Train Epoch: 8 [0/50000 (0%)]\tLoss: 0.049794\n",
      "Train Epoch: 8 [25600/50000 (51%)]\tLoss: 0.044055\n",
      "\n",
      "--------- Testing in global mode ---------\n",
      "size of test_loader dataset:  10000\n",
      "\n",
      "Test set: Avg. loss: 0.0003, Accuracy: 9038/10000 (90%)\n",
      "\n",
      "Train Epoch: 9 [0/50000 (0%)]\tLoss: 0.041390\n",
      "Train Epoch: 9 [25600/50000 (51%)]\tLoss: 0.052762\n",
      "\n",
      "--------- Testing in global mode ---------\n",
      "size of test_loader dataset:  10000\n",
      "\n",
      "Test set: Avg. loss: 0.0003, Accuracy: 9035/10000 (90%)\n",
      "\n",
      "Train Epoch: 10 [0/50000 (0%)]\tLoss: 0.037116\n",
      "Train Epoch: 10 [25600/50000 (51%)]\tLoss: 0.051481\n",
      "\n",
      "--------- Testing in global mode ---------\n",
      "size of test_loader dataset:  10000\n",
      "\n",
      "Test set: Avg. loss: 0.0003, Accuracy: 9038/10000 (90%)\n",
      "\n",
      "in times 2, now cumm_ratio 0.3,accuracies:[90.3, 90.38],losses:[0.0003071047902107239, 0.00030301153659820554]\n",
      "===============================================\n",
      "===============================================\n",
      "before Ensemble,acc: [90.3, 90.38] loss [0.0003071047902107239, 0.00030301153659820554]\n",
      "===============================================\n",
      "===============================================\n",
      "check zero rate for model 0\n",
      "odict_keys(['features.0.weight', 'features.3.weight', 'features.6.weight', 'features.8.weight', 'features.11.weight', 'features.13.weight', 'features.16.weight', 'features.18.weight', 'classifier.weight'])\n",
      "the ratio of zero for features.0.weight\n",
      "tensor(0.2998, device='cuda:0')\n",
      "the ratio of zero for features.3.weight\n",
      "tensor(0.3000, device='cuda:0')\n",
      "the ratio of zero for features.6.weight\n",
      "tensor(0.3000, device='cuda:0')\n",
      "the ratio of zero for features.8.weight\n",
      "tensor(0.3000, device='cuda:0')\n",
      "the ratio of zero for features.11.weight\n",
      "tensor(0.3000, device='cuda:0')\n",
      "the ratio of zero for features.13.weight\n",
      "tensor(0.3000, device='cuda:0')\n",
      "the ratio of zero for features.16.weight\n",
      "tensor(0.3000, device='cuda:0')\n",
      "the ratio of zero for features.18.weight\n",
      "tensor(0.3000, device='cuda:0')\n",
      "the ratio of zero for classifier.weight\n",
      "tensor(0.3000, device='cuda:0')\n",
      "check zero rate for model 1\n",
      "odict_keys(['features.0.weight', 'features.3.weight', 'features.6.weight', 'features.8.weight', 'features.11.weight', 'features.13.weight', 'features.16.weight', 'features.18.weight', 'classifier.weight'])\n",
      "the ratio of zero for features.0.weight\n",
      "tensor(0.2998, device='cuda:0')\n",
      "the ratio of zero for features.3.weight\n",
      "tensor(0.3000, device='cuda:0')\n",
      "the ratio of zero for features.6.weight\n",
      "tensor(0.3000, device='cuda:0')\n",
      "the ratio of zero for features.8.weight\n",
      "tensor(0.3000, device='cuda:0')\n",
      "the ratio of zero for features.11.weight\n",
      "tensor(0.3000, device='cuda:0')\n",
      "the ratio of zero for features.13.weight\n",
      "tensor(0.3000, device='cuda:0')\n",
      "the ratio of zero for features.16.weight\n",
      "tensor(0.3000, device='cuda:0')\n",
      "the ratio of zero for features.18.weight\n",
      "tensor(0.3000, device='cuda:0')\n",
      "the ratio of zero for classifier.weight\n",
      "tensor(0.3000, device='cuda:0')\n",
      "layer features.0.weight has #params  1728\n",
      "layer features.3.weight has #params  73728\n",
      "layer features.6.weight has #params  294912\n",
      "layer features.8.weight has #params  589824\n",
      "layer features.11.weight has #params  1179648\n",
      "layer features.13.weight has #params  2359296\n",
      "layer features.16.weight has #params  2359296\n",
      "layer features.18.weight has #params  2359296\n",
      "layer classifier.weight has #params  5120\n",
      "Activation Timer start\n",
      "Activation Timer ends\n",
      "------- Geometric Ensembling -------\n",
      "Timer start\n",
      "Previous layer shape is  None\n",
      "Processing the coordinates to form ground_metric\n",
      "dont leave off the squaring of the ground metric\n",
      "returns a uniform measure of cardinality:  64\n",
      "returns a uniform measure of cardinality:  64\n",
      "the transport map is  tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')\n",
      "Ratio of trace to the matrix sum:  tensor(0.0312, device='cuda:0')\n",
      "Here, trace is 1.9999871253967285 and matrix sum is 63.99958801269531 \n",
      "this is past correction for weight mode\n",
      "Shape of aligned wt is  torch.Size([64, 3, 9])\n",
      "Shape of fc_layer0_weight_data is  torch.Size([64, 3, 9])\n",
      "Previous layer shape is  torch.Size([64, 3, 3, 3])\n",
      "shape of layer: model 0 torch.Size([128, 64, 9])\n",
      "shape of layer: model 1 torch.Size([128, 64, 9])\n",
      "shape of previous transport map torch.Size([64, 64])\n",
      "Processing the coordinates to form ground_metric\n",
      "dont leave off the squaring of the ground metric\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "returns a uniform measure of cardinality:  128\n",
      "returns a uniform measure of cardinality:  128\n",
      "the transport map is  tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        ...,\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0078],\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],\n",
      "       device='cuda:0')\n",
      "Ratio of trace to the matrix sum:  tensor(0., device='cuda:0')\n",
      "Here, trace is 0.0 and matrix sum is 127.99836730957031 \n",
      "this is past correction for weight mode\n",
      "Shape of aligned wt is  torch.Size([128, 64, 9])\n",
      "Shape of fc_layer0_weight_data is  torch.Size([128, 64, 9])\n",
      "Previous layer shape is  torch.Size([128, 64, 3, 3])\n",
      "shape of layer: model 0 torch.Size([256, 128, 9])\n",
      "shape of layer: model 1 torch.Size([256, 128, 9])\n",
      "shape of previous transport map torch.Size([128, 128])\n",
      "Processing the coordinates to form ground_metric\n",
      "dont leave off the squaring of the ground metric\n",
      "returns a uniform measure of cardinality:  256\n",
      "returns a uniform measure of cardinality:  256\n",
      "the transport map is  tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')\n",
      "Ratio of trace to the matrix sum:  tensor(0., device='cuda:0')\n",
      "Here, trace is 0.0 and matrix sum is 255.99343872070312 \n",
      "this is past correction for weight mode\n",
      "Shape of aligned wt is  torch.Size([256, 128, 9])\n",
      "Shape of fc_layer0_weight_data is  torch.Size([256, 128, 9])\n",
      "Previous layer shape is  torch.Size([256, 128, 3, 3])\n",
      "shape of layer: model 0 torch.Size([256, 256, 9])\n",
      "shape of layer: model 1 torch.Size([256, 256, 9])\n",
      "shape of previous transport map torch.Size([256, 256])\n",
      "Processing the coordinates to form ground_metric\n",
      "dont leave off the squaring of the ground metric\n",
      "returns a uniform measure of cardinality:  256\n",
      "returns a uniform measure of cardinality:  256\n",
      "the transport map is  tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        ...,\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.0039, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],\n",
      "       device='cuda:0')\n",
      "Ratio of trace to the matrix sum:  tensor(0.0078, device='cuda:0')\n",
      "Here, trace is 1.9999487400054932 and matrix sum is 255.99343872070312 \n",
      "this is past correction for weight mode\n",
      "Shape of aligned wt is  torch.Size([256, 256, 9])\n",
      "Shape of fc_layer0_weight_data is  torch.Size([256, 256, 9])\n",
      "Previous layer shape is  torch.Size([256, 256, 3, 3])\n",
      "shape of layer: model 0 torch.Size([512, 256, 9])\n",
      "shape of layer: model 1 torch.Size([512, 256, 9])\n",
      "shape of previous transport map torch.Size([256, 256])\n",
      "Processing the coordinates to form ground_metric\n",
      "dont leave off the squaring of the ground metric\n",
      "returns a uniform measure of cardinality:  512\n",
      "returns a uniform measure of cardinality:  512\n",
      "the transport map is  tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')\n",
      "Ratio of trace to the matrix sum:  tensor(0.0020, device='cuda:0')\n",
      "Here, trace is 0.9999488592147827 and matrix sum is 511.97381591796875 \n",
      "this is past correction for weight mode\n",
      "Shape of aligned wt is  torch.Size([512, 256, 9])\n",
      "Shape of fc_layer0_weight_data is  torch.Size([512, 256, 9])\n",
      "Previous layer shape is  torch.Size([512, 256, 3, 3])\n",
      "shape of layer: model 0 torch.Size([512, 512, 9])\n",
      "shape of layer: model 1 torch.Size([512, 512, 9])\n",
      "shape of previous transport map torch.Size([512, 512])\n",
      "Processing the coordinates to form ground_metric\n",
      "dont leave off the squaring of the ground metric\n",
      "returns a uniform measure of cardinality:  512\n",
      "returns a uniform measure of cardinality:  512\n",
      "the transport map is  tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')\n",
      "Ratio of trace to the matrix sum:  tensor(0.0020, device='cuda:0')\n",
      "Here, trace is 0.9999488592147827 and matrix sum is 511.97381591796875 \n",
      "this is past correction for weight mode\n",
      "Shape of aligned wt is  torch.Size([512, 512, 9])\n",
      "Shape of fc_layer0_weight_data is  torch.Size([512, 512, 9])\n",
      "Previous layer shape is  torch.Size([512, 512, 3, 3])\n",
      "shape of layer: model 0 torch.Size([512, 512, 9])\n",
      "shape of layer: model 1 torch.Size([512, 512, 9])\n",
      "shape of previous transport map torch.Size([512, 512])\n",
      "Processing the coordinates to form ground_metric\n",
      "dont leave off the squaring of the ground metric\n",
      "returns a uniform measure of cardinality:  512\n",
      "returns a uniform measure of cardinality:  512\n",
      "the transport map is  tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')\n",
      "Ratio of trace to the matrix sum:  tensor(0.0020, device='cuda:0')\n",
      "Here, trace is 0.9999488592147827 and matrix sum is 511.97381591796875 \n",
      "this is past correction for weight mode\n",
      "Shape of aligned wt is  torch.Size([512, 512, 9])\n",
      "Shape of fc_layer0_weight_data is  torch.Size([512, 512, 9])\n",
      "Previous layer shape is  torch.Size([512, 512, 3, 3])\n",
      "shape of layer: model 0 torch.Size([512, 512, 9])\n",
      "shape of layer: model 1 torch.Size([512, 512, 9])\n",
      "shape of previous transport map torch.Size([512, 512])\n",
      "Processing the coordinates to form ground_metric\n",
      "dont leave off the squaring of the ground metric\n",
      "returns a uniform measure of cardinality:  512\n",
      "returns a uniform measure of cardinality:  512\n",
      "the transport map is  tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        [0.0020, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        ...,\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],\n",
      "       device='cuda:0')\n",
      "Ratio of trace to the matrix sum:  tensor(0., device='cuda:0')\n",
      "Here, trace is 0.0 and matrix sum is 511.97381591796875 \n",
      "this is past correction for weight mode\n",
      "Shape of aligned wt is  torch.Size([512, 512, 9])\n",
      "Shape of fc_layer0_weight_data is  torch.Size([512, 512, 9])\n",
      "Previous layer shape is  torch.Size([512, 512, 3, 3])\n",
      "shape of layer: model 0 torch.Size([10, 512])\n",
      "shape of layer: model 1 torch.Size([10, 512])\n",
      "shape of previous transport map torch.Size([512, 512])\n",
      "Processing the coordinates to form ground_metric\n",
      "dont leave off the squaring of the ground metric\n",
      "ground metric is  tensor([[1.4606, 2.5042, 2.8065, 2.8669, 2.6955, 2.8700, 2.7283, 2.6381, 2.5896,\n",
      "         2.6121],\n",
      "        [2.4822, 1.3260, 2.6890, 2.5873, 2.7631, 2.5190, 2.4353, 2.5535, 2.3147,\n",
      "         2.4199],\n",
      "        [2.7673, 2.6585, 1.3782, 2.9791, 2.9159, 2.8719, 2.7393, 2.8571, 2.7597,\n",
      "         2.7621],\n",
      "        [2.8479, 2.6691, 2.9010, 1.5104, 2.8485, 2.9509, 2.7459, 2.8213, 2.8178,\n",
      "         2.6765],\n",
      "        [2.7604, 2.5483, 2.8648, 2.8877, 1.5623, 2.7463, 2.7706, 2.6808, 2.6843,\n",
      "         2.7223],\n",
      "        [2.9467, 2.5926, 2.9517, 2.8072, 2.7613, 1.5581, 2.7626, 2.6925, 2.8180,\n",
      "         2.6799],\n",
      "        [2.6755, 2.4348, 2.7218, 2.9270, 2.6578, 2.8671, 1.4088, 2.7071, 2.4891,\n",
      "         2.5379],\n",
      "        [2.7396, 2.5518, 2.7563, 2.7923, 2.7272, 2.7125, 2.6967, 1.3956, 2.5359,\n",
      "         2.5180],\n",
      "        [2.5554, 2.3775, 2.7756, 2.7405, 2.7145, 2.8213, 2.5396, 2.6412, 1.3137,\n",
      "         2.3707],\n",
      "        [2.5932, 2.2715, 2.7998, 2.7451, 2.6729, 2.7101, 2.5760, 2.5112, 2.5267,\n",
      "         1.2905]], device='cuda:0', grad_fn=<PowBackward0>)\n",
      "returns a uniform measure of cardinality:  10\n",
      "returns a uniform measure of cardinality:  10\n",
      "the transport map is  tensor([[0.1000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0000, 0.1000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0000, 0.0000, 0.1000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.1000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.1000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.1000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.1000, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.1000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.1000,\n",
      "         0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.1000]], device='cuda:0')\n",
      "Ratio of trace to the matrix sum:  tensor(1., device='cuda:0')\n",
      "Here, trace is 9.999990463256836 and matrix sum is 9.999990463256836 \n",
      "this is past correction for weight mode\n",
      "Shape of aligned wt is  torch.Size([10, 512])\n",
      "Shape of fc_layer0_weight_data is  torch.Size([10, 512])\n",
      "using independent method\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "in _make_layers [Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), AvgPool2d(kernel_size=1, stride=1, padding=0)]\n",
      "Relu Inplace is  False\n",
      "model parameters are \n",
      " [torch.Size([64, 3, 3, 3]), torch.Size([128, 64, 3, 3]), torch.Size([256, 128, 3, 3]), torch.Size([256, 256, 3, 3]), torch.Size([512, 256, 3, 3]), torch.Size([512, 512, 3, 3]), torch.Size([512, 512, 3, 3]), torch.Size([512, 512, 3, 3]), torch.Size([10, 512])]\n",
      "\n",
      "--------- Testing in global mode ---------\n",
      "size of test_loader dataset:  10000\n",
      "\n",
      "Test set: Avg. loss: 0.0023, Accuracy: 1000/10000 (10%)\n",
      "\n",
      "len of model parameters and avg aligned layers is  9 9\n",
      "len of model_state_dict is  9\n",
      "len of param_list is  9\n",
      "\n",
      "--------- Testing in global mode ---------\n",
      "size of test_loader dataset:  10000\n",
      "\n",
      "Test set: Avg. loss: 0.0013, Accuracy: 8590/10000 (86%)\n",
      "\n",
      "===============================================\n",
      "===============================================\n",
      "After Ensemble,acc: 85.9 loss 0.0012730895280838013\n",
      "===============================================\n",
      "===============================================\n",
      "Timer ends\n",
      "Time taken for geometric ensembling is 3.210039851954207 seconds\n",
      "------- Prediction based ensembling -------\n",
      "\n",
      "Test set: Avg. loss: 0.0003, Accuracy: 9110/10000 (91%)\n",
      "\n",
      "------- Naive ensembling of weights -------\n",
      "[torch.Size([64, 3, 3, 3]), torch.Size([64, 3, 3, 3])]\n",
      "torch.Size([64, 3, 3, 3])\n",
      "[torch.Size([128, 64, 3, 3]), torch.Size([128, 64, 3, 3])]\n",
      "torch.Size([128, 64, 3, 3])\n",
      "[torch.Size([256, 128, 3, 3]), torch.Size([256, 128, 3, 3])]\n",
      "torch.Size([256, 128, 3, 3])\n",
      "[torch.Size([256, 256, 3, 3]), torch.Size([256, 256, 3, 3])]\n",
      "torch.Size([256, 256, 3, 3])\n",
      "[torch.Size([512, 256, 3, 3]), torch.Size([512, 256, 3, 3])]\n",
      "torch.Size([512, 256, 3, 3])\n",
      "[torch.Size([512, 512, 3, 3]), torch.Size([512, 512, 3, 3])]\n",
      "torch.Size([512, 512, 3, 3])\n",
      "[torch.Size([512, 512, 3, 3]), torch.Size([512, 512, 3, 3])]\n",
      "torch.Size([512, 512, 3, 3])\n",
      "[torch.Size([512, 512, 3, 3]), torch.Size([512, 512, 3, 3])]\n",
      "torch.Size([512, 512, 3, 3])\n",
      "[torch.Size([10, 512]), torch.Size([10, 512])]\n",
      "torch.Size([10, 512])\n",
      "in _make_layers [Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), AvgPool2d(kernel_size=1, stride=1, padding=0)]\n",
      "Relu Inplace is  False\n",
      "model parameters are \n",
      " [torch.Size([64, 3, 3, 3]), torch.Size([128, 64, 3, 3]), torch.Size([256, 128, 3, 3]), torch.Size([256, 256, 3, 3]), torch.Size([512, 256, 3, 3]), torch.Size([512, 512, 3, 3]), torch.Size([512, 512, 3, 3]), torch.Size([512, 512, 3, 3]), torch.Size([10, 512])]\n",
      "\n",
      "--------- Testing in global mode ---------\n",
      "size of test_loader dataset:  10000\n",
      "\n",
      "Test set: Avg. loss: 0.0023, Accuracy: 904/10000 (9%)\n",
      "\n",
      "\n",
      "--------- Testing in global mode ---------\n",
      "size of test_loader dataset:  10000\n",
      "\n",
      "Test set: Avg. loss: 0.0023, Accuracy: 1483/10000 (15%)\n",
      "\n",
      "-------- Retraining the models ---------\n",
      "Retraining model :  model_0\n",
      "num_epochs---------------+++))))))+++ 300\n",
      "num_epochs---------------+++))))))+++ 300\n",
      "lr is  0.05\n",
      "number of epochs would be  30\n",
      "num_epochs-------------- 30\n",
      "Epoch 000\n",
      "/usr/local/pace-apps/manual/packages/pytorch/1.12.0/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:131: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\"Detected call of `lr_scheduler.step()` before `optimizer.step()`. \"\n",
      "/usr/local/pace-apps/manual/packages/pytorch/1.12.0/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:156: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "accuracy: {'epoch': 0, 'value': 0.8951199999809262} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 0, 'value': 0.31531580252647406} ({'split': 'train'})\n",
      "accuracy: {'epoch': 0, 'value': 0.8362000346183777} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 0, 'value': 0.5080202281475067} ({'split': 'test'})\n",
      "Epoch 001\n",
      "accuracy: {'epoch': 1, 'value': 0.928460000038147} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 1, 'value': 0.2110305256271363} ({'split': 'train'})\n",
      "accuracy: {'epoch': 1, 'value': 0.8566000461578369} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 1, 'value': 0.46844792664051055} ({'split': 'test'})\n",
      "Epoch 002\n",
      "accuracy: {'epoch': 2, 'value': 0.9473800000190737} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 2, 'value': 0.15643289404869087} ({'split': 'train'})\n",
      "accuracy: {'epoch': 2, 'value': 0.8508000433444977} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 2, 'value': 0.5109203040599823} ({'split': 'test'})\n",
      "Epoch 003\n",
      "accuracy: {'epoch': 3, 'value': 0.9602400000572207} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 3, 'value': 0.11812330454111102} ({'split': 'train'})\n",
      "accuracy: {'epoch': 3, 'value': 0.8539000451564789} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 3, 'value': 0.5465173959732056} ({'split': 'test'})\n",
      "Epoch 004\n",
      "accuracy: {'epoch': 4, 'value': 0.9722399999809265} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 4, 'value': 0.08543080784797667} ({'split': 'train'})\n",
      "accuracy: {'epoch': 4, 'value': 0.8571000397205353} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 4, 'value': 0.5684880077838897} ({'split': 'test'})\n",
      "Epoch 005\n",
      "accuracy: {'epoch': 5, 'value': 0.9731000000572207} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 5, 'value': 0.08177937172412873} ({'split': 'train'})\n",
      "accuracy: {'epoch': 5, 'value': 0.8588000357151031} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 5, 'value': 0.5209989368915557} ({'split': 'test'})\n",
      "Epoch 006\n",
      "accuracy: {'epoch': 6, 'value': 0.9793800000381467} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 6, 'value': 0.06294088052272796} ({'split': 'train'})\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: {'epoch': 6, 'value': 0.8548000395298004} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 6, 'value': 0.541737824678421} ({'split': 'test'})\n",
      "Epoch 007\n",
      "accuracy: {'epoch': 7, 'value': 0.9823800000572204} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 7, 'value': 0.05421690947294234} ({'split': 'train'})\n",
      "accuracy: {'epoch': 7, 'value': 0.8576000392436981} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 7, 'value': 0.57166628241539} ({'split': 'test'})\n",
      "Epoch 008\n",
      "accuracy: {'epoch': 8, 'value': 0.9826999999999998} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 8, 'value': 0.05260873134136198} ({'split': 'train'})\n",
      "accuracy: {'epoch': 8, 'value': 0.8576000392436981} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 8, 'value': 0.5883015036582947} ({'split': 'test'})\n",
      "Epoch 009\n",
      "accuracy: {'epoch': 9, 'value': 0.9872800000381472} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 9, 'value': 0.03973255690813065} ({'split': 'train'})\n",
      "accuracy: {'epoch': 9, 'value': 0.8630000412464142} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 9, 'value': 0.542350333929062} ({'split': 'test'})\n",
      "Epoch 010\n",
      "accuracy: {'epoch': 10, 'value': 0.9862200000381475} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 10, 'value': 0.04390260297775268} ({'split': 'train'})\n",
      "accuracy: {'epoch': 10, 'value': 0.8625000476837158} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 10, 'value': 0.6117210984230042} ({'split': 'test'})\n",
      "Epoch 011\n",
      "accuracy: {'epoch': 11, 'value': 0.9838200000381462} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 11, 'value': 0.050769693970680235} ({'split': 'train'})\n",
      "accuracy: {'epoch': 11, 'value': 0.8510000348091126} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 11, 'value': 0.6166756093502045} ({'split': 'test'})\n",
      "Epoch 012\n",
      "accuracy: {'epoch': 12, 'value': 0.9871000000381468} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 12, 'value': 0.041150264220237744} ({'split': 'train'})\n",
      "accuracy: {'epoch': 12, 'value': 0.8700000345706941} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 12, 'value': 0.5384641021490096} ({'split': 'test'})\n",
      "Epoch 013\n",
      "accuracy: {'epoch': 13, 'value': 0.985480000038147} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 13, 'value': 0.04616463696241379} ({'split': 'train'})\n",
      "accuracy: {'epoch': 13, 'value': 0.8471000373363495} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 13, 'value': 0.6122187852859498} ({'split': 'test'})\n",
      "Epoch 014\n",
      "accuracy: {'epoch': 14, 'value': 0.9863000000190734} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 14, 'value': 0.04251446803212165} ({'split': 'train'})\n",
      "accuracy: {'epoch': 14, 'value': 0.8573000371456145} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 14, 'value': 0.6290091276168823} ({'split': 'test'})\n",
      "Epoch 015\n",
      "accuracy: {'epoch': 15, 'value': 0.9913400000000003} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 15, 'value': 0.03060065492868424} ({'split': 'train'})\n",
      "accuracy: {'epoch': 15, 'value': 0.856700038909912} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 15, 'value': 0.6045291662216187} ({'split': 'test'})\n",
      "Epoch 016\n",
      "accuracy: {'epoch': 16, 'value': 0.9917799999999998} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 16, 'value': 0.02747287947773934} ({'split': 'train'})\n",
      "accuracy: {'epoch': 16, 'value': 0.8429000437259674} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 16, 'value': 0.7050644040107727} ({'split': 'test'})\n",
      "Epoch 017\n",
      "accuracy: {'epoch': 17, 'value': 0.9904000000190737} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 17, 'value': 0.031438325119018555} ({'split': 'train'})\n",
      "accuracy: {'epoch': 17, 'value': 0.8457000494003296} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 17, 'value': 0.5913499951362609} ({'split': 'test'})\n",
      "Epoch 018\n",
      "accuracy: {'epoch': 18, 'value': 0.9875000000000002} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 18, 'value': 0.0401956430476904} ({'split': 'train'})\n",
      "accuracy: {'epoch': 18, 'value': 0.8437000334262846} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 18, 'value': 0.6273028492927551} ({'split': 'test'})\n",
      "Epoch 019\n",
      "accuracy: {'epoch': 19, 'value': 0.9871800000190736} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 19, 'value': 0.0404216042804718} ({'split': 'train'})\n",
      "accuracy: {'epoch': 19, 'value': 0.844400042295456} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 19, 'value': 0.6520635962486268} ({'split': 'test'})\n",
      "Epoch 020\n",
      "accuracy: {'epoch': 20, 'value': 0.99032} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 20, 'value': 0.031608952068686476} ({'split': 'train'})\n",
      "accuracy: {'epoch': 20, 'value': 0.8541000425815581} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 20, 'value': 0.6049115002155304} ({'split': 'test'})\n",
      "Epoch 021\n",
      "accuracy: {'epoch': 21, 'value': 0.9870199999999999} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 21, 'value': 0.041737373306453246} ({'split': 'train'})\n",
      "accuracy: {'epoch': 21, 'value': 0.8472000420093536} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 21, 'value': 0.6425936698913575} ({'split': 'test'})\n",
      "Epoch 022\n",
      "accuracy: {'epoch': 22, 'value': 0.9895400000190735} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 22, 'value': 0.03339559181451798} ({'split': 'train'})\n",
      "accuracy: {'epoch': 22, 'value': 0.8457000315189361} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 22, 'value': 0.624459046125412} ({'split': 'test'})\n",
      "Epoch 023\n",
      "accuracy: {'epoch': 23, 'value': 0.9861800000381471} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 23, 'value': 0.04276735854387283} ({'split': 'train'})\n",
      "accuracy: {'epoch': 23, 'value': 0.8499000370502472} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 23, 'value': 0.600737327337265} ({'split': 'test'})\n",
      "Epoch 024\n",
      "accuracy: {'epoch': 24, 'value': 0.9869400000190737} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 24, 'value': 0.04210019931316375} ({'split': 'train'})\n",
      "accuracy: {'epoch': 24, 'value': 0.8506000339984894} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 24, 'value': 0.6017744064331054} ({'split': 'test'})\n",
      "Epoch 025\n",
      "accuracy: {'epoch': 25, 'value': 0.9871800000000001} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 25, 'value': 0.041868797129392614} ({'split': 'train'})\n",
      "accuracy: {'epoch': 25, 'value': 0.850700032711029} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 25, 'value': 0.6068979471921921} ({'split': 'test'})\n",
      "Epoch 026\n",
      "accuracy: {'epoch': 26, 'value': 0.98506} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 26, 'value': 0.048100949170589435} ({'split': 'train'})\n",
      "accuracy: {'epoch': 26, 'value': 0.8514000356197357} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 26, 'value': 0.5958176851272583} ({'split': 'test'})\n",
      "Epoch 027\n",
      "accuracy: {'epoch': 27, 'value': 0.9872200000190735} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 27, 'value': 0.038802852343320836} ({'split': 'train'})\n",
      "accuracy: {'epoch': 27, 'value': 0.8512000441551208} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 27, 'value': 0.5957448720932007} ({'split': 'test'})\n",
      "Epoch 028\n",
      "accuracy: {'epoch': 28, 'value': 0.9894199999999997} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 28, 'value': 0.03373419820308686} ({'split': 'train'})\n",
      "accuracy: {'epoch': 28, 'value': 0.8464000403881073} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 28, 'value': 0.6652878582477569} ({'split': 'test'})\n",
      "Epoch 029\n",
      "accuracy: {'epoch': 29, 'value': 0.9889200000572206} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 29, 'value': 0.035994002885818485} ({'split': 'train'})\n",
      "accuracy: {'epoch': 29, 'value': 0.8504000365734099} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 29, 'value': 0.6120037317276003} ({'split': 'test'})\n",
      "mean_test_accuracy <cifar_utils.accumulators.Mean object at 0x2aab8d7b42b0>\n",
      "QWERTY: Enter Cifar 2, acc 0.8700000345706941\n",
      "Retraining model :  model_1\n",
      "num_epochs---------------+++))))))+++ 30\n",
      "num_epochs---------------+++))))))+++ 30\n",
      "lr is  0.05\n",
      "number of epochs would be  30\n",
      "num_epochs-------------- 30\n",
      "Epoch 000\n",
      "accuracy: {'epoch': 0, 'value': 0.8917000000190733} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 0, 'value': 0.3296694714546203} ({'split': 'train'})\n",
      "accuracy: {'epoch': 0, 'value': 0.8387000381946563} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 0, 'value': 0.5002811312675476} ({'split': 'test'})\n",
      "Epoch 001\n",
      "accuracy: {'epoch': 1, 'value': 0.9272799999809263} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 1, 'value': 0.21600715390205388} ({'split': 'train'})\n",
      "accuracy: {'epoch': 1, 'value': 0.8346000373363494} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 1, 'value': 0.5658823698759079} ({'split': 'test'})\n",
      "Epoch 002\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: {'epoch': 2, 'value': 0.9483400000572206} ({'split': 'train'})\r\n",
      "cross_entropy: {'epoch': 2, 'value': 0.15412278239011776} ({'split': 'train'})\r\n",
      "^C\r\n",
      "Traceback (most recent call last):\r\n",
      "  File \"/storage/coda1/p-jli3175/0/mwu385/OTfusion_pruning_final/otfusion/main.py\", line 460, in <module>\r\n",
      "    _, best_retrain_acc = routines.retrain_models(args, [*original_models, geometric_model, naive_model], retrain_loader, test_loader, config, tensorboard_obj=tensorboard_obj, initial_acc=initial_acc, nicks=nicks)\r\n",
      "  File \"/storage/coda1/p-jli3175/0/mwu385/OTfusion_pruning_final/otfusion/routines.py\", line 394, in retrain_models\r\n",
      "    retrained_network, acc = cifar_train.get_retrained_model(args, retrain_loader, test_loader, old_networks[i], config, output_root_dir, tensorboard_obj=tensorboard_obj, nick=nick, start_acc=initial_acc[i])\r\n",
      "  File \"/storage/coda1/p-jli3175/0/mwu385/OTfusion_pruning_final/otfusion/./cifar/train.py\", line 333, in get_retrained_model\r\n",
      "    best_acc,model = main(config, output_dir, args.gpu_id, pretrained_model=old_network, pretrained_dataset=(train_loader, test_loader), tensorboard_obj=tensorboard_obj,return_model=True)\r\n",
      "  File \"/storage/coda1/p-jli3175/0/mwu385/OTfusion_pruning_final/otfusion/./cifar/train.py\", line 100, in main\r\n",
      "    for batch_x, batch_y in test_loader:\r\n",
      "  File \"/usr/local/pace-apps/manual/packages/pytorch/1.12.0/lib/python3.9/site-packages/torch/utils/data/dataloader.py\", line 652, in __next__\r\n",
      "    data = self._next_data()\r\n",
      "  File \"/usr/local/pace-apps/manual/packages/pytorch/1.12.0/lib/python3.9/site-packages/torch/utils/data/dataloader.py\", line 692, in _next_data\r\n",
      "    data = self._dataset_fetcher.fetch(index)  # may raise StopIteration\r\n",
      "  File \"/usr/local/pace-apps/manual/packages/pytorch/1.12.0/lib/python3.9/site-packages/torch/utils/data/_utils/fetch.py\", line 49, in fetch\r\n",
      "    data = [self.dataset[idx] for idx in possibly_batched_index]\r\n",
      "  File \"/usr/local/pace-apps/manual/packages/pytorch/1.12.0/lib/python3.9/site-packages/torch/utils/data/_utils/fetch.py\", line 49, in <listcomp>\r\n",
      "    data = [self.dataset[idx] for idx in possibly_batched_index]\r\n",
      "  File \"/usr/local/pace-apps/manual/packages/pytorch/1.12.0/lib/python3.9/site-packages/torchvision/datasets/cifar.py\", line 118, in __getitem__\r\n",
      "    img = self.transform(img)\r\n",
      "  File \"/usr/local/pace-apps/manual/packages/pytorch/1.12.0/lib/python3.9/site-packages/torchvision/transforms/transforms.py\", line 94, in __call__\r\n",
      "    img = t(img)\r\n",
      "  File \"/usr/local/pace-apps/manual/packages/pytorch/1.12.0/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 1130, in _call_impl\r\n",
      "    return forward_call(*input, **kwargs)\r\n",
      "  File \"/usr/local/pace-apps/manual/packages/pytorch/1.12.0/lib/python3.9/site-packages/torchvision/transforms/transforms.py\", line 269, in forward\r\n",
      "    return F.normalize(tensor, self.mean, self.std, self.inplace)\r\n",
      "  File \"/usr/local/pace-apps/manual/packages/pytorch/1.12.0/lib/python3.9/site-packages/torchvision/transforms/functional.py\", line 360, in normalize\r\n",
      "    return F_t.normalize(tensor, mean=mean, std=std, inplace=inplace)\r\n",
      "  File \"/usr/local/pace-apps/manual/packages/pytorch/1.12.0/lib/python3.9/site-packages/torchvision/transforms/functional_tensor.py\", line 936, in normalize\r\n",
      "    def normalize(tensor: Tensor, mean: List[float], std: List[float], inplace: bool = False) -> Tensor:\r\n",
      "KeyboardInterrupt\r\n"
     ]
    }
   ],
   "source": [
    "!python main.py --gpu-id 0 --model-name vgg11_nobias --n-epochs 10 --save-result-file sample.csv \\\n",
    "--sweep-name exp_sample --correction --ground-metric euclidean --weight-stats \\\n",
    "--geom-ensemble-type wts --ground-metric-normalize none --sweep-id 90 \\\n",
    "--ckpt-type best --dataset Cifar10 --ground-metric-eff --recheck-cifar --activation-seed {activation_seed} \\\n",
    "--prelu-acts --past-correction --not-squared --exact\\\n",
    "--learning-rate 0.0001\\\n",
    "--momentum 0.5\\\n",
    "--batch-size-train 256 --experiment-dir {experiment_dir}\\\n",
    "--prunint-rate {prunint_rate}\\\n",
    "--run-mode {run_mode}\\\n",
    "--to-download\\\n",
    "--load-model-dir {load_model_dir}\\\n",
    "--retrain 30\\\n",
    "--prune_times 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "29a14370-8231-4282-b050-cda48f71b3f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#剪枝实验1:prune without finetune\n",
    "activation_seed=21\n",
    "experiment_dir=\"experiment1.5\"\n",
    "prunint_rate=0.3\n",
    "run_mode='prune'\n",
    "load_model_dir='./cifar_models'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a2488ee6-9aab-4165-9663-cf713df42dee",
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------- Setting up parameters -------\n",
      "dumping parameters at  /storage/coda1/p-jli3175/0/mwu385/OTfusion_pruning_final/otfusion/exp_sample/configurations\n",
      "The parameters are: \n",
      " Namespace(experiment_dir='experiment1.5', prunint_rate=0.3, run_mode='prune', load_model_dir='./cifar_models', prune_times=4, finetunetimes_benchmark=0, n_epochs=90, batch_size_train=64, batch_size_test=1000, learning_rate=0.08, momentum=0.5, log_interval=100, to_download=True, disable_bias=True, dataset='Cifar10', num_models=2, model_name='vgg11_nobias', config_file=None, config_dir='/storage/coda1/p-jli3175/0/mwu385/OTfusion_pruning_final/otfusion/exp_sample/configurations', num_hidden_nodes=400, num_hidden_nodes1=400, num_hidden_nodes2=200, num_hidden_nodes3=100, num_hidden_nodes4=50, sweep_id=90, gpu_id=0, skip_last_layer=False, skip_last_layer_type='average', debug=False, cifar_style_data=False, activation_histograms=False, act_num_samples=100, softmax_temperature=1, activation_mode=None, options_type='generic', deprecated=None, save_result_file='sample.csv', sweep_name='exp_sample', reg=0.01, reg_m=0.001, ground_metric='euclidean', ground_metric_normalize='none', not_squared=True, clip_gm=False, clip_min=0, clip_max=5, tmap_stats=False, ensemble_step=0.5, ground_metric_eff=True, retrain=30, retrain_lr_decay=-1, retrain_lr_decay_factor=None, retrain_lr_decay_epochs=None, retrain_avg_only=False, retrain_geometric_only=False, load_models='', ckpt_type='best', recheck_cifar=True, recheck_acc=False, eval_aligned=False, enable_dropout=False, dump_model=False, dump_final_models=False, correction=True, activation_seed=21, weight_stats=True, sinkhorn_type='normal', geom_ensemble_type='wts', act_bug=False, standardize_acts=False, transform_acts=False, center_acts=False, prelu_acts=True, pool_acts=False, pool_relu=False, normalize_acts=False, normalize_wts=False, gromov=False, gromov_loss='square_loss', tensorboard_root='./tensorboard', tensorboard=False, same_model=-1, dist_normalize=False, update_acts=False, past_correction=True, partial_reshape=False, choice='0 2 4 6 8', diff_init=False, partition_type='labels', personal_class_idx=9, partition_dataloader=-1, personal_split_frac=0.1, exact=True, skip_personal_idx=False, prediction_wts=False, width_ratio=1, proper_marginals=False, retrain_seed=-1, no_random_trainloaders=False, reinit_trainloaders=False, second_model_name=None, print_distances=False, deterministic=False, skip_retrain=-1, importance=None, unbalanced=False, temperature=20, alpha=0.7, dist_epochs=60, handle_skips=False, timestamp='2023-09-19_00-14-06_568261', rootdir='/storage/coda1/p-jli3175/0/mwu385/OTfusion_pruning_final/otfusion/exp_sample', baseroot='/storage/coda1/p-jli3175/0/mwu385/OTfusion_pruning_final/otfusion', result_dir='/storage/coda1/p-jli3175/0/mwu385/OTfusion_pruning_final/otfusion/exp_sample/results', exp_name='exp_2023-09-19_00-14-06_568261', csv_dir='/storage/coda1/p-jli3175/0/mwu385/OTfusion_pruning_final/otfusion/exp_sample/csv')\n",
      "refactored get_config\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "in _make_layers [Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), AvgPool2d(kernel_size=1, stride=1, padding=0)]\n",
      "Relu Inplace is  False\n",
      "model parameters are \n",
      " [torch.Size([64, 3, 3, 3]), torch.Size([128, 64, 3, 3]), torch.Size([256, 128, 3, 3]), torch.Size([256, 256, 3, 3]), torch.Size([512, 256, 3, 3]), torch.Size([512, 512, 3, 3]), torch.Size([512, 512, 3, 3]), torch.Size([512, 512, 3, 3]), torch.Size([10, 512])]\n",
      "in _make_layers [Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), AvgPool2d(kernel_size=1, stride=1, padding=0)]\n",
      "Relu Inplace is  False\n",
      "model parameters are \n",
      " [torch.Size([64, 3, 3, 3]), torch.Size([128, 64, 3, 3]), torch.Size([256, 128, 3, 3]), torch.Size([256, 256, 3, 3]), torch.Size([512, 256, 3, 3]), torch.Size([512, 512, 3, 3]), torch.Size([512, 512, 3, 3]), torch.Size([512, 512, 3, 3]), torch.Size([10, 512])]\n",
      "in _make_layers [Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), AvgPool2d(kernel_size=1, stride=1, padding=0)]\n",
      "Relu Inplace is  False\n",
      "model parameters are \n",
      " [torch.Size([64, 3, 3, 3]), torch.Size([128, 64, 3, 3]), torch.Size([256, 128, 3, 3]), torch.Size([256, 256, 3, 3]), torch.Size([512, 256, 3, 3]), torch.Size([512, 512, 3, 3]), torch.Size([512, 512, 3, 3]), torch.Size([512, 512, 3, 3]), torch.Size([10, 512])]\n",
      "in _make_layers [Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), AvgPool2d(kernel_size=1, stride=1, padding=0)]\n",
      "Relu Inplace is  False\n",
      "model parameters are \n",
      " [torch.Size([64, 3, 3, 3]), torch.Size([128, 64, 3, 3]), torch.Size([256, 128, 3, 3]), torch.Size([256, 256, 3, 3]), torch.Size([512, 256, 3, 3]), torch.Size([512, 512, 3, 3]), torch.Size([512, 512, 3, 3]), torch.Size([512, 512, 3, 3]), torch.Size([10, 512])]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--------- Testing in global mode ---------\n",
      "size of test_loader dataset:  10000\n",
      "\n",
      "Test set: Avg. loss: 0.0003, Accuracy: 8982/10000 (90%)\n",
      "\n",
      "\n",
      "--------- Testing in global mode ---------\n",
      "size of test_loader dataset:  10000\n",
      "\n",
      "Test set: Avg. loss: 0.0003, Accuracy: 9017/10000 (90%)\n",
      "\n",
      "===============================================\n",
      "===============================================\n",
      "before Ensemble,acc: [89.82, 90.17] loss [0.00033039932250976563, 0.00033331955671310427]\n",
      "===============================================\n",
      "===============================================\n",
      "check zero rate for model 0\n",
      "odict_keys(['features.0.weight', 'features.3.weight', 'features.6.weight', 'features.8.weight', 'features.11.weight', 'features.13.weight', 'features.16.weight', 'features.18.weight', 'classifier.weight'])\n",
      "the ratio of zero for features.0.weight\n",
      "tensor(0.2998, device='cuda:0')\n",
      "the ratio of zero for features.3.weight\n",
      "tensor(0.3000, device='cuda:0')\n",
      "the ratio of zero for features.6.weight\n",
      "tensor(0.3000, device='cuda:0')\n",
      "the ratio of zero for features.8.weight\n",
      "tensor(0.3000, device='cuda:0')\n",
      "the ratio of zero for features.11.weight\n",
      "tensor(0.3000, device='cuda:0')\n",
      "the ratio of zero for features.13.weight\n",
      "tensor(0.3000, device='cuda:0')\n",
      "the ratio of zero for features.16.weight\n",
      "tensor(0.3000, device='cuda:0')\n",
      "the ratio of zero for features.18.weight\n",
      "tensor(0.3000, device='cuda:0')\n",
      "the ratio of zero for classifier.weight\n",
      "tensor(0.3000, device='cuda:0')\n",
      "check zero rate for model 1\n",
      "odict_keys(['features.0.weight', 'features.3.weight', 'features.6.weight', 'features.8.weight', 'features.11.weight', 'features.13.weight', 'features.16.weight', 'features.18.weight', 'classifier.weight'])\n",
      "the ratio of zero for features.0.weight\n",
      "tensor(0.2998, device='cuda:0')\n",
      "the ratio of zero for features.3.weight\n",
      "tensor(0.3000, device='cuda:0')\n",
      "the ratio of zero for features.6.weight\n",
      "tensor(0.3000, device='cuda:0')\n",
      "the ratio of zero for features.8.weight\n",
      "tensor(0.3000, device='cuda:0')\n",
      "the ratio of zero for features.11.weight\n",
      "tensor(0.3000, device='cuda:0')\n",
      "the ratio of zero for features.13.weight\n",
      "tensor(0.3000, device='cuda:0')\n",
      "the ratio of zero for features.16.weight\n",
      "tensor(0.3000, device='cuda:0')\n",
      "the ratio of zero for features.18.weight\n",
      "tensor(0.3000, device='cuda:0')\n",
      "the ratio of zero for classifier.weight\n",
      "tensor(0.3000, device='cuda:0')\n",
      "layer features.0.weight has #params  1728\n",
      "layer features.3.weight has #params  73728\n",
      "layer features.6.weight has #params  294912\n",
      "layer features.8.weight has #params  589824\n",
      "layer features.11.weight has #params  1179648\n",
      "layer features.13.weight has #params  2359296\n",
      "layer features.16.weight has #params  2359296\n",
      "layer features.18.weight has #params  2359296\n",
      "layer classifier.weight has #params  5120\n",
      "Activation Timer start\n",
      "Activation Timer ends\n",
      "------- Geometric Ensembling -------\n",
      "Timer start\n",
      "Previous layer shape is  None\n",
      "Processing the coordinates to form ground_metric\n",
      "dont leave off the squaring of the ground metric\n",
      "returns a uniform measure of cardinality:  64\n",
      "returns a uniform measure of cardinality:  64\n",
      "the transport map is  tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')\n",
      "Ratio of trace to the matrix sum:  tensor(0.0312, device='cuda:0')\n",
      "Here, trace is 1.9999871253967285 and matrix sum is 63.99958801269531 \n",
      "this is past correction for weight mode\n",
      "Shape of aligned wt is  torch.Size([64, 3, 9])\n",
      "Shape of fc_layer0_weight_data is  torch.Size([64, 3, 9])\n",
      "Previous layer shape is  torch.Size([64, 3, 3, 3])\n",
      "shape of layer: model 0 torch.Size([128, 64, 9])\n",
      "shape of layer: model 1 torch.Size([128, 64, 9])\n",
      "shape of previous transport map torch.Size([64, 64])\n",
      "Processing the coordinates to form ground_metric\n",
      "dont leave off the squaring of the ground metric\n",
      "returns a uniform measure of cardinality:  128\n",
      "returns a uniform measure of cardinality:  128\n",
      "the transport map is  tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        ...,\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0078],\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],\n",
      "       device='cuda:0')\n",
      "Ratio of trace to the matrix sum:  tensor(0.0078, device='cuda:0')\n",
      "Here, trace is 0.9999872446060181 and matrix sum is 127.99836730957031 \n",
      "this is past correction for weight mode\n",
      "Shape of aligned wt is  torch.Size([128, 64, 9])\n",
      "Shape of fc_layer0_weight_data is  torch.Size([128, 64, 9])\n",
      "Previous layer shape is  torch.Size([128, 64, 3, 3])\n",
      "shape of layer: model 0 torch.Size([256, 128, 9])\n",
      "shape of layer: model 1 torch.Size([256, 128, 9])\n",
      "shape of previous transport map torch.Size([128, 128])\n",
      "Processing the coordinates to form ground_metric\n",
      "dont leave off the squaring of the ground metric\n",
      "returns a uniform measure of cardinality:  256\n",
      "returns a uniform measure of cardinality:  256\n",
      "the transport map is  tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')\n",
      "Ratio of trace to the matrix sum:  tensor(0., device='cuda:0')\n",
      "Here, trace is 0.0 and matrix sum is 255.99343872070312 \n",
      "this is past correction for weight mode\n",
      "Shape of aligned wt is  torch.Size([256, 128, 9])\n",
      "Shape of fc_layer0_weight_data is  torch.Size([256, 128, 9])\n",
      "Previous layer shape is  torch.Size([256, 128, 3, 3])\n",
      "shape of layer: model 0 torch.Size([256, 256, 9])\n",
      "shape of layer: model 1 torch.Size([256, 256, 9])\n",
      "shape of previous transport map torch.Size([256, 256])\n",
      "Processing the coordinates to form ground_metric\n",
      "dont leave off the squaring of the ground metric\n",
      "returns a uniform measure of cardinality:  256\n",
      "returns a uniform measure of cardinality:  256\n",
      "the transport map is  tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        ...,\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.0039, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],\n",
      "       device='cuda:0')\n",
      "Ratio of trace to the matrix sum:  tensor(0.0117, device='cuda:0')\n",
      "Here, trace is 2.9999232292175293 and matrix sum is 255.99343872070312 \n",
      "this is past correction for weight mode\n",
      "Shape of aligned wt is  torch.Size([256, 256, 9])\n",
      "Shape of fc_layer0_weight_data is  torch.Size([256, 256, 9])\n",
      "Previous layer shape is  torch.Size([256, 256, 3, 3])\n",
      "shape of layer: model 0 torch.Size([512, 256, 9])\n",
      "shape of layer: model 1 torch.Size([512, 256, 9])\n",
      "shape of previous transport map torch.Size([256, 256])\n",
      "Processing the coordinates to form ground_metric\n",
      "dont leave off the squaring of the ground metric\n",
      "returns a uniform measure of cardinality:  512\n",
      "returns a uniform measure of cardinality:  512\n",
      "the transport map is  tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')\n",
      "Ratio of trace to the matrix sum:  tensor(0.0039, device='cuda:0')\n",
      "Here, trace is 1.9998977184295654 and matrix sum is 511.97381591796875 \n",
      "this is past correction for weight mode\n",
      "Shape of aligned wt is  torch.Size([512, 256, 9])\n",
      "Shape of fc_layer0_weight_data is  torch.Size([512, 256, 9])\n",
      "Previous layer shape is  torch.Size([512, 256, 3, 3])\n",
      "shape of layer: model 0 torch.Size([512, 512, 9])\n",
      "shape of layer: model 1 torch.Size([512, 512, 9])\n",
      "shape of previous transport map torch.Size([512, 512])\n",
      "Processing the coordinates to form ground_metric\n",
      "dont leave off the squaring of the ground metric\n",
      "returns a uniform measure of cardinality:  512\n",
      "returns a uniform measure of cardinality:  512\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the transport map is  tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')\n",
      "Ratio of trace to the matrix sum:  tensor(0.0020, device='cuda:0')\n",
      "Here, trace is 0.9999488592147827 and matrix sum is 511.97381591796875 \n",
      "this is past correction for weight mode\n",
      "Shape of aligned wt is  torch.Size([512, 512, 9])\n",
      "Shape of fc_layer0_weight_data is  torch.Size([512, 512, 9])\n",
      "Previous layer shape is  torch.Size([512, 512, 3, 3])\n",
      "shape of layer: model 0 torch.Size([512, 512, 9])\n",
      "shape of layer: model 1 torch.Size([512, 512, 9])\n",
      "shape of previous transport map torch.Size([512, 512])\n",
      "Processing the coordinates to form ground_metric\n",
      "dont leave off the squaring of the ground metric\n",
      "returns a uniform measure of cardinality:  512\n",
      "returns a uniform measure of cardinality:  512\n",
      "the transport map is  tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')\n",
      "Ratio of trace to the matrix sum:  tensor(0.0020, device='cuda:0')\n",
      "Here, trace is 0.9999488592147827 and matrix sum is 511.97381591796875 \n",
      "this is past correction for weight mode\n",
      "Shape of aligned wt is  torch.Size([512, 512, 9])\n",
      "Shape of fc_layer0_weight_data is  torch.Size([512, 512, 9])\n",
      "Previous layer shape is  torch.Size([512, 512, 3, 3])\n",
      "shape of layer: model 0 torch.Size([512, 512, 9])\n",
      "shape of layer: model 1 torch.Size([512, 512, 9])\n",
      "shape of previous transport map torch.Size([512, 512])\n",
      "Processing the coordinates to form ground_metric\n",
      "dont leave off the squaring of the ground metric\n",
      "returns a uniform measure of cardinality:  512\n",
      "returns a uniform measure of cardinality:  512\n",
      "the transport map is  tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        [0.0020, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        ...,\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],\n",
      "       device='cuda:0')\n",
      "Ratio of trace to the matrix sum:  tensor(0., device='cuda:0')\n",
      "Here, trace is 0.0 and matrix sum is 511.97381591796875 \n",
      "this is past correction for weight mode\n",
      "Shape of aligned wt is  torch.Size([512, 512, 9])\n",
      "Shape of fc_layer0_weight_data is  torch.Size([512, 512, 9])\n",
      "Previous layer shape is  torch.Size([512, 512, 3, 3])\n",
      "shape of layer: model 0 torch.Size([10, 512])\n",
      "shape of layer: model 1 torch.Size([10, 512])\n",
      "shape of previous transport map torch.Size([512, 512])\n",
      "Processing the coordinates to form ground_metric\n",
      "dont leave off the squaring of the ground metric\n",
      "ground metric is  tensor([[1.4431, 2.4814, 2.7640, 2.8642, 2.7381, 2.8541, 2.7144, 2.6338, 2.5652,\n",
      "         2.5807],\n",
      "        [2.4832, 1.3134, 2.6957, 2.5580, 2.7329, 2.5275, 2.4074, 2.5345, 2.3182,\n",
      "         2.4110],\n",
      "        [2.7668, 2.6553, 1.3792, 2.9559, 2.9218, 2.8444, 2.7036, 2.8443, 2.7224,\n",
      "         2.7683],\n",
      "        [2.8034, 2.6659, 2.8862, 1.5381, 2.8264, 2.9462, 2.7411, 2.8098, 2.8102,\n",
      "         2.6539],\n",
      "        [2.7712, 2.5508, 2.8131, 2.8944, 1.5168, 2.7311, 2.7914, 2.6537, 2.6861,\n",
      "         2.7012],\n",
      "        [2.9167, 2.5796, 2.9437, 2.7910, 2.7188, 1.5508, 2.7595, 2.7077, 2.7962,\n",
      "         2.6762],\n",
      "        [2.6810, 2.4087, 2.7155, 2.9203, 2.6225, 2.8564, 1.4231, 2.6895, 2.4879,\n",
      "         2.5324],\n",
      "        [2.7341, 2.5314, 2.7699, 2.7636, 2.7151, 2.6796, 2.6878, 1.3887, 2.5378,\n",
      "         2.5107],\n",
      "        [2.5475, 2.3703, 2.7718, 2.7028, 2.7099, 2.8053, 2.5222, 2.6313, 1.2765,\n",
      "         2.3832],\n",
      "        [2.5801, 2.2704, 2.7707, 2.7432, 2.6737, 2.7151, 2.5468, 2.4910, 2.5154,\n",
      "         1.2773]], device='cuda:0', grad_fn=<PowBackward0>)\n",
      "returns a uniform measure of cardinality:  10\n",
      "returns a uniform measure of cardinality:  10\n",
      "the transport map is  tensor([[0.1000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0000, 0.1000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0000, 0.0000, 0.1000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.1000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.1000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.1000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.1000, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.1000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.1000,\n",
      "         0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.1000]], device='cuda:0')\n",
      "Ratio of trace to the matrix sum:  tensor(1., device='cuda:0')\n",
      "Here, trace is 9.999990463256836 and matrix sum is 9.999990463256836 \n",
      "this is past correction for weight mode\n",
      "Shape of aligned wt is  torch.Size([10, 512])\n",
      "Shape of fc_layer0_weight_data is  torch.Size([10, 512])\n",
      "using independent method\n",
      "in _make_layers [Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), AvgPool2d(kernel_size=1, stride=1, padding=0)]\n",
      "Relu Inplace is  False\n",
      "model parameters are \n",
      " [torch.Size([64, 3, 3, 3]), torch.Size([128, 64, 3, 3]), torch.Size([256, 128, 3, 3]), torch.Size([256, 256, 3, 3]), torch.Size([512, 256, 3, 3]), torch.Size([512, 512, 3, 3]), torch.Size([512, 512, 3, 3]), torch.Size([512, 512, 3, 3]), torch.Size([10, 512])]\n",
      "\n",
      "--------- Testing in global mode ---------\n",
      "size of test_loader dataset:  10000\n",
      "\n",
      "Test set: Avg. loss: 0.0023, Accuracy: 1019/10000 (10%)\n",
      "\n",
      "len of model parameters and avg aligned layers is  9 9\n",
      "len of model_state_dict is  9\n",
      "len of param_list is  9\n",
      "\n",
      "--------- Testing in global mode ---------\n",
      "size of test_loader dataset:  10000\n",
      "\n",
      "Test set: Avg. loss: 0.0015, Accuracy: 8499/10000 (85%)\n",
      "\n",
      "===============================================\n",
      "===============================================\n",
      "After Ensemble,acc: 84.99 loss 0.0015218741536140441\n",
      "===============================================\n",
      "===============================================\n",
      "Timer ends\n",
      "Time taken for geometric ensembling is 2.931522934930399 seconds\n",
      "------- Prediction based ensembling -------\n",
      "\n",
      "Test set: Avg. loss: 0.0003, Accuracy: 9121/10000 (91%)\n",
      "\n",
      "------- Naive ensembling of weights -------\n",
      "[torch.Size([64, 3, 3, 3]), torch.Size([64, 3, 3, 3])]\n",
      "torch.Size([64, 3, 3, 3])\n",
      "[torch.Size([128, 64, 3, 3]), torch.Size([128, 64, 3, 3])]\n",
      "torch.Size([128, 64, 3, 3])\n",
      "[torch.Size([256, 128, 3, 3]), torch.Size([256, 128, 3, 3])]\n",
      "torch.Size([256, 128, 3, 3])\n",
      "[torch.Size([256, 256, 3, 3]), torch.Size([256, 256, 3, 3])]\n",
      "torch.Size([256, 256, 3, 3])\n",
      "[torch.Size([512, 256, 3, 3]), torch.Size([512, 256, 3, 3])]\n",
      "torch.Size([512, 256, 3, 3])\n",
      "[torch.Size([512, 512, 3, 3]), torch.Size([512, 512, 3, 3])]\n",
      "torch.Size([512, 512, 3, 3])\n",
      "[torch.Size([512, 512, 3, 3]), torch.Size([512, 512, 3, 3])]\n",
      "torch.Size([512, 512, 3, 3])\n",
      "[torch.Size([512, 512, 3, 3]), torch.Size([512, 512, 3, 3])]\n",
      "torch.Size([512, 512, 3, 3])\n",
      "[torch.Size([10, 512]), torch.Size([10, 512])]\n",
      "torch.Size([10, 512])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "in _make_layers [Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), AvgPool2d(kernel_size=1, stride=1, padding=0)]\n",
      "Relu Inplace is  False\n",
      "model parameters are \n",
      " [torch.Size([64, 3, 3, 3]), torch.Size([128, 64, 3, 3]), torch.Size([256, 128, 3, 3]), torch.Size([256, 256, 3, 3]), torch.Size([512, 256, 3, 3]), torch.Size([512, 512, 3, 3]), torch.Size([512, 512, 3, 3]), torch.Size([512, 512, 3, 3]), torch.Size([10, 512])]\n",
      "\n",
      "--------- Testing in global mode ---------\n",
      "size of test_loader dataset:  10000\n",
      "\n",
      "Test set: Avg. loss: 0.0023, Accuracy: 1143/10000 (11%)\n",
      "\n",
      "\n",
      "--------- Testing in global mode ---------\n",
      "size of test_loader dataset:  10000\n",
      "\n",
      "Test set: Avg. loss: 0.0023, Accuracy: 1759/10000 (18%)\n",
      "\n",
      "-------- Retraining the models ---------\n",
      "Retraining model :  model_0\n",
      "num_epochs---------------+++))))))+++ 300\n",
      "num_epochs---------------+++))))))+++ 300\n",
      "lr is  0.05\n",
      "number of epochs would be  30\n",
      "num_epochs-------------- 30\n",
      "Epoch 000\n",
      "/usr/local/pace-apps/manual/packages/pytorch/1.12.0/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:131: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\"Detected call of `lr_scheduler.step()` before `optimizer.step()`. \"\n",
      "/usr/local/pace-apps/manual/packages/pytorch/1.12.0/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:156: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "accuracy: {'epoch': 0, 'value': 0.6949800000000007} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 0, 'value': 0.9317241025543201} ({'split': 'train'})\n",
      "accuracy: {'epoch': 0, 'value': 0.7600000381469727} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 0, 'value': 0.7242591977119446} ({'split': 'test'})\n",
      "Epoch 001\n",
      "accuracy: {'epoch': 1, 'value': 0.7896799999999997} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 1, 'value': 0.64471085395813} ({'split': 'train'})\n",
      "accuracy: {'epoch': 1, 'value': 0.7909000396728515} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 1, 'value': 0.6574820578098297} ({'split': 'test'})\n",
      "Epoch 002\n",
      "accuracy: {'epoch': 2, 'value': 0.8194000000000004} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 2, 'value': 0.552467917346955} ({'split': 'train'})\n",
      "accuracy: {'epoch': 2, 'value': 0.7778000354766846} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 2, 'value': 0.7160158157348633} ({'split': 'test'})\n",
      "Epoch 003\n",
      "accuracy: {'epoch': 3, 'value': 0.8372600000000001} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 3, 'value': 0.5023458707809444} ({'split': 'train'})\n",
      "accuracy: {'epoch': 3, 'value': 0.7883000373840332} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 3, 'value': 0.7103352010250091} ({'split': 'test'})\n",
      "Epoch 004\n",
      "accuracy: {'epoch': 4, 'value': 0.8505999999999986} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 4, 'value': 0.4595854912185672} ({'split': 'train'})\n",
      "accuracy: {'epoch': 4, 'value': 0.7689000427722931} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 4, 'value': 0.7072589218616486} ({'split': 'test'})\n",
      "Epoch 005\n",
      "accuracy: {'epoch': 5, 'value': 0.8570800000000008} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 5, 'value': 0.43406744623184185} ({'split': 'train'})\n",
      "accuracy: {'epoch': 5, 'value': 0.7644000351428986} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 5, 'value': 0.7627931237220764} ({'split': 'test'})\n",
      "Epoch 006\n",
      "accuracy: {'epoch': 6, 'value': 0.86542} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 6, 'value': 0.41401328595161435} ({'split': 'train'})\n",
      "accuracy: {'epoch': 6, 'value': 0.7886000454425812} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 6, 'value': 0.7653741419315339} ({'split': 'test'})\n",
      "Epoch 007\n",
      "accuracy: {'epoch': 7, 'value': 0.8690999999999999} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 7, 'value': 0.3966676432514192} ({'split': 'train'})\n",
      "accuracy: {'epoch': 7, 'value': 0.7823000431060791} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 7, 'value': 0.6994630634784699} ({'split': 'test'})\n",
      "Epoch 008\n",
      "accuracy: {'epoch': 8, 'value': 0.8726999999999986} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 8, 'value': 0.3908747737312314} ({'split': 'train'})\n",
      "accuracy: {'epoch': 8, 'value': 0.7784000277519226} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 8, 'value': 0.744156938791275} ({'split': 'test'})\n",
      "Epoch 009\n",
      "accuracy: {'epoch': 9, 'value': 0.8751399999999998} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 9, 'value': 0.39148441466808365} ({'split': 'train'})\n",
      "accuracy: {'epoch': 9, 'value': 0.7951000452041626} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 9, 'value': 0.7677066922187805} ({'split': 'test'})\n",
      "Epoch 010\n",
      "accuracy: {'epoch': 10, 'value': 0.873719999999999} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 10, 'value': 0.3867607792377475} ({'split': 'train'})\n",
      "accuracy: {'epoch': 10, 'value': 0.7948000371456146} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 10, 'value': 0.6880967140197755} ({'split': 'test'})\n",
      "Epoch 011\n",
      "accuracy: {'epoch': 11, 'value': 0.8846999999999995} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 11, 'value': 0.3603943016052246} ({'split': 'train'})\n",
      "accuracy: {'epoch': 11, 'value': 0.7918000340461732} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 11, 'value': 0.7201961934566498} ({'split': 'test'})\n",
      "Epoch 012\n",
      "accuracy: {'epoch': 12, 'value': 0.8829399999999997} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 12, 'value': 0.358083543801308} ({'split': 'train'})\n",
      "accuracy: {'epoch': 12, 'value': 0.7788000345230103} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 12, 'value': 0.792179799079895} ({'split': 'test'})\n",
      "Epoch 013\n",
      "accuracy: {'epoch': 13, 'value': 0.8848200000000005} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 13, 'value': 0.35622121481895447} ({'split': 'train'})\n",
      "accuracy: {'epoch': 13, 'value': 0.7922000408172607} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 13, 'value': 0.7132372915744781} ({'split': 'test'})\n",
      "Epoch 014\n",
      "accuracy: {'epoch': 14, 'value': 0.8854400000000004} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 14, 'value': 0.35618562400817844} ({'split': 'train'})\n",
      "accuracy: {'epoch': 14, 'value': 0.77540003657341} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 14, 'value': 0.7455909729003906} ({'split': 'test'})\n",
      "Epoch 015\n",
      "accuracy: {'epoch': 15, 'value': 0.8842800000000006} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 15, 'value': 0.35750245300292977} ({'split': 'train'})\n",
      "accuracy: {'epoch': 15, 'value': 0.7819000244140625} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 15, 'value': 0.8150813817977904} ({'split': 'test'})\n",
      "Epoch 016\n",
      "accuracy: {'epoch': 16, 'value': 0.8881799999999997} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 16, 'value': 0.3444518243074415} ({'split': 'train'})\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: {'epoch': 16, 'value': 0.7779000401496887} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 16, 'value': 0.8065318703651428} ({'split': 'test'})\n",
      "Epoch 017\n",
      "accuracy: {'epoch': 17, 'value': 0.8848999999999995} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 17, 'value': 0.3553845950460436} ({'split': 'train'})\n",
      "accuracy: {'epoch': 17, 'value': 0.7890000343322754} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 17, 'value': 0.7083560705184936} ({'split': 'test'})\n",
      "Epoch 018\n",
      "accuracy: {'epoch': 18, 'value': 0.8879000000000004} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 18, 'value': 0.3512283482837679} ({'split': 'train'})\n",
      "accuracy: {'epoch': 18, 'value': 0.7909000337123872} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 18, 'value': 0.815596479177475} ({'split': 'test'})\n",
      "Epoch 019\n",
      "accuracy: {'epoch': 19, 'value': 0.8873200000000001} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 19, 'value': 0.3460617665290832} ({'split': 'train'})\n",
      "accuracy: {'epoch': 19, 'value': 0.8002000391483307} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 19, 'value': 0.6858282148838043} ({'split': 'test'})\n",
      "Epoch 020\n",
      "accuracy: {'epoch': 20, 'value': 0.8919999999999988} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 20, 'value': 0.3381205030441282} ({'split': 'train'})\n",
      "accuracy: {'epoch': 20, 'value': 0.785200047492981} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 20, 'value': 0.7464113414287568} ({'split': 'test'})\n",
      "Epoch 021\n",
      "accuracy: {'epoch': 21, 'value': 0.8916200000000005} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 21, 'value': 0.33472841708660117} ({'split': 'train'})\n",
      "accuracy: {'epoch': 21, 'value': 0.798600047826767} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 21, 'value': 0.7240138351917267} ({'split': 'test'})\n",
      "Epoch 022\n",
      "accuracy: {'epoch': 22, 'value': 0.8878199999999996} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 22, 'value': 0.34710214271068557} ({'split': 'train'})\n",
      "accuracy: {'epoch': 22, 'value': 0.7900000393390656} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 22, 'value': 0.7809044539928436} ({'split': 'test'})\n",
      "Epoch 023\n",
      "accuracy: {'epoch': 23, 'value': 0.8912599999999998} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 23, 'value': 0.3330700536441805} ({'split': 'train'})\n",
      "accuracy: {'epoch': 23, 'value': 0.8031000435352327} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 23, 'value': 0.6967456758022308} ({'split': 'test'})\n",
      "Epoch 024\n",
      "accuracy: {'epoch': 24, 'value': 0.8971800000000001} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 24, 'value': 0.32025232432365364} ({'split': 'train'})\n",
      "accuracy: {'epoch': 24, 'value': 0.7528000354766846} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 24, 'value': 0.891483598947525} ({'split': 'test'})\n",
      "Epoch 025\n",
      "accuracy: {'epoch': 25, 'value': 0.89204} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 25, 'value': 0.3394712126874926} ({'split': 'train'})\n",
      "accuracy: {'epoch': 25, 'value': 0.781000030040741} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 25, 'value': 0.7742364883422852} ({'split': 'test'})\n",
      "Epoch 026\n",
      "accuracy: {'epoch': 26, 'value': 0.8893800000000002} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 26, 'value': 0.34148203535079985} ({'split': 'train'})\n",
      "accuracy: {'epoch': 26, 'value': 0.8025000333786011} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 26, 'value': 0.6751730263233184} ({'split': 'test'})\n",
      "Epoch 027\n",
      "accuracy: {'epoch': 27, 'value': 0.8948600000000004} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 27, 'value': 0.32510761394023874} ({'split': 'train'})\n",
      "accuracy: {'epoch': 27, 'value': 0.780400037765503} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 27, 'value': 0.7854975044727325} ({'split': 'test'})\n",
      "Epoch 028\n",
      "accuracy: {'epoch': 28, 'value': 0.8955200000000009} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 28, 'value': 0.328130712451935} ({'split': 'train'})\n",
      "accuracy: {'epoch': 28, 'value': 0.7712000370025635} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 28, 'value': 0.7494247436523437} ({'split': 'test'})\n",
      "Epoch 029\n",
      "accuracy: {'epoch': 29, 'value': 0.8921800000000002} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 29, 'value': 0.333532668223381} ({'split': 'train'})\n",
      "accuracy: {'epoch': 29, 'value': 0.7956000506877898} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 29, 'value': 0.7465797424316406} ({'split': 'test'})\n",
      "mean_test_accuracy <cifar_utils.accumulators.Mean object at 0x2aabe0bd7df0>\n",
      "QWERTY: Enter Cifar 2, acc 0.8031000435352327\n",
      "Retraining model :  model_1\n",
      "num_epochs---------------+++))))))+++ 30\n",
      "num_epochs---------------+++))))))+++ 30\n",
      "lr is  0.05\n",
      "number of epochs would be  30\n",
      "num_epochs-------------- 30\n",
      "Epoch 000\n",
      "accuracy: {'epoch': 0, 'value': 0.6551599999999991} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 0, 'value': 1.033191127514837} ({'split': 'train'})\n",
      "accuracy: {'epoch': 0, 'value': 0.7352000355720519} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 0, 'value': 0.7897428393363953} ({'split': 'test'})\n",
      "Epoch 001\n",
      "accuracy: {'epoch': 1, 'value': 0.7803000000000005} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 1, 'value': 0.6708096678733831} ({'split': 'train'})\n",
      "accuracy: {'epoch': 1, 'value': 0.7546000361442566} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 1, 'value': 0.7210745036602021} ({'split': 'test'})\n",
      "Epoch 002\n",
      "accuracy: {'epoch': 2, 'value': 0.81074} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 2, 'value': 0.5801458586406706} ({'split': 'train'})\n",
      "accuracy: {'epoch': 2, 'value': 0.7644000351428986} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 2, 'value': 0.7942703187465668} ({'split': 'test'})\n",
      "Epoch 003\n",
      "accuracy: {'epoch': 3, 'value': 0.8344999999999998} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 3, 'value': 0.5052483861446381} ({'split': 'train'})\n",
      "accuracy: {'epoch': 3, 'value': 0.7919000387191772} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 3, 'value': 0.7135947346687317} ({'split': 'test'})\n",
      "Epoch 004\n",
      "accuracy: {'epoch': 4, 'value': 0.8435000000000001} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 4, 'value': 0.4716795811653137} ({'split': 'train'})\n",
      "accuracy: {'epoch': 4, 'value': 0.7820000410079956} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 4, 'value': 0.6964006125926971} ({'split': 'test'})\n",
      "Epoch 005\n",
      "accuracy: {'epoch': 5, 'value': 0.8555999999999998} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 5, 'value': 0.43813507627487175} ({'split': 'train'})\n",
      "accuracy: {'epoch': 5, 'value': 0.7952000439167023} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 5, 'value': 0.6482356727123261} ({'split': 'test'})\n",
      "Epoch 006\n",
      "accuracy: {'epoch': 6, 'value': 0.8596999999999998} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 6, 'value': 0.426826815986633} ({'split': 'train'})\n",
      "accuracy: {'epoch': 6, 'value': 0.7584000289440155} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 6, 'value': 0.8442609071731567} ({'split': 'test'})\n",
      "Epoch 007\n",
      "accuracy: {'epoch': 7, 'value': 0.8646799999999998} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 7, 'value': 0.40962935649871823} ({'split': 'train'})\n",
      "accuracy: {'epoch': 7, 'value': 0.7905000448226929} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 7, 'value': 0.6858637809753418} ({'split': 'test'})\n",
      "Epoch 008\n",
      "accuracy: {'epoch': 8, 'value': 0.87268} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 8, 'value': 0.391927499332428} ({'split': 'train'})\n",
      "accuracy: {'epoch': 8, 'value': 0.8024000406265258} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 8, 'value': 0.6257424294948578} ({'split': 'test'})\n",
      "Epoch 009\n",
      "accuracy: {'epoch': 9, 'value': 0.8744999999999995} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 9, 'value': 0.3857785383772851} ({'split': 'train'})\n",
      "accuracy: {'epoch': 9, 'value': 0.784000039100647} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 9, 'value': 0.7147150397300721} ({'split': 'test'})\n",
      "Epoch 010\n",
      "accuracy: {'epoch': 10, 'value': 0.8801799999999994} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 10, 'value': 0.36518762001991256} ({'split': 'train'})\n",
      "accuracy: {'epoch': 10, 'value': 0.7908000349998474} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 10, 'value': 0.6906944811344147} ({'split': 'test'})\n",
      "Epoch 011\n",
      "accuracy: {'epoch': 11, 'value': 0.8779199999999995} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 11, 'value': 0.3752987222957612} ({'split': 'train'})\n",
      "accuracy: {'epoch': 11, 'value': 0.811100035905838} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 11, 'value': 0.6380331277847289} ({'split': 'test'})\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 012\n",
      "accuracy: {'epoch': 12, 'value': 0.8815600000000005} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 12, 'value': 0.3687025043678282} ({'split': 'train'})\n",
      "accuracy: {'epoch': 12, 'value': 0.7923000395298004} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 12, 'value': 0.685784786939621} ({'split': 'test'})\n",
      "Epoch 013\n",
      "accuracy: {'epoch': 13, 'value': 0.8831400000000003} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 13, 'value': 0.35945278398513775} ({'split': 'train'})\n",
      "accuracy: {'epoch': 13, 'value': 0.7787000298500061} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 13, 'value': 0.7176323056221009} ({'split': 'test'})\n",
      "Epoch 014\n",
      "accuracy: {'epoch': 14, 'value': 0.8871600000000007} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 14, 'value': 0.3441870903301241} ({'split': 'train'})\n",
      "accuracy: {'epoch': 14, 'value': 0.794800043106079} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 14, 'value': 0.7330583214759826} ({'split': 'test'})\n",
      "Epoch 015\n",
      "accuracy: {'epoch': 15, 'value': 0.8853000000000004} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 15, 'value': 0.35217336451530434} ({'split': 'train'})\n",
      "accuracy: {'epoch': 15, 'value': 0.7845000445842744} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 15, 'value': 0.7594782888889313} ({'split': 'test'})\n",
      "Epoch 016\n",
      "accuracy: {'epoch': 16, 'value': 0.8839} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 16, 'value': 0.3569721854686734} ({'split': 'train'})\n",
      "accuracy: {'epoch': 16, 'value': 0.7700000286102294} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 16, 'value': 0.7467237114906311} ({'split': 'test'})\n",
      "Epoch 017\n",
      "accuracy: {'epoch': 17, 'value': 0.8894199999999994} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 17, 'value': 0.3438211313915252} ({'split': 'train'})\n",
      "accuracy: {'epoch': 17, 'value': 0.7891000390052795} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 17, 'value': 0.7271245419979095} ({'split': 'test'})\n",
      "Epoch 018\n",
      "accuracy: {'epoch': 18, 'value': 0.8896600000000003} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 18, 'value': 0.34198487934589356} ({'split': 'train'})\n",
      "accuracy: {'epoch': 18, 'value': 0.8033000349998474} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 18, 'value': 0.7302846431732178} ({'split': 'test'})\n",
      "Epoch 019\n",
      "accuracy: {'epoch': 19, 'value': 0.8858000000000001} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 19, 'value': 0.35139252688407885} ({'split': 'train'})\n",
      "accuracy: {'epoch': 19, 'value': 0.7444000363349915} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 19, 'value': 0.9390414237976074} ({'split': 'test'})\n",
      "Epoch 020\n",
      "accuracy: {'epoch': 20, 'value': 0.8872800000000011} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 20, 'value': 0.353183264331818} ({'split': 'train'})\n",
      "accuracy: {'epoch': 20, 'value': 0.7768000364303589} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 20, 'value': 0.7603942453861237} ({'split': 'test'})\n",
      "Epoch 021\n",
      "accuracy: {'epoch': 21, 'value': 0.8894999999999998} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 21, 'value': 0.34125597115516687} ({'split': 'train'})\n",
      "accuracy: {'epoch': 21, 'value': 0.7801000416278839} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 21, 'value': 0.7097148358821869} ({'split': 'test'})\n",
      "Epoch 022\n",
      "accuracy: {'epoch': 22, 'value': 0.8886999999999997} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 22, 'value': 0.3411502166557312} ({'split': 'train'})\n",
      "accuracy: {'epoch': 22, 'value': 0.7833000421524048} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 22, 'value': 0.7465688526630402} ({'split': 'test'})\n",
      "Epoch 023\n",
      "^C\n",
      "Traceback (most recent call last):\n",
      "  File \"/storage/coda1/p-jli3175/0/mwu385/OTfusion_pruning_final/otfusion/main.py\", line 460, in <module>\n",
      "    _, best_retrain_acc = routines.retrain_models(args, [*original_models, geometric_model, naive_model], retrain_loader, test_loader, config, tensorboard_obj=tensorboard_obj, initial_acc=initial_acc, nicks=nicks)\n",
      "  File \"/storage/coda1/p-jli3175/0/mwu385/OTfusion_pruning_final/otfusion/routines.py\", line 394, in retrain_models\n",
      "    retrained_network, acc = cifar_train.get_retrained_model(args, retrain_loader, test_loader, old_networks[i], config, output_root_dir, tensorboard_obj=tensorboard_obj, nick=nick, start_acc=initial_acc[i])\n",
      "  File \"/storage/coda1/p-jli3175/0/mwu385/OTfusion_pruning_final/otfusion/./cifar/train.py\", line 333, in get_retrained_model\n",
      "    best_acc,model = main(config, output_dir, args.gpu_id, pretrained_model=old_network, pretrained_dataset=(train_loader, test_loader), tensorboard_obj=tensorboard_obj,return_model=True)\n",
      "  File \"/storage/coda1/p-jli3175/0/mwu385/OTfusion_pruning_final/otfusion/./cifar/train.py\", line 67, in main\n",
      "    for batch_x, batch_y in training_loader:\n",
      "  File \"/usr/local/pace-apps/manual/packages/pytorch/1.12.0/lib/python3.9/site-packages/torch/utils/data/dataloader.py\", line 652, in __next__\n",
      "    data = self._next_data()\n",
      "  File \"/usr/local/pace-apps/manual/packages/pytorch/1.12.0/lib/python3.9/site-packages/torch/utils/data/dataloader.py\", line 692, in _next_data\n",
      "    data = self._dataset_fetcher.fetch(index)  # may raise StopIteration\n",
      "  File \"/usr/local/pace-apps/manual/packages/pytorch/1.12.0/lib/python3.9/site-packages/torch/utils/data/_utils/fetch.py\", line 49, in fetch\n",
      "    data = [self.dataset[idx] for idx in possibly_batched_index]\n",
      "  File \"/usr/local/pace-apps/manual/packages/pytorch/1.12.0/lib/python3.9/site-packages/torch/utils/data/_utils/fetch.py\", line 49, in <listcomp>\n",
      "    data = [self.dataset[idx] for idx in possibly_batched_index]\n",
      "  File \"/usr/local/pace-apps/manual/packages/pytorch/1.12.0/lib/python3.9/site-packages/torchvision/datasets/cifar.py\", line 118, in __getitem__\n",
      "    img = self.transform(img)\n",
      "  File \"/usr/local/pace-apps/manual/packages/pytorch/1.12.0/lib/python3.9/site-packages/torchvision/transforms/transforms.py\", line 94, in __call__\n",
      "    img = t(img)\n",
      "  File \"/usr/local/pace-apps/manual/packages/pytorch/1.12.0/lib/python3.9/site-packages/torchvision/transforms/transforms.py\", line 134, in __call__\n",
      "    return F.to_tensor(pic)\n",
      "  File \"/usr/local/pace-apps/manual/packages/pytorch/1.12.0/lib/python3.9/site-packages/torchvision/transforms/functional.py\", line 171, in to_tensor\n",
      "    if isinstance(img, torch.ByteTensor):\n",
      "KeyboardInterrupt\n"
     ]
    }
   ],
   "source": [
    "!python main.py --gpu-id 0 --model-name vgg11_nobias --n-epochs 90 --save-result-file sample.csv \\\n",
    "--sweep-name exp_sample --correction --ground-metric euclidean --weight-stats \\\n",
    "--geom-ensemble-type wts --ground-metric-normalize none --sweep-id 90 \\\n",
    "--ckpt-type best --dataset Cifar10 --ground-metric-eff --recheck-cifar --activation-seed {activation_seed} \\\n",
    "--prelu-acts --past-correction --not-squared --exact\\\n",
    "--learning-rate 0.08\\\n",
    "--momentum 0.5\\\n",
    "--batch-size-train 64 --experiment-dir {experiment_dir}\\\n",
    "--prunint-rate {prunint_rate}\\\n",
    "--run-mode {run_mode}\\\n",
    "--to-download\\\n",
    "--load-model-dir {load_model_dir}\\\n",
    "--retrain 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "19ce2567-adad-4f09-95a3-4449ed54d54d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#剪枝实验2:fine prune\n",
    "activation_seed=21\n",
    "experiment_dir=\"experiment1.5\"\n",
    "prunint_rate=0.3\n",
    "run_mode='finetune'\n",
    "load_model_dir='./cifar_models'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "85412569-b402-468e-9014-991148ecb970",
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------- Setting up parameters -------\n",
      "dumping parameters at  /storage/coda1/p-jli3175/0/mwu385/OTfusion_pruning_final/otfusion/exp_sample/configurations\n",
      "The parameters are: \n",
      " Namespace(experiment_dir='experiment1.5', prunint_rate=0.3, run_mode='finetune', load_model_dir='./cifar_models', prune_times=4, finetunetimes_benchmark=0, n_epochs=30, batch_size_train=128, batch_size_test=1000, learning_rate=0.0003, momentum=0.5, log_interval=100, to_download=True, disable_bias=True, dataset='Cifar10', num_models=2, model_name='vgg11_nobias', config_file=None, config_dir='/storage/coda1/p-jli3175/0/mwu385/OTfusion_pruning_final/otfusion/exp_sample/configurations', num_hidden_nodes=400, num_hidden_nodes1=400, num_hidden_nodes2=200, num_hidden_nodes3=100, num_hidden_nodes4=50, sweep_id=90, gpu_id=0, skip_last_layer=False, skip_last_layer_type='average', debug=False, cifar_style_data=False, activation_histograms=False, act_num_samples=100, softmax_temperature=1, activation_mode=None, options_type='generic', deprecated=None, save_result_file='sample.csv', sweep_name='exp_sample', reg=0.01, reg_m=0.001, ground_metric='euclidean', ground_metric_normalize='none', not_squared=True, clip_gm=False, clip_min=0, clip_max=5, tmap_stats=False, ensemble_step=0.5, ground_metric_eff=True, retrain=30, retrain_lr_decay=-1, retrain_lr_decay_factor=None, retrain_lr_decay_epochs=None, retrain_avg_only=False, retrain_geometric_only=False, load_models='', ckpt_type='best', recheck_cifar=True, recheck_acc=False, eval_aligned=False, enable_dropout=False, dump_model=False, dump_final_models=False, correction=True, activation_seed=21, weight_stats=True, sinkhorn_type='normal', geom_ensemble_type='wts', act_bug=False, standardize_acts=False, transform_acts=False, center_acts=False, prelu_acts=True, pool_acts=False, pool_relu=False, normalize_acts=False, normalize_wts=False, gromov=False, gromov_loss='square_loss', tensorboard_root='./tensorboard', tensorboard=False, same_model=-1, dist_normalize=False, update_acts=False, past_correction=True, partial_reshape=False, choice='0 2 4 6 8', diff_init=False, partition_type='labels', personal_class_idx=9, partition_dataloader=-1, personal_split_frac=0.1, exact=True, skip_personal_idx=False, prediction_wts=False, width_ratio=1, proper_marginals=False, retrain_seed=-1, no_random_trainloaders=False, reinit_trainloaders=False, second_model_name=None, print_distances=False, deterministic=False, skip_retrain=-1, importance=None, unbalanced=False, temperature=20, alpha=0.7, dist_epochs=60, handle_skips=False, timestamp='2023-09-19_00-40-03_603111', rootdir='/storage/coda1/p-jli3175/0/mwu385/OTfusion_pruning_final/otfusion/exp_sample', baseroot='/storage/coda1/p-jli3175/0/mwu385/OTfusion_pruning_final/otfusion', result_dir='/storage/coda1/p-jli3175/0/mwu385/OTfusion_pruning_final/otfusion/exp_sample/results', exp_name='exp_2023-09-19_00-40-03_603111', csv_dir='/storage/coda1/p-jli3175/0/mwu385/OTfusion_pruning_final/otfusion/exp_sample/csv')\n",
      "refactored get_config\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "in _make_layers [Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), AvgPool2d(kernel_size=1, stride=1, padding=0)]\n",
      "Relu Inplace is  False\n",
      "model parameters are \n",
      " [torch.Size([64, 3, 3, 3]), torch.Size([128, 64, 3, 3]), torch.Size([256, 128, 3, 3]), torch.Size([256, 256, 3, 3]), torch.Size([512, 256, 3, 3]), torch.Size([512, 512, 3, 3]), torch.Size([512, 512, 3, 3]), torch.Size([512, 512, 3, 3]), torch.Size([10, 512])]\n",
      "in _make_layers [Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), AvgPool2d(kernel_size=1, stride=1, padding=0)]\n",
      "Relu Inplace is  False\n",
      "model parameters are \n",
      " [torch.Size([64, 3, 3, 3]), torch.Size([128, 64, 3, 3]), torch.Size([256, 128, 3, 3]), torch.Size([256, 256, 3, 3]), torch.Size([512, 256, 3, 3]), torch.Size([512, 512, 3, 3]), torch.Size([512, 512, 3, 3]), torch.Size([512, 512, 3, 3]), torch.Size([10, 512])]\n",
      "in _make_layers [Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), AvgPool2d(kernel_size=1, stride=1, padding=0)]\n",
      "Relu Inplace is  False\n",
      "model parameters are \n",
      " [torch.Size([64, 3, 3, 3]), torch.Size([128, 64, 3, 3]), torch.Size([256, 128, 3, 3]), torch.Size([256, 256, 3, 3]), torch.Size([512, 256, 3, 3]), torch.Size([512, 512, 3, 3]), torch.Size([512, 512, 3, 3]), torch.Size([512, 512, 3, 3]), torch.Size([10, 512])]\n",
      "in _make_layers [Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), AvgPool2d(kernel_size=1, stride=1, padding=0)]\n",
      "Relu Inplace is  False\n",
      "model parameters are \n",
      " [torch.Size([64, 3, 3, 3]), torch.Size([128, 64, 3, 3]), torch.Size([256, 128, 3, 3]), torch.Size([256, 256, 3, 3]), torch.Size([512, 256, 3, 3]), torch.Size([512, 512, 3, 3]), torch.Size([512, 512, 3, 3]), torch.Size([512, 512, 3, 3]), torch.Size([10, 512])]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [0/50000 (0%)]\tLoss: 0.107499\n",
      "Train Epoch: 1 [12800/50000 (26%)]\tLoss: 0.106098\n",
      "Train Epoch: 1 [25600/50000 (51%)]\tLoss: 0.100583\n",
      "Train Epoch: 1 [38400/50000 (77%)]\tLoss: 0.059528\n",
      "\n",
      "--------- Testing in global mode ---------\n",
      "size of test_loader dataset:  10000\n",
      "\n",
      "Test set: Avg. loss: 0.0003, Accuracy: 9025/10000 (90%)\n",
      "\n",
      "Train Epoch: 2 [0/50000 (0%)]\tLoss: 0.062414\n",
      "Train Epoch: 2 [12800/50000 (26%)]\tLoss: 0.044833\n",
      "Train Epoch: 2 [25600/50000 (51%)]\tLoss: 0.076413\n",
      "Train Epoch: 2 [38400/50000 (77%)]\tLoss: 0.051220\n",
      "\n",
      "--------- Testing in global mode ---------\n",
      "size of test_loader dataset:  10000\n",
      "\n",
      "Test set: Avg. loss: 0.0003, Accuracy: 9029/10000 (90%)\n",
      "\n",
      "Train Epoch: 3 [0/50000 (0%)]\tLoss: 0.047788\n",
      "Train Epoch: 3 [12800/50000 (26%)]\tLoss: 0.052018\n",
      "Train Epoch: 3 [25600/50000 (51%)]\tLoss: 0.046932\n",
      "Train Epoch: 3 [38400/50000 (77%)]\tLoss: 0.044914\n",
      "\n",
      "--------- Testing in global mode ---------\n",
      "size of test_loader dataset:  10000\n",
      "\n",
      "Test set: Avg. loss: 0.0003, Accuracy: 9034/10000 (90%)\n",
      "\n",
      "Train Epoch: 4 [0/50000 (0%)]\tLoss: 0.051103\n",
      "Train Epoch: 4 [12800/50000 (26%)]\tLoss: 0.037135\n",
      "Train Epoch: 4 [25600/50000 (51%)]\tLoss: 0.034624\n",
      "Train Epoch: 4 [38400/50000 (77%)]\tLoss: 0.039544\n",
      "\n",
      "--------- Testing in global mode ---------\n",
      "size of test_loader dataset:  10000\n",
      "\n",
      "Test set: Avg. loss: 0.0003, Accuracy: 9030/10000 (90%)\n",
      "\n",
      "Train Epoch: 5 [0/50000 (0%)]\tLoss: 0.052140\n",
      "Train Epoch: 5 [12800/50000 (26%)]\tLoss: 0.038869\n",
      "Train Epoch: 5 [25600/50000 (51%)]\tLoss: 0.050808\n",
      "Train Epoch: 5 [38400/50000 (77%)]\tLoss: 0.045030\n",
      "\n",
      "--------- Testing in global mode ---------\n",
      "size of test_loader dataset:  10000\n",
      "\n",
      "Test set: Avg. loss: 0.0003, Accuracy: 9032/10000 (90%)\n",
      "\n",
      "Train Epoch: 6 [0/50000 (0%)]\tLoss: 0.042637\n",
      "Train Epoch: 6 [12800/50000 (26%)]\tLoss: 0.038961\n",
      "Train Epoch: 6 [25600/50000 (51%)]\tLoss: 0.054680\n",
      "Train Epoch: 6 [38400/50000 (77%)]\tLoss: 0.033008\n",
      "\n",
      "--------- Testing in global mode ---------\n",
      "size of test_loader dataset:  10000\n",
      "\n",
      "Test set: Avg. loss: 0.0003, Accuracy: 9031/10000 (90%)\n",
      "\n",
      "Train Epoch: 7 [0/50000 (0%)]\tLoss: 0.041460\n",
      "Train Epoch: 7 [12800/50000 (26%)]\tLoss: 0.050253\n",
      "Train Epoch: 7 [25600/50000 (51%)]\tLoss: 0.051426\n",
      "Train Epoch: 7 [38400/50000 (77%)]\tLoss: 0.053019\n",
      "\n",
      "--------- Testing in global mode ---------\n",
      "size of test_loader dataset:  10000\n",
      "\n",
      "Test set: Avg. loss: 0.0003, Accuracy: 9026/10000 (90%)\n",
      "\n",
      "Train Epoch: 8 [0/50000 (0%)]\tLoss: 0.027368\n",
      "Train Epoch: 8 [12800/50000 (26%)]\tLoss: 0.031691\n",
      "Train Epoch: 8 [25600/50000 (51%)]\tLoss: 0.057592\n",
      "Train Epoch: 8 [38400/50000 (77%)]\tLoss: 0.027700\n",
      "\n",
      "--------- Testing in global mode ---------\n",
      "size of test_loader dataset:  10000\n",
      "\n",
      "Test set: Avg. loss: 0.0003, Accuracy: 9036/10000 (90%)\n",
      "\n",
      "Train Epoch: 9 [0/50000 (0%)]\tLoss: 0.035068\n",
      "Train Epoch: 9 [12800/50000 (26%)]\tLoss: 0.041315\n",
      "Train Epoch: 9 [25600/50000 (51%)]\tLoss: 0.044457\n",
      "Train Epoch: 9 [38400/50000 (77%)]\tLoss: 0.038739\n",
      "\n",
      "--------- Testing in global mode ---------\n",
      "size of test_loader dataset:  10000\n",
      "\n",
      "Test set: Avg. loss: 0.0003, Accuracy: 9032/10000 (90%)\n",
      "\n",
      "Train Epoch: 10 [0/50000 (0%)]\tLoss: 0.052934\n",
      "Train Epoch: 10 [12800/50000 (26%)]\tLoss: 0.042984\n",
      "Train Epoch: 10 [25600/50000 (51%)]\tLoss: 0.040697\n",
      "Train Epoch: 10 [38400/50000 (77%)]\tLoss: 0.038955\n",
      "\n",
      "--------- Testing in global mode ---------\n",
      "size of test_loader dataset:  10000\n",
      "\n",
      "Test set: Avg. loss: 0.0003, Accuracy: 9027/10000 (90%)\n",
      "\n",
      "Train Epoch: 11 [0/50000 (0%)]\tLoss: 0.034035\n",
      "Train Epoch: 11 [12800/50000 (26%)]\tLoss: 0.038787\n",
      "Train Epoch: 11 [25600/50000 (51%)]\tLoss: 0.018903\n",
      "Train Epoch: 11 [38400/50000 (77%)]\tLoss: 0.029173\n",
      "\n",
      "--------- Testing in global mode ---------\n",
      "size of test_loader dataset:  10000\n",
      "\n",
      "Test set: Avg. loss: 0.0003, Accuracy: 9027/10000 (90%)\n",
      "\n",
      "Train Epoch: 12 [0/50000 (0%)]\tLoss: 0.028632\n",
      "Train Epoch: 12 [12800/50000 (26%)]\tLoss: 0.038014\n",
      "Train Epoch: 12 [25600/50000 (51%)]\tLoss: 0.045334\n",
      "Train Epoch: 12 [38400/50000 (77%)]\tLoss: 0.037902\n",
      "\n",
      "--------- Testing in global mode ---------\n",
      "size of test_loader dataset:  10000\n",
      "\n",
      "Test set: Avg. loss: 0.0003, Accuracy: 9028/10000 (90%)\n",
      "\n",
      "Train Epoch: 13 [0/50000 (0%)]\tLoss: 0.024560\n",
      "Train Epoch: 13 [12800/50000 (26%)]\tLoss: 0.033791\n",
      "Train Epoch: 13 [25600/50000 (51%)]\tLoss: 0.022823\n",
      "Train Epoch: 13 [38400/50000 (77%)]\tLoss: 0.028708\n",
      "\n",
      "--------- Testing in global mode ---------\n",
      "size of test_loader dataset:  10000\n",
      "\n",
      "Test set: Avg. loss: 0.0003, Accuracy: 9028/10000 (90%)\n",
      "\n",
      "Train Epoch: 14 [0/50000 (0%)]\tLoss: 0.049624\n",
      "Train Epoch: 14 [12800/50000 (26%)]\tLoss: 0.029383\n",
      "Train Epoch: 14 [25600/50000 (51%)]\tLoss: 0.031141\n",
      "Train Epoch: 14 [38400/50000 (77%)]\tLoss: 0.032142\n",
      "\n",
      "--------- Testing in global mode ---------\n",
      "size of test_loader dataset:  10000\n",
      "\n",
      "Test set: Avg. loss: 0.0003, Accuracy: 9028/10000 (90%)\n",
      "\n",
      "Train Epoch: 15 [0/50000 (0%)]\tLoss: 0.042474\n",
      "Train Epoch: 15 [12800/50000 (26%)]\tLoss: 0.036536\n",
      "Train Epoch: 15 [25600/50000 (51%)]\tLoss: 0.030691\n",
      "Train Epoch: 15 [38400/50000 (77%)]\tLoss: 0.037974\n",
      "\n",
      "--------- Testing in global mode ---------\n",
      "size of test_loader dataset:  10000\n",
      "\n",
      "Test set: Avg. loss: 0.0003, Accuracy: 9032/10000 (90%)\n",
      "\n",
      "Train Epoch: 16 [0/50000 (0%)]\tLoss: 0.029278\n",
      "Train Epoch: 16 [12800/50000 (26%)]\tLoss: 0.017581\n",
      "Train Epoch: 16 [25600/50000 (51%)]\tLoss: 0.014313\n",
      "Train Epoch: 16 [38400/50000 (77%)]\tLoss: 0.028721\n",
      "\n",
      "--------- Testing in global mode ---------\n",
      "size of test_loader dataset:  10000\n",
      "\n",
      "Test set: Avg. loss: 0.0003, Accuracy: 9026/10000 (90%)\n",
      "\n",
      "Train Epoch: 17 [0/50000 (0%)]\tLoss: 0.045079\n",
      "Train Epoch: 17 [12800/50000 (26%)]\tLoss: 0.018341\n",
      "Train Epoch: 17 [25600/50000 (51%)]\tLoss: 0.016601\n",
      "Train Epoch: 17 [38400/50000 (77%)]\tLoss: 0.036918\n",
      "\n",
      "--------- Testing in global mode ---------\n",
      "size of test_loader dataset:  10000\n",
      "\n",
      "Test set: Avg. loss: 0.0003, Accuracy: 9030/10000 (90%)\n",
      "\n",
      "Train Epoch: 18 [0/50000 (0%)]\tLoss: 0.021934\n",
      "Train Epoch: 18 [12800/50000 (26%)]\tLoss: 0.023729\n",
      "Train Epoch: 18 [25600/50000 (51%)]\tLoss: 0.027788\n",
      "Train Epoch: 18 [38400/50000 (77%)]\tLoss: 0.024681\n",
      "\n",
      "--------- Testing in global mode ---------\n",
      "size of test_loader dataset:  10000\n",
      "\n",
      "Test set: Avg. loss: 0.0003, Accuracy: 9035/10000 (90%)\n",
      "\n",
      "Train Epoch: 19 [0/50000 (0%)]\tLoss: 0.026217\n",
      "Train Epoch: 19 [12800/50000 (26%)]\tLoss: 0.024923\n",
      "Train Epoch: 19 [25600/50000 (51%)]\tLoss: 0.020692\n",
      "Train Epoch: 19 [38400/50000 (77%)]\tLoss: 0.030952\n",
      "\n",
      "--------- Testing in global mode ---------\n",
      "size of test_loader dataset:  10000\n",
      "\n",
      "Test set: Avg. loss: 0.0003, Accuracy: 9026/10000 (90%)\n",
      "\n",
      "Train Epoch: 20 [0/50000 (0%)]\tLoss: 0.027375\n",
      "Train Epoch: 20 [12800/50000 (26%)]\tLoss: 0.024873\n",
      "Train Epoch: 20 [25600/50000 (51%)]\tLoss: 0.022178\n",
      "Train Epoch: 20 [38400/50000 (77%)]\tLoss: 0.024388\n",
      "\n",
      "--------- Testing in global mode ---------\n",
      "size of test_loader dataset:  10000\n",
      "\n",
      "Test set: Avg. loss: 0.0003, Accuracy: 9026/10000 (90%)\n",
      "\n",
      "Train Epoch: 21 [0/50000 (0%)]\tLoss: 0.016751\n",
      "Train Epoch: 21 [12800/50000 (26%)]\tLoss: 0.009613\n",
      "Train Epoch: 21 [25600/50000 (51%)]\tLoss: 0.042852\n",
      "Train Epoch: 21 [38400/50000 (77%)]\tLoss: 0.016484\n",
      "\n",
      "--------- Testing in global mode ---------\n",
      "size of test_loader dataset:  10000\n",
      "\n",
      "Test set: Avg. loss: 0.0003, Accuracy: 9024/10000 (90%)\n",
      "\n",
      "Train Epoch: 22 [0/50000 (0%)]\tLoss: 0.040455\n",
      "Train Epoch: 22 [12800/50000 (26%)]\tLoss: 0.023962\n",
      "Train Epoch: 22 [25600/50000 (51%)]\tLoss: 0.025231\n",
      "Train Epoch: 22 [38400/50000 (77%)]\tLoss: 0.015957\n",
      "\n",
      "--------- Testing in global mode ---------\n",
      "size of test_loader dataset:  10000\n",
      "\n",
      "Test set: Avg. loss: 0.0003, Accuracy: 9026/10000 (90%)\n",
      "\n",
      "Train Epoch: 23 [0/50000 (0%)]\tLoss: 0.029708\n",
      "Train Epoch: 23 [12800/50000 (26%)]\tLoss: 0.019373\n",
      "Train Epoch: 23 [25600/50000 (51%)]\tLoss: 0.030349\n",
      "Train Epoch: 23 [38400/50000 (77%)]\tLoss: 0.023012\n",
      "\n",
      "--------- Testing in global mode ---------\n",
      "size of test_loader dataset:  10000\n",
      "\n",
      "Test set: Avg. loss: 0.0003, Accuracy: 9025/10000 (90%)\n",
      "\n",
      "Train Epoch: 24 [0/50000 (0%)]\tLoss: 0.016480\n",
      "Train Epoch: 24 [12800/50000 (26%)]\tLoss: 0.027412\n",
      "Train Epoch: 24 [25600/50000 (51%)]\tLoss: 0.016082\n",
      "Train Epoch: 24 [38400/50000 (77%)]\tLoss: 0.021596\n",
      "\n",
      "--------- Testing in global mode ---------\n",
      "size of test_loader dataset:  10000\n",
      "\n",
      "Test set: Avg. loss: 0.0003, Accuracy: 9024/10000 (90%)\n",
      "\n",
      "Train Epoch: 25 [0/50000 (0%)]\tLoss: 0.019538\n",
      "Train Epoch: 25 [12800/50000 (26%)]\tLoss: 0.023037\n",
      "Train Epoch: 25 [25600/50000 (51%)]\tLoss: 0.021114\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 25 [38400/50000 (77%)]\tLoss: 0.021692\n",
      "\n",
      "--------- Testing in global mode ---------\n",
      "size of test_loader dataset:  10000\n",
      "\n",
      "Test set: Avg. loss: 0.0003, Accuracy: 9026/10000 (90%)\n",
      "\n",
      "Train Epoch: 26 [0/50000 (0%)]\tLoss: 0.021995\n",
      "Train Epoch: 26 [12800/50000 (26%)]\tLoss: 0.024158\n",
      "Train Epoch: 26 [25600/50000 (51%)]\tLoss: 0.014361\n",
      "Train Epoch: 26 [38400/50000 (77%)]\tLoss: 0.037262\n",
      "\n",
      "--------- Testing in global mode ---------\n",
      "size of test_loader dataset:  10000\n",
      "\n",
      "Test set: Avg. loss: 0.0003, Accuracy: 9024/10000 (90%)\n",
      "\n",
      "Train Epoch: 27 [0/50000 (0%)]\tLoss: 0.011086\n",
      "Train Epoch: 27 [12800/50000 (26%)]\tLoss: 0.022403\n",
      "Train Epoch: 27 [25600/50000 (51%)]\tLoss: 0.024830\n",
      "Train Epoch: 27 [38400/50000 (77%)]\tLoss: 0.017481\n",
      "\n",
      "--------- Testing in global mode ---------\n",
      "size of test_loader dataset:  10000\n",
      "\n",
      "Test set: Avg. loss: 0.0003, Accuracy: 9025/10000 (90%)\n",
      "\n",
      "Train Epoch: 28 [0/50000 (0%)]\tLoss: 0.024920\n",
      "Train Epoch: 28 [12800/50000 (26%)]\tLoss: 0.015651\n",
      "Train Epoch: 28 [25600/50000 (51%)]\tLoss: 0.015037\n",
      "Train Epoch: 28 [38400/50000 (77%)]\tLoss: 0.027473\n",
      "\n",
      "--------- Testing in global mode ---------\n",
      "size of test_loader dataset:  10000\n",
      "\n",
      "Test set: Avg. loss: 0.0003, Accuracy: 9020/10000 (90%)\n",
      "\n",
      "Train Epoch: 29 [0/50000 (0%)]\tLoss: 0.017876\n",
      "Train Epoch: 29 [12800/50000 (26%)]\tLoss: 0.020739\n",
      "Train Epoch: 29 [25600/50000 (51%)]\tLoss: 0.019402\n",
      "Train Epoch: 29 [38400/50000 (77%)]\tLoss: 0.010182\n",
      "\n",
      "--------- Testing in global mode ---------\n",
      "size of test_loader dataset:  10000\n",
      "\n",
      "Test set: Avg. loss: 0.0003, Accuracy: 9027/10000 (90%)\n",
      "\n",
      "Train Epoch: 30 [0/50000 (0%)]\tLoss: 0.021572\n",
      "Train Epoch: 30 [12800/50000 (26%)]\tLoss: 0.009700\n",
      "Train Epoch: 30 [25600/50000 (51%)]\tLoss: 0.009873\n",
      "Train Epoch: 30 [38400/50000 (77%)]\tLoss: 0.011762\n",
      "\n",
      "--------- Testing in global mode ---------\n",
      "size of test_loader dataset:  10000\n",
      "\n",
      "Test set: Avg. loss: 0.0003, Accuracy: 9022/10000 (90%)\n",
      "\n",
      "Train Epoch: 1 [0/50000 (0%)]\tLoss: 0.099543\n",
      "Train Epoch: 1 [12800/50000 (26%)]\tLoss: 0.077175\n",
      "Train Epoch: 1 [25600/50000 (51%)]\tLoss: 0.075169\n",
      "Train Epoch: 1 [38400/50000 (77%)]\tLoss: 0.081884\n",
      "\n",
      "--------- Testing in global mode ---------\n",
      "size of test_loader dataset:  10000\n",
      "\n",
      "Test set: Avg. loss: 0.0003, Accuracy: 9051/10000 (91%)\n",
      "\n",
      "Train Epoch: 2 [0/50000 (0%)]\tLoss: 0.071398\n",
      "Train Epoch: 2 [12800/50000 (26%)]\tLoss: 0.071126\n",
      "Train Epoch: 2 [25600/50000 (51%)]\tLoss: 0.065763\n",
      "Train Epoch: 2 [38400/50000 (77%)]\tLoss: 0.045883\n",
      "\n",
      "--------- Testing in global mode ---------\n",
      "size of test_loader dataset:  10000\n",
      "\n",
      "Test set: Avg. loss: 0.0003, Accuracy: 9046/10000 (90%)\n",
      "\n",
      "Train Epoch: 3 [0/50000 (0%)]\tLoss: 0.070329\n",
      "Train Epoch: 3 [12800/50000 (26%)]\tLoss: 0.053054\n",
      "Train Epoch: 3 [25600/50000 (51%)]\tLoss: 0.039511\n",
      "Train Epoch: 3 [38400/50000 (77%)]\tLoss: 0.046425\n",
      "\n",
      "--------- Testing in global mode ---------\n",
      "size of test_loader dataset:  10000\n",
      "\n",
      "Test set: Avg. loss: 0.0003, Accuracy: 9046/10000 (90%)\n",
      "\n",
      "Train Epoch: 4 [0/50000 (0%)]\tLoss: 0.066092\n",
      "Train Epoch: 4 [12800/50000 (26%)]\tLoss: 0.039984\n",
      "Train Epoch: 4 [25600/50000 (51%)]\tLoss: 0.056482\n",
      "Train Epoch: 4 [38400/50000 (77%)]\tLoss: 0.059905\n",
      "\n",
      "--------- Testing in global mode ---------\n",
      "size of test_loader dataset:  10000\n",
      "\n",
      "Test set: Avg. loss: 0.0003, Accuracy: 9042/10000 (90%)\n",
      "\n",
      "Train Epoch: 5 [0/50000 (0%)]\tLoss: 0.040956\n",
      "Train Epoch: 5 [12800/50000 (26%)]\tLoss: 0.041585\n",
      "Train Epoch: 5 [25600/50000 (51%)]\tLoss: 0.062064\n",
      "Train Epoch: 5 [38400/50000 (77%)]\tLoss: 0.045941\n",
      "\n",
      "--------- Testing in global mode ---------\n",
      "size of test_loader dataset:  10000\n",
      "\n",
      "Test set: Avg. loss: 0.0003, Accuracy: 9037/10000 (90%)\n",
      "\n",
      "Train Epoch: 6 [0/50000 (0%)]\tLoss: 0.055750\n",
      "Train Epoch: 6 [12800/50000 (26%)]\tLoss: 0.032442\n",
      "Train Epoch: 6 [25600/50000 (51%)]\tLoss: 0.043586\n",
      "Train Epoch: 6 [38400/50000 (77%)]\tLoss: 0.042376\n",
      "\n",
      "--------- Testing in global mode ---------\n",
      "size of test_loader dataset:  10000\n",
      "\n",
      "Test set: Avg. loss: 0.0003, Accuracy: 9040/10000 (90%)\n",
      "\n",
      "Train Epoch: 7 [0/50000 (0%)]\tLoss: 0.032447\n",
      "Train Epoch: 7 [12800/50000 (26%)]\tLoss: 0.052955\n",
      "Train Epoch: 7 [25600/50000 (51%)]\tLoss: 0.034173\n",
      "Train Epoch: 7 [38400/50000 (77%)]\tLoss: 0.042301\n",
      "\n",
      "--------- Testing in global mode ---------\n",
      "size of test_loader dataset:  10000\n",
      "\n",
      "Test set: Avg. loss: 0.0003, Accuracy: 9037/10000 (90%)\n",
      "\n",
      "Train Epoch: 8 [0/50000 (0%)]\tLoss: 0.033425\n",
      "Train Epoch: 8 [12800/50000 (26%)]\tLoss: 0.050169\n",
      "Train Epoch: 8 [25600/50000 (51%)]\tLoss: 0.045604\n",
      "Train Epoch: 8 [38400/50000 (77%)]\tLoss: 0.041821\n",
      "\n",
      "--------- Testing in global mode ---------\n",
      "size of test_loader dataset:  10000\n",
      "\n",
      "Test set: Avg. loss: 0.0003, Accuracy: 9032/10000 (90%)\n",
      "\n",
      "Train Epoch: 9 [0/50000 (0%)]\tLoss: 0.056688\n",
      "Train Epoch: 9 [12800/50000 (26%)]\tLoss: 0.032782\n",
      "Train Epoch: 9 [25600/50000 (51%)]\tLoss: 0.032062\n",
      "Train Epoch: 9 [38400/50000 (77%)]\tLoss: 0.030242\n",
      "\n",
      "--------- Testing in global mode ---------\n",
      "size of test_loader dataset:  10000\n",
      "\n",
      "Test set: Avg. loss: 0.0003, Accuracy: 9037/10000 (90%)\n",
      "\n",
      "Train Epoch: 10 [0/50000 (0%)]\tLoss: 0.044234\n",
      "Train Epoch: 10 [12800/50000 (26%)]\tLoss: 0.044055\n",
      "Train Epoch: 10 [25600/50000 (51%)]\tLoss: 0.043108\n",
      "Train Epoch: 10 [38400/50000 (77%)]\tLoss: 0.049696\n",
      "\n",
      "--------- Testing in global mode ---------\n",
      "size of test_loader dataset:  10000\n",
      "\n",
      "Test set: Avg. loss: 0.0003, Accuracy: 9036/10000 (90%)\n",
      "\n",
      "Train Epoch: 11 [0/50000 (0%)]\tLoss: 0.035775\n",
      "Train Epoch: 11 [12800/50000 (26%)]\tLoss: 0.038012\n",
      "Train Epoch: 11 [25600/50000 (51%)]\tLoss: 0.035603\n",
      "Train Epoch: 11 [38400/50000 (77%)]\tLoss: 0.042513\n",
      "\n",
      "--------- Testing in global mode ---------\n",
      "size of test_loader dataset:  10000\n",
      "\n",
      "Test set: Avg. loss: 0.0003, Accuracy: 9035/10000 (90%)\n",
      "\n",
      "Train Epoch: 12 [0/50000 (0%)]\tLoss: 0.035021\n",
      "Train Epoch: 12 [12800/50000 (26%)]\tLoss: 0.032292\n",
      "Train Epoch: 12 [25600/50000 (51%)]\tLoss: 0.018737\n",
      "Train Epoch: 12 [38400/50000 (77%)]\tLoss: 0.043416\n",
      "\n",
      "--------- Testing in global mode ---------\n",
      "size of test_loader dataset:  10000\n",
      "\n",
      "Test set: Avg. loss: 0.0003, Accuracy: 9041/10000 (90%)\n",
      "\n",
      "Train Epoch: 13 [0/50000 (0%)]\tLoss: 0.043412\n",
      "Train Epoch: 13 [12800/50000 (26%)]\tLoss: 0.032004\n",
      "Train Epoch: 13 [25600/50000 (51%)]\tLoss: 0.038078\n",
      "Train Epoch: 13 [38400/50000 (77%)]\tLoss: 0.040133\n",
      "\n",
      "--------- Testing in global mode ---------\n",
      "size of test_loader dataset:  10000\n",
      "\n",
      "Test set: Avg. loss: 0.0003, Accuracy: 9037/10000 (90%)\n",
      "\n",
      "Train Epoch: 14 [0/50000 (0%)]\tLoss: 0.035781\n",
      "Train Epoch: 14 [12800/50000 (26%)]\tLoss: 0.022686\n",
      "Train Epoch: 14 [25600/50000 (51%)]\tLoss: 0.025956\n",
      "Train Epoch: 14 [38400/50000 (77%)]\tLoss: 0.052127\n",
      "\n",
      "--------- Testing in global mode ---------\n",
      "size of test_loader dataset:  10000\n",
      "\n",
      "Test set: Avg. loss: 0.0003, Accuracy: 9039/10000 (90%)\n",
      "\n",
      "Train Epoch: 15 [0/50000 (0%)]\tLoss: 0.025346\n",
      "Train Epoch: 15 [12800/50000 (26%)]\tLoss: 0.020094\n",
      "Train Epoch: 15 [25600/50000 (51%)]\tLoss: 0.031621\n",
      "Train Epoch: 15 [38400/50000 (77%)]\tLoss: 0.028268\n",
      "\n",
      "--------- Testing in global mode ---------\n",
      "size of test_loader dataset:  10000\n",
      "\n",
      "Test set: Avg. loss: 0.0003, Accuracy: 9038/10000 (90%)\n",
      "\n",
      "Train Epoch: 16 [0/50000 (0%)]\tLoss: 0.041763\n",
      "Train Epoch: 16 [12800/50000 (26%)]\tLoss: 0.041937\n",
      "Train Epoch: 16 [25600/50000 (51%)]\tLoss: 0.026226\n",
      "Train Epoch: 16 [38400/50000 (77%)]\tLoss: 0.040836\n",
      "\n",
      "--------- Testing in global mode ---------\n",
      "size of test_loader dataset:  10000\n",
      "\n",
      "Test set: Avg. loss: 0.0003, Accuracy: 9036/10000 (90%)\n",
      "\n",
      "Train Epoch: 17 [0/50000 (0%)]\tLoss: 0.036102\n",
      "Train Epoch: 17 [12800/50000 (26%)]\tLoss: 0.044344\n",
      "Train Epoch: 17 [25600/50000 (51%)]\tLoss: 0.029349\n",
      "Train Epoch: 17 [38400/50000 (77%)]\tLoss: 0.033999\n",
      "\n",
      "--------- Testing in global mode ---------\n",
      "size of test_loader dataset:  10000\n",
      "\n",
      "Test set: Avg. loss: 0.0003, Accuracy: 9043/10000 (90%)\n",
      "\n",
      "Train Epoch: 18 [0/50000 (0%)]\tLoss: 0.024463\n",
      "Train Epoch: 18 [12800/50000 (26%)]\tLoss: 0.018379\n",
      "Train Epoch: 18 [25600/50000 (51%)]\tLoss: 0.032299\n",
      "Train Epoch: 18 [38400/50000 (77%)]\tLoss: 0.019897\n",
      "\n",
      "--------- Testing in global mode ---------\n",
      "size of test_loader dataset:  10000\n",
      "\n",
      "Test set: Avg. loss: 0.0003, Accuracy: 9035/10000 (90%)\n",
      "\n",
      "Train Epoch: 19 [0/50000 (0%)]\tLoss: 0.020105\n",
      "Train Epoch: 19 [12800/50000 (26%)]\tLoss: 0.024358\n",
      "Train Epoch: 19 [25600/50000 (51%)]\tLoss: 0.021997\n",
      "Train Epoch: 19 [38400/50000 (77%)]\tLoss: 0.017812\n",
      "\n",
      "--------- Testing in global mode ---------\n",
      "size of test_loader dataset:  10000\n",
      "\n",
      "Test set: Avg. loss: 0.0003, Accuracy: 9036/10000 (90%)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 20 [0/50000 (0%)]\tLoss: 0.029504\n",
      "Train Epoch: 20 [12800/50000 (26%)]\tLoss: 0.027408\n",
      "Train Epoch: 20 [25600/50000 (51%)]\tLoss: 0.020884\n",
      "Train Epoch: 20 [38400/50000 (77%)]\tLoss: 0.038957\n",
      "\n",
      "--------- Testing in global mode ---------\n",
      "size of test_loader dataset:  10000\n",
      "\n",
      "Test set: Avg. loss: 0.0003, Accuracy: 9041/10000 (90%)\n",
      "\n",
      "Train Epoch: 21 [0/50000 (0%)]\tLoss: 0.029821\n",
      "Train Epoch: 21 [12800/50000 (26%)]\tLoss: 0.026474\n",
      "Train Epoch: 21 [25600/50000 (51%)]\tLoss: 0.019179\n",
      "Train Epoch: 21 [38400/50000 (77%)]\tLoss: 0.026278\n",
      "\n",
      "--------- Testing in global mode ---------\n",
      "size of test_loader dataset:  10000\n",
      "\n",
      "Test set: Avg. loss: 0.0003, Accuracy: 9041/10000 (90%)\n",
      "\n",
      "Train Epoch: 22 [0/50000 (0%)]\tLoss: 0.014093\n",
      "Train Epoch: 22 [12800/50000 (26%)]\tLoss: 0.022799\n",
      "Train Epoch: 22 [25600/50000 (51%)]\tLoss: 0.020141\n",
      "Train Epoch: 22 [38400/50000 (77%)]\tLoss: 0.024382\n",
      "\n",
      "--------- Testing in global mode ---------\n",
      "size of test_loader dataset:  10000\n",
      "\n",
      "Test set: Avg. loss: 0.0003, Accuracy: 9039/10000 (90%)\n",
      "\n",
      "Train Epoch: 23 [0/50000 (0%)]\tLoss: 0.020831\n",
      "Train Epoch: 23 [12800/50000 (26%)]\tLoss: 0.014318\n",
      "Train Epoch: 23 [25600/50000 (51%)]\tLoss: 0.015911\n",
      "Train Epoch: 23 [38400/50000 (77%)]\tLoss: 0.015533\n",
      "\n",
      "--------- Testing in global mode ---------\n",
      "size of test_loader dataset:  10000\n",
      "\n",
      "Test set: Avg. loss: 0.0003, Accuracy: 9039/10000 (90%)\n",
      "\n",
      "Train Epoch: 24 [0/50000 (0%)]\tLoss: 0.020140\n",
      "Train Epoch: 24 [12800/50000 (26%)]\tLoss: 0.013470\n",
      "Train Epoch: 24 [25600/50000 (51%)]\tLoss: 0.023286\n",
      "Train Epoch: 24 [38400/50000 (77%)]\tLoss: 0.022283\n",
      "\n",
      "--------- Testing in global mode ---------\n",
      "size of test_loader dataset:  10000\n",
      "\n",
      "Test set: Avg. loss: 0.0003, Accuracy: 9040/10000 (90%)\n",
      "\n",
      "Train Epoch: 25 [0/50000 (0%)]\tLoss: 0.019270\n",
      "Train Epoch: 25 [12800/50000 (26%)]\tLoss: 0.017107\n",
      "Train Epoch: 25 [25600/50000 (51%)]\tLoss: 0.027440\n",
      "Train Epoch: 25 [38400/50000 (77%)]\tLoss: 0.028404\n",
      "\n",
      "--------- Testing in global mode ---------\n",
      "size of test_loader dataset:  10000\n",
      "\n",
      "Test set: Avg. loss: 0.0003, Accuracy: 9041/10000 (90%)\n",
      "\n",
      "Train Epoch: 26 [0/50000 (0%)]\tLoss: 0.023928\n",
      "Train Epoch: 26 [12800/50000 (26%)]\tLoss: 0.028706\n",
      "Train Epoch: 26 [25600/50000 (51%)]\tLoss: 0.017168\n",
      "Train Epoch: 26 [38400/50000 (77%)]\tLoss: 0.029579\n",
      "\n",
      "--------- Testing in global mode ---------\n",
      "size of test_loader dataset:  10000\n",
      "\n",
      "Test set: Avg. loss: 0.0003, Accuracy: 9038/10000 (90%)\n",
      "\n",
      "Train Epoch: 27 [0/50000 (0%)]\tLoss: 0.017689\n",
      "Train Epoch: 27 [12800/50000 (26%)]\tLoss: 0.020042\n",
      "Train Epoch: 27 [25600/50000 (51%)]\tLoss: 0.020521\n",
      "Train Epoch: 27 [38400/50000 (77%)]\tLoss: 0.026791\n",
      "\n",
      "--------- Testing in global mode ---------\n",
      "size of test_loader dataset:  10000\n",
      "\n",
      "Test set: Avg. loss: 0.0003, Accuracy: 9040/10000 (90%)\n",
      "\n",
      "Train Epoch: 28 [0/50000 (0%)]\tLoss: 0.028522\n",
      "Train Epoch: 28 [12800/50000 (26%)]\tLoss: 0.015453\n",
      "Train Epoch: 28 [25600/50000 (51%)]\tLoss: 0.025645\n",
      "Train Epoch: 28 [38400/50000 (77%)]\tLoss: 0.019263\n",
      "\n",
      "--------- Testing in global mode ---------\n",
      "size of test_loader dataset:  10000\n",
      "\n",
      "Test set: Avg. loss: 0.0003, Accuracy: 9039/10000 (90%)\n",
      "\n",
      "Train Epoch: 29 [0/50000 (0%)]\tLoss: 0.017217\n",
      "Train Epoch: 29 [12800/50000 (26%)]\tLoss: 0.012955\n",
      "Train Epoch: 29 [25600/50000 (51%)]\tLoss: 0.032514\n",
      "Train Epoch: 29 [38400/50000 (77%)]\tLoss: 0.025837\n",
      "\n",
      "--------- Testing in global mode ---------\n",
      "size of test_loader dataset:  10000\n",
      "\n",
      "Test set: Avg. loss: 0.0003, Accuracy: 9038/10000 (90%)\n",
      "\n",
      "Train Epoch: 30 [0/50000 (0%)]\tLoss: 0.019868\n",
      "Train Epoch: 30 [12800/50000 (26%)]\tLoss: 0.016498\n",
      "Train Epoch: 30 [25600/50000 (51%)]\tLoss: 0.033259\n",
      "Train Epoch: 30 [38400/50000 (77%)]\tLoss: 0.015216\n",
      "\n",
      "--------- Testing in global mode ---------\n",
      "size of test_loader dataset:  10000\n",
      "\n",
      "Test set: Avg. loss: 0.0003, Accuracy: 9038/10000 (90%)\n",
      "\n",
      "===============================================\n",
      "===============================================\n",
      "before Ensemble,acc: [90.22, 90.38] loss [0.00033342576622962954, 0.00032726884484291077]\n",
      "===============================================\n",
      "===============================================\n",
      "check zero rate for model 0\n",
      "odict_keys(['features.0.weight', 'features.3.weight', 'features.6.weight', 'features.8.weight', 'features.11.weight', 'features.13.weight', 'features.16.weight', 'features.18.weight', 'classifier.weight'])\n",
      "the ratio of zero for features.0.weight\n",
      "tensor(0.2998, device='cuda:0')\n",
      "the ratio of zero for features.3.weight\n",
      "tensor(0.3000, device='cuda:0')\n",
      "the ratio of zero for features.6.weight\n",
      "tensor(0.3000, device='cuda:0')\n",
      "the ratio of zero for features.8.weight\n",
      "tensor(0.3000, device='cuda:0')\n",
      "the ratio of zero for features.11.weight\n",
      "tensor(0.3000, device='cuda:0')\n",
      "the ratio of zero for features.13.weight\n",
      "tensor(0.3000, device='cuda:0')\n",
      "the ratio of zero for features.16.weight\n",
      "tensor(0.3000, device='cuda:0')\n",
      "the ratio of zero for features.18.weight\n",
      "tensor(0.3000, device='cuda:0')\n",
      "the ratio of zero for classifier.weight\n",
      "tensor(0.3000, device='cuda:0')\n",
      "check zero rate for model 1\n",
      "odict_keys(['features.0.weight', 'features.3.weight', 'features.6.weight', 'features.8.weight', 'features.11.weight', 'features.13.weight', 'features.16.weight', 'features.18.weight', 'classifier.weight'])\n",
      "the ratio of zero for features.0.weight\n",
      "tensor(0.2998, device='cuda:0')\n",
      "the ratio of zero for features.3.weight\n",
      "tensor(0.3000, device='cuda:0')\n",
      "the ratio of zero for features.6.weight\n",
      "tensor(0.3000, device='cuda:0')\n",
      "the ratio of zero for features.8.weight\n",
      "tensor(0.3000, device='cuda:0')\n",
      "the ratio of zero for features.11.weight\n",
      "tensor(0.3000, device='cuda:0')\n",
      "the ratio of zero for features.13.weight\n",
      "tensor(0.3000, device='cuda:0')\n",
      "the ratio of zero for features.16.weight\n",
      "tensor(0.3000, device='cuda:0')\n",
      "the ratio of zero for features.18.weight\n",
      "tensor(0.3000, device='cuda:0')\n",
      "the ratio of zero for classifier.weight\n",
      "tensor(0.3000, device='cuda:0')\n",
      "layer features.0.weight has #params  1728\n",
      "layer features.3.weight has #params  73728\n",
      "layer features.6.weight has #params  294912\n",
      "layer features.8.weight has #params  589824\n",
      "layer features.11.weight has #params  1179648\n",
      "layer features.13.weight has #params  2359296\n",
      "layer features.16.weight has #params  2359296\n",
      "layer features.18.weight has #params  2359296\n",
      "layer classifier.weight has #params  5120\n",
      "Activation Timer start\n",
      "Activation Timer ends\n",
      "------- Geometric Ensembling -------\n",
      "Timer start\n",
      "Previous layer shape is  None\n",
      "Processing the coordinates to form ground_metric\n",
      "dont leave off the squaring of the ground metric\n",
      "returns a uniform measure of cardinality:  64\n",
      "returns a uniform measure of cardinality:  64\n",
      "the transport map is  tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')\n",
      "Ratio of trace to the matrix sum:  tensor(0.0312, device='cuda:0')\n",
      "Here, trace is 1.9999871253967285 and matrix sum is 63.99958801269531 \n",
      "this is past correction for weight mode\n",
      "Shape of aligned wt is  torch.Size([64, 3, 9])\n",
      "Shape of fc_layer0_weight_data is  torch.Size([64, 3, 9])\n",
      "Previous layer shape is  torch.Size([64, 3, 3, 3])\n",
      "shape of layer: model 0 torch.Size([128, 64, 9])\n",
      "shape of layer: model 1 torch.Size([128, 64, 9])\n",
      "shape of previous transport map torch.Size([64, 64])\n",
      "Processing the coordinates to form ground_metric\n",
      "dont leave off the squaring of the ground metric\n",
      "returns a uniform measure of cardinality:  128\n",
      "returns a uniform measure of cardinality:  128\n",
      "the transport map is  tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        ...,\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0078],\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],\n",
      "       device='cuda:0')\n",
      "Ratio of trace to the matrix sum:  tensor(0., device='cuda:0')\n",
      "Here, trace is 0.0 and matrix sum is 127.99836730957031 \n",
      "this is past correction for weight mode\n",
      "Shape of aligned wt is  torch.Size([128, 64, 9])\n",
      "Shape of fc_layer0_weight_data is  torch.Size([128, 64, 9])\n",
      "Previous layer shape is  torch.Size([128, 64, 3, 3])\n",
      "shape of layer: model 0 torch.Size([256, 128, 9])\n",
      "shape of layer: model 1 torch.Size([256, 128, 9])\n",
      "shape of previous transport map torch.Size([128, 128])\n",
      "Processing the coordinates to form ground_metric\n",
      "dont leave off the squaring of the ground metric\n",
      "returns a uniform measure of cardinality:  256\n",
      "returns a uniform measure of cardinality:  256\n",
      "the transport map is  tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')\n",
      "Ratio of trace to the matrix sum:  tensor(0., device='cuda:0')\n",
      "Here, trace is 0.0 and matrix sum is 255.99343872070312 \n",
      "this is past correction for weight mode\n",
      "Shape of aligned wt is  torch.Size([256, 128, 9])\n",
      "Shape of fc_layer0_weight_data is  torch.Size([256, 128, 9])\n",
      "Previous layer shape is  torch.Size([256, 128, 3, 3])\n",
      "shape of layer: model 0 torch.Size([256, 256, 9])\n",
      "shape of layer: model 1 torch.Size([256, 256, 9])\n",
      "shape of previous transport map torch.Size([256, 256])\n",
      "Processing the coordinates to form ground_metric\n",
      "dont leave off the squaring of the ground metric\n",
      "returns a uniform measure of cardinality:  256\n",
      "returns a uniform measure of cardinality:  256\n",
      "the transport map is  tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        ...,\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.0039, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],\n",
      "       device='cuda:0')\n",
      "Ratio of trace to the matrix sum:  tensor(0.0078, device='cuda:0')\n",
      "Here, trace is 1.9999487400054932 and matrix sum is 255.99343872070312 \n",
      "this is past correction for weight mode\n",
      "Shape of aligned wt is  torch.Size([256, 256, 9])\n",
      "Shape of fc_layer0_weight_data is  torch.Size([256, 256, 9])\n",
      "Previous layer shape is  torch.Size([256, 256, 3, 3])\n",
      "shape of layer: model 0 torch.Size([512, 256, 9])\n",
      "shape of layer: model 1 torch.Size([512, 256, 9])\n",
      "shape of previous transport map torch.Size([256, 256])\n",
      "Processing the coordinates to form ground_metric\n",
      "dont leave off the squaring of the ground metric\n",
      "returns a uniform measure of cardinality:  512\n",
      "returns a uniform measure of cardinality:  512\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the transport map is  tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')\n",
      "Ratio of trace to the matrix sum:  tensor(0.0020, device='cuda:0')\n",
      "Here, trace is 0.9999488592147827 and matrix sum is 511.97381591796875 \n",
      "this is past correction for weight mode\n",
      "Shape of aligned wt is  torch.Size([512, 256, 9])\n",
      "Shape of fc_layer0_weight_data is  torch.Size([512, 256, 9])\n",
      "Previous layer shape is  torch.Size([512, 256, 3, 3])\n",
      "shape of layer: model 0 torch.Size([512, 512, 9])\n",
      "shape of layer: model 1 torch.Size([512, 512, 9])\n",
      "shape of previous transport map torch.Size([512, 512])\n",
      "Processing the coordinates to form ground_metric\n",
      "dont leave off the squaring of the ground metric\n",
      "returns a uniform measure of cardinality:  512\n",
      "returns a uniform measure of cardinality:  512\n",
      "the transport map is  tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')\n",
      "Ratio of trace to the matrix sum:  tensor(0.0020, device='cuda:0')\n",
      "Here, trace is 0.9999488592147827 and matrix sum is 511.97381591796875 \n",
      "this is past correction for weight mode\n",
      "Shape of aligned wt is  torch.Size([512, 512, 9])\n",
      "Shape of fc_layer0_weight_data is  torch.Size([512, 512, 9])\n",
      "Previous layer shape is  torch.Size([512, 512, 3, 3])\n",
      "shape of layer: model 0 torch.Size([512, 512, 9])\n",
      "shape of layer: model 1 torch.Size([512, 512, 9])\n",
      "shape of previous transport map torch.Size([512, 512])\n",
      "Processing the coordinates to form ground_metric\n",
      "dont leave off the squaring of the ground metric\n",
      "returns a uniform measure of cardinality:  512\n",
      "returns a uniform measure of cardinality:  512\n",
      "the transport map is  tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')\n",
      "Ratio of trace to the matrix sum:  tensor(0.0020, device='cuda:0')\n",
      "Here, trace is 0.9999488592147827 and matrix sum is 511.97381591796875 \n",
      "this is past correction for weight mode\n",
      "Shape of aligned wt is  torch.Size([512, 512, 9])\n",
      "Shape of fc_layer0_weight_data is  torch.Size([512, 512, 9])\n",
      "Previous layer shape is  torch.Size([512, 512, 3, 3])\n",
      "shape of layer: model 0 torch.Size([512, 512, 9])\n",
      "shape of layer: model 1 torch.Size([512, 512, 9])\n",
      "shape of previous transport map torch.Size([512, 512])\n",
      "Processing the coordinates to form ground_metric\n",
      "dont leave off the squaring of the ground metric\n",
      "returns a uniform measure of cardinality:  512\n",
      "returns a uniform measure of cardinality:  512\n",
      "the transport map is  tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        [0.0020, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        ...,\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],\n",
      "       device='cuda:0')\n",
      "Ratio of trace to the matrix sum:  tensor(0., device='cuda:0')\n",
      "Here, trace is 0.0 and matrix sum is 511.97381591796875 \n",
      "this is past correction for weight mode\n",
      "Shape of aligned wt is  torch.Size([512, 512, 9])\n",
      "Shape of fc_layer0_weight_data is  torch.Size([512, 512, 9])\n",
      "Previous layer shape is  torch.Size([512, 512, 3, 3])\n",
      "shape of layer: model 0 torch.Size([10, 512])\n",
      "shape of layer: model 1 torch.Size([10, 512])\n",
      "shape of previous transport map torch.Size([512, 512])\n",
      "Processing the coordinates to form ground_metric\n",
      "dont leave off the squaring of the ground metric\n",
      "ground metric is  tensor([[1.4630, 2.5301, 2.8444, 2.9091, 2.7701, 2.9031, 2.7344, 2.6780, 2.6169,\n",
      "         2.6228],\n",
      "        [2.5094, 1.3518, 2.7284, 2.6266, 2.7859, 2.5665, 2.4574, 2.5658, 2.3498,\n",
      "         2.4390],\n",
      "        [2.8073, 2.6804, 1.3803, 3.0058, 2.9756, 2.8895, 2.7735, 2.8903, 2.7965,\n",
      "         2.8163],\n",
      "        [2.8549, 2.6999, 2.9600, 1.5582, 2.8875, 3.0019, 2.7924, 2.8544, 2.8565,\n",
      "         2.6940],\n",
      "        [2.7895, 2.5701, 2.9017, 2.9262, 1.5653, 2.7862, 2.8077, 2.7044, 2.7425,\n",
      "         2.7479],\n",
      "        [2.9789, 2.6254, 3.0068, 2.8525, 2.7752, 1.5628, 2.8046, 2.7226, 2.8426,\n",
      "         2.7213],\n",
      "        [2.7146, 2.4651, 2.7303, 2.9604, 2.6933, 2.8911, 1.4452, 2.7521, 2.5009,\n",
      "         2.5671],\n",
      "        [2.7709, 2.5918, 2.7923, 2.8355, 2.7479, 2.7451, 2.7252, 1.4149, 2.5736,\n",
      "         2.5322],\n",
      "        [2.6278, 2.4174, 2.8125, 2.7525, 2.7207, 2.8666, 2.5622, 2.6731, 1.2794,\n",
      "         2.4183],\n",
      "        [2.6263, 2.2893, 2.8288, 2.7820, 2.6943, 2.7480, 2.6012, 2.5360, 2.5486,\n",
      "         1.3207]], device='cuda:0', grad_fn=<PowBackward0>)\n",
      "returns a uniform measure of cardinality:  10\n",
      "returns a uniform measure of cardinality:  10\n",
      "the transport map is  tensor([[0.1000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0000, 0.1000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0000, 0.0000, 0.1000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.1000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.1000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.1000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.1000, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.1000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.1000,\n",
      "         0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.1000]], device='cuda:0')\n",
      "Ratio of trace to the matrix sum:  tensor(1., device='cuda:0')\n",
      "Here, trace is 9.999990463256836 and matrix sum is 9.999990463256836 \n",
      "this is past correction for weight mode\n",
      "Shape of aligned wt is  torch.Size([10, 512])\n",
      "Shape of fc_layer0_weight_data is  torch.Size([10, 512])\n",
      "using independent method\n",
      "in _make_layers [Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), AvgPool2d(kernel_size=1, stride=1, padding=0)]\n",
      "Relu Inplace is  False\n",
      "model parameters are \n",
      " [torch.Size([64, 3, 3, 3]), torch.Size([128, 64, 3, 3]), torch.Size([256, 128, 3, 3]), torch.Size([256, 256, 3, 3]), torch.Size([512, 256, 3, 3]), torch.Size([512, 512, 3, 3]), torch.Size([512, 512, 3, 3]), torch.Size([512, 512, 3, 3]), torch.Size([10, 512])]\n",
      "\n",
      "--------- Testing in global mode ---------\n",
      "size of test_loader dataset:  10000\n",
      "\n",
      "Test set: Avg. loss: 0.0023, Accuracy: 1000/10000 (10%)\n",
      "\n",
      "len of model parameters and avg aligned layers is  9 9\n",
      "len of model_state_dict is  9\n",
      "len of param_list is  9\n",
      "\n",
      "--------- Testing in global mode ---------\n",
      "size of test_loader dataset:  10000\n",
      "\n",
      "Test set: Avg. loss: 0.0010, Accuracy: 8556/10000 (86%)\n",
      "\n",
      "===============================================\n",
      "===============================================\n",
      "After Ensemble,acc: 85.56 loss 0.001046433126926422\n",
      "===============================================\n",
      "===============================================\n",
      "Timer ends\n",
      "Time taken for geometric ensembling is 2.9432703009806573 seconds\n",
      "------- Prediction based ensembling -------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Avg. loss: 0.0003, Accuracy: 9117/10000 (91%)\n",
      "\n",
      "------- Naive ensembling of weights -------\n",
      "[torch.Size([64, 3, 3, 3]), torch.Size([64, 3, 3, 3])]\n",
      "torch.Size([64, 3, 3, 3])\n",
      "[torch.Size([128, 64, 3, 3]), torch.Size([128, 64, 3, 3])]\n",
      "torch.Size([128, 64, 3, 3])\n",
      "[torch.Size([256, 128, 3, 3]), torch.Size([256, 128, 3, 3])]\n",
      "torch.Size([256, 128, 3, 3])\n",
      "[torch.Size([256, 256, 3, 3]), torch.Size([256, 256, 3, 3])]\n",
      "torch.Size([256, 256, 3, 3])\n",
      "[torch.Size([512, 256, 3, 3]), torch.Size([512, 256, 3, 3])]\n",
      "torch.Size([512, 256, 3, 3])\n",
      "[torch.Size([512, 512, 3, 3]), torch.Size([512, 512, 3, 3])]\n",
      "torch.Size([512, 512, 3, 3])\n",
      "[torch.Size([512, 512, 3, 3]), torch.Size([512, 512, 3, 3])]\n",
      "torch.Size([512, 512, 3, 3])\n",
      "[torch.Size([512, 512, 3, 3]), torch.Size([512, 512, 3, 3])]\n",
      "torch.Size([512, 512, 3, 3])\n",
      "[torch.Size([10, 512]), torch.Size([10, 512])]\n",
      "torch.Size([10, 512])\n",
      "in _make_layers [Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), AvgPool2d(kernel_size=1, stride=1, padding=0)]\n",
      "Relu Inplace is  False\n",
      "model parameters are \n",
      " [torch.Size([64, 3, 3, 3]), torch.Size([128, 64, 3, 3]), torch.Size([256, 128, 3, 3]), torch.Size([256, 256, 3, 3]), torch.Size([512, 256, 3, 3]), torch.Size([512, 512, 3, 3]), torch.Size([512, 512, 3, 3]), torch.Size([512, 512, 3, 3]), torch.Size([10, 512])]\n",
      "\n",
      "--------- Testing in global mode ---------\n",
      "size of test_loader dataset:  10000\n",
      "\n",
      "Test set: Avg. loss: 0.0023, Accuracy: 904/10000 (9%)\n",
      "\n",
      "\n",
      "--------- Testing in global mode ---------\n",
      "size of test_loader dataset:  10000\n",
      "\n",
      "Test set: Avg. loss: 0.0023, Accuracy: 1405/10000 (14%)\n",
      "\n",
      "-------- Retraining the models ---------\n",
      "Retraining model :  model_0\n",
      "num_epochs---------------+++))))))+++ 300\n",
      "num_epochs---------------+++))))))+++ 300\n",
      "lr is  0.05\n",
      "number of epochs would be  30\n",
      "num_epochs-------------- 30\n",
      "Epoch 000\n",
      "/usr/local/pace-apps/manual/packages/pytorch/1.12.0/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:131: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\"Detected call of `lr_scheduler.step()` before `optimizer.step()`. \"\n",
      "/usr/local/pace-apps/manual/packages/pytorch/1.12.0/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:156: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "accuracy: {'epoch': 0, 'value': 0.8141400000190728} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 0, 'value': 0.5706454984903332} ({'split': 'train'})\n",
      "accuracy: {'epoch': 0, 'value': 0.8106000423431396} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 0, 'value': 0.5900417327880859} ({'split': 'test'})\n",
      "Epoch 001\n",
      "accuracy: {'epoch': 1, 'value': 0.8742600000190739} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 1, 'value': 0.37527672813415547} ({'split': 'train'})\n",
      "accuracy: {'epoch': 1, 'value': 0.8184000432491302} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 1, 'value': 0.5879074573516846} ({'split': 'test'})\n",
      "Epoch 002\n",
      "accuracy: {'epoch': 2, 'value': 0.9033000000572207} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 2, 'value': 0.28566226284027085} ({'split': 'train'})\n",
      "accuracy: {'epoch': 2, 'value': 0.8292000353336335} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 2, 'value': 0.5601330071687699} ({'split': 'test'})\n",
      "Epoch 003\n",
      "accuracy: {'epoch': 3, 'value': 0.922760000038147} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 3, 'value': 0.23217077801227562} ({'split': 'train'})\n",
      "accuracy: {'epoch': 3, 'value': 0.834600031375885} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 3, 'value': 0.5669786632061005} ({'split': 'test'})\n",
      "Epoch 004\n",
      "accuracy: {'epoch': 4, 'value': 0.9304200000572197} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 4, 'value': 0.2111923619651795} ({'split': 'train'})\n",
      "accuracy: {'epoch': 4, 'value': 0.8279000401496887} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 4, 'value': 0.5746641397476197} ({'split': 'test'})\n",
      "Epoch 005\n",
      "accuracy: {'epoch': 5, 'value': 0.9406000000190734} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 5, 'value': 0.17837551941871638} ({'split': 'train'})\n",
      "accuracy: {'epoch': 5, 'value': 0.819100034236908} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 5, 'value': 0.63096843957901} ({'split': 'test'})\n",
      "Epoch 006\n",
      "accuracy: {'epoch': 6, 'value': 0.9480400000190735} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 6, 'value': 0.1581753240871429} ({'split': 'train'})\n",
      "accuracy: {'epoch': 6, 'value': 0.8481000304222107} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 6, 'value': 0.576449054479599} ({'split': 'test'})\n",
      "Epoch 007\n",
      "accuracy: {'epoch': 7, 'value': 0.9453800000572209} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 7, 'value': 0.16539756085872645} ({'split': 'train'})\n",
      "accuracy: {'epoch': 7, 'value': 0.8250000417232514} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 7, 'value': 0.6385862886905671} ({'split': 'test'})\n",
      "Epoch 008\n",
      "accuracy: {'epoch': 8, 'value': 0.9530800000190736} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 8, 'value': 0.14221414058685303} ({'split': 'train'})\n",
      "accuracy: {'epoch': 8, 'value': 0.8056000411510468} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 8, 'value': 0.7120107114315033} ({'split': 'test'})\n",
      "Epoch 009\n",
      "accuracy: {'epoch': 9, 'value': 0.9547600000190741} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 9, 'value': 0.1395307765388488} ({'split': 'train'})\n",
      "accuracy: {'epoch': 9, 'value': 0.8378000378608703} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 9, 'value': 0.6473852217197418} ({'split': 'test'})\n",
      "Epoch 010\n",
      "accuracy: {'epoch': 10, 'value': 0.9568600000572204} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 10, 'value': 0.1356471933364869} ({'split': 'train'})\n",
      "accuracy: {'epoch': 10, 'value': 0.8275000393390656} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 10, 'value': 0.6041347086429595} ({'split': 'test'})\n",
      "Epoch 011\n",
      "accuracy: {'epoch': 11, 'value': 0.9554599999809265} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 11, 'value': 0.13546198210477836} ({'split': 'train'})\n",
      "accuracy: {'epoch': 11, 'value': 0.8327000379562378} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 11, 'value': 0.5863363862037658} ({'split': 'test'})\n",
      "Epoch 012\n",
      "accuracy: {'epoch': 12, 'value': 0.9619000000381474} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 12, 'value': 0.11638832125425336} ({'split': 'train'})\n",
      "accuracy: {'epoch': 12, 'value': 0.813400036096573} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 12, 'value': 0.7109529495239258} ({'split': 'test'})\n",
      "Epoch 013\n",
      "accuracy: {'epoch': 13, 'value': 0.9568000000381468} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 13, 'value': 0.1339322737169265} ({'split': 'train'})\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: {'epoch': 13, 'value': 0.8253000438213349} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 13, 'value': 0.6976024329662321} ({'split': 'test'})\n",
      "Epoch 014\n",
      "accuracy: {'epoch': 14, 'value': 0.9589400000190729} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 14, 'value': 0.12490061977863313} ({'split': 'train'})\n",
      "accuracy: {'epoch': 14, 'value': 0.8268000364303588} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 14, 'value': 0.6103038847446443} ({'split': 'test'})\n",
      "Epoch 015\n",
      "accuracy: {'epoch': 15, 'value': 0.9626000000190729} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 15, 'value': 0.11472394806861876} ({'split': 'train'})\n",
      "accuracy: {'epoch': 15, 'value': 0.8201000332832337} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 15, 'value': 0.7450841188430787} ({'split': 'test'})\n",
      "Epoch 016\n",
      "accuracy: {'epoch': 16, 'value': 0.9609400000572198} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 16, 'value': 0.12077561809062956} ({'split': 'train'})\n",
      "accuracy: {'epoch': 16, 'value': 0.8282000303268433} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 16, 'value': 0.6155348300933838} ({'split': 'test'})\n",
      "Epoch 017\n",
      "accuracy: {'epoch': 17, 'value': 0.9591800000572205} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 17, 'value': 0.12715106862068165} ({'split': 'train'})\n",
      "accuracy: {'epoch': 17, 'value': 0.8249000489711762} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 17, 'value': 0.6573211371898652} ({'split': 'test'})\n",
      "Epoch 018\n",
      "accuracy: {'epoch': 18, 'value': 0.9644600000572204} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 18, 'value': 0.1109698359823227} ({'split': 'train'})\n",
      "accuracy: {'epoch': 18, 'value': 0.8356000423431396} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 18, 'value': 0.7073285758495331} ({'split': 'test'})\n",
      "Epoch 019\n",
      "accuracy: {'epoch': 19, 'value': 0.9625199999809266} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 19, 'value': 0.11306913742065428} ({'split': 'train'})\n",
      "accuracy: {'epoch': 19, 'value': 0.8147000372409821} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 19, 'value': 0.6604127407073974} ({'split': 'test'})\n",
      "Epoch 020\n",
      "accuracy: {'epoch': 20, 'value': 0.9639600000190736} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 20, 'value': 0.11407978266716001} ({'split': 'train'})\n",
      "accuracy: {'epoch': 20, 'value': 0.8292000412940979} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 20, 'value': 0.685973620414734} ({'split': 'test'})\n",
      "Epoch 021\n",
      "accuracy: {'epoch': 21, 'value': 0.9637400000000005} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 21, 'value': 0.111420422949791} ({'split': 'train'})\n",
      "accuracy: {'epoch': 21, 'value': 0.8217000365257263} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 21, 'value': 0.7119944810867309} ({'split': 'test'})\n",
      "Epoch 022\n",
      "accuracy: {'epoch': 22, 'value': 0.9600400000190742} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 22, 'value': 0.12791573692321773} ({'split': 'train'})\n",
      "accuracy: {'epoch': 22, 'value': 0.8150000393390655} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 22, 'value': 0.6974963843822479} ({'split': 'test'})\n",
      "Epoch 023\n",
      "accuracy: {'epoch': 23, 'value': 0.962979999980927} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 23, 'value': 0.11725367122650153} ({'split': 'train'})\n",
      "accuracy: {'epoch': 23, 'value': 0.8119000434875487} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 23, 'value': 0.6978908240795135} ({'split': 'test'})\n",
      "Epoch 024\n",
      "accuracy: {'epoch': 24, 'value': 0.9618200000572202} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 24, 'value': 0.1183246151828767} ({'split': 'train'})\n",
      "accuracy: {'epoch': 24, 'value': 0.8183000445365906} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 24, 'value': 0.7609285175800323} ({'split': 'test'})\n",
      "Epoch 025\n",
      "accuracy: {'epoch': 25, 'value': 0.9636800000572207} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 25, 'value': 0.11264088063955302} ({'split': 'train'})\n",
      "accuracy: {'epoch': 25, 'value': 0.8237000405788422} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 25, 'value': 0.6851243197917938} ({'split': 'test'})\n",
      "Epoch 026\n",
      "accuracy: {'epoch': 26, 'value': 0.9652399999809264} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 26, 'value': 0.10863073498010639} ({'split': 'train'})\n",
      "accuracy: {'epoch': 26, 'value': 0.8307000339031219} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 26, 'value': 0.6479323327541352} ({'split': 'test'})\n",
      "Epoch 027\n",
      "accuracy: {'epoch': 27, 'value': 0.9657200000381465} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 27, 'value': 0.1049334424066543} ({'split': 'train'})\n",
      "accuracy: {'epoch': 27, 'value': 0.8250000417232514} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 27, 'value': 0.7729537189006805} ({'split': 'test'})\n",
      "Epoch 028\n",
      "accuracy: {'epoch': 28, 'value': 0.9655000000381464} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 28, 'value': 0.10833053930759429} ({'split': 'train'})\n",
      "accuracy: {'epoch': 28, 'value': 0.8185000360012055} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 28, 'value': 0.6739295065402985} ({'split': 'test'})\n",
      "Epoch 029\n",
      "accuracy: {'epoch': 29, 'value': 0.9658400000572203} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 29, 'value': 0.10793667633056639} ({'split': 'train'})\n",
      "accuracy: {'epoch': 29, 'value': 0.8283000409603118} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 29, 'value': 0.7437346518039704} ({'split': 'test'})\n",
      "mean_test_accuracy <cifar_utils.accumulators.Mean object at 0x2aabde7d18b0>\n",
      "QWERTY: Enter Cifar 2, acc 0.8481000304222107\n",
      "Retraining model :  model_1\n",
      "num_epochs---------------+++))))))+++ 30\n",
      "num_epochs---------------+++))))))+++ 30\n",
      "lr is  0.05\n",
      "number of epochs would be  30\n",
      "num_epochs-------------- 30\n",
      "Epoch 000\n",
      "accuracy: {'epoch': 0, 'value': 0.7971400000381471} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 0, 'value': 0.6138636363697051} ({'split': 'train'})\n",
      "accuracy: {'epoch': 0, 'value': 0.8091000497341155} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 0, 'value': 0.5865806937217714} ({'split': 'test'})\n",
      "Epoch 001\n",
      "accuracy: {'epoch': 1, 'value': 0.8671} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 1, 'value': 0.39900440132141074} ({'split': 'train'})\n",
      "accuracy: {'epoch': 1, 'value': 0.8352000415325165} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 1, 'value': 0.5205959767103194} ({'split': 'test'})\n",
      "Epoch 002\n",
      "accuracy: {'epoch': 2, 'value': 0.9034800000572205} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 2, 'value': 0.28946346643447873} ({'split': 'train'})\n",
      "accuracy: {'epoch': 2, 'value': 0.82990003824234} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 2, 'value': 0.5649041593074798} ({'split': 'test'})\n",
      "Epoch 003\n",
      "accuracy: {'epoch': 3, 'value': 0.9221199999809263} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 3, 'value': 0.2349290325021744} ({'split': 'train'})\n",
      "accuracy: {'epoch': 3, 'value': 0.8345000386238098} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 3, 'value': 0.5966291904449463} ({'split': 'test'})\n",
      "Epoch 004\n",
      "accuracy: {'epoch': 4, 'value': 0.9295} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 4, 'value': 0.20725084144592285} ({'split': 'train'})\n",
      "accuracy: {'epoch': 4, 'value': 0.8404000401496886} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 4, 'value': 0.5508099913597108} ({'split': 'test'})\n",
      "Epoch 005\n",
      "^C\n",
      "Traceback (most recent call last):\n",
      "  File \"/storage/coda1/p-jli3175/0/mwu385/OTfusion_pruning_final/otfusion/main.py\", line 460, in <module>\n",
      "    _, best_retrain_acc = routines.retrain_models(args, [*original_models, geometric_model, naive_model], retrain_loader, test_loader, config, tensorboard_obj=tensorboard_obj, initial_acc=initial_acc, nicks=nicks)\n",
      "  File \"/storage/coda1/p-jli3175/0/mwu385/OTfusion_pruning_final/otfusion/routines.py\", line 394, in retrain_models\n",
      "    retrained_network, acc = cifar_train.get_retrained_model(args, retrain_loader, test_loader, old_networks[i], config, output_root_dir, tensorboard_obj=tensorboard_obj, nick=nick, start_acc=initial_acc[i])\n",
      "  File \"/storage/coda1/p-jli3175/0/mwu385/OTfusion_pruning_final/otfusion/./cifar/train.py\", line 333, in get_retrained_model\n",
      "    best_acc,model = main(config, output_dir, args.gpu_id, pretrained_model=old_network, pretrained_dataset=(train_loader, test_loader), tensorboard_obj=tensorboard_obj,return_model=True)\n",
      "  File \"/storage/coda1/p-jli3175/0/mwu385/OTfusion_pruning_final/otfusion/./cifar/train.py\", line 67, in main\n",
      "    for batch_x, batch_y in training_loader:\n",
      "  File \"/usr/local/pace-apps/manual/packages/pytorch/1.12.0/lib/python3.9/site-packages/torch/utils/data/dataloader.py\", line 652, in __next__\n",
      "    data = self._next_data()\n",
      "  File \"/usr/local/pace-apps/manual/packages/pytorch/1.12.0/lib/python3.9/site-packages/torch/utils/data/dataloader.py\", line 692, in _next_data\n",
      "    data = self._dataset_fetcher.fetch(index)  # may raise StopIteration\n",
      "  File \"/usr/local/pace-apps/manual/packages/pytorch/1.12.0/lib/python3.9/site-packages/torch/utils/data/_utils/fetch.py\", line 49, in fetch\n",
      "    data = [self.dataset[idx] for idx in possibly_batched_index]\n",
      "  File \"/usr/local/pace-apps/manual/packages/pytorch/1.12.0/lib/python3.9/site-packages/torch/utils/data/_utils/fetch.py\", line 49, in <listcomp>\n",
      "    data = [self.dataset[idx] for idx in possibly_batched_index]\n",
      "  File \"/usr/local/pace-apps/manual/packages/pytorch/1.12.0/lib/python3.9/site-packages/torchvision/datasets/cifar.py\", line 118, in __getitem__\n",
      "    img = self.transform(img)\n",
      "  File \"/usr/local/pace-apps/manual/packages/pytorch/1.12.0/lib/python3.9/site-packages/torchvision/transforms/transforms.py\", line 94, in __call__\n",
      "    img = t(img)\n",
      "  File \"/usr/local/pace-apps/manual/packages/pytorch/1.12.0/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 1130, in _call_impl\n",
      "    return forward_call(*input, **kwargs)\n",
      "  File \"/usr/local/pace-apps/manual/packages/pytorch/1.12.0/lib/python3.9/site-packages/torchvision/transforms/transforms.py\", line 269, in forward\n",
      "    return F.normalize(tensor, self.mean, self.std, self.inplace)\n",
      "  File \"/usr/local/pace-apps/manual/packages/pytorch/1.12.0/lib/python3.9/site-packages/torchvision/transforms/functional.py\", line 356, in normalize\n",
      "    _log_api_usage_once(normalize)\n",
      "  File \"/usr/local/pace-apps/manual/packages/pytorch/1.12.0/lib/python3.9/site-packages/torchvision/utils.py\", line 566, in _log_api_usage_once\n",
      "    torch._C._log_api_usage_once(f\"{obj.__module__}.{name}\")\n",
      "KeyboardInterrupt\n"
     ]
    }
   ],
   "source": [
    "!python main.py --gpu-id 0 --model-name vgg11_nobias --n-epochs 30 --save-result-file sample.csv \\\n",
    "--sweep-name exp_sample --correction --ground-metric euclidean --weight-stats \\\n",
    "--geom-ensemble-type wts --ground-metric-normalize none --sweep-id 90 \\\n",
    "--ckpt-type best --dataset Cifar10 --ground-metric-eff --recheck-cifar --activation-seed {activation_seed} \\\n",
    "--prelu-acts --past-correction --not-squared --exact\\\n",
    "--learning-rate 0.0003\\\n",
    "--momentum 0.5\\\n",
    "--batch-size-train 128 --experiment-dir {experiment_dir}\\\n",
    "--prunint-rate {prunint_rate}\\\n",
    "--run-mode {run_mode}\\\n",
    "--to-download\\\n",
    "--load-model-dir {load_model_dir}\\\n",
    "--retrain 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e0df533",
   "metadata": {},
   "outputs": [],
   "source": [
    "#剪枝实验2:fine prune\n",
    "activation_seed=21\n",
    "experiment_dir=\"experiment1.5\"\n",
    "prunint_rate=0.3\n",
    "run_mode='finetune'\n",
    "load_model_dir='./cifar_models'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b59b6d6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python main.py --gpu-id 0 --model-name vgg11_nobias --n-epochs 30 --save-result-file sample.csv \\\n",
    "--sweep-name exp_sample --correction --ground-metric euclidean --weight-stats \\\n",
    "--geom-ensemble-type wts --ground-metric-normalize none --sweep-id 90 \\\n",
    "--ckpt-type best --dataset Cifar10 --ground-metric-eff --recheck-cifar --activation-seed {activation_seed} \\\n",
    "--prelu-acts --past-correction --not-squared --exact\\\n",
    "--learning-rate 0.0001\\\n",
    "--momentum 0.5\\\n",
    "--batch-size-train 256 --experiment-dir {experiment_dir}\\\n",
    "--prunint-rate {prunint_rate}\\\n",
    "--run-mode {run_mode}\\\n",
    "--to-download\\\n",
    "--load-model-dir {load_model_dir}\\\n",
    "--retrain 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "59f7e765-a8f1-447c-959a-56518a74919f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#剪枝实验3:prune times without finetune\n",
    "activation_seed=21\n",
    "experiment_dir=\"experiment1.5\"\n",
    "prunint_rate=0.3\n",
    "run_mode='prune_times'\n",
    "load_model_dir='./cifar_models'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "232a1e48-dda3-44cc-9412-ca248f03cc3f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------- Setting up parameters -------\n",
      "dumping parameters at  /root/autodl-tmp/OTfusion_pruning/otfusion/exp_sample/configurations\n",
      "The parameters are: \n",
      " Namespace(act_bug=False, act_num_samples=100, activation_histograms=False, activation_mode=None, activation_seed=21, alpha=0.7, baseroot='/root/autodl-tmp/OTfusion_pruning/otfusion', batch_size_test=1000, batch_size_train=256, center_acts=False, choice='0 2 4 6 8', cifar_style_data=False, ckpt_type='best', clip_gm=False, clip_max=5, clip_min=0, config_dir='/root/autodl-tmp/OTfusion_pruning/otfusion/exp_sample/configurations', config_file=None, correction=True, csv_dir='/root/autodl-tmp/OTfusion_pruning/otfusion/exp_sample/csv', dataset='Cifar10', debug=False, deprecated=None, deterministic=False, diff_init=False, disable_bias=True, dist_epochs=60, dist_normalize=False, dump_final_models=False, dump_model=False, enable_dropout=False, ensemble_step=0.5, eval_aligned=False, exact=True, exp_name='exp_2023-09-17_13-23-31_267121', experiment_dir='experiment3', geom_ensemble_type='wts', gpu_id=0, gromov=False, gromov_loss='square_loss', ground_metric='euclidean', ground_metric_eff=True, ground_metric_normalize='none', handle_skips=False, importance=None, learning_rate=0.0001, load_model_dir='./cifar_models', load_models='', log_interval=100, model_name='vgg11_nobias', momentum=0.5, n_epochs=5, no_random_trainloaders=False, normalize_acts=False, normalize_wts=False, not_squared=True, num_hidden_nodes=400, num_hidden_nodes1=400, num_hidden_nodes2=200, num_hidden_nodes3=100, num_hidden_nodes4=50, num_models=2, options_type='generic', partial_reshape=False, partition_dataloader=-1, partition_type='labels', past_correction=True, personal_class_idx=9, personal_split_frac=0.1, pool_acts=False, pool_relu=False, prediction_wts=False, prelu_acts=True, print_distances=False, proper_marginals=False, prune_times=6, prunint_rate=0.3, recheck_acc=False, recheck_cifar=True, reg=0.01, reg_m=0.001, reinit_trainloaders=False, result_dir='/root/autodl-tmp/OTfusion_pruning/otfusion/exp_sample/results', retrain=30, retrain_avg_only=False, retrain_geometric_only=False, retrain_lr_decay=-1, retrain_lr_decay_epochs=None, retrain_lr_decay_factor=None, retrain_seed=-1, rootdir='/root/autodl-tmp/OTfusion_pruning/otfusion/exp_sample', run_mode='prune_times', same_model=-1, save_result_file='sample.csv', second_model_name=None, sinkhorn_type='normal', skip_last_layer=False, skip_last_layer_type='average', skip_personal_idx=False, skip_retrain=-1, softmax_temperature=1, standardize_acts=False, sweep_id=90, sweep_name='exp_sample', temperature=20, tensorboard=False, tensorboard_root='./tensorboard', timestamp='2023-09-17_13-23-31_267121', tmap_stats=False, to_download=True, transform_acts=False, unbalanced=False, update_acts=False, weight_stats=True, width_ratio=1)\n",
      "refactored get_config\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "in _make_layers [Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), AvgPool2d(kernel_size=1, stride=1, padding=0)]\n",
      "Relu Inplace is  False\n",
      "model parameters are \n",
      " [torch.Size([64, 3, 3, 3]), torch.Size([128, 64, 3, 3]), torch.Size([256, 128, 3, 3]), torch.Size([256, 256, 3, 3]), torch.Size([512, 256, 3, 3]), torch.Size([512, 512, 3, 3]), torch.Size([512, 512, 3, 3]), torch.Size([512, 512, 3, 3]), torch.Size([10, 512])]\n",
      "in _make_layers [Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), AvgPool2d(kernel_size=1, stride=1, padding=0)]\n",
      "Relu Inplace is  False\n",
      "model parameters are \n",
      " [torch.Size([64, 3, 3, 3]), torch.Size([128, 64, 3, 3]), torch.Size([256, 128, 3, 3]), torch.Size([256, 256, 3, 3]), torch.Size([512, 256, 3, 3]), torch.Size([512, 512, 3, 3]), torch.Size([512, 512, 3, 3]), torch.Size([512, 512, 3, 3]), torch.Size([10, 512])]\n",
      "in _make_layers [Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), AvgPool2d(kernel_size=1, stride=1, padding=0)]\n",
      "Relu Inplace is  False\n",
      "model parameters are \n",
      " [torch.Size([64, 3, 3, 3]), torch.Size([128, 64, 3, 3]), torch.Size([256, 128, 3, 3]), torch.Size([256, 256, 3, 3]), torch.Size([512, 256, 3, 3]), torch.Size([512, 512, 3, 3]), torch.Size([512, 512, 3, 3]), torch.Size([512, 512, 3, 3]), torch.Size([10, 512])]\n",
      "in _make_layers [Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), AvgPool2d(kernel_size=1, stride=1, padding=0)]\n",
      "Relu Inplace is  False\n",
      "model parameters are \n",
      " [torch.Size([64, 3, 3, 3]), torch.Size([128, 64, 3, 3]), torch.Size([256, 128, 3, 3]), torch.Size([256, 256, 3, 3]), torch.Size([512, 256, 3, 3]), torch.Size([512, 512, 3, 3]), torch.Size([512, 512, 3, 3]), torch.Size([512, 512, 3, 3]), torch.Size([10, 512])]\n",
      "enter prune_times, the times is 6,ratio:0.049999999999999996\n",
      "Train Epoch: 1 [0/50000 (0%)]\tLoss: 0.101607\n",
      "Train Epoch: 1 [25600/50000 (51%)]\tLoss: 0.094466\n",
      "\n",
      "--------- Testing in global mode ---------\n",
      "size of test_loader dataset:  10000\n",
      "\n",
      "Test set: Avg. loss: 0.0003, Accuracy: 9027/10000 (90%)\n",
      "\n",
      "Train Epoch: 2 [0/50000 (0%)]\tLoss: 0.071986\n",
      "Train Epoch: 2 [25600/50000 (51%)]\tLoss: 0.088799\n",
      "\n",
      "--------- Testing in global mode ---------\n",
      "size of test_loader dataset:  10000\n",
      "\n",
      "Test set: Avg. loss: 0.0003, Accuracy: 9041/10000 (90%)\n",
      "\n",
      "Train Epoch: 3 [0/50000 (0%)]\tLoss: 0.064529\n",
      "Train Epoch: 3 [25600/50000 (51%)]\tLoss: 0.059589\n",
      "\n",
      "--------- Testing in global mode ---------\n",
      "size of test_loader dataset:  10000\n",
      "\n",
      "Test set: Avg. loss: 0.0003, Accuracy: 9050/10000 (90%)\n",
      "\n",
      "Train Epoch: 4 [0/50000 (0%)]\tLoss: 0.069565\n",
      "Train Epoch: 4 [25600/50000 (51%)]\tLoss: 0.055295\n",
      "\n",
      "--------- Testing in global mode ---------\n",
      "size of test_loader dataset:  10000\n",
      "\n",
      "Test set: Avg. loss: 0.0003, Accuracy: 9050/10000 (90%)\n",
      "\n",
      "Train Epoch: 5 [0/50000 (0%)]\tLoss: 0.066640\n",
      "Train Epoch: 5 [25600/50000 (51%)]\tLoss: 0.069415\n",
      "\n",
      "--------- Testing in global mode ---------\n",
      "size of test_loader dataset:  10000\n",
      "\n",
      "Test set: Avg. loss: 0.0003, Accuracy: 9045/10000 (90%)\n",
      "\n",
      "Train Epoch: 1 [0/50000 (0%)]\tLoss: 0.100916\n",
      "Train Epoch: 1 [25600/50000 (51%)]\tLoss: 0.088565\n",
      "\n",
      "--------- Testing in global mode ---------\n",
      "size of test_loader dataset:  10000\n",
      "\n",
      "Test set: Avg. loss: 0.0003, Accuracy: 9059/10000 (91%)\n",
      "\n",
      "Train Epoch: 2 [0/50000 (0%)]\tLoss: 0.081772\n",
      "Train Epoch: 2 [25600/50000 (51%)]\tLoss: 0.092069\n",
      "\n",
      "--------- Testing in global mode ---------\n",
      "size of test_loader dataset:  10000\n",
      "\n",
      "Test set: Avg. loss: 0.0003, Accuracy: 9076/10000 (91%)\n",
      "\n",
      "Train Epoch: 3 [0/50000 (0%)]\tLoss: 0.072218\n",
      "Train Epoch: 3 [25600/50000 (51%)]\tLoss: 0.086950\n",
      "\n",
      "--------- Testing in global mode ---------\n",
      "size of test_loader dataset:  10000\n",
      "\n",
      "Test set: Avg. loss: 0.0003, Accuracy: 9071/10000 (91%)\n",
      "\n",
      "Train Epoch: 4 [0/50000 (0%)]\tLoss: 0.092203\n",
      "Train Epoch: 4 [25600/50000 (51%)]\tLoss: 0.075020\n",
      "\n",
      "--------- Testing in global mode ---------\n",
      "size of test_loader dataset:  10000\n",
      "\n",
      "Test set: Avg. loss: 0.0003, Accuracy: 9065/10000 (91%)\n",
      "\n",
      "Train Epoch: 5 [0/50000 (0%)]\tLoss: 0.069902\n",
      "Train Epoch: 5 [25600/50000 (51%)]\tLoss: 0.064976\n",
      "\n",
      "--------- Testing in global mode ---------\n",
      "size of test_loader dataset:  10000\n",
      "\n",
      "Test set: Avg. loss: 0.0003, Accuracy: 9057/10000 (91%)\n",
      "\n",
      "in times 0, now cumm_ratio 0.049999999999999996,accuracies:[90.45, 90.57],losses:[0.00030069350600242615, 0.0002984473496675491]\n",
      "Train Epoch: 1 [0/50000 (0%)]\tLoss: 0.060230\n",
      "Train Epoch: 1 [25600/50000 (51%)]\tLoss: 0.050899\n",
      "\n",
      "--------- Testing in global mode ---------\n",
      "size of test_loader dataset:  10000\n",
      "\n",
      "Test set: Avg. loss: 0.0003, Accuracy: 9049/10000 (90%)\n",
      "\n",
      "Train Epoch: 2 [0/50000 (0%)]\tLoss: 0.047751\n",
      "Train Epoch: 2 [25600/50000 (51%)]\tLoss: 0.070797\n",
      "\n",
      "--------- Testing in global mode ---------\n",
      "size of test_loader dataset:  10000\n",
      "\n",
      "Test set: Avg. loss: 0.0003, Accuracy: 9047/10000 (90%)\n",
      "\n",
      "Train Epoch: 3 [0/50000 (0%)]\tLoss: 0.054113\n",
      "Train Epoch: 3 [25600/50000 (51%)]\tLoss: 0.053716\n",
      "\n",
      "--------- Testing in global mode ---------\n",
      "size of test_loader dataset:  10000\n",
      "\n",
      "Test set: Avg. loss: 0.0003, Accuracy: 9049/10000 (90%)\n",
      "\n",
      "Train Epoch: 4 [0/50000 (0%)]\tLoss: 0.061361\n",
      "Train Epoch: 4 [25600/50000 (51%)]\tLoss: 0.053931\n",
      "\n",
      "--------- Testing in global mode ---------\n",
      "size of test_loader dataset:  10000\n",
      "\n",
      "Test set: Avg. loss: 0.0003, Accuracy: 9046/10000 (90%)\n",
      "\n",
      "Train Epoch: 5 [0/50000 (0%)]\tLoss: 0.058132\n",
      "Train Epoch: 5 [25600/50000 (51%)]\tLoss: 0.056490\n",
      "\n",
      "--------- Testing in global mode ---------\n",
      "size of test_loader dataset:  10000\n",
      "\n",
      "Test set: Avg. loss: 0.0003, Accuracy: 9048/10000 (90%)\n",
      "\n",
      "Train Epoch: 1 [0/50000 (0%)]\tLoss: 0.061815\n",
      "Train Epoch: 1 [25600/50000 (51%)]\tLoss: 0.059492\n",
      "\n",
      "--------- Testing in global mode ---------\n",
      "size of test_loader dataset:  10000\n",
      "\n",
      "Test set: Avg. loss: 0.0003, Accuracy: 9062/10000 (91%)\n",
      "\n",
      "Train Epoch: 2 [0/50000 (0%)]\tLoss: 0.063887\n",
      "Train Epoch: 2 [25600/50000 (51%)]\tLoss: 0.051710\n",
      "\n",
      "--------- Testing in global mode ---------\n",
      "size of test_loader dataset:  10000\n",
      "\n",
      "Test set: Avg. loss: 0.0003, Accuracy: 9065/10000 (91%)\n",
      "\n",
      "Train Epoch: 3 [0/50000 (0%)]\tLoss: 0.051443\n",
      "Train Epoch: 3 [25600/50000 (51%)]\tLoss: 0.062346\n",
      "\n",
      "--------- Testing in global mode ---------\n",
      "size of test_loader dataset:  10000\n",
      "\n",
      "Test set: Avg. loss: 0.0003, Accuracy: 9060/10000 (91%)\n",
      "\n",
      "Train Epoch: 4 [0/50000 (0%)]\tLoss: 0.058576\n",
      "Train Epoch: 4 [25600/50000 (51%)]\tLoss: 0.060837\n",
      "\n",
      "--------- Testing in global mode ---------\n",
      "size of test_loader dataset:  10000\n",
      "\n",
      "Test set: Avg. loss: 0.0003, Accuracy: 9057/10000 (91%)\n",
      "\n",
      "Train Epoch: 5 [0/50000 (0%)]\tLoss: 0.055335\n",
      "Train Epoch: 5 [25600/50000 (51%)]\tLoss: 0.052082\n",
      "\n",
      "--------- Testing in global mode ---------\n",
      "size of test_loader dataset:  10000\n",
      "\n",
      "Test set: Avg. loss: 0.0003, Accuracy: 9057/10000 (91%)\n",
      "\n",
      "in times 1, now cumm_ratio 0.09999999999999999,accuracies:[90.48, 90.57],losses:[0.00029933132231235504, 0.0002961380332708359]\n",
      "Train Epoch: 1 [0/50000 (0%)]\tLoss: 0.042468\n",
      "Train Epoch: 1 [25600/50000 (51%)]\tLoss: 0.059078\n",
      "\n",
      "--------- Testing in global mode ---------\n",
      "size of test_loader dataset:  10000\n",
      "\n",
      "Test set: Avg. loss: 0.0003, Accuracy: 9053/10000 (91%)\n",
      "\n",
      "Train Epoch: 2 [0/50000 (0%)]\tLoss: 0.068352\n",
      "Train Epoch: 2 [25600/50000 (51%)]\tLoss: 0.049185\n",
      "\n",
      "--------- Testing in global mode ---------\n",
      "size of test_loader dataset:  10000\n",
      "\n",
      "Test set: Avg. loss: 0.0003, Accuracy: 9054/10000 (91%)\n",
      "\n",
      "Train Epoch: 3 [0/50000 (0%)]\tLoss: 0.049418\n",
      "Train Epoch: 3 [25600/50000 (51%)]\tLoss: 0.053414\n",
      "\n",
      "--------- Testing in global mode ---------\n",
      "size of test_loader dataset:  10000\n",
      "\n",
      "Test set: Avg. loss: 0.0003, Accuracy: 9051/10000 (91%)\n",
      "\n",
      "Train Epoch: 4 [0/50000 (0%)]\tLoss: 0.040274\n",
      "Train Epoch: 4 [25600/50000 (51%)]\tLoss: 0.051211\n",
      "\n",
      "--------- Testing in global mode ---------\n",
      "size of test_loader dataset:  10000\n",
      "\n",
      "Test set: Avg. loss: 0.0003, Accuracy: 9051/10000 (91%)\n",
      "\n",
      "Train Epoch: 5 [0/50000 (0%)]\tLoss: 0.040381\n",
      "Train Epoch: 5 [25600/50000 (51%)]\tLoss: 0.044499\n",
      "\n",
      "--------- Testing in global mode ---------\n",
      "size of test_loader dataset:  10000\n",
      "\n",
      "Test set: Avg. loss: 0.0003, Accuracy: 9052/10000 (91%)\n",
      "\n",
      "Train Epoch: 1 [0/50000 (0%)]\tLoss: 0.067314\n",
      "Train Epoch: 1 [25600/50000 (51%)]\tLoss: 0.043373\n",
      "\n",
      "--------- Testing in global mode ---------\n",
      "size of test_loader dataset:  10000\n",
      "\n",
      "Test set: Avg. loss: 0.0003, Accuracy: 9052/10000 (91%)\n",
      "\n",
      "Train Epoch: 2 [0/50000 (0%)]\tLoss: 0.047621\n",
      "Train Epoch: 2 [25600/50000 (51%)]\tLoss: 0.048761\n",
      "\n",
      "--------- Testing in global mode ---------\n",
      "size of test_loader dataset:  10000\n",
      "\n",
      "Test set: Avg. loss: 0.0003, Accuracy: 9051/10000 (91%)\n",
      "\n",
      "Train Epoch: 3 [0/50000 (0%)]\tLoss: 0.052236\n",
      "Train Epoch: 3 [25600/50000 (51%)]\tLoss: 0.044773\n",
      "\n",
      "--------- Testing in global mode ---------\n",
      "size of test_loader dataset:  10000\n",
      "\n",
      "Test set: Avg. loss: 0.0003, Accuracy: 9044/10000 (90%)\n",
      "\n",
      "Train Epoch: 4 [0/50000 (0%)]\tLoss: 0.059296\n",
      "Train Epoch: 4 [25600/50000 (51%)]\tLoss: 0.043340\n",
      "\n",
      "--------- Testing in global mode ---------\n",
      "size of test_loader dataset:  10000\n",
      "\n",
      "Test set: Avg. loss: 0.0003, Accuracy: 9043/10000 (90%)\n",
      "\n",
      "Train Epoch: 5 [0/50000 (0%)]\tLoss: 0.053493\n",
      "Train Epoch: 5 [25600/50000 (51%)]\tLoss: 0.051335\n",
      "\n",
      "--------- Testing in global mode ---------\n",
      "size of test_loader dataset:  10000\n",
      "\n",
      "Test set: Avg. loss: 0.0003, Accuracy: 9044/10000 (90%)\n",
      "\n",
      "in times 2, now cumm_ratio 0.15,accuracies:[90.52, 90.44],losses:[0.00029983029663562774, 0.00029635486602783205]\n",
      "Train Epoch: 1 [0/50000 (0%)]\tLoss: 0.047175\n",
      "Train Epoch: 1 [25600/50000 (51%)]\tLoss: 0.042709\n",
      "\n",
      "--------- Testing in global mode ---------\n",
      "size of test_loader dataset:  10000\n",
      "\n",
      "Test set: Avg. loss: 0.0003, Accuracy: 9048/10000 (90%)\n",
      "\n",
      "Train Epoch: 2 [0/50000 (0%)]\tLoss: 0.034754\n",
      "Train Epoch: 2 [25600/50000 (51%)]\tLoss: 0.034363\n",
      "\n",
      "--------- Testing in global mode ---------\n",
      "size of test_loader dataset:  10000\n",
      "\n",
      "Test set: Avg. loss: 0.0003, Accuracy: 9044/10000 (90%)\n",
      "\n",
      "Train Epoch: 3 [0/50000 (0%)]\tLoss: 0.051147\n",
      "Train Epoch: 3 [25600/50000 (51%)]\tLoss: 0.040479\n",
      "\n",
      "--------- Testing in global mode ---------\n",
      "size of test_loader dataset:  10000\n",
      "\n",
      "Test set: Avg. loss: 0.0003, Accuracy: 9045/10000 (90%)\n",
      "\n",
      "Train Epoch: 4 [0/50000 (0%)]\tLoss: 0.053561\n",
      "Train Epoch: 4 [25600/50000 (51%)]\tLoss: 0.040511\n",
      "\n",
      "--------- Testing in global mode ---------\n",
      "size of test_loader dataset:  10000\n",
      "\n",
      "Test set: Avg. loss: 0.0003, Accuracy: 9045/10000 (90%)\n",
      "\n",
      "Train Epoch: 5 [0/50000 (0%)]\tLoss: 0.041064\n",
      "Train Epoch: 5 [25600/50000 (51%)]\tLoss: 0.051423\n",
      "\n",
      "--------- Testing in global mode ---------\n",
      "size of test_loader dataset:  10000\n",
      "\n",
      "Test set: Avg. loss: 0.0003, Accuracy: 9046/10000 (90%)\n",
      "\n",
      "Train Epoch: 1 [0/50000 (0%)]\tLoss: 0.051889\n",
      "Train Epoch: 1 [25600/50000 (51%)]\tLoss: 0.040852\n",
      "\n",
      "--------- Testing in global mode ---------\n",
      "size of test_loader dataset:  10000\n",
      "\n",
      "Test set: Avg. loss: 0.0003, Accuracy: 9041/10000 (90%)\n",
      "\n",
      "Train Epoch: 2 [0/50000 (0%)]\tLoss: 0.034726\n",
      "Train Epoch: 2 [25600/50000 (51%)]\tLoss: 0.042077\n",
      "\n",
      "--------- Testing in global mode ---------\n",
      "size of test_loader dataset:  10000\n",
      "\n",
      "Test set: Avg. loss: 0.0003, Accuracy: 9042/10000 (90%)\n",
      "\n",
      "Train Epoch: 3 [0/50000 (0%)]\tLoss: 0.049467\n",
      "Train Epoch: 3 [25600/50000 (51%)]\tLoss: 0.051107\n",
      "\n",
      "--------- Testing in global mode ---------\n",
      "size of test_loader dataset:  10000\n",
      "\n",
      "Test set: Avg. loss: 0.0003, Accuracy: 9041/10000 (90%)\n",
      "\n",
      "Train Epoch: 4 [0/50000 (0%)]\tLoss: 0.054461\n",
      "Train Epoch: 4 [25600/50000 (51%)]\tLoss: 0.045085\n",
      "\n",
      "--------- Testing in global mode ---------\n",
      "size of test_loader dataset:  10000\n",
      "\n",
      "Test set: Avg. loss: 0.0003, Accuracy: 9040/10000 (90%)\n",
      "\n",
      "Train Epoch: 5 [0/50000 (0%)]\tLoss: 0.042276\n",
      "Train Epoch: 5 [25600/50000 (51%)]\tLoss: 0.053963\n",
      "\n",
      "--------- Testing in global mode ---------\n",
      "size of test_loader dataset:  10000\n",
      "\n",
      "Test set: Avg. loss: 0.0003, Accuracy: 9038/10000 (90%)\n",
      "\n",
      "in times 3, now cumm_ratio 0.19999999999999998,accuracies:[90.46, 90.38],losses:[0.0003019609525799751, 0.0002976515859365463]\n",
      "Train Epoch: 1 [0/50000 (0%)]\tLoss: 0.042371\n",
      "Train Epoch: 1 [25600/50000 (51%)]\tLoss: 0.048950\n",
      "\n",
      "--------- Testing in global mode ---------\n",
      "size of test_loader dataset:  10000\n",
      "\n",
      "Test set: Avg. loss: 0.0003, Accuracy: 9030/10000 (90%)\n",
      "\n",
      "Train Epoch: 2 [0/50000 (0%)]\tLoss: 0.041815\n",
      "Train Epoch: 2 [25600/50000 (51%)]\tLoss: 0.043534\n",
      "\n",
      "--------- Testing in global mode ---------\n",
      "size of test_loader dataset:  10000\n",
      "\n",
      "Test set: Avg. loss: 0.0003, Accuracy: 9035/10000 (90%)\n",
      "\n",
      "Train Epoch: 3 [0/50000 (0%)]\tLoss: 0.060089\n",
      "Train Epoch: 3 [25600/50000 (51%)]\tLoss: 0.044052\n",
      "\n",
      "--------- Testing in global mode ---------\n",
      "size of test_loader dataset:  10000\n",
      "\n",
      "Test set: Avg. loss: 0.0003, Accuracy: 9035/10000 (90%)\n",
      "\n",
      "Train Epoch: 4 [0/50000 (0%)]\tLoss: 0.046658\n",
      "Train Epoch: 4 [25600/50000 (51%)]\tLoss: 0.044399\n",
      "\n",
      "--------- Testing in global mode ---------\n",
      "size of test_loader dataset:  10000\n",
      "\n",
      "Test set: Avg. loss: 0.0003, Accuracy: 9032/10000 (90%)\n",
      "\n",
      "Train Epoch: 5 [0/50000 (0%)]\tLoss: 0.048275\n",
      "Train Epoch: 5 [25600/50000 (51%)]\tLoss: 0.039424\n",
      "\n",
      "--------- Testing in global mode ---------\n",
      "size of test_loader dataset:  10000\n",
      "\n",
      "Test set: Avg. loss: 0.0003, Accuracy: 9032/10000 (90%)\n",
      "\n",
      "Train Epoch: 1 [0/50000 (0%)]\tLoss: 0.058785\n",
      "Train Epoch: 1 [25600/50000 (51%)]\tLoss: 0.035397\n",
      "\n",
      "--------- Testing in global mode ---------\n",
      "size of test_loader dataset:  10000\n",
      "\n",
      "Test set: Avg. loss: 0.0003, Accuracy: 9034/10000 (90%)\n",
      "\n",
      "Train Epoch: 2 [0/50000 (0%)]\tLoss: 0.041309\n",
      "Train Epoch: 2 [25600/50000 (51%)]\tLoss: 0.044635\n",
      "\n",
      "--------- Testing in global mode ---------\n",
      "size of test_loader dataset:  10000\n",
      "\n",
      "Test set: Avg. loss: 0.0003, Accuracy: 9036/10000 (90%)\n",
      "\n",
      "Train Epoch: 3 [0/50000 (0%)]\tLoss: 0.053062\n",
      "Train Epoch: 3 [25600/50000 (51%)]\tLoss: 0.048245\n",
      "\n",
      "--------- Testing in global mode ---------\n",
      "size of test_loader dataset:  10000\n",
      "\n",
      "Test set: Avg. loss: 0.0003, Accuracy: 9032/10000 (90%)\n",
      "\n",
      "Train Epoch: 4 [0/50000 (0%)]\tLoss: 0.039306\n",
      "Train Epoch: 4 [25600/50000 (51%)]\tLoss: 0.038803\n",
      "\n",
      "--------- Testing in global mode ---------\n",
      "size of test_loader dataset:  10000\n",
      "\n",
      "Test set: Avg. loss: 0.0003, Accuracy: 9031/10000 (90%)\n",
      "\n",
      "Train Epoch: 5 [0/50000 (0%)]\tLoss: 0.049448\n",
      "Train Epoch: 5 [25600/50000 (51%)]\tLoss: 0.048671\n",
      "\n",
      "--------- Testing in global mode ---------\n",
      "size of test_loader dataset:  10000\n",
      "\n",
      "Test set: Avg. loss: 0.0003, Accuracy: 9030/10000 (90%)\n",
      "\n",
      "in times 4, now cumm_ratio 0.24999999999999997,accuracies:[90.32, 90.3],losses:[0.0003051861494779587, 0.0003003058195114136]\n",
      "Train Epoch: 1 [0/50000 (0%)]\tLoss: 0.045835\n",
      "Train Epoch: 1 [25600/50000 (51%)]\tLoss: 0.038378\n",
      "\n",
      "--------- Testing in global mode ---------\n",
      "size of test_loader dataset:  10000\n",
      "\n",
      "Test set: Avg. loss: 0.0003, Accuracy: 9021/10000 (90%)\n",
      "\n",
      "Train Epoch: 2 [0/50000 (0%)]\tLoss: 0.036805\n",
      "Train Epoch: 2 [25600/50000 (51%)]\tLoss: 0.037534\n",
      "\n",
      "--------- Testing in global mode ---------\n",
      "size of test_loader dataset:  10000\n",
      "\n",
      "Test set: Avg. loss: 0.0003, Accuracy: 9023/10000 (90%)\n",
      "\n",
      "Train Epoch: 3 [0/50000 (0%)]\tLoss: 0.042298\n",
      "Train Epoch: 3 [25600/50000 (51%)]\tLoss: 0.044288\n",
      "\n",
      "--------- Testing in global mode ---------\n",
      "size of test_loader dataset:  10000\n",
      "\n",
      "Test set: Avg. loss: 0.0003, Accuracy: 9026/10000 (90%)\n",
      "\n",
      "Train Epoch: 4 [0/50000 (0%)]\tLoss: 0.037782\n",
      "Train Epoch: 4 [25600/50000 (51%)]\tLoss: 0.032487\n",
      "\n",
      "--------- Testing in global mode ---------\n",
      "size of test_loader dataset:  10000\n",
      "\n",
      "Test set: Avg. loss: 0.0003, Accuracy: 9030/10000 (90%)\n",
      "\n",
      "Train Epoch: 5 [0/50000 (0%)]\tLoss: 0.048911\n",
      "Train Epoch: 5 [25600/50000 (51%)]\tLoss: 0.044069\n",
      "\n",
      "--------- Testing in global mode ---------\n",
      "size of test_loader dataset:  10000\n",
      "\n",
      "Test set: Avg. loss: 0.0003, Accuracy: 9036/10000 (90%)\n",
      "\n",
      "Train Epoch: 1 [0/50000 (0%)]\tLoss: 0.042938\n",
      "Train Epoch: 1 [25600/50000 (51%)]\tLoss: 0.044251\n",
      "\n",
      "--------- Testing in global mode ---------\n",
      "size of test_loader dataset:  10000\n",
      "\n",
      "Test set: Avg. loss: 0.0003, Accuracy: 9034/10000 (90%)\n",
      "\n",
      "Train Epoch: 2 [0/50000 (0%)]\tLoss: 0.054605\n",
      "Train Epoch: 2 [25600/50000 (51%)]\tLoss: 0.054013\n",
      "\n",
      "--------- Testing in global mode ---------\n",
      "size of test_loader dataset:  10000\n",
      "\n",
      "Test set: Avg. loss: 0.0003, Accuracy: 9034/10000 (90%)\n",
      "\n",
      "Train Epoch: 3 [0/50000 (0%)]\tLoss: 0.050810\n",
      "Train Epoch: 3 [25600/50000 (51%)]\tLoss: 0.044486\n",
      "\n",
      "--------- Testing in global mode ---------\n",
      "size of test_loader dataset:  10000\n",
      "\n",
      "Test set: Avg. loss: 0.0003, Accuracy: 9038/10000 (90%)\n",
      "\n",
      "Train Epoch: 4 [0/50000 (0%)]\tLoss: 0.041026\n",
      "Train Epoch: 4 [25600/50000 (51%)]\tLoss: 0.052041\n",
      "\n",
      "--------- Testing in global mode ---------\n",
      "size of test_loader dataset:  10000\n",
      "\n",
      "Test set: Avg. loss: 0.0003, Accuracy: 9033/10000 (90%)\n",
      "\n",
      "Train Epoch: 5 [0/50000 (0%)]\tLoss: 0.037089\n",
      "Train Epoch: 5 [25600/50000 (51%)]\tLoss: 0.051727\n",
      "\n",
      "--------- Testing in global mode ---------\n",
      "size of test_loader dataset:  10000\n",
      "\n",
      "Test set: Avg. loss: 0.0003, Accuracy: 9033/10000 (90%)\n",
      "\n",
      "in times 5, now cumm_ratio 0.3,accuracies:[90.36, 90.33],losses:[0.00030814808309078214, 0.0003035434454679489]\n",
      "===============================================\n",
      "===============================================\n",
      "before Ensemble,acc: [90.36, 90.33] loss [0.00030814808309078214, 0.0003035434454679489]\n",
      "===============================================\n",
      "===============================================\n",
      "check zero rate for model 0\n",
      "odict_keys(['features.0.weight', 'features.3.weight', 'features.6.weight', 'features.8.weight', 'features.11.weight', 'features.13.weight', 'features.16.weight', 'features.18.weight', 'classifier.weight'])\n",
      "the ratio of zero for features.0.weight\n",
      "tensor(0.2998, device='cuda:0')\n",
      "the ratio of zero for features.3.weight\n",
      "tensor(0.3000, device='cuda:0')\n",
      "the ratio of zero for features.6.weight\n",
      "tensor(0.3000, device='cuda:0')\n",
      "the ratio of zero for features.8.weight\n",
      "tensor(0.3000, device='cuda:0')\n",
      "the ratio of zero for features.11.weight\n",
      "tensor(0.3000, device='cuda:0')\n",
      "the ratio of zero for features.13.weight\n",
      "tensor(0.3000, device='cuda:0')\n",
      "the ratio of zero for features.16.weight\n",
      "tensor(0.3000, device='cuda:0')\n",
      "the ratio of zero for features.18.weight\n",
      "tensor(0.3000, device='cuda:0')\n",
      "the ratio of zero for classifier.weight\n",
      "tensor(0.3000, device='cuda:0')\n",
      "check zero rate for model 1\n",
      "odict_keys(['features.0.weight', 'features.3.weight', 'features.6.weight', 'features.8.weight', 'features.11.weight', 'features.13.weight', 'features.16.weight', 'features.18.weight', 'classifier.weight'])\n",
      "the ratio of zero for features.0.weight\n",
      "tensor(0.2998, device='cuda:0')\n",
      "the ratio of zero for features.3.weight\n",
      "tensor(0.3000, device='cuda:0')\n",
      "the ratio of zero for features.6.weight\n",
      "tensor(0.3000, device='cuda:0')\n",
      "the ratio of zero for features.8.weight\n",
      "tensor(0.3000, device='cuda:0')\n",
      "the ratio of zero for features.11.weight\n",
      "tensor(0.3000, device='cuda:0')\n",
      "the ratio of zero for features.13.weight\n",
      "tensor(0.3000, device='cuda:0')\n",
      "the ratio of zero for features.16.weight\n",
      "tensor(0.3000, device='cuda:0')\n",
      "the ratio of zero for features.18.weight\n",
      "tensor(0.3000, device='cuda:0')\n",
      "the ratio of zero for classifier.weight\n",
      "tensor(0.3000, device='cuda:0')\n",
      "layer features.0.weight has #params  1728\n",
      "layer features.3.weight has #params  73728\n",
      "layer features.6.weight has #params  294912\n",
      "layer features.8.weight has #params  589824\n",
      "layer features.11.weight has #params  1179648\n",
      "layer features.13.weight has #params  2359296\n",
      "layer features.16.weight has #params  2359296\n",
      "layer features.18.weight has #params  2359296\n",
      "layer classifier.weight has #params  5120\n",
      "Activation Timer start\n",
      "Activation Timer ends\n",
      "------- Geometric Ensembling -------\n",
      "Timer start\n",
      "odict_keys(['features.0.weight', 'features.3.weight', 'features.6.weight', 'features.8.weight', 'features.11.weight', 'features.13.weight', 'features.16.weight', 'features.18.weight', 'classifier.weight'])\n",
      "the ratio of zero for features.0.weight\n",
      "tensor(0.2998, device='cuda:0')\n",
      "the ratio of zero for features.3.weight\n",
      "tensor(0.3000, device='cuda:0')\n",
      "the ratio of zero for features.6.weight\n",
      "tensor(0.3000, device='cuda:0')\n",
      "the ratio of zero for features.8.weight\n",
      "tensor(0.3000, device='cuda:0')\n",
      "the ratio of zero for features.11.weight\n",
      "tensor(0.3000, device='cuda:0')\n",
      "the ratio of zero for features.13.weight\n",
      "tensor(0.3000, device='cuda:0')\n",
      "the ratio of zero for features.16.weight\n",
      "tensor(0.3000, device='cuda:0')\n",
      "the ratio of zero for features.18.weight\n",
      "tensor(0.3000, device='cuda:0')\n",
      "the ratio of zero for classifier.weight\n",
      "tensor(0.3000, device='cuda:0')\n",
      "Previous layer shape is  None\n",
      "Processing the coordinates to form ground_metric\n",
      "dont leave off the squaring of the ground metric\n",
      "returns a uniform measure of cardinality:  64\n",
      "returns a uniform measure of cardinality:  64\n",
      "the transport map is  tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')\n",
      "Ratio of trace to the matrix sum:  tensor(0.0312, device='cuda:0')\n",
      "Here, trace is 2.0 and matrix sum is 64.0 \n",
      "this is past correction for weight mode\n",
      "Shape of aligned wt is  torch.Size([64, 3, 9])\n",
      "Shape of fc_layer0_weight_data is  torch.Size([64, 3, 9])\n",
      "Previous layer shape is  torch.Size([64, 3, 3, 3])\n",
      "shape of layer: model 0 torch.Size([128, 64, 9])\n",
      "shape of layer: model 1 torch.Size([128, 64, 9])\n",
      "shape of previous transport map torch.Size([64, 64])\n",
      "Processing the coordinates to form ground_metric\n",
      "dont leave off the squaring of the ground metric\n",
      "returns a uniform measure of cardinality:  128\n",
      "returns a uniform measure of cardinality:  128\n",
      "the transport map is  tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        ...,\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0078],\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],\n",
      "       device='cuda:0')\n",
      "Ratio of trace to the matrix sum:  tensor(0., device='cuda:0')\n",
      "Here, trace is 0.0 and matrix sum is 128.0 \n",
      "this is past correction for weight mode\n",
      "Shape of aligned wt is  torch.Size([128, 64, 9])\n",
      "Shape of fc_layer0_weight_data is  torch.Size([128, 64, 9])\n",
      "Previous layer shape is  torch.Size([128, 64, 3, 3])\n",
      "shape of layer: model 0 torch.Size([256, 128, 9])\n",
      "shape of layer: model 1 torch.Size([256, 128, 9])\n",
      "shape of previous transport map torch.Size([128, 128])\n",
      "Processing the coordinates to form ground_metric\n",
      "dont leave off the squaring of the ground metric\n",
      "returns a uniform measure of cardinality:  256\n",
      "returns a uniform measure of cardinality:  256\n",
      "the transport map is  tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')\n",
      "Ratio of trace to the matrix sum:  tensor(0., device='cuda:0')\n",
      "Here, trace is 0.0 and matrix sum is 256.0 \n",
      "this is past correction for weight mode\n",
      "Shape of aligned wt is  torch.Size([256, 128, 9])\n",
      "Shape of fc_layer0_weight_data is  torch.Size([256, 128, 9])\n",
      "Previous layer shape is  torch.Size([256, 128, 3, 3])\n",
      "shape of layer: model 0 torch.Size([256, 256, 9])\n",
      "shape of layer: model 1 torch.Size([256, 256, 9])\n",
      "shape of previous transport map torch.Size([256, 256])\n",
      "Processing the coordinates to form ground_metric\n",
      "dont leave off the squaring of the ground metric\n",
      "returns a uniform measure of cardinality:  256\n",
      "returns a uniform measure of cardinality:  256\n",
      "the transport map is  tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        ...,\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.0039, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],\n",
      "       device='cuda:0')\n",
      "Ratio of trace to the matrix sum:  tensor(0.0078, device='cuda:0')\n",
      "Here, trace is 2.0 and matrix sum is 256.0 \n",
      "this is past correction for weight mode\n",
      "Shape of aligned wt is  torch.Size([256, 256, 9])\n",
      "Shape of fc_layer0_weight_data is  torch.Size([256, 256, 9])\n",
      "Previous layer shape is  torch.Size([256, 256, 3, 3])\n",
      "shape of layer: model 0 torch.Size([512, 256, 9])\n",
      "shape of layer: model 1 torch.Size([512, 256, 9])\n",
      "shape of previous transport map torch.Size([256, 256])\n",
      "Processing the coordinates to form ground_metric\n",
      "dont leave off the squaring of the ground metric\n",
      "returns a uniform measure of cardinality:  512\n",
      "returns a uniform measure of cardinality:  512\n",
      "the transport map is  tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')\n",
      "Ratio of trace to the matrix sum:  tensor(0.0020, device='cuda:0')\n",
      "Here, trace is 1.0 and matrix sum is 512.0 \n",
      "this is past correction for weight mode\n",
      "Shape of aligned wt is  torch.Size([512, 256, 9])\n",
      "Shape of fc_layer0_weight_data is  torch.Size([512, 256, 9])\n",
      "Previous layer shape is  torch.Size([512, 256, 3, 3])\n",
      "shape of layer: model 0 torch.Size([512, 512, 9])\n",
      "shape of layer: model 1 torch.Size([512, 512, 9])\n",
      "shape of previous transport map torch.Size([512, 512])\n",
      "Processing the coordinates to form ground_metric\n",
      "dont leave off the squaring of the ground metric\n",
      "returns a uniform measure of cardinality:  512\n",
      "returns a uniform measure of cardinality:  512\n",
      "the transport map is  tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')\n",
      "Ratio of trace to the matrix sum:  tensor(0.0020, device='cuda:0')\n",
      "Here, trace is 1.0 and matrix sum is 512.0 \n",
      "this is past correction for weight mode\n",
      "Shape of aligned wt is  torch.Size([512, 512, 9])\n",
      "Shape of fc_layer0_weight_data is  torch.Size([512, 512, 9])\n",
      "Previous layer shape is  torch.Size([512, 512, 3, 3])\n",
      "shape of layer: model 0 torch.Size([512, 512, 9])\n",
      "shape of layer: model 1 torch.Size([512, 512, 9])\n",
      "shape of previous transport map torch.Size([512, 512])\n",
      "Processing the coordinates to form ground_metric\n",
      "dont leave off the squaring of the ground metric\n",
      "returns a uniform measure of cardinality:  512\n",
      "returns a uniform measure of cardinality:  512\n",
      "the transport map is  tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')\n",
      "Ratio of trace to the matrix sum:  tensor(0.0020, device='cuda:0')\n",
      "Here, trace is 1.0 and matrix sum is 512.0 \n",
      "this is past correction for weight mode\n",
      "Shape of aligned wt is  torch.Size([512, 512, 9])\n",
      "Shape of fc_layer0_weight_data is  torch.Size([512, 512, 9])\n",
      "Previous layer shape is  torch.Size([512, 512, 3, 3])\n",
      "shape of layer: model 0 torch.Size([512, 512, 9])\n",
      "shape of layer: model 1 torch.Size([512, 512, 9])\n",
      "shape of previous transport map torch.Size([512, 512])\n",
      "Processing the coordinates to form ground_metric\n",
      "dont leave off the squaring of the ground metric\n",
      "returns a uniform measure of cardinality:  512\n",
      "returns a uniform measure of cardinality:  512\n",
      "the transport map is  tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        [0.0020, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        ...,\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],\n",
      "       device='cuda:0')\n",
      "Ratio of trace to the matrix sum:  tensor(0., device='cuda:0')\n",
      "Here, trace is 0.0 and matrix sum is 512.0 \n",
      "this is past correction for weight mode\n",
      "Shape of aligned wt is  torch.Size([512, 512, 9])\n",
      "Shape of fc_layer0_weight_data is  torch.Size([512, 512, 9])\n",
      "Previous layer shape is  torch.Size([512, 512, 3, 3])\n",
      "shape of layer: model 0 torch.Size([10, 512])\n",
      "shape of layer: model 1 torch.Size([10, 512])\n",
      "shape of previous transport map torch.Size([512, 512])\n",
      "Processing the coordinates to form ground_metric\n",
      "dont leave off the squaring of the ground metric\n",
      "ground metric is  tensor([[1.4642, 2.5078, 2.8024, 2.8813, 2.6926, 2.8742, 2.7234, 2.6453, 2.5786,\n",
      "         2.6029],\n",
      "        [2.4737, 1.3481, 2.6841, 2.5886, 2.7753, 2.5260, 2.4259, 2.5360, 2.3173,\n",
      "         2.4198],\n",
      "        [2.7759, 2.6643, 1.3718, 2.9766, 2.9029, 2.8572, 2.7530, 2.8285, 2.7650,\n",
      "         2.7865],\n",
      "        [2.8403, 2.6432, 2.9049, 1.5382, 2.8522, 2.9644, 2.7427, 2.8368, 2.8237,\n",
      "         2.6608],\n",
      "        [2.7739, 2.5339, 2.8855, 2.8890, 1.5432, 2.7461, 2.7694, 2.6744, 2.7002,\n",
      "         2.7140],\n",
      "        [2.9384, 2.5983, 2.9602, 2.8103, 2.7634, 1.5249, 2.7733, 2.6859, 2.8210,\n",
      "         2.6734],\n",
      "        [2.6858, 2.4528, 2.6877, 2.9081, 2.6625, 2.8541, 1.4164, 2.7240, 2.4745,\n",
      "         2.5564],\n",
      "        [2.7176, 2.5470, 2.7543, 2.7844, 2.7381, 2.7291, 2.6847, 1.4193, 2.5482,\n",
      "         2.5109],\n",
      "        [2.5693, 2.3897, 2.7881, 2.7297, 2.7001, 2.8155, 2.5450, 2.6450, 1.2811,\n",
      "         2.3769],\n",
      "        [2.5952, 2.2636, 2.7947, 2.7496, 2.6754, 2.7221, 2.5690, 2.5120, 2.5170,\n",
      "         1.2983]], device='cuda:0', grad_fn=<PowBackward0>)\n",
      "returns a uniform measure of cardinality:  10\n",
      "returns a uniform measure of cardinality:  10\n",
      "the transport map is  tensor([[0.1000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0000, 0.1000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0000, 0.0000, 0.1000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.1000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.1000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.1000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.1000, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.1000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.1000,\n",
      "         0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.1000]], device='cuda:0')\n",
      "Ratio of trace to the matrix sum:  tensor(1., device='cuda:0')\n",
      "Here, trace is 9.99755859375 and matrix sum is 9.99755859375 \n",
      "this is past correction for weight mode\n",
      "Shape of aligned wt is  torch.Size([10, 512])\n",
      "Shape of fc_layer0_weight_data is  torch.Size([10, 512])\n",
      "using independent method\n",
      "in _make_layers [Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), AvgPool2d(kernel_size=1, stride=1, padding=0)]\n",
      "Relu Inplace is  False\n",
      "model parameters are \n",
      " [torch.Size([64, 3, 3, 3]), torch.Size([128, 64, 3, 3]), torch.Size([256, 128, 3, 3]), torch.Size([256, 256, 3, 3]), torch.Size([512, 256, 3, 3]), torch.Size([512, 512, 3, 3]), torch.Size([512, 512, 3, 3]), torch.Size([512, 512, 3, 3]), torch.Size([10, 512])]\n",
      "\n",
      "--------- Testing in global mode ---------\n",
      "size of test_loader dataset:  10000\n",
      "\n",
      "Test set: Avg. loss: 0.0023, Accuracy: 1000/10000 (10%)\n",
      "\n",
      "len of model parameters and avg aligned layers is  9 9\n",
      "len of model_state_dict is  9\n",
      "len of param_list is  9\n",
      "\n",
      "--------- Testing in global mode ---------\n",
      "size of test_loader dataset:  10000\n",
      "\n",
      "Test set: Avg. loss: 0.0013, Accuracy: 8612/10000 (86%)\n",
      "\n",
      "===============================================\n",
      "===============================================\n",
      "After Ensemble,acc: 86.12 loss 0.0012734648704528808\n",
      "===============================================\n",
      "===============================================\n",
      "Timer ends\n",
      "Time taken for geometric ensembling is 3.4202709458768368 seconds\n",
      "------- Prediction based ensembling -------\n",
      "\n",
      "Test set: Avg. loss: 0.0003, Accuracy: 9109/10000 (91%)\n",
      "\n",
      "------- Naive ensembling of weights -------\n",
      "[torch.Size([64, 3, 3, 3]), torch.Size([64, 3, 3, 3])]\n",
      "torch.Size([64, 3, 3, 3])\n",
      "[torch.Size([128, 64, 3, 3]), torch.Size([128, 64, 3, 3])]\n",
      "torch.Size([128, 64, 3, 3])\n",
      "[torch.Size([256, 128, 3, 3]), torch.Size([256, 128, 3, 3])]\n",
      "torch.Size([256, 128, 3, 3])\n",
      "[torch.Size([256, 256, 3, 3]), torch.Size([256, 256, 3, 3])]\n",
      "torch.Size([256, 256, 3, 3])\n",
      "[torch.Size([512, 256, 3, 3]), torch.Size([512, 256, 3, 3])]\n",
      "torch.Size([512, 256, 3, 3])\n",
      "[torch.Size([512, 512, 3, 3]), torch.Size([512, 512, 3, 3])]\n",
      "torch.Size([512, 512, 3, 3])\n",
      "[torch.Size([512, 512, 3, 3]), torch.Size([512, 512, 3, 3])]\n",
      "torch.Size([512, 512, 3, 3])\n",
      "[torch.Size([512, 512, 3, 3]), torch.Size([512, 512, 3, 3])]\n",
      "torch.Size([512, 512, 3, 3])\n",
      "[torch.Size([10, 512]), torch.Size([10, 512])]\n",
      "torch.Size([10, 512])\n",
      "in _make_layers [Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), AvgPool2d(kernel_size=1, stride=1, padding=0)]\n",
      "Relu Inplace is  False\n",
      "model parameters are \n",
      " [torch.Size([64, 3, 3, 3]), torch.Size([128, 64, 3, 3]), torch.Size([256, 128, 3, 3]), torch.Size([256, 256, 3, 3]), torch.Size([512, 256, 3, 3]), torch.Size([512, 512, 3, 3]), torch.Size([512, 512, 3, 3]), torch.Size([512, 512, 3, 3]), torch.Size([10, 512])]\n",
      "\n",
      "--------- Testing in global mode ---------\n",
      "size of test_loader dataset:  10000\n",
      "\n",
      "Test set: Avg. loss: 0.0023, Accuracy: 904/10000 (9%)\n",
      "\n",
      "\n",
      "--------- Testing in global mode ---------\n",
      "size of test_loader dataset:  10000\n",
      "\n",
      "Test set: Avg. loss: 0.0023, Accuracy: 1484/10000 (15%)\n",
      "\n",
      "-------- Retraining the models ---------\n",
      "Retraining model :  model_0\n",
      "num_epochs---------------+++))))))+++ 300\n",
      "num_epochs---------------+++))))))+++ 300\n",
      "lr is  0.05\n",
      "number of epochs would be  30\n",
      "num_epochs-------------- 30\n",
      "Epoch 000\n",
      "/root/miniconda3/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:129: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\"Detected call of `lr_scheduler.step()` before `optimizer.step()`. \"\n",
      "/root/miniconda3/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:154: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "accuracy: {'epoch': 0, 'value': 0.9034600000572204} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 0, 'value': 0.2861940524578096} ({'split': 'train'})\n",
      "accuracy: {'epoch': 0, 'value': 0.8322000443935393} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 0, 'value': 0.5332091838121413} ({'split': 'test'})\n",
      "Epoch 001\n",
      "accuracy: {'epoch': 1, 'value': 0.9306000000381469} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 1, 'value': 0.2062341377162933} ({'split': 'train'})\n",
      "accuracy: {'epoch': 1, 'value': 0.8257000327110291} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 1, 'value': 0.5655232071876526} ({'split': 'test'})\n",
      "Epoch 002\n",
      "accuracy: {'epoch': 2, 'value': 0.9523600000572203} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 2, 'value': 0.14104558134317383} ({'split': 'train'})\n",
      "accuracy: {'epoch': 2, 'value': 0.8502000451087952} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 2, 'value': 0.5423282623291016} ({'split': 'test'})\n",
      "Epoch 003\n",
      "accuracy: {'epoch': 3, 'value': 0.9609799999809265} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 3, 'value': 0.11707838998794556} ({'split': 'train'})\n",
      "accuracy: {'epoch': 3, 'value': 0.8652000367641448} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 3, 'value': 0.47779696583747866} ({'split': 'test'})\n",
      "Epoch 004\n",
      "accuracy: {'epoch': 4, 'value': 0.9676200000190737} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 4, 'value': 0.09817684835135938} ({'split': 'train'})\n",
      "accuracy: {'epoch': 4, 'value': 0.8578000426292419} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 4, 'value': 0.529752019047737} ({'split': 'test'})\n",
      "Epoch 005\n",
      "accuracy: {'epoch': 5, 'value': 0.9767200000000005} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 5, 'value': 0.07055917090415957} ({'split': 'train'})\n",
      "accuracy: {'epoch': 5, 'value': 0.8577000379562378} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 5, 'value': 0.5138664931058884} ({'split': 'test'})\n",
      "Epoch 006\n",
      "accuracy: {'epoch': 6, 'value': 0.9802000000381472} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 6, 'value': 0.060092167692184444} ({'split': 'train'})\n",
      "accuracy: {'epoch': 6, 'value': 0.8554000437259675} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 6, 'value': 0.5283318877220153} ({'split': 'test'})\n",
      "Epoch 007\n",
      "accuracy: {'epoch': 7, 'value': 0.9822799999999999} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 7, 'value': 0.05610418575644491} ({'split': 'train'})\n",
      "accuracy: {'epoch': 7, 'value': 0.8531000435352326} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 7, 'value': 0.5842208325862884} ({'split': 'test'})\n",
      "Epoch 008\n",
      "accuracy: {'epoch': 8, 'value': 0.983120000019074} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 8, 'value': 0.052588160436153414} ({'split': 'train'})\n",
      "accuracy: {'epoch': 8, 'value': 0.8552000403404236} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 8, 'value': 0.5693721771240234} ({'split': 'test'})\n",
      "Epoch 009\n",
      "accuracy: {'epoch': 9, 'value': 0.98232} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 9, 'value': 0.0534888467204571} ({'split': 'train'})\n",
      "accuracy: {'epoch': 9, 'value': 0.8543000340461732} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 9, 'value': 0.5928713917732239} ({'split': 'test'})\n",
      "Epoch 010\n",
      "accuracy: {'epoch': 10, 'value': 0.9836599999999999} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 10, 'value': 0.052074470153450944} ({'split': 'train'})\n",
      "accuracy: {'epoch': 10, 'value': 0.8490000307559967} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 10, 'value': 0.6220249533653259} ({'split': 'test'})\n",
      "Epoch 011\n",
      "accuracy: {'epoch': 11, 'value': 0.9865400000381471} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 11, 'value': 0.04145857124805447} ({'split': 'train'})\n",
      "accuracy: {'epoch': 11, 'value': 0.8530000388622283} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 11, 'value': 0.6340257287025451} ({'split': 'test'})\n",
      "Epoch 012\n",
      "accuracy: {'epoch': 12, 'value': 0.9846000000381474} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 12, 'value': 0.04755685933351517} ({'split': 'train'})\n",
      "accuracy: {'epoch': 12, 'value': 0.8610000431537629} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 12, 'value': 0.5898884028196335} ({'split': 'test'})\n",
      "Epoch 013\n",
      "accuracy: {'epoch': 13, 'value': 0.9834400000381467} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 13, 'value': 0.051918923590183276} ({'split': 'train'})\n",
      "accuracy: {'epoch': 13, 'value': 0.8544000387191772} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 13, 'value': 0.612633776664734} ({'split': 'test'})\n",
      "Epoch 014\n",
      "accuracy: {'epoch': 14, 'value': 0.9875199999999998} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 14, 'value': 0.040493200426697754} ({'split': 'train'})\n",
      "accuracy: {'epoch': 14, 'value': 0.8641000330448151} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 14, 'value': 0.5741070449352266} ({'split': 'test'})\n",
      "Epoch 015\n",
      "accuracy: {'epoch': 15, 'value': 0.9909000000190734} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 15, 'value': 0.031229211916923513} ({'split': 'train'})\n",
      "accuracy: {'epoch': 15, 'value': 0.863800036907196} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 15, 'value': 0.567335706949234} ({'split': 'test'})\n",
      "Epoch 016\n",
      "accuracy: {'epoch': 16, 'value': 0.9890599999999998} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 16, 'value': 0.03529059117794037} ({'split': 'train'})\n",
      "accuracy: {'epoch': 16, 'value': 0.8636000514030456} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 16, 'value': 0.5998180627822877} ({'split': 'test'})\n",
      "Epoch 017\n",
      "accuracy: {'epoch': 17, 'value': 0.9910400000190734} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 17, 'value': 0.029078066592216484} ({'split': 'train'})\n",
      "accuracy: {'epoch': 17, 'value': 0.8628000378608703} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 17, 'value': 0.5651193976402282} ({'split': 'test'})\n",
      "Epoch 018\n",
      "accuracy: {'epoch': 18, 'value': 0.9902799999999996} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 18, 'value': 0.03261485529094932} ({'split': 'train'})\n",
      "accuracy: {'epoch': 18, 'value': 0.8512000322341919} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 18, 'value': 0.5779894888401031} ({'split': 'test'})\n",
      "Epoch 019\n",
      "accuracy: {'epoch': 19, 'value': 0.9897599999999996} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 19, 'value': 0.033220017830133466} ({'split': 'train'})\n",
      "accuracy: {'epoch': 19, 'value': 0.8636000335216523} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 19, 'value': 0.5949508368968963} ({'split': 'test'})\n",
      "Epoch 020\n",
      "accuracy: {'epoch': 20, 'value': 0.9902600000190732} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 20, 'value': 0.032277221642136575} ({'split': 'train'})\n",
      "accuracy: {'epoch': 20, 'value': 0.8490000545978548} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 20, 'value': 0.63849396109581} ({'split': 'test'})\n",
      "Epoch 021\n",
      "accuracy: {'epoch': 21, 'value': 0.9889400000190736} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 21, 'value': 0.03574433617115021} ({'split': 'train'})\n",
      "accuracy: {'epoch': 21, 'value': 0.8530000448226929} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 21, 'value': 0.5930671751499176} ({'split': 'test'})\n",
      "Epoch 022\n",
      "accuracy: {'epoch': 22, 'value': 0.9909800000572199} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 22, 'value': 0.029097452056407944} ({'split': 'train'})\n",
      "accuracy: {'epoch': 22, 'value': 0.8498000383377077} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 22, 'value': 0.6541174232959747} ({'split': 'test'})\n",
      "Epoch 023\n",
      "accuracy: {'epoch': 23, 'value': 0.9828600000381472} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 23, 'value': 0.05637620221137999} ({'split': 'train'})\n",
      "accuracy: {'epoch': 23, 'value': 0.8485000371932983} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 23, 'value': 0.5877056181430816} ({'split': 'test'})\n",
      "Epoch 024\n",
      "accuracy: {'epoch': 24, 'value': 0.9830600000190741} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 24, 'value': 0.052264558696746836} ({'split': 'train'})\n",
      "accuracy: {'epoch': 24, 'value': 0.8469000339508057} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 24, 'value': 0.6026497989892959} ({'split': 'test'})\n",
      "Epoch 025\n",
      "accuracy: {'epoch': 25, 'value': 0.9856600000572203} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 25, 'value': 0.04563972810983657} ({'split': 'train'})\n",
      "accuracy: {'epoch': 25, 'value': 0.8545000433921814} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 25, 'value': 0.6201868414878845} ({'split': 'test'})\n",
      "Epoch 026\n",
      "accuracy: {'epoch': 26, 'value': 0.9840400000381471} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 26, 'value': 0.050633746323585506} ({'split': 'train'})\n",
      "accuracy: {'epoch': 26, 'value': 0.8382000386714935} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 26, 'value': 0.6363924384117127} ({'split': 'test'})\n",
      "Epoch 027\n",
      "accuracy: {'epoch': 27, 'value': 0.9850600000572204} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 27, 'value': 0.04701965092658998} ({'split': 'train'})\n",
      "accuracy: {'epoch': 27, 'value': 0.8363000452518463} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 27, 'value': 0.6974831700325013} ({'split': 'test'})\n",
      "Epoch 028\n",
      "accuracy: {'epoch': 28, 'value': 0.9878200000381466} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 28, 'value': 0.038004105551242826} ({'split': 'train'})\n",
      "accuracy: {'epoch': 28, 'value': 0.8417000353336334} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 28, 'value': 0.6688791453838349} ({'split': 'test'})\n",
      "Epoch 029\n",
      "accuracy: {'epoch': 29, 'value': 0.9888400000190735} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 29, 'value': 0.03589627307295798} ({'split': 'train'})\n",
      "accuracy: {'epoch': 29, 'value': 0.8550000429153443} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 29, 'value': 0.601151192188263} ({'split': 'test'})\n",
      "mean_test_accuracy <cifar_utils.accumulators.Mean object at 0x7f0f02f1a7f0>\n",
      "QWERTY: Enter Cifar 2, acc 0.8652000367641448\n",
      "Retraining model :  model_1\n",
      "num_epochs---------------+++))))))+++ 30\n",
      "num_epochs---------------+++))))))+++ 30\n",
      "lr is  0.05\n",
      "number of epochs would be  30\n",
      "num_epochs-------------- 30\n",
      "Epoch 000\n",
      "accuracy: {'epoch': 0, 'value': 0.8965200000190733} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 0, 'value': 0.30901001655578586} ({'split': 'train'})\n",
      "accuracy: {'epoch': 0, 'value': 0.8347000420093537} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 0, 'value': 0.5112851291894912} ({'split': 'test'})\n",
      "Epoch 001\n",
      "accuracy: {'epoch': 1, 'value': 0.9284200000572203} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 1, 'value': 0.21400561015129083} ({'split': 'train'})\n",
      "accuracy: {'epoch': 1, 'value': 0.8544000387191772} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 1, 'value': 0.46467748284339905} ({'split': 'test'})\n",
      "Epoch 002\n",
      "accuracy: {'epoch': 2, 'value': 0.9453000000381468} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 2, 'value': 0.1634138419103623} ({'split': 'train'})\n",
      "accuracy: {'epoch': 2, 'value': 0.848300039768219} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 2, 'value': 0.5638387441635132} ({'split': 'test'})\n",
      "Epoch 003\n",
      "accuracy: {'epoch': 3, 'value': 0.9629800000381472} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 3, 'value': 0.10882959683418271} ({'split': 'train'})\n",
      "accuracy: {'epoch': 3, 'value': 0.8508000433444977} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 3, 'value': 0.5663427859544754} ({'split': 'test'})\n",
      "Epoch 004\n",
      "accuracy: {'epoch': 4, 'value': 0.9711400000190733} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 4, 'value': 0.08968882656931876} ({'split': 'train'})\n",
      "accuracy: {'epoch': 4, 'value': 0.853300040960312} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 4, 'value': 0.5653102934360503} ({'split': 'test'})\n",
      "Epoch 005\n",
      "accuracy: {'epoch': 5, 'value': 0.9720599999809271} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 5, 'value': 0.08425876945495607} ({'split': 'train'})\n",
      "accuracy: {'epoch': 5, 'value': 0.8591000378131867} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 5, 'value': 0.5364825159311295} ({'split': 'test'})\n",
      "Epoch 006\n",
      "accuracy: {'epoch': 6, 'value': 0.9792600000190732} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 6, 'value': 0.06493564519166943} ({'split': 'train'})\n",
      "accuracy: {'epoch': 6, 'value': 0.8645000398159027} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 6, 'value': 0.5012237340211869} ({'split': 'test'})\n",
      "Epoch 007\n",
      "accuracy: {'epoch': 7, 'value': 0.9809199999999998} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 7, 'value': 0.05560121849060058} ({'split': 'train'})\n",
      "accuracy: {'epoch': 7, 'value': 0.8651000320911407} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 7, 'value': 0.5313334137201309} ({'split': 'test'})\n",
      "Epoch 008\n",
      "accuracy: {'epoch': 8, 'value': 0.9825800000190739} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 8, 'value': 0.05460666907072066} ({'split': 'train'})\n",
      "accuracy: {'epoch': 8, 'value': 0.8538000404834748} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 8, 'value': 0.5830058515071869} ({'split': 'test'})\n",
      "Epoch 009\n",
      "accuracy: {'epoch': 9, 'value': 0.9843000000572205} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 9, 'value': 0.050371813454627996} ({'split': 'train'})\n",
      "accuracy: {'epoch': 9, 'value': 0.8455000340938568} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 9, 'value': 0.6305307984352112} ({'split': 'test'})\n",
      "Epoch 010\n",
      "accuracy: {'epoch': 10, 'value': 0.984200000038147} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 10, 'value': 0.049891167089939126} ({'split': 'train'})\n",
      "accuracy: {'epoch': 10, 'value': 0.8592000424861908} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 10, 'value': 0.5381639957427978} ({'split': 'test'})\n",
      "Epoch 011\n",
      "accuracy: {'epoch': 11, 'value': 0.9878399999999998} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 11, 'value': 0.0376664188438654} ({'split': 'train'})\n",
      "accuracy: {'epoch': 11, 'value': 0.849800032377243} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 11, 'value': 0.6446761429309844} ({'split': 'test'})\n",
      "Epoch 012\n",
      "accuracy: {'epoch': 12, 'value': 0.98868} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 12, 'value': 0.035790787287056446} ({'split': 'train'})\n",
      "accuracy: {'epoch': 12, 'value': 0.8660000503063202} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 12, 'value': 0.5660721182823181} ({'split': 'test'})\n",
      "Epoch 013\n",
      "accuracy: {'epoch': 13, 'value': 0.9910400000190734} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 13, 'value': 0.028924685460329065} ({'split': 'train'})\n",
      "accuracy: {'epoch': 13, 'value': 0.8530000269412994} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 13, 'value': 0.5974088788032532} ({'split': 'test'})\n",
      "Epoch 014\n",
      "accuracy: {'epoch': 14, 'value': 0.987380000019074} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 14, 'value': 0.04089335276126862} ({'split': 'train'})\n",
      "accuracy: {'epoch': 14, 'value': 0.855500054359436} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 14, 'value': 0.5753581047058105} ({'split': 'test'})\n",
      "Epoch 015\n",
      "accuracy: {'epoch': 15, 'value': 0.9881200000572208} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 15, 'value': 0.03680537763357163} ({'split': 'train'})\n",
      "accuracy: {'epoch': 15, 'value': 0.8480000317096711} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 15, 'value': 0.6610167264938355} ({'split': 'test'})\n",
      "Epoch 016\n",
      "accuracy: {'epoch': 16, 'value': 0.9880800000190729} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 16, 'value': 0.03927777709126474} ({'split': 'train'})\n",
      "accuracy: {'epoch': 16, 'value': 0.8593000292778016} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 16, 'value': 0.6063423871994019} ({'split': 'test'})\n",
      "Epoch 017\n",
      "accuracy: {'epoch': 17, 'value': 0.9938400000000002} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 17, 'value': 0.022680407299101334} ({'split': 'train'})\n",
      "accuracy: {'epoch': 17, 'value': 0.866400045156479} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 17, 'value': 0.5770237833261489} ({'split': 'test'})\n",
      "Epoch 018\n",
      "accuracy: {'epoch': 18, 'value': 0.9908399999999998} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 18, 'value': 0.03030792966842653} ({'split': 'train'})\n",
      "accuracy: {'epoch': 18, 'value': 0.8639000415802002} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 18, 'value': 0.54256611764431} ({'split': 'test'})\n",
      "Epoch 019\n",
      "accuracy: {'epoch': 19, 'value': 0.9885400000190737} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 19, 'value': 0.03512513783454897} ({'split': 'train'})\n",
      "accuracy: {'epoch': 19, 'value': 0.8491000354290008} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 19, 'value': 0.5734287083148957} ({'split': 'test'})\n",
      "Epoch 020\n",
      "accuracy: {'epoch': 20, 'value': 0.9880399999999999} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 20, 'value': 0.03834976524293426} ({'split': 'train'})\n",
      "accuracy: {'epoch': 20, 'value': 0.8681000351905823} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 20, 'value': 0.5545712292194367} ({'split': 'test'})\n",
      "Epoch 021\n",
      "accuracy: {'epoch': 21, 'value': 0.9871000000190734} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 21, 'value': 0.04154652908086776} ({'split': 'train'})\n",
      "accuracy: {'epoch': 21, 'value': 0.8593000411987304} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 21, 'value': 0.5932292759418487} ({'split': 'test'})\n",
      "Epoch 022\n",
      "accuracy: {'epoch': 22, 'value': 0.9869800000190738} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 22, 'value': 0.04131829721689226} ({'split': 'train'})\n",
      "accuracy: {'epoch': 22, 'value': 0.8626000463962554} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 22, 'value': 0.5883871555328369} ({'split': 'test'})\n",
      "Epoch 023\n",
      "accuracy: {'epoch': 23, 'value': 0.9885800000190733} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 23, 'value': 0.038162579710483584} ({'split': 'train'})\n",
      "accuracy: {'epoch': 23, 'value': 0.8568000495433807} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 23, 'value': 0.5593375742435456} ({'split': 'test'})\n",
      "Epoch 024\n",
      "accuracy: {'epoch': 24, 'value': 0.9871800000381468} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 24, 'value': 0.040670193493366265} ({'split': 'train'})\n",
      "accuracy: {'epoch': 24, 'value': 0.8512000441551208} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 24, 'value': 0.6475919246673585} ({'split': 'test'})\n",
      "Epoch 025\n",
      "accuracy: {'epoch': 25, 'value': 0.9851400000190736} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 25, 'value': 0.04787382166624068} ({'split': 'train'})\n",
      "accuracy: {'epoch': 25, 'value': 0.8451000392436981} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 25, 'value': 0.6508484005928039} ({'split': 'test'})\n",
      "Epoch 026\n",
      "accuracy: {'epoch': 26, 'value': 0.9875400000572202} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 26, 'value': 0.038936395449638395} ({'split': 'train'})\n",
      "accuracy: {'epoch': 26, 'value': 0.8532000482082367} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 26, 'value': 0.5802248805761338} ({'split': 'test'})\n",
      "Epoch 027\n",
      "accuracy: {'epoch': 27, 'value': 0.9862200000000001} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 27, 'value': 0.043081625930666924} ({'split': 'train'})\n",
      "accuracy: {'epoch': 27, 'value': 0.8517000377178192} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 27, 'value': 0.6305249989032745} ({'split': 'test'})\n",
      "Epoch 028\n",
      "accuracy: {'epoch': 28, 'value': 0.9881800000000001} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 28, 'value': 0.03901504044115543} ({'split': 'train'})\n",
      "accuracy: {'epoch': 28, 'value': 0.8475000381469726} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 28, 'value': 0.632063913345337} ({'split': 'test'})\n",
      "Epoch 029\n",
      "accuracy: {'epoch': 29, 'value': 0.9893799999999999} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 29, 'value': 0.03574592933416366} ({'split': 'train'})\n",
      "accuracy: {'epoch': 29, 'value': 0.8612000465393066} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 29, 'value': 0.5977804124355316} ({'split': 'test'})\n",
      "mean_test_accuracy <cifar_utils.accumulators.Mean object at 0x7f0f02ef9430>\n",
      "QWERTY: Enter Cifar 2, acc 0.8681000351905823\n",
      "Retraining model :  geometric\n",
      "num_epochs---------------+++))))))+++ 30\n",
      "num_epochs---------------+++))))))+++ 30\n",
      "lr is  0.05\n",
      "number of epochs would be  30\n",
      "num_epochs-------------- 30\n",
      "Epoch 000\n",
      "accuracy: {'epoch': 0, 'value': 0.12818000000476837} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 0, 'value': 2.2704399839782714} ({'split': 'train'})\n",
      "accuracy: {'epoch': 0, 'value': 0.10000000447034836} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 0, 'value': 2.3025848865509033} ({'split': 'test'})\n",
      "Epoch 001\n",
      "accuracy: {'epoch': 1, 'value': 0.10000000000953674} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 1, 'value': 2.3025848854064943} ({'split': 'train'})\n",
      "accuracy: {'epoch': 1, 'value': 0.10000000521540642} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 1, 'value': 2.3025848865509033} ({'split': 'test'})\n",
      "Epoch 002\n",
      "accuracy: {'epoch': 2, 'value': 0.09999999999999999} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 2, 'value': 2.3025848854064943} ({'split': 'train'})\n",
      "accuracy: {'epoch': 2, 'value': 0.10000000447034835} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 2, 'value': 2.3025848865509033} ({'split': 'test'})\n",
      "Epoch 003\n",
      "accuracy: {'epoch': 3, 'value': 0.1000000000023842} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 3, 'value': 2.3025848854064943} ({'split': 'train'})\n",
      "accuracy: {'epoch': 3, 'value': 0.1000000037252903} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 3, 'value': 2.3025848865509033} ({'split': 'test'})\n",
      "Epoch 004\n",
      "accuracy: {'epoch': 4, 'value': 0.10000000000238415} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 4, 'value': 2.3025848854064943} ({'split': 'train'})\n",
      "accuracy: {'epoch': 4, 'value': 0.1000000037252903} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 4, 'value': 2.3025848865509033} ({'split': 'test'})\n",
      "Epoch 005\n",
      "accuracy: {'epoch': 5, 'value': 0.10000000000476833} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 5, 'value': 2.3025848854064943} ({'split': 'train'})\n",
      "accuracy: {'epoch': 5, 'value': 0.10000000447034836} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 5, 'value': 2.3025848865509033} ({'split': 'test'})\n",
      "Epoch 006\n",
      "accuracy: {'epoch': 6, 'value': 0.09999999999761584} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 6, 'value': 2.3025848854064943} ({'split': 'train'})\n",
      "accuracy: {'epoch': 6, 'value': 0.10000000447034835} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 6, 'value': 2.3025848865509033} ({'split': 'test'})\n",
      "Epoch 007\n",
      "^C\n",
      "Traceback (most recent call last):\n",
      "  File \"main.py\", line 458, in <module>\n",
      "    _, best_retrain_acc = routines.retrain_models(args, [*original_models, geometric_model, naive_model], retrain_loader, test_loader, config, tensorboard_obj=tensorboard_obj, initial_acc=initial_acc, nicks=nicks)\n",
      "  File \"/root/autodl-tmp/OTfusion_pruning/otfusion/routines.py\", line 394, in retrain_models\n",
      "    retrained_network, acc = cifar_train.get_retrained_model(args, retrain_loader, test_loader, old_networks[i], config, output_root_dir, tensorboard_obj=tensorboard_obj, nick=nick, start_acc=initial_acc[i])\n",
      "  File \"/root/autodl-tmp/OTfusion_pruning/otfusion/./cifar/train.py\", line 333, in get_retrained_model\n",
      "    best_acc,model = main(config, output_dir, args.gpu_id, pretrained_model=old_network, pretrained_dataset=(train_loader, test_loader), tensorboard_obj=tensorboard_obj,return_model=True)\n",
      "  File \"/root/autodl-tmp/OTfusion_pruning/otfusion/./cifar/train.py\", line 81, in main\n",
      "    mean_train_loss.add(loss.item(), weight=len(batch_x))\n",
      "KeyboardInterrupt\n"
     ]
    }
   ],
   "source": [
    "!python main.py --gpu-id 0 --model-name vgg11_nobias --n-epochs 5 --save-result-file sample.csv \\\n",
    "--sweep-name exp_sample --correction --ground-metric euclidean --weight-stats \\\n",
    "--geom-ensemble-type wts --ground-metric-normalize none --sweep-id 90 \\\n",
    "--ckpt-type best --dataset Cifar10 --ground-metric-eff --recheck-cifar --activation-seed {activation_seed} \\\n",
    "--prelu-acts --past-correction --not-squared --exact\\\n",
    "--learning-rate 0.0001\\\n",
    "--momentum 0.5\\\n",
    "--batch-size-train 256 --experiment-dir {experiment_dir}\\\n",
    "--prunint-rate {prunint_rate}\\\n",
    "--run-mode {run_mode}\\\n",
    "--to-download\\\n",
    "--load-model-dir {load_model_dir}\\\n",
    "--retrain 30\\\n",
    "--prune_times 6\n",
    "# 86.12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31fbce8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#剪枝实验3:prune times without finetune\n",
    "activation_seed=21\n",
    "experiment_dir=\"experiment1.5\"\n",
    "prunint_rate=0.3\n",
    "run_mode='prune_times'\n",
    "load_model_dir='./cifar_models'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2aa57e2f-73c5-4a0d-8bac-daa1ce10e108",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python main.py --gpu-id 0 --model-name vgg11_nobias --n-epochs 10 --save-result-file sample.csv \\\n",
    "--sweep-name exp_sample --correction --ground-metric euclidean --weight-stats \\\n",
    "--geom-ensemble-type wts --ground-metric-normalize none --sweep-id 90 \\\n",
    "--ckpt-type best --dataset Cifar10 --ground-metric-eff --recheck-cifar --activation-seed {activation_seed} \\\n",
    "--prelu-acts --past-correction --not-squared --exact\\\n",
    "--learning-rate 0.0001\\\n",
    "--momentum 0.5\\\n",
    "--batch-size-train 256 --experiment-dir {experiment_dir}\\\n",
    "--prunint-rate {prunint_rate}\\\n",
    "--run-mode {run_mode}\\\n",
    "--to-download\\\n",
    "--load-model-dir {load_model_dir}\\\n",
    "--retrain 30\\\n",
    "--prune_times 3\n",
    "# 85.91"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6dd3f2b3-a28e-48de-9734-3c48c3d6514d",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------- Setting up parameters -------\n",
      "dumping parameters at  /root/autodl-tmp/OTfusion_pruning/otfusion/exp_sample/configurations\n",
      "The parameters are: \n",
      " Namespace(act_bug=False, act_num_samples=100, activation_histograms=False, activation_mode=None, activation_seed=21, alpha=0.7, baseroot='/root/autodl-tmp/OTfusion_pruning/otfusion', batch_size_test=1000, batch_size_train=256, center_acts=False, choice='0 2 4 6 8', cifar_style_data=False, ckpt_type='best', clip_gm=False, clip_max=5, clip_min=0, config_dir='/root/autodl-tmp/OTfusion_pruning/otfusion/exp_sample/configurations', config_file=None, correction=True, csv_dir='/root/autodl-tmp/OTfusion_pruning/otfusion/exp_sample/csv', dataset='Cifar10', debug=False, deprecated=None, deterministic=False, diff_init=False, disable_bias=True, dist_epochs=60, dist_normalize=False, dump_final_models=False, dump_model=False, enable_dropout=False, ensemble_step=0.5, eval_aligned=False, exact=True, exp_name='exp_2023-09-16_17-12-35_115737', experiment_dir='experiment2', geom_ensemble_type='wts', gpu_id=0, gromov=False, gromov_loss='square_loss', ground_metric='euclidean', ground_metric_eff=True, ground_metric_normalize='none', handle_skips=False, importance=None, learning_rate=0.0001, load_model_dir='./cifar_models', load_models='', log_interval=100, model_name='vgg11_nobias', momentum=0.5, n_epochs=10, no_random_trainloaders=False, normalize_acts=False, normalize_wts=False, not_squared=True, num_hidden_nodes=400, num_hidden_nodes1=400, num_hidden_nodes2=200, num_hidden_nodes3=100, num_hidden_nodes4=50, num_models=2, options_type='generic', partial_reshape=False, partition_dataloader=-1, partition_type='labels', past_correction=True, personal_class_idx=9, personal_split_frac=0.1, pool_acts=False, pool_relu=False, prediction_wts=False, prelu_acts=True, print_distances=False, proper_marginals=False, prunint_rate=0.2, recheck_acc=False, recheck_cifar=True, reg=0.01, reg_m=0.001, reinit_trainloaders=False, result_dir='/root/autodl-tmp/OTfusion_pruning/otfusion/exp_sample/results', retrain=30, retrain_avg_only=False, retrain_geometric_only=False, retrain_lr_decay=-1, retrain_lr_decay_epochs=None, retrain_lr_decay_factor=None, retrain_seed=-1, rootdir='/root/autodl-tmp/OTfusion_pruning/otfusion/exp_sample', run_mode='finetune', same_model=-1, save_result_file='sample.csv', second_model_name=None, sinkhorn_type='normal', skip_last_layer=False, skip_last_layer_type='average', skip_personal_idx=False, skip_retrain=-1, softmax_temperature=1, standardize_acts=False, sweep_id=90, sweep_name='exp_sample', temperature=20, tensorboard=False, tensorboard_root='./tensorboard', timestamp='2023-09-16_17-12-35_115737', tmap_stats=False, to_download=True, transform_acts=False, unbalanced=False, update_acts=False, weight_stats=True, width_ratio=1)\n",
      "refactored get_config\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "in _make_layers [Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), AvgPool2d(kernel_size=1, stride=1, padding=0)]\n",
      "Relu Inplace is  False\n",
      "model parameters are \n",
      " [torch.Size([64, 3, 3, 3]), torch.Size([128, 64, 3, 3]), torch.Size([256, 128, 3, 3]), torch.Size([256, 256, 3, 3]), torch.Size([512, 256, 3, 3]), torch.Size([512, 512, 3, 3]), torch.Size([512, 512, 3, 3]), torch.Size([512, 512, 3, 3]), torch.Size([10, 512])]\n",
      "in _make_layers [Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), AvgPool2d(kernel_size=1, stride=1, padding=0)]\n",
      "Relu Inplace is  False\n",
      "model parameters are \n",
      " [torch.Size([64, 3, 3, 3]), torch.Size([128, 64, 3, 3]), torch.Size([256, 128, 3, 3]), torch.Size([256, 256, 3, 3]), torch.Size([512, 256, 3, 3]), torch.Size([512, 512, 3, 3]), torch.Size([512, 512, 3, 3]), torch.Size([512, 512, 3, 3]), torch.Size([10, 512])]\n",
      "in _make_layers [Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), AvgPool2d(kernel_size=1, stride=1, padding=0)]\n",
      "Relu Inplace is  False\n",
      "model parameters are \n",
      " [torch.Size([64, 3, 3, 3]), torch.Size([128, 64, 3, 3]), torch.Size([256, 128, 3, 3]), torch.Size([256, 256, 3, 3]), torch.Size([512, 256, 3, 3]), torch.Size([512, 512, 3, 3]), torch.Size([512, 512, 3, 3]), torch.Size([512, 512, 3, 3]), torch.Size([10, 512])]\n",
      "in _make_layers [Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), AvgPool2d(kernel_size=1, stride=1, padding=0)]\n",
      "Relu Inplace is  False\n",
      "model parameters are \n",
      " [torch.Size([64, 3, 3, 3]), torch.Size([128, 64, 3, 3]), torch.Size([256, 128, 3, 3]), torch.Size([256, 256, 3, 3]), torch.Size([512, 256, 3, 3]), torch.Size([512, 512, 3, 3]), torch.Size([512, 512, 3, 3]), torch.Size([512, 512, 3, 3]), torch.Size([10, 512])]\n",
      "Train Epoch: 1 [0/50000 (0%)]\tLoss: 0.104261\n",
      "Train Epoch: 1 [25600/50000 (51%)]\tLoss: 0.096386\n",
      "\n",
      "--------- Testing in global mode ---------\n",
      "size of test_loader dataset:  10000\n",
      "\n",
      "Test set: Avg. loss: 0.0003, Accuracy: 9038/10000 (90%)\n",
      "\n",
      "Train Epoch: 2 [0/50000 (0%)]\tLoss: 0.073829\n",
      "Train Epoch: 2 [25600/50000 (51%)]\tLoss: 0.090221\n",
      "\n",
      "--------- Testing in global mode ---------\n",
      "size of test_loader dataset:  10000\n",
      "\n",
      "Test set: Avg. loss: 0.0003, Accuracy: 9043/10000 (90%)\n",
      "\n",
      "Train Epoch: 3 [0/50000 (0%)]\tLoss: 0.064929\n",
      "Train Epoch: 3 [25600/50000 (51%)]\tLoss: 0.059550\n",
      "\n",
      "--------- Testing in global mode ---------\n",
      "size of test_loader dataset:  10000\n",
      "\n",
      "Test set: Avg. loss: 0.0003, Accuracy: 9050/10000 (90%)\n",
      "\n",
      "Train Epoch: 4 [0/50000 (0%)]\tLoss: 0.071164\n",
      "Train Epoch: 4 [25600/50000 (51%)]\tLoss: 0.056062\n",
      "\n",
      "--------- Testing in global mode ---------\n",
      "size of test_loader dataset:  10000\n",
      "\n",
      "Test set: Avg. loss: 0.0003, Accuracy: 9048/10000 (90%)\n",
      "\n",
      "Train Epoch: 5 [0/50000 (0%)]\tLoss: 0.069778\n",
      "Train Epoch: 5 [25600/50000 (51%)]\tLoss: 0.068953\n",
      "\n",
      "--------- Testing in global mode ---------\n",
      "size of test_loader dataset:  10000\n",
      "\n",
      "Test set: Avg. loss: 0.0003, Accuracy: 9040/10000 (90%)\n",
      "\n",
      "Train Epoch: 6 [0/50000 (0%)]\tLoss: 0.064667\n",
      "Train Epoch: 6 [25600/50000 (51%)]\tLoss: 0.064324\n",
      "\n",
      "--------- Testing in global mode ---------\n",
      "size of test_loader dataset:  10000\n",
      "\n",
      "Test set: Avg. loss: 0.0003, Accuracy: 9040/10000 (90%)\n",
      "\n",
      "Train Epoch: 7 [0/50000 (0%)]\tLoss: 0.070104\n",
      "Train Epoch: 7 [25600/50000 (51%)]\tLoss: 0.074657\n",
      "\n",
      "--------- Testing in global mode ---------\n",
      "size of test_loader dataset:  10000\n",
      "\n",
      "Test set: Avg. loss: 0.0003, Accuracy: 9044/10000 (90%)\n",
      "\n",
      "Train Epoch: 8 [0/50000 (0%)]\tLoss: 0.056472\n",
      "Train Epoch: 8 [25600/50000 (51%)]\tLoss: 0.074212\n",
      "\n",
      "--------- Testing in global mode ---------\n",
      "size of test_loader dataset:  10000\n",
      "\n",
      "Test set: Avg. loss: 0.0003, Accuracy: 9047/10000 (90%)\n",
      "\n",
      "Train Epoch: 9 [0/50000 (0%)]\tLoss: 0.058916\n",
      "Train Epoch: 9 [25600/50000 (51%)]\tLoss: 0.066633\n",
      "\n",
      "--------- Testing in global mode ---------\n",
      "size of test_loader dataset:  10000\n",
      "\n",
      "Test set: Avg. loss: 0.0003, Accuracy: 9046/10000 (90%)\n",
      "\n",
      "Train Epoch: 10 [0/50000 (0%)]\tLoss: 0.062924\n",
      "Train Epoch: 10 [25600/50000 (51%)]\tLoss: 0.063471\n",
      "\n",
      "--------- Testing in global mode ---------\n",
      "size of test_loader dataset:  10000\n",
      "\n",
      "Test set: Avg. loss: 0.0003, Accuracy: 9044/10000 (90%)\n",
      "\n",
      "Train Epoch: 1 [0/50000 (0%)]\tLoss: 0.097751\n",
      "Train Epoch: 1 [25600/50000 (51%)]\tLoss: 0.080212\n",
      "\n",
      "--------- Testing in global mode ---------\n",
      "size of test_loader dataset:  10000\n",
      "\n",
      "Test set: Avg. loss: 0.0003, Accuracy: 9056/10000 (91%)\n",
      "\n",
      "Train Epoch: 2 [0/50000 (0%)]\tLoss: 0.072035\n",
      "Train Epoch: 2 [25600/50000 (51%)]\tLoss: 0.098448\n",
      "\n",
      "--------- Testing in global mode ---------\n",
      "size of test_loader dataset:  10000\n",
      "\n",
      "Test set: Avg. loss: 0.0003, Accuracy: 9065/10000 (91%)\n",
      "\n",
      "Train Epoch: 3 [0/50000 (0%)]\tLoss: 0.075295\n",
      "Train Epoch: 3 [25600/50000 (51%)]\tLoss: 0.076242\n",
      "\n",
      "--------- Testing in global mode ---------\n",
      "size of test_loader dataset:  10000\n",
      "\n",
      "Test set: Avg. loss: 0.0003, Accuracy: 9066/10000 (91%)\n",
      "\n",
      "Train Epoch: 4 [0/50000 (0%)]\tLoss: 0.089911\n",
      "Train Epoch: 4 [25600/50000 (51%)]\tLoss: 0.069176\n",
      "\n",
      "--------- Testing in global mode ---------\n",
      "size of test_loader dataset:  10000\n",
      "\n",
      "Test set: Avg. loss: 0.0003, Accuracy: 9068/10000 (91%)\n",
      "\n",
      "Train Epoch: 5 [0/50000 (0%)]\tLoss: 0.066074\n",
      "Train Epoch: 5 [25600/50000 (51%)]\tLoss: 0.093553\n",
      "\n",
      "--------- Testing in global mode ---------\n",
      "size of test_loader dataset:  10000\n",
      "\n",
      "Test set: Avg. loss: 0.0003, Accuracy: 9067/10000 (91%)\n",
      "\n",
      "Train Epoch: 6 [0/50000 (0%)]\tLoss: 0.065930\n",
      "Train Epoch: 6 [25600/50000 (51%)]\tLoss: 0.063445\n",
      "\n",
      "--------- Testing in global mode ---------\n",
      "size of test_loader dataset:  10000\n",
      "\n",
      "Test set: Avg. loss: 0.0003, Accuracy: 9068/10000 (91%)\n",
      "\n",
      "Train Epoch: 7 [0/50000 (0%)]\tLoss: 0.067796\n",
      "Train Epoch: 7 [25600/50000 (51%)]\tLoss: 0.056102\n",
      "\n",
      "--------- Testing in global mode ---------\n",
      "size of test_loader dataset:  10000\n",
      "\n",
      "Test set: Avg. loss: 0.0003, Accuracy: 9064/10000 (91%)\n",
      "\n",
      "Train Epoch: 8 [0/50000 (0%)]\tLoss: 0.053087\n",
      "Train Epoch: 8 [25600/50000 (51%)]\tLoss: 0.066485\n",
      "\n",
      "--------- Testing in global mode ---------\n",
      "size of test_loader dataset:  10000\n",
      "\n",
      "Test set: Avg. loss: 0.0003, Accuracy: 9067/10000 (91%)\n",
      "\n",
      "Train Epoch: 9 [0/50000 (0%)]\tLoss: 0.061531\n",
      "Train Epoch: 9 [25600/50000 (51%)]\tLoss: 0.062868\n",
      "\n",
      "--------- Testing in global mode ---------\n",
      "size of test_loader dataset:  10000\n",
      "\n",
      "Test set: Avg. loss: 0.0003, Accuracy: 9067/10000 (91%)\n",
      "\n",
      "Train Epoch: 10 [0/50000 (0%)]\tLoss: 0.057892\n",
      "Train Epoch: 10 [25600/50000 (51%)]\tLoss: 0.056442\n",
      "\n",
      "--------- Testing in global mode ---------\n",
      "size of test_loader dataset:  10000\n",
      "\n",
      "Test set: Avg. loss: 0.0003, Accuracy: 9058/10000 (91%)\n",
      "\n",
      "========================================================\n",
      "accuracies [90.44, 90.58]\n",
      "=========================================================\n",
      "layer features.0.weight_orig has #params  1728\n",
      "layer features.3.weight_orig has #params  73728\n",
      "layer features.6.weight_orig has #params  294912\n",
      "layer features.8.weight_orig has #params  589824\n",
      "layer features.11.weight_orig has #params  1179648\n",
      "layer features.13.weight_orig has #params  2359296\n",
      "layer features.16.weight_orig has #params  2359296\n",
      "layer features.18.weight_orig has #params  2359296\n",
      "layer classifier.weight has #params  5120\n",
      "Activation Timer start\n",
      "Activation Timer ends\n",
      "------- Geometric Ensembling -------\n",
      "Timer start\n",
      "Previous layer shape is  None\n",
      "Processing the coordinates to form ground_metric\n",
      "dont leave off the squaring of the ground metric\n",
      "returns a uniform measure of cardinality:  64\n",
      "returns a uniform measure of cardinality:  64\n",
      "the transport map is  tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')\n",
      "Ratio of trace to the matrix sum:  tensor(0.0469, device='cuda:0')\n",
      "Here, trace is 3.0 and matrix sum is 64.0 \n",
      "this is past correction for weight mode\n",
      "Shape of aligned wt is  torch.Size([64, 3, 9])\n",
      "Shape of fc_layer0_weight_data is  torch.Size([64, 3, 9])\n",
      "Previous layer shape is  torch.Size([64, 3, 3, 3])\n",
      "shape of layer: model 0 torch.Size([128, 64, 9])\n",
      "shape of layer: model 1 torch.Size([128, 64, 9])\n",
      "shape of previous transport map torch.Size([64, 64])\n",
      "Processing the coordinates to form ground_metric\n",
      "dont leave off the squaring of the ground metric\n",
      "returns a uniform measure of cardinality:  128\n",
      "returns a uniform measure of cardinality:  128\n",
      "the transport map is  tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        ...,\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0078],\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],\n",
      "       device='cuda:0')\n",
      "Ratio of trace to the matrix sum:  tensor(0.0078, device='cuda:0')\n",
      "Here, trace is 1.0 and matrix sum is 128.0 \n",
      "this is past correction for weight mode\n",
      "Shape of aligned wt is  torch.Size([128, 64, 9])\n",
      "Shape of fc_layer0_weight_data is  torch.Size([128, 64, 9])\n",
      "Previous layer shape is  torch.Size([128, 64, 3, 3])\n",
      "shape of layer: model 0 torch.Size([256, 128, 9])\n",
      "shape of layer: model 1 torch.Size([256, 128, 9])\n",
      "shape of previous transport map torch.Size([128, 128])\n",
      "Processing the coordinates to form ground_metric\n",
      "dont leave off the squaring of the ground metric\n",
      "returns a uniform measure of cardinality:  256\n",
      "returns a uniform measure of cardinality:  256\n",
      "the transport map is  tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')\n",
      "Ratio of trace to the matrix sum:  tensor(0., device='cuda:0')\n",
      "Here, trace is 0.0 and matrix sum is 256.0 \n",
      "this is past correction for weight mode\n",
      "Shape of aligned wt is  torch.Size([256, 128, 9])\n",
      "Shape of fc_layer0_weight_data is  torch.Size([256, 128, 9])\n",
      "Previous layer shape is  torch.Size([256, 128, 3, 3])\n",
      "shape of layer: model 0 torch.Size([256, 256, 9])\n",
      "shape of layer: model 1 torch.Size([256, 256, 9])\n",
      "shape of previous transport map torch.Size([256, 256])\n",
      "Processing the coordinates to form ground_metric\n",
      "dont leave off the squaring of the ground metric\n",
      "returns a uniform measure of cardinality:  256\n",
      "returns a uniform measure of cardinality:  256\n",
      "the transport map is  tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        ...,\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.0039, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],\n",
      "       device='cuda:0')\n",
      "Ratio of trace to the matrix sum:  tensor(0.0078, device='cuda:0')\n",
      "Here, trace is 2.0 and matrix sum is 256.0 \n",
      "this is past correction for weight mode\n",
      "Shape of aligned wt is  torch.Size([256, 256, 9])\n",
      "Shape of fc_layer0_weight_data is  torch.Size([256, 256, 9])\n",
      "Previous layer shape is  torch.Size([256, 256, 3, 3])\n",
      "shape of layer: model 0 torch.Size([512, 256, 9])\n",
      "shape of layer: model 1 torch.Size([512, 256, 9])\n",
      "shape of previous transport map torch.Size([256, 256])\n",
      "Processing the coordinates to form ground_metric\n",
      "dont leave off the squaring of the ground metric\n",
      "returns a uniform measure of cardinality:  512\n",
      "returns a uniform measure of cardinality:  512\n",
      "the transport map is  tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')\n",
      "Ratio of trace to the matrix sum:  tensor(0.0039, device='cuda:0')\n",
      "Here, trace is 2.0 and matrix sum is 512.0 \n",
      "this is past correction for weight mode\n",
      "Shape of aligned wt is  torch.Size([512, 256, 9])\n",
      "Shape of fc_layer0_weight_data is  torch.Size([512, 256, 9])\n",
      "Previous layer shape is  torch.Size([512, 256, 3, 3])\n",
      "shape of layer: model 0 torch.Size([512, 512, 9])\n",
      "shape of layer: model 1 torch.Size([512, 512, 9])\n",
      "shape of previous transport map torch.Size([512, 512])\n",
      "Processing the coordinates to form ground_metric\n",
      "dont leave off the squaring of the ground metric\n",
      "returns a uniform measure of cardinality:  512\n",
      "returns a uniform measure of cardinality:  512\n",
      "the transport map is  tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')\n",
      "Ratio of trace to the matrix sum:  tensor(0.0020, device='cuda:0')\n",
      "Here, trace is 1.0 and matrix sum is 512.0 \n",
      "this is past correction for weight mode\n",
      "Shape of aligned wt is  torch.Size([512, 512, 9])\n",
      "Shape of fc_layer0_weight_data is  torch.Size([512, 512, 9])\n",
      "Previous layer shape is  torch.Size([512, 512, 3, 3])\n",
      "shape of layer: model 0 torch.Size([512, 512, 9])\n",
      "shape of layer: model 1 torch.Size([512, 512, 9])\n",
      "shape of previous transport map torch.Size([512, 512])\n",
      "Processing the coordinates to form ground_metric\n",
      "dont leave off the squaring of the ground metric\n",
      "returns a uniform measure of cardinality:  512\n",
      "returns a uniform measure of cardinality:  512\n",
      "the transport map is  tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')\n",
      "Ratio of trace to the matrix sum:  tensor(0.0020, device='cuda:0')\n",
      "Here, trace is 1.0 and matrix sum is 512.0 \n",
      "this is past correction for weight mode\n",
      "Shape of aligned wt is  torch.Size([512, 512, 9])\n",
      "Shape of fc_layer0_weight_data is  torch.Size([512, 512, 9])\n",
      "Previous layer shape is  torch.Size([512, 512, 3, 3])\n",
      "shape of layer: model 0 torch.Size([512, 512, 9])\n",
      "shape of layer: model 1 torch.Size([512, 512, 9])\n",
      "shape of previous transport map torch.Size([512, 512])\n",
      "Processing the coordinates to form ground_metric\n",
      "dont leave off the squaring of the ground metric\n",
      "returns a uniform measure of cardinality:  512\n",
      "returns a uniform measure of cardinality:  512\n",
      "the transport map is  tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        [0.0020, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        ...,\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],\n",
      "       device='cuda:0')\n",
      "Ratio of trace to the matrix sum:  tensor(0., device='cuda:0')\n",
      "Here, trace is 0.0 and matrix sum is 512.0 \n",
      "this is past correction for weight mode\n",
      "Shape of aligned wt is  torch.Size([512, 512, 9])\n",
      "Shape of fc_layer0_weight_data is  torch.Size([512, 512, 9])\n",
      "Previous layer shape is  torch.Size([512, 512, 3, 3])\n",
      "shape of layer: model 0 torch.Size([10, 512])\n",
      "shape of layer: model 1 torch.Size([10, 512])\n",
      "shape of previous transport map torch.Size([512, 512])\n",
      "Processing the coordinates to form ground_metric\n",
      "dont leave off the squaring of the ground metric\n",
      "ground metric is  tensor([[1.4271, 2.4927, 2.7867, 2.8786, 2.7436, 2.8868, 2.7326, 2.6399, 2.5700,\n",
      "         2.5976],\n",
      "        [2.4939, 1.3272, 2.6784, 2.5964, 2.7632, 2.5546, 2.4204, 2.5255, 2.3095,\n",
      "         2.4138],\n",
      "        [2.7656, 2.6762, 1.3861, 2.9526, 2.9123, 2.8761, 2.7322, 2.8437, 2.7544,\n",
      "         2.7917],\n",
      "        [2.8411, 2.6845, 2.8984, 1.5462, 2.8664, 2.9223, 2.7624, 2.8035, 2.8226,\n",
      "         2.6405],\n",
      "        [2.7609, 2.6010, 2.8692, 2.8510, 1.5072, 2.7504, 2.7899, 2.6860, 2.7091,\n",
      "         2.7345],\n",
      "        [2.9250, 2.5443, 2.9528, 2.8477, 2.7327, 1.5533, 2.7827, 2.7352, 2.8038,\n",
      "         2.6747],\n",
      "        [2.6826, 2.4488, 2.7238, 2.9161, 2.6529, 2.8572, 1.3947, 2.6929, 2.5024,\n",
      "         2.5595],\n",
      "        [2.7443, 2.5535, 2.7852, 2.7956, 2.7201, 2.6639, 2.7226, 1.3946, 2.5633,\n",
      "         2.5158],\n",
      "        [2.6031, 2.3702, 2.7753, 2.6925, 2.7078, 2.8341, 2.5361, 2.6413, 1.3028,\n",
      "         2.4083],\n",
      "        [2.5967, 2.2764, 2.7867, 2.7625, 2.6741, 2.7321, 2.5512, 2.5326, 2.5255,\n",
      "         1.2897]], device='cuda:0', grad_fn=<PowBackward0>)\n",
      "returns a uniform measure of cardinality:  10\n",
      "returns a uniform measure of cardinality:  10\n",
      "the transport map is  tensor([[0.1000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0000, 0.1000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0000, 0.0000, 0.1000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.1000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.1000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.1000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.1000, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.1000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.1000,\n",
      "         0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.1000]], device='cuda:0')\n",
      "Ratio of trace to the matrix sum:  tensor(1., device='cuda:0')\n",
      "Here, trace is 9.99755859375 and matrix sum is 9.99755859375 \n",
      "this is past correction for weight mode\n",
      "Shape of aligned wt is  torch.Size([10, 512])\n",
      "Shape of fc_layer0_weight_data is  torch.Size([10, 512])\n",
      "using independent method\n",
      "in _make_layers [Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), AvgPool2d(kernel_size=1, stride=1, padding=0)]\n",
      "Relu Inplace is  False\n",
      "model parameters are \n",
      " [torch.Size([64, 3, 3, 3]), torch.Size([128, 64, 3, 3]), torch.Size([256, 128, 3, 3]), torch.Size([256, 256, 3, 3]), torch.Size([512, 256, 3, 3]), torch.Size([512, 512, 3, 3]), torch.Size([512, 512, 3, 3]), torch.Size([512, 512, 3, 3]), torch.Size([10, 512])]\n",
      "\n",
      "--------- Testing in global mode ---------\n",
      "size of test_loader dataset:  10000\n",
      "\n",
      "Test set: Avg. loss: 0.0023, Accuracy: 776/10000 (8%)\n",
      "\n",
      "len of model parameters and avg aligned layers is  9 9\n",
      "len of model_state_dict is  9\n",
      "len of param_list is  9\n",
      "\n",
      "--------- Testing in global mode ---------\n",
      "size of test_loader dataset:  10000\n",
      "\n",
      "Test set: Avg. loss: 0.0014, Accuracy: 8693/10000 (87%)\n",
      "\n",
      "===============================================\n",
      "===============================================\n",
      "After Ensemble: 86.93\n",
      "===============================================\n",
      "===============================================\n",
      "Timer ends\n",
      "Time taken for geometric ensembling is 4.122280936688185 seconds\n",
      "------- Prediction based ensembling -------\n",
      "\n",
      "Test set: Avg. loss: 0.0003, Accuracy: 9125/10000 (91%)\n",
      "\n",
      "------- Naive ensembling of weights -------\n",
      "[torch.Size([64, 3, 3, 3]), torch.Size([64, 3, 3, 3])]\n",
      "torch.Size([64, 3, 3, 3])\n",
      "[torch.Size([128, 64, 3, 3]), torch.Size([128, 64, 3, 3])]\n",
      "torch.Size([128, 64, 3, 3])\n",
      "[torch.Size([256, 128, 3, 3]), torch.Size([256, 128, 3, 3])]\n",
      "torch.Size([256, 128, 3, 3])\n",
      "[torch.Size([256, 256, 3, 3]), torch.Size([256, 256, 3, 3])]\n",
      "torch.Size([256, 256, 3, 3])\n",
      "[torch.Size([512, 256, 3, 3]), torch.Size([512, 256, 3, 3])]\n",
      "torch.Size([512, 256, 3, 3])\n",
      "[torch.Size([512, 512, 3, 3]), torch.Size([512, 512, 3, 3])]\n",
      "torch.Size([512, 512, 3, 3])\n",
      "[torch.Size([512, 512, 3, 3]), torch.Size([512, 512, 3, 3])]\n",
      "torch.Size([512, 512, 3, 3])\n",
      "[torch.Size([512, 512, 3, 3]), torch.Size([512, 512, 3, 3])]\n",
      "torch.Size([512, 512, 3, 3])\n",
      "[torch.Size([10, 512]), torch.Size([10, 512])]\n",
      "torch.Size([10, 512])\n",
      "in _make_layers [Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), AvgPool2d(kernel_size=1, stride=1, padding=0)]\n",
      "Relu Inplace is  False\n",
      "model parameters are \n",
      " [torch.Size([64, 3, 3, 3]), torch.Size([128, 64, 3, 3]), torch.Size([256, 128, 3, 3]), torch.Size([256, 256, 3, 3]), torch.Size([512, 256, 3, 3]), torch.Size([512, 512, 3, 3]), torch.Size([512, 512, 3, 3]), torch.Size([512, 512, 3, 3]), torch.Size([10, 512])]\n",
      "\n",
      "--------- Testing in global mode ---------\n",
      "size of test_loader dataset:  10000\n",
      "\n",
      "Test set: Avg. loss: 0.0023, Accuracy: 1066/10000 (11%)\n",
      "\n",
      "\n",
      "--------- Testing in global mode ---------\n",
      "size of test_loader dataset:  10000\n",
      "\n",
      "Test set: Avg. loss: 0.0023, Accuracy: 1596/10000 (16%)\n",
      "\n",
      "-------- Retraining the models ---------\n",
      "Retraining model :  model_0\n",
      "num_epochs---------------+++))))))+++ 300\n",
      "num_epochs---------------+++))))))+++ 300\n",
      "lr is  0.05\n",
      "number of epochs would be  30\n",
      "num_epochs-------------- 30\n",
      "Epoch 000\n",
      "/root/miniconda3/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:129: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\"Detected call of `lr_scheduler.step()` before `optimizer.step()`. \"\n",
      "/root/miniconda3/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:154: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "accuracy: {'epoch': 0, 'value': 0.9122000000190741} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 0, 'value': 0.2636602558898925} ({'split': 'train'})\n",
      "accuracy: {'epoch': 0, 'value': 0.8344000399112701} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 0, 'value': 0.5101864248514175} ({'split': 'test'})\n",
      "Epoch 001\n",
      "accuracy: {'epoch': 1, 'value': 0.9403200000190738} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 1, 'value': 0.17580926229715352} ({'split': 'train'})\n",
      "accuracy: {'epoch': 1, 'value': 0.8468000352382661} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 1, 'value': 0.5193602144718171} ({'split': 'test'})\n",
      "Epoch 002\n",
      "accuracy: {'epoch': 2, 'value': 0.9563600000381468} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 2, 'value': 0.13225128568172462} ({'split': 'train'})\n",
      "accuracy: {'epoch': 2, 'value': 0.8551000356674194} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 2, 'value': 0.5112047284841538} ({'split': 'test'})\n",
      "Epoch 003\n",
      "accuracy: {'epoch': 3, 'value': 0.9671600000572209} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 3, 'value': 0.10211080060482026} ({'split': 'train'})\n",
      "accuracy: {'epoch': 3, 'value': 0.8414000451564789} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 3, 'value': 0.5935918271541595} ({'split': 'test'})\n",
      "Epoch 004\n",
      "accuracy: {'epoch': 4, 'value': 0.9735000000572205} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 4, 'value': 0.07740002972126014} ({'split': 'train'})\n",
      "accuracy: {'epoch': 4, 'value': 0.8538000345230102} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 4, 'value': 0.576252418756485} ({'split': 'test'})\n",
      "Epoch 005\n",
      "accuracy: {'epoch': 5, 'value': 0.9776800000572204} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 5, 'value': 0.06936297762870791} ({'split': 'train'})\n",
      "accuracy: {'epoch': 5, 'value': 0.8592000424861909} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 5, 'value': 0.5628400206565856} ({'split': 'test'})\n",
      "Epoch 006\n",
      "accuracy: {'epoch': 6, 'value': 0.9816400000190737} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 6, 'value': 0.05853538508176803} ({'split': 'train'})\n",
      "accuracy: {'epoch': 6, 'value': 0.866800045967102} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 6, 'value': 0.5465849697589873} ({'split': 'test'})\n",
      "Epoch 007\n",
      "accuracy: {'epoch': 7, 'value': 0.984740000019074} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 7, 'value': 0.04710702549099921} ({'split': 'train'})\n",
      "accuracy: {'epoch': 7, 'value': 0.8397000372409821} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 7, 'value': 0.6637592494487763} ({'split': 'test'})\n",
      "Epoch 008\n",
      "accuracy: {'epoch': 8, 'value': 0.9859000000572207} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 8, 'value': 0.043933117284774746} ({'split': 'train'})\n",
      "accuracy: {'epoch': 8, 'value': 0.8613000571727752} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 8, 'value': 0.5364737719297409} ({'split': 'test'})\n",
      "Epoch 009\n",
      "accuracy: {'epoch': 9, 'value': 0.985540000038147} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 9, 'value': 0.04429035544872284} ({'split': 'train'})\n",
      "accuracy: {'epoch': 9, 'value': 0.84560005068779} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 9, 'value': 0.6203998327255248} ({'split': 'test'})\n",
      "Epoch 010\n",
      "accuracy: {'epoch': 10, 'value': 0.980520000019073} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 10, 'value': 0.062353112208843216} ({'split': 'train'})\n",
      "accuracy: {'epoch': 10, 'value': 0.8550000369548798} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 10, 'value': 0.5902862787246704} ({'split': 'test'})\n",
      "Epoch 011\n",
      "accuracy: {'epoch': 11, 'value': 0.9872200000381468} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 11, 'value': 0.042148954392671585} ({'split': 'train'})\n",
      "accuracy: {'epoch': 11, 'value': 0.8621000409126282} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 11, 'value': 0.6437871098518372} ({'split': 'test'})\n",
      "Epoch 012\n",
      "accuracy: {'epoch': 12, 'value': 0.9894400000190738} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 12, 'value': 0.03598721664547921} ({'split': 'train'})\n",
      "accuracy: {'epoch': 12, 'value': 0.8698000431060791} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 12, 'value': 0.5256512731313705} ({'split': 'test'})\n",
      "Epoch 013\n",
      "accuracy: {'epoch': 13, 'value': 0.990660000038147} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 13, 'value': 0.030652406332492835} ({'split': 'train'})\n",
      "accuracy: {'epoch': 13, 'value': 0.8585000395774841} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 13, 'value': 0.5822527706623077} ({'split': 'test'})\n",
      "Epoch 014\n",
      "accuracy: {'epoch': 14, 'value': 0.9904200000000004} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 14, 'value': 0.031495134933590886} ({'split': 'train'})\n",
      "accuracy: {'epoch': 14, 'value': 0.8659000396728516} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 14, 'value': 0.5479746639728547} ({'split': 'test'})\n",
      "Epoch 015\n",
      "accuracy: {'epoch': 15, 'value': 0.9911200000381467} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 15, 'value': 0.02920728697061539} ({'split': 'train'})\n",
      "accuracy: {'epoch': 15, 'value': 0.8625000357627868} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 15, 'value': 0.5630833506584167} ({'split': 'test'})\n",
      "Epoch 016\n",
      "accuracy: {'epoch': 16, 'value': 0.9894400000190737} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 16, 'value': 0.0346684301567078} ({'split': 'train'})\n",
      "accuracy: {'epoch': 16, 'value': 0.8657000362873077} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 16, 'value': 0.5487367331981658} ({'split': 'test'})\n",
      "Epoch 017\n",
      "accuracy: {'epoch': 17, 'value': 0.9887399999999998} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 17, 'value': 0.03685286352962254} ({'split': 'train'})\n",
      "accuracy: {'epoch': 17, 'value': 0.8545000314712525} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 17, 'value': 0.6119182616472244} ({'split': 'test'})\n",
      "Epoch 018\n",
      "accuracy: {'epoch': 18, 'value': 0.9890800000190734} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 18, 'value': 0.035313584553003305} ({'split': 'train'})\n",
      "accuracy: {'epoch': 18, 'value': 0.8621000349521637} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 18, 'value': 0.555291822552681} ({'split': 'test'})\n",
      "Epoch 019\n",
      "accuracy: {'epoch': 19, 'value': 0.9899600000190744} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 19, 'value': 0.03316129726171496} ({'split': 'train'})\n",
      "accuracy: {'epoch': 19, 'value': 0.8682000398635864} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 19, 'value': 0.5559415608644485} ({'split': 'test'})\n",
      "Epoch 020\n",
      "accuracy: {'epoch': 20, 'value': 0.9898600000190733} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 20, 'value': 0.03351216415524483} ({'split': 'train'})\n",
      "accuracy: {'epoch': 20, 'value': 0.8601000487804413} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 20, 'value': 0.5782888352870942} ({'split': 'test'})\n",
      "Epoch 021\n",
      "accuracy: {'epoch': 21, 'value': 0.9883600000381472} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 21, 'value': 0.03587706937551499} ({'split': 'train'})\n",
      "accuracy: {'epoch': 21, 'value': 0.8587000370025635} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 21, 'value': 0.6062435984611511} ({'split': 'test'})\n",
      "Epoch 022\n",
      "accuracy: {'epoch': 22, 'value': 0.9858000000381465} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 22, 'value': 0.04532035360813142} ({'split': 'train'})\n",
      "accuracy: {'epoch': 22, 'value': 0.8450000405311585} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 22, 'value': 0.6278650104999542} ({'split': 'test'})\n",
      "Epoch 023\n",
      "accuracy: {'epoch': 23, 'value': 0.9867000000190733} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 23, 'value': 0.04363761785984038} ({'split': 'train'})\n",
      "accuracy: {'epoch': 23, 'value': 0.847100043296814} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 23, 'value': 0.6292963147163391} ({'split': 'test'})\n",
      "Epoch 024\n",
      "accuracy: {'epoch': 24, 'value': 0.9867600000190734} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 24, 'value': 0.042320175387263334} ({'split': 'train'})\n",
      "accuracy: {'epoch': 24, 'value': 0.8553000450134276} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 24, 'value': 0.630251544713974} ({'split': 'test'})\n",
      "Epoch 025\n",
      "^C\n",
      "Traceback (most recent call last):\n",
      "  File \"main.py\", line 351, in <module>\n",
      "    _, best_retrain_acc = routines.retrain_models(args, [*original_models, geometric_model, naive_model], retrain_loader, test_loader, config, tensorboard_obj=tensorboard_obj, initial_acc=initial_acc, nicks=nicks)\n",
      "  File \"/root/autodl-tmp/OTfusion_pruning/otfusion/routines.py\", line 394, in retrain_models\n",
      "    retrained_network, acc = cifar_train.get_retrained_model(args, retrain_loader, test_loader, old_networks[i], config, output_root_dir, tensorboard_obj=tensorboard_obj, nick=nick, start_acc=initial_acc[i])\n",
      "  File \"/root/autodl-tmp/OTfusion_pruning/otfusion/./cifar/train.py\", line 333, in get_retrained_model\n",
      "    best_acc,model = main(config, output_dir, args.gpu_id, pretrained_model=old_network, pretrained_dataset=(train_loader, test_loader), tensorboard_obj=tensorboard_obj,return_model=True)\n",
      "  File \"/root/autodl-tmp/OTfusion_pruning/otfusion/./cifar/train.py\", line 67, in main\n",
      "    for batch_x, batch_y in training_loader:\n",
      "  File \"/root/miniconda3/lib/python3.8/site-packages/torch/utils/data/dataloader.py\", line 521, in __next__\n",
      "    data = self._next_data()\n",
      "  File \"/root/miniconda3/lib/python3.8/site-packages/torch/utils/data/dataloader.py\", line 561, in _next_data\n",
      "    data = self._dataset_fetcher.fetch(index)  # may raise StopIteration\n",
      "  File \"/root/miniconda3/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py\", line 49, in fetch\n",
      "    data = [self.dataset[idx] for idx in possibly_batched_index]\n",
      "  File \"/root/miniconda3/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py\", line 49, in <listcomp>\n",
      "    data = [self.dataset[idx] for idx in possibly_batched_index]\n",
      "  File \"/root/miniconda3/lib/python3.8/site-packages/torchvision/datasets/cifar.py\", line 121, in __getitem__\n",
      "    img = self.transform(img)\n",
      "  File \"/root/miniconda3/lib/python3.8/site-packages/torchvision/transforms/transforms.py\", line 61, in __call__\n",
      "    img = t(img)\n",
      "  File \"/root/miniconda3/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1102, in _call_impl\n",
      "    return forward_call(*input, **kwargs)\n",
      "  File \"/root/miniconda3/lib/python3.8/site-packages/torchvision/transforms/transforms.py\", line 226, in forward\n",
      "    return F.normalize(tensor, self.mean, self.std, self.inplace)\n",
      "  File \"/root/miniconda3/lib/python3.8/site-packages/torchvision/transforms/functional.py\", line 351, in normalize\n",
      "    tensor.sub_(mean).div_(std)\n",
      "KeyboardInterrupt\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m~/miniconda3/lib/python3.8/site-packages/IPython/utils/_process_posix.py\u001b[0m in \u001b[0;36msystem\u001b[0;34m(self, cmd)\u001b[0m\n\u001b[1;32m    161\u001b[0m                 \u001b[0;31m# know whether we've finished (if we matched EOF) or not\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 162\u001b[0;31m                 \u001b[0mres_idx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mchild\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexpect_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpatterns\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_timeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    163\u001b[0m                 \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchild\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbefore\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mout_size\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'replace'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m''\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.8/site-packages/pexpect/spawnbase.py\u001b[0m in \u001b[0;36mexpect_list\u001b[0;34m(self, pattern_list, timeout, searchwindowsize, async_, **kw)\u001b[0m\n\u001b[1;32m    371\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 372\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mexp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexpect_loop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    373\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.8/site-packages/pexpect/expect.py\u001b[0m in \u001b[0;36mexpect_loop\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    168\u001b[0m                 \u001b[0;31m# Still have time left, so read more data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 169\u001b[0;31m                 \u001b[0mincoming\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mspawn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_nonblocking\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mspawn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmaxread\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    170\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mspawn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdelayafterread\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.8/site-packages/pexpect/pty_spawn.py\u001b[0m in \u001b[0;36mread_nonblocking\u001b[0;34m(self, size, timeout)\u001b[0m\n\u001b[1;32m    499\u001b[0m         \u001b[0;31m# (possibly timeout=None), we call select() with a timeout.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 500\u001b[0;31m         \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mselect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    501\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mspawn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_nonblocking\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.8/site-packages/pexpect/pty_spawn.py\u001b[0m in \u001b[0;36mselect\u001b[0;34m(timeout)\u001b[0m\n\u001b[1;32m    449\u001b[0m             \u001b[0;32mdef\u001b[0m \u001b[0mselect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 450\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mselect_ignore_interrupts\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchild_fd\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    451\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.8/site-packages/pexpect/utils.py\u001b[0m in \u001b[0;36mselect_ignore_interrupts\u001b[0;34m(iwtd, owtd, ewtd, timeout)\u001b[0m\n\u001b[1;32m    142\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 143\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mselect\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miwtd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mowtd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mewtd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    144\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mInterruptedError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: ",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_9919/1363029856.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msystem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'python main.py --gpu-id 0 --model-name vgg11_nobias --n-epochs 10 --save-result-file sample.csv  --sweep-name exp_sample --correction --ground-metric euclidean --weight-stats  --geom-ensemble-type wts --ground-metric-normalize none --sweep-id 90  --ckpt-type best --dataset Cifar10 --ground-metric-eff --recheck-cifar --activation-seed {activation_seed}  --prelu-acts --past-correction --not-squared --exact --learning-rate 0.0001 --momentum 0.5 --batch-size-train 256 --experiment-dir {experiment_dir} --prunint-rate {prunint_rate} --run-mode {run_mode} --to-download --load-model-dir {load_model_dir} --retrain 30'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/miniconda3/lib/python3.8/site-packages/ipykernel/zmqshell.py\u001b[0m in \u001b[0;36msystem_piped\u001b[0;34m(self, cmd)\u001b[0m\n\u001b[1;32m    635\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muser_ns\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'_exit_code'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msystem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcmd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    636\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 637\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muser_ns\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'_exit_code'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msystem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvar_expand\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcmd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdepth\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    638\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    639\u001b[0m     \u001b[0;31m# Ensure new system_piped implementation is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.8/site-packages/IPython/utils/_process_posix.py\u001b[0m in \u001b[0;36msystem\u001b[0;34m(self, cmd)\u001b[0m\n\u001b[1;32m    184\u001b[0m             \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    185\u001b[0m                 \u001b[0;31m# Ensure the subprocess really is terminated\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 186\u001b[0;31m                 \u001b[0mchild\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mterminate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mforce\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    187\u001b[0m         \u001b[0;31m# add isalive check, to ensure exitstatus is set:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    188\u001b[0m         \u001b[0mchild\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misalive\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.8/site-packages/pexpect/pty_spawn.py\u001b[0m in \u001b[0;36mterminate\u001b[0;34m(self, force)\u001b[0m\n\u001b[1;32m    653\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mforce\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    654\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkill\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msignal\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSIGKILL\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 655\u001b[0;31m                 \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdelayafterterminate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    656\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misalive\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    657\u001b[0m                     \u001b[0;32mreturn\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e1701397-8c63-4095-be56-b3464267c277",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------- Setting up parameters -------\n",
      "dumping parameters at  /root/autodl-tmp/OTfusion_pruning/otfusion/exp_sample/configurations\n",
      "The parameters are: \n",
      " Namespace(act_bug=False, act_num_samples=100, activation_histograms=False, activation_mode=None, activation_seed=21, alpha=0.7, baseroot='/root/autodl-tmp/OTfusion_pruning/otfusion', batch_size_test=1000, batch_size_train=64, center_acts=False, choice='0 2 4 6 8', cifar_style_data=False, ckpt_type='best', clip_gm=False, clip_max=5, clip_min=0, config_dir='/root/autodl-tmp/OTfusion_pruning/otfusion/exp_sample/configurations', config_file=None, correction=True, csv_dir='/root/autodl-tmp/OTfusion_pruning/otfusion/exp_sample/csv', dataset='Cifar10', debug=False, deprecated=None, deterministic=False, diff_init=False, disable_bias=True, dist_epochs=60, dist_normalize=False, dump_final_models=False, dump_model=False, enable_dropout=False, ensemble_step=0.5, eval_aligned=False, exact=True, exp_name='exp_2023-09-16_17-01-31_730310', experiment_dir='experiment2', geom_ensemble_type='wts', gpu_id=0, gromov=False, gromov_loss='square_loss', ground_metric='euclidean', ground_metric_eff=True, ground_metric_normalize='none', handle_skips=False, importance=None, learning_rate=0.0001, load_model_dir='./cifar_models', load_models='', log_interval=100, model_name='vgg11_nobias', momentum=0.5, n_epochs=10, no_random_trainloaders=False, normalize_acts=False, normalize_wts=False, not_squared=True, num_hidden_nodes=400, num_hidden_nodes1=400, num_hidden_nodes2=200, num_hidden_nodes3=100, num_hidden_nodes4=50, num_models=2, options_type='generic', partial_reshape=False, partition_dataloader=-1, partition_type='labels', past_correction=True, personal_class_idx=9, personal_split_frac=0.1, pool_acts=False, pool_relu=False, prediction_wts=False, prelu_acts=True, print_distances=False, proper_marginals=False, prunint_rate=0.2, recheck_acc=False, recheck_cifar=True, reg=0.01, reg_m=0.001, reinit_trainloaders=False, result_dir='/root/autodl-tmp/OTfusion_pruning/otfusion/exp_sample/results', retrain=30, retrain_avg_only=False, retrain_geometric_only=False, retrain_lr_decay=-1, retrain_lr_decay_epochs=None, retrain_lr_decay_factor=None, retrain_seed=-1, rootdir='/root/autodl-tmp/OTfusion_pruning/otfusion/exp_sample', run_mode='finetune', same_model=-1, save_result_file='sample.csv', second_model_name=None, sinkhorn_type='normal', skip_last_layer=False, skip_last_layer_type='average', skip_personal_idx=False, skip_retrain=-1, softmax_temperature=1, standardize_acts=False, sweep_id=90, sweep_name='exp_sample', temperature=20, tensorboard=False, tensorboard_root='./tensorboard', timestamp='2023-09-16_17-01-31_730310', tmap_stats=False, to_download=True, transform_acts=False, unbalanced=False, update_acts=False, weight_stats=True, width_ratio=1)\n",
      "refactored get_config\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "in _make_layers [Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), AvgPool2d(kernel_size=1, stride=1, padding=0)]\n",
      "Relu Inplace is  False\n",
      "model parameters are \n",
      " [torch.Size([64, 3, 3, 3]), torch.Size([128, 64, 3, 3]), torch.Size([256, 128, 3, 3]), torch.Size([256, 256, 3, 3]), torch.Size([512, 256, 3, 3]), torch.Size([512, 512, 3, 3]), torch.Size([512, 512, 3, 3]), torch.Size([512, 512, 3, 3]), torch.Size([10, 512])]\n",
      "in _make_layers [Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), AvgPool2d(kernel_size=1, stride=1, padding=0)]\n",
      "Relu Inplace is  False\n",
      "model parameters are \n",
      " [torch.Size([64, 3, 3, 3]), torch.Size([128, 64, 3, 3]), torch.Size([256, 128, 3, 3]), torch.Size([256, 256, 3, 3]), torch.Size([512, 256, 3, 3]), torch.Size([512, 512, 3, 3]), torch.Size([512, 512, 3, 3]), torch.Size([512, 512, 3, 3]), torch.Size([10, 512])]\n",
      "in _make_layers [Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), AvgPool2d(kernel_size=1, stride=1, padding=0)]\n",
      "Relu Inplace is  False\n",
      "model parameters are \n",
      " [torch.Size([64, 3, 3, 3]), torch.Size([128, 64, 3, 3]), torch.Size([256, 128, 3, 3]), torch.Size([256, 256, 3, 3]), torch.Size([512, 256, 3, 3]), torch.Size([512, 512, 3, 3]), torch.Size([512, 512, 3, 3]), torch.Size([512, 512, 3, 3]), torch.Size([10, 512])]\n",
      "in _make_layers [Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), AvgPool2d(kernel_size=1, stride=1, padding=0)]\n",
      "Relu Inplace is  False\n",
      "model parameters are \n",
      " [torch.Size([64, 3, 3, 3]), torch.Size([128, 64, 3, 3]), torch.Size([256, 128, 3, 3]), torch.Size([256, 256, 3, 3]), torch.Size([512, 256, 3, 3]), torch.Size([512, 512, 3, 3]), torch.Size([512, 512, 3, 3]), torch.Size([512, 512, 3, 3]), torch.Size([10, 512])]\n",
      "Train Epoch: 1 [0/50000 (0%)]\tLoss: 0.116993\n",
      "Train Epoch: 1 [6400/50000 (13%)]\tLoss: 0.069534\n",
      "Train Epoch: 1 [12800/50000 (26%)]\tLoss: 0.115613\n",
      "Train Epoch: 1 [19200/50000 (38%)]\tLoss: 0.062993\n",
      "Train Epoch: 1 [25600/50000 (51%)]\tLoss: 0.083842\n",
      "Train Epoch: 1 [32000/50000 (64%)]\tLoss: 0.060105\n",
      "Train Epoch: 1 [38400/50000 (77%)]\tLoss: 0.070053\n",
      "Train Epoch: 1 [44800/50000 (90%)]\tLoss: 0.066773\n",
      "\n",
      "--------- Testing in global mode ---------\n",
      "size of test_loader dataset:  10000\n",
      "\n",
      "Test set: Avg. loss: 0.0003, Accuracy: 9049/10000 (90%)\n",
      "\n",
      "Train Epoch: 2 [0/50000 (0%)]\tLoss: 0.059949\n",
      "Train Epoch: 2 [6400/50000 (13%)]\tLoss: 0.060339\n",
      "Train Epoch: 2 [12800/50000 (26%)]\tLoss: 0.044316\n",
      "Train Epoch: 2 [19200/50000 (38%)]\tLoss: 0.052728\n",
      "Train Epoch: 2 [25600/50000 (51%)]\tLoss: 0.062013\n",
      "Train Epoch: 2 [32000/50000 (64%)]\tLoss: 0.036824\n",
      "Train Epoch: 2 [38400/50000 (77%)]\tLoss: 0.046963\n",
      "Train Epoch: 2 [44800/50000 (90%)]\tLoss: 0.037231\n",
      "\n",
      "--------- Testing in global mode ---------\n",
      "size of test_loader dataset:  10000\n",
      "\n",
      "Test set: Avg. loss: 0.0003, Accuracy: 9044/10000 (90%)\n",
      "\n",
      "Train Epoch: 3 [0/50000 (0%)]\tLoss: 0.050803\n",
      "Train Epoch: 3 [6400/50000 (13%)]\tLoss: 0.039283\n",
      "Train Epoch: 3 [12800/50000 (26%)]\tLoss: 0.063139\n",
      "Train Epoch: 3 [19200/50000 (38%)]\tLoss: 0.077094\n",
      "Train Epoch: 3 [25600/50000 (51%)]\tLoss: 0.052441\n",
      "Train Epoch: 3 [32000/50000 (64%)]\tLoss: 0.038409\n",
      "Train Epoch: 3 [38400/50000 (77%)]\tLoss: 0.046758\n",
      "Train Epoch: 3 [44800/50000 (90%)]\tLoss: 0.033687\n",
      "\n",
      "--------- Testing in global mode ---------\n",
      "size of test_loader dataset:  10000\n",
      "\n",
      "Test set: Avg. loss: 0.0003, Accuracy: 9044/10000 (90%)\n",
      "\n",
      "Train Epoch: 4 [0/50000 (0%)]\tLoss: 0.052528\n",
      "Train Epoch: 4 [6400/50000 (13%)]\tLoss: 0.055730\n",
      "Train Epoch: 4 [12800/50000 (26%)]\tLoss: 0.040544\n",
      "Train Epoch: 4 [19200/50000 (38%)]\tLoss: 0.040515\n",
      "Train Epoch: 4 [25600/50000 (51%)]\tLoss: 0.040505\n",
      "Train Epoch: 4 [32000/50000 (64%)]\tLoss: 0.044889\n",
      "Train Epoch: 4 [38400/50000 (77%)]\tLoss: 0.032337\n",
      "Train Epoch: 4 [44800/50000 (90%)]\tLoss: 0.031939\n",
      "\n",
      "--------- Testing in global mode ---------\n",
      "size of test_loader dataset:  10000\n",
      "\n",
      "Test set: Avg. loss: 0.0003, Accuracy: 9049/10000 (90%)\n",
      "\n",
      "Train Epoch: 5 [0/50000 (0%)]\tLoss: 0.054305\n",
      "Train Epoch: 5 [6400/50000 (13%)]\tLoss: 0.024743\n",
      "Train Epoch: 5 [12800/50000 (26%)]\tLoss: 0.029276\n",
      "Train Epoch: 5 [19200/50000 (38%)]\tLoss: 0.028308\n",
      "Train Epoch: 5 [25600/50000 (51%)]\tLoss: 0.063834\n",
      "Train Epoch: 5 [32000/50000 (64%)]\tLoss: 0.038146\n",
      "Train Epoch: 5 [38400/50000 (77%)]\tLoss: 0.038030\n",
      "Train Epoch: 5 [44800/50000 (90%)]\tLoss: 0.045305\n",
      "\n",
      "--------- Testing in global mode ---------\n",
      "size of test_loader dataset:  10000\n",
      "\n",
      "Test set: Avg. loss: 0.0003, Accuracy: 9047/10000 (90%)\n",
      "\n",
      "Train Epoch: 6 [0/50000 (0%)]\tLoss: 0.043455\n",
      "Train Epoch: 6 [6400/50000 (13%)]\tLoss: 0.025087\n",
      "Train Epoch: 6 [12800/50000 (26%)]\tLoss: 0.029782\n",
      "Train Epoch: 6 [19200/50000 (38%)]\tLoss: 0.026195\n",
      "Train Epoch: 6 [25600/50000 (51%)]\tLoss: 0.041571\n",
      "Train Epoch: 6 [32000/50000 (64%)]\tLoss: 0.041046\n",
      "Train Epoch: 6 [38400/50000 (77%)]\tLoss: 0.033723\n",
      "Train Epoch: 6 [44800/50000 (90%)]\tLoss: 0.025436\n",
      "\n",
      "--------- Testing in global mode ---------\n",
      "size of test_loader dataset:  10000\n",
      "\n",
      "Test set: Avg. loss: 0.0003, Accuracy: 9043/10000 (90%)\n",
      "\n",
      "Train Epoch: 7 [0/50000 (0%)]\tLoss: 0.038252\n",
      "Train Epoch: 7 [6400/50000 (13%)]\tLoss: 0.047840\n",
      "Train Epoch: 7 [12800/50000 (26%)]\tLoss: 0.070959\n",
      "Train Epoch: 7 [19200/50000 (38%)]\tLoss: 0.033207\n",
      "Train Epoch: 7 [25600/50000 (51%)]\tLoss: 0.050586\n",
      "Train Epoch: 7 [32000/50000 (64%)]\tLoss: 0.029546\n",
      "Train Epoch: 7 [38400/50000 (77%)]\tLoss: 0.061426\n",
      "Train Epoch: 7 [44800/50000 (90%)]\tLoss: 0.057670\n",
      "\n",
      "--------- Testing in global mode ---------\n",
      "size of test_loader dataset:  10000\n",
      "\n",
      "Test set: Avg. loss: 0.0003, Accuracy: 9041/10000 (90%)\n",
      "\n",
      "Train Epoch: 8 [0/50000 (0%)]\tLoss: 0.032923\n",
      "Train Epoch: 8 [6400/50000 (13%)]\tLoss: 0.028961\n",
      "Train Epoch: 8 [12800/50000 (26%)]\tLoss: 0.033977\n",
      "Train Epoch: 8 [19200/50000 (38%)]\tLoss: 0.038018\n",
      "Train Epoch: 8 [25600/50000 (51%)]\tLoss: 0.065654\n",
      "Train Epoch: 8 [32000/50000 (64%)]\tLoss: 0.035545\n",
      "Train Epoch: 8 [38400/50000 (77%)]\tLoss: 0.021207\n",
      "Train Epoch: 8 [44800/50000 (90%)]\tLoss: 0.038753\n",
      "\n",
      "--------- Testing in global mode ---------\n",
      "size of test_loader dataset:  10000\n",
      "\n",
      "Test set: Avg. loss: 0.0003, Accuracy: 9035/10000 (90%)\n",
      "\n",
      "Train Epoch: 9 [0/50000 (0%)]\tLoss: 0.026012\n",
      "Train Epoch: 9 [6400/50000 (13%)]\tLoss: 0.019942\n",
      "Train Epoch: 9 [12800/50000 (26%)]\tLoss: 0.039452\n",
      "Train Epoch: 9 [19200/50000 (38%)]\tLoss: 0.027666\n",
      "Train Epoch: 9 [25600/50000 (51%)]\tLoss: 0.038081\n",
      "Train Epoch: 9 [32000/50000 (64%)]\tLoss: 0.053346\n",
      "Train Epoch: 9 [38400/50000 (77%)]\tLoss: 0.049406\n",
      "Train Epoch: 9 [44800/50000 (90%)]\tLoss: 0.057711\n",
      "\n",
      "--------- Testing in global mode ---------\n",
      "size of test_loader dataset:  10000\n",
      "\n",
      "Test set: Avg. loss: 0.0003, Accuracy: 9031/10000 (90%)\n",
      "\n",
      "Train Epoch: 10 [0/50000 (0%)]\tLoss: 0.052621\n",
      "Train Epoch: 10 [6400/50000 (13%)]\tLoss: 0.036410\n",
      "Train Epoch: 10 [12800/50000 (26%)]\tLoss: 0.038551\n",
      "Train Epoch: 10 [19200/50000 (38%)]\tLoss: 0.024619\n",
      "Train Epoch: 10 [25600/50000 (51%)]\tLoss: 0.056248\n",
      "Train Epoch: 10 [32000/50000 (64%)]\tLoss: 0.025337\n",
      "Train Epoch: 10 [38400/50000 (77%)]\tLoss: 0.039602\n",
      "Train Epoch: 10 [44800/50000 (90%)]\tLoss: 0.037355\n",
      "\n",
      "--------- Testing in global mode ---------\n",
      "size of test_loader dataset:  10000\n",
      "\n",
      "Test set: Avg. loss: 0.0003, Accuracy: 9032/10000 (90%)\n",
      "\n",
      "Train Epoch: 1 [0/50000 (0%)]\tLoss: 0.142689\n",
      "Train Epoch: 1 [6400/50000 (13%)]\tLoss: 0.052603\n",
      "Train Epoch: 1 [12800/50000 (26%)]\tLoss: 0.093420\n",
      "Train Epoch: 1 [19200/50000 (38%)]\tLoss: 0.100826\n",
      "Train Epoch: 1 [25600/50000 (51%)]\tLoss: 0.070845\n",
      "Train Epoch: 1 [32000/50000 (64%)]\tLoss: 0.096255\n",
      "Train Epoch: 1 [38400/50000 (77%)]\tLoss: 0.041809\n",
      "Train Epoch: 1 [44800/50000 (90%)]\tLoss: 0.077177\n",
      "\n",
      "--------- Testing in global mode ---------\n",
      "size of test_loader dataset:  10000\n",
      "\n",
      "Test set: Avg. loss: 0.0003, Accuracy: 9069/10000 (91%)\n",
      "\n",
      "Train Epoch: 2 [0/50000 (0%)]\tLoss: 0.084418\n",
      "Train Epoch: 2 [6400/50000 (13%)]\tLoss: 0.082418\n",
      "Train Epoch: 2 [12800/50000 (26%)]\tLoss: 0.057694\n",
      "Train Epoch: 2 [19200/50000 (38%)]\tLoss: 0.075087\n",
      "Train Epoch: 2 [25600/50000 (51%)]\tLoss: 0.113169\n",
      "Train Epoch: 2 [32000/50000 (64%)]\tLoss: 0.061594\n",
      "Train Epoch: 2 [38400/50000 (77%)]\tLoss: 0.083725\n",
      "Train Epoch: 2 [44800/50000 (90%)]\tLoss: 0.055700\n",
      "\n",
      "--------- Testing in global mode ---------\n",
      "size of test_loader dataset:  10000\n",
      "\n",
      "Test set: Avg. loss: 0.0003, Accuracy: 9069/10000 (91%)\n",
      "\n",
      "Train Epoch: 3 [0/50000 (0%)]\tLoss: 0.065325\n",
      "Train Epoch: 3 [6400/50000 (13%)]\tLoss: 0.036698\n",
      "Train Epoch: 3 [12800/50000 (26%)]\tLoss: 0.071318\n",
      "Train Epoch: 3 [19200/50000 (38%)]\tLoss: 0.054485\n",
      "Train Epoch: 3 [25600/50000 (51%)]\tLoss: 0.058405\n",
      "Train Epoch: 3 [32000/50000 (64%)]\tLoss: 0.041059\n",
      "Train Epoch: 3 [38400/50000 (77%)]\tLoss: 0.044897\n",
      "Train Epoch: 3 [44800/50000 (90%)]\tLoss: 0.085841\n",
      "\n",
      "--------- Testing in global mode ---------\n",
      "size of test_loader dataset:  10000\n",
      "\n",
      "Test set: Avg. loss: 0.0003, Accuracy: 9060/10000 (91%)\n",
      "\n",
      "Train Epoch: 4 [0/50000 (0%)]\tLoss: 0.088576\n",
      "Train Epoch: 4 [6400/50000 (13%)]\tLoss: 0.050531\n",
      "Train Epoch: 4 [12800/50000 (26%)]\tLoss: 0.083464\n",
      "Train Epoch: 4 [19200/50000 (38%)]\tLoss: 0.039458\n",
      "Train Epoch: 4 [25600/50000 (51%)]\tLoss: 0.040206\n",
      "Train Epoch: 4 [32000/50000 (64%)]\tLoss: 0.070399\n",
      "Train Epoch: 4 [38400/50000 (77%)]\tLoss: 0.072445\n",
      "Train Epoch: 4 [44800/50000 (90%)]\tLoss: 0.071258\n",
      "\n",
      "--------- Testing in global mode ---------\n",
      "size of test_loader dataset:  10000\n",
      "\n",
      "Test set: Avg. loss: 0.0003, Accuracy: 9050/10000 (90%)\n",
      "\n",
      "Train Epoch: 5 [0/50000 (0%)]\tLoss: 0.052611\n",
      "Train Epoch: 5 [6400/50000 (13%)]\tLoss: 0.061676\n",
      "Train Epoch: 5 [12800/50000 (26%)]\tLoss: 0.049378\n",
      "Train Epoch: 5 [19200/50000 (38%)]\tLoss: 0.048139\n",
      "Train Epoch: 5 [25600/50000 (51%)]\tLoss: 0.042785\n",
      "Train Epoch: 5 [32000/50000 (64%)]\tLoss: 0.037133\n",
      "Train Epoch: 5 [38400/50000 (77%)]\tLoss: 0.076163\n",
      "Train Epoch: 5 [44800/50000 (90%)]\tLoss: 0.027200\n",
      "\n",
      "--------- Testing in global mode ---------\n",
      "size of test_loader dataset:  10000\n",
      "\n",
      "Test set: Avg. loss: 0.0003, Accuracy: 9043/10000 (90%)\n",
      "\n",
      "Train Epoch: 6 [0/50000 (0%)]\tLoss: 0.032286\n",
      "Train Epoch: 6 [6400/50000 (13%)]\tLoss: 0.055140\n",
      "Train Epoch: 6 [12800/50000 (26%)]\tLoss: 0.044526\n",
      "Train Epoch: 6 [19200/50000 (38%)]\tLoss: 0.038840\n",
      "Train Epoch: 6 [25600/50000 (51%)]\tLoss: 0.022438\n",
      "Train Epoch: 6 [32000/50000 (64%)]\tLoss: 0.055642\n",
      "Train Epoch: 6 [38400/50000 (77%)]\tLoss: 0.045887\n",
      "Train Epoch: 6 [44800/50000 (90%)]\tLoss: 0.034372\n",
      "\n",
      "--------- Testing in global mode ---------\n",
      "size of test_loader dataset:  10000\n",
      "\n",
      "Test set: Avg. loss: 0.0003, Accuracy: 9037/10000 (90%)\n",
      "\n",
      "Train Epoch: 7 [0/50000 (0%)]\tLoss: 0.086913\n",
      "Train Epoch: 7 [6400/50000 (13%)]\tLoss: 0.054592\n",
      "Train Epoch: 7 [12800/50000 (26%)]\tLoss: 0.040316\n",
      "Train Epoch: 7 [19200/50000 (38%)]\tLoss: 0.038977\n",
      "Train Epoch: 7 [25600/50000 (51%)]\tLoss: 0.042053\n",
      "Train Epoch: 7 [32000/50000 (64%)]\tLoss: 0.047258\n",
      "Train Epoch: 7 [38400/50000 (77%)]\tLoss: 0.033528\n",
      "Train Epoch: 7 [44800/50000 (90%)]\tLoss: 0.064025\n",
      "\n",
      "--------- Testing in global mode ---------\n",
      "size of test_loader dataset:  10000\n",
      "\n",
      "Test set: Avg. loss: 0.0003, Accuracy: 9036/10000 (90%)\n",
      "\n",
      "Train Epoch: 8 [0/50000 (0%)]\tLoss: 0.038836\n",
      "Train Epoch: 8 [6400/50000 (13%)]\tLoss: 0.025743\n",
      "Train Epoch: 8 [12800/50000 (26%)]\tLoss: 0.024755\n",
      "Train Epoch: 8 [19200/50000 (38%)]\tLoss: 0.039706\n",
      "Train Epoch: 8 [25600/50000 (51%)]\tLoss: 0.056052\n",
      "Train Epoch: 8 [32000/50000 (64%)]\tLoss: 0.033363\n",
      "Train Epoch: 8 [38400/50000 (77%)]\tLoss: 0.028756\n",
      "Train Epoch: 8 [44800/50000 (90%)]\tLoss: 0.029979\n",
      "\n",
      "--------- Testing in global mode ---------\n",
      "size of test_loader dataset:  10000\n",
      "\n",
      "Test set: Avg. loss: 0.0003, Accuracy: 9030/10000 (90%)\n",
      "\n",
      "Train Epoch: 9 [0/50000 (0%)]\tLoss: 0.037950\n",
      "Train Epoch: 9 [6400/50000 (13%)]\tLoss: 0.018366\n",
      "Train Epoch: 9 [12800/50000 (26%)]\tLoss: 0.028059\n",
      "Train Epoch: 9 [19200/50000 (38%)]\tLoss: 0.046587\n",
      "Train Epoch: 9 [25600/50000 (51%)]\tLoss: 0.012991\n",
      "Train Epoch: 9 [32000/50000 (64%)]\tLoss: 0.055657\n",
      "Train Epoch: 9 [38400/50000 (77%)]\tLoss: 0.052003\n",
      "Train Epoch: 9 [44800/50000 (90%)]\tLoss: 0.021347\n",
      "\n",
      "--------- Testing in global mode ---------\n",
      "size of test_loader dataset:  10000\n",
      "\n",
      "Test set: Avg. loss: 0.0003, Accuracy: 9029/10000 (90%)\n",
      "\n",
      "Train Epoch: 10 [0/50000 (0%)]\tLoss: 0.032026\n",
      "Train Epoch: 10 [6400/50000 (13%)]\tLoss: 0.039842\n",
      "Train Epoch: 10 [12800/50000 (26%)]\tLoss: 0.050016\n",
      "Train Epoch: 10 [19200/50000 (38%)]\tLoss: 0.027208\n",
      "Train Epoch: 10 [25600/50000 (51%)]\tLoss: 0.032963\n",
      "Train Epoch: 10 [32000/50000 (64%)]\tLoss: 0.055067\n",
      "Train Epoch: 10 [38400/50000 (77%)]\tLoss: 0.031083\n",
      "Train Epoch: 10 [44800/50000 (90%)]\tLoss: 0.043165\n",
      "\n",
      "--------- Testing in global mode ---------\n",
      "size of test_loader dataset:  10000\n",
      "\n",
      "Test set: Avg. loss: 0.0003, Accuracy: 9027/10000 (90%)\n",
      "\n",
      "========================================================\n",
      "accuracies [90.32, 90.27]\n",
      "=========================================================\n",
      "layer features.0.weight_orig has #params  1728\n",
      "layer features.3.weight_orig has #params  73728\n",
      "layer features.6.weight_orig has #params  294912\n",
      "layer features.8.weight_orig has #params  589824\n",
      "layer features.11.weight_orig has #params  1179648\n",
      "layer features.13.weight_orig has #params  2359296\n",
      "layer features.16.weight_orig has #params  2359296\n",
      "layer features.18.weight_orig has #params  2359296\n",
      "layer classifier.weight has #params  5120\n",
      "Activation Timer start\n",
      "Activation Timer ends\n",
      "------- Geometric Ensembling -------\n",
      "Timer start\n",
      "Previous layer shape is  None\n",
      "Processing the coordinates to form ground_metric\n",
      "dont leave off the squaring of the ground metric\n",
      "returns a uniform measure of cardinality:  64\n",
      "returns a uniform measure of cardinality:  64\n",
      "the transport map is  tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')\n",
      "Ratio of trace to the matrix sum:  tensor(0.0469, device='cuda:0')\n",
      "Here, trace is 3.0 and matrix sum is 64.0 \n",
      "this is past correction for weight mode\n",
      "Shape of aligned wt is  torch.Size([64, 3, 9])\n",
      "Shape of fc_layer0_weight_data is  torch.Size([64, 3, 9])\n",
      "Previous layer shape is  torch.Size([64, 3, 3, 3])\n",
      "shape of layer: model 0 torch.Size([128, 64, 9])\n",
      "shape of layer: model 1 torch.Size([128, 64, 9])\n",
      "shape of previous transport map torch.Size([64, 64])\n",
      "Processing the coordinates to form ground_metric\n",
      "dont leave off the squaring of the ground metric\n",
      "returns a uniform measure of cardinality:  128\n",
      "returns a uniform measure of cardinality:  128\n",
      "the transport map is  tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        ...,\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0078],\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],\n",
      "       device='cuda:0')\n",
      "Ratio of trace to the matrix sum:  tensor(0.0078, device='cuda:0')\n",
      "Here, trace is 1.0 and matrix sum is 128.0 \n",
      "this is past correction for weight mode\n",
      "Shape of aligned wt is  torch.Size([128, 64, 9])\n",
      "Shape of fc_layer0_weight_data is  torch.Size([128, 64, 9])\n",
      "Previous layer shape is  torch.Size([128, 64, 3, 3])\n",
      "shape of layer: model 0 torch.Size([256, 128, 9])\n",
      "shape of layer: model 1 torch.Size([256, 128, 9])\n",
      "shape of previous transport map torch.Size([128, 128])\n",
      "Processing the coordinates to form ground_metric\n",
      "dont leave off the squaring of the ground metric\n",
      "returns a uniform measure of cardinality:  256\n",
      "returns a uniform measure of cardinality:  256\n",
      "the transport map is  tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')\n",
      "Ratio of trace to the matrix sum:  tensor(0., device='cuda:0')\n",
      "Here, trace is 0.0 and matrix sum is 256.0 \n",
      "this is past correction for weight mode\n",
      "Shape of aligned wt is  torch.Size([256, 128, 9])\n",
      "Shape of fc_layer0_weight_data is  torch.Size([256, 128, 9])\n",
      "Previous layer shape is  torch.Size([256, 128, 3, 3])\n",
      "shape of layer: model 0 torch.Size([256, 256, 9])\n",
      "shape of layer: model 1 torch.Size([256, 256, 9])\n",
      "shape of previous transport map torch.Size([256, 256])\n",
      "Processing the coordinates to form ground_metric\n",
      "dont leave off the squaring of the ground metric\n",
      "returns a uniform measure of cardinality:  256\n",
      "returns a uniform measure of cardinality:  256\n",
      "the transport map is  tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        ...,\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.0039, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],\n",
      "       device='cuda:0')\n",
      "Ratio of trace to the matrix sum:  tensor(0.0117, device='cuda:0')\n",
      "Here, trace is 3.0 and matrix sum is 256.0 \n",
      "this is past correction for weight mode\n",
      "Shape of aligned wt is  torch.Size([256, 256, 9])\n",
      "Shape of fc_layer0_weight_data is  torch.Size([256, 256, 9])\n",
      "Previous layer shape is  torch.Size([256, 256, 3, 3])\n",
      "shape of layer: model 0 torch.Size([512, 256, 9])\n",
      "shape of layer: model 1 torch.Size([512, 256, 9])\n",
      "shape of previous transport map torch.Size([256, 256])\n",
      "Processing the coordinates to form ground_metric\n",
      "dont leave off the squaring of the ground metric\n",
      "returns a uniform measure of cardinality:  512\n",
      "returns a uniform measure of cardinality:  512\n",
      "the transport map is  tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')\n",
      "Ratio of trace to the matrix sum:  tensor(0.0039, device='cuda:0')\n",
      "Here, trace is 2.0 and matrix sum is 512.0 \n",
      "this is past correction for weight mode\n",
      "Shape of aligned wt is  torch.Size([512, 256, 9])\n",
      "Shape of fc_layer0_weight_data is  torch.Size([512, 256, 9])\n",
      "Previous layer shape is  torch.Size([512, 256, 3, 3])\n",
      "shape of layer: model 0 torch.Size([512, 512, 9])\n",
      "shape of layer: model 1 torch.Size([512, 512, 9])\n",
      "shape of previous transport map torch.Size([512, 512])\n",
      "Processing the coordinates to form ground_metric\n",
      "dont leave off the squaring of the ground metric\n",
      "returns a uniform measure of cardinality:  512\n",
      "returns a uniform measure of cardinality:  512\n",
      "the transport map is  tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')\n",
      "Ratio of trace to the matrix sum:  tensor(0.0020, device='cuda:0')\n",
      "Here, trace is 1.0 and matrix sum is 512.0 \n",
      "this is past correction for weight mode\n",
      "Shape of aligned wt is  torch.Size([512, 512, 9])\n",
      "Shape of fc_layer0_weight_data is  torch.Size([512, 512, 9])\n",
      "Previous layer shape is  torch.Size([512, 512, 3, 3])\n",
      "shape of layer: model 0 torch.Size([512, 512, 9])\n",
      "shape of layer: model 1 torch.Size([512, 512, 9])\n",
      "shape of previous transport map torch.Size([512, 512])\n",
      "Processing the coordinates to form ground_metric\n",
      "dont leave off the squaring of the ground metric\n",
      "returns a uniform measure of cardinality:  512\n",
      "returns a uniform measure of cardinality:  512\n",
      "the transport map is  tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')\n",
      "Ratio of trace to the matrix sum:  tensor(0.0020, device='cuda:0')\n",
      "Here, trace is 1.0 and matrix sum is 512.0 \n",
      "this is past correction for weight mode\n",
      "Shape of aligned wt is  torch.Size([512, 512, 9])\n",
      "Shape of fc_layer0_weight_data is  torch.Size([512, 512, 9])\n",
      "Previous layer shape is  torch.Size([512, 512, 3, 3])\n",
      "shape of layer: model 0 torch.Size([512, 512, 9])\n",
      "shape of layer: model 1 torch.Size([512, 512, 9])\n",
      "shape of previous transport map torch.Size([512, 512])\n",
      "Processing the coordinates to form ground_metric\n",
      "dont leave off the squaring of the ground metric\n",
      "returns a uniform measure of cardinality:  512\n",
      "returns a uniform measure of cardinality:  512\n",
      "the transport map is  tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        [0.0020, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        ...,\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],\n",
      "       device='cuda:0')\n",
      "Ratio of trace to the matrix sum:  tensor(0., device='cuda:0')\n",
      "Here, trace is 0.0 and matrix sum is 512.0 \n",
      "this is past correction for weight mode\n",
      "Shape of aligned wt is  torch.Size([512, 512, 9])\n",
      "Shape of fc_layer0_weight_data is  torch.Size([512, 512, 9])\n",
      "Previous layer shape is  torch.Size([512, 512, 3, 3])\n",
      "shape of layer: model 0 torch.Size([10, 512])\n",
      "shape of layer: model 1 torch.Size([10, 512])\n",
      "shape of previous transport map torch.Size([512, 512])\n",
      "Processing the coordinates to form ground_metric\n",
      "dont leave off the squaring of the ground metric\n",
      "ground metric is  tensor([[1.4325, 2.4911, 2.8079, 2.8856, 2.7468, 2.8870, 2.7353, 2.6645, 2.5804,\n",
      "         2.6195],\n",
      "        [2.5208, 1.3107, 2.7097, 2.5832, 2.7581, 2.5509, 2.4414, 2.5393, 2.3284,\n",
      "         2.4193],\n",
      "        [2.7692, 2.6807, 1.3577, 2.9864, 2.9482, 2.8599, 2.7595, 2.8709, 2.7691,\n",
      "         2.7716],\n",
      "        [2.8418, 2.6804, 2.9077, 1.5495, 2.8531, 2.9723, 2.7705, 2.8076, 2.8298,\n",
      "         2.6796],\n",
      "        [2.8005, 2.5947, 2.8385, 2.9068, 1.5305, 2.7492, 2.8050, 2.6826, 2.6967,\n",
      "         2.7531],\n",
      "        [2.9442, 2.5952, 2.9950, 2.8124, 2.7448, 1.5445, 2.7571, 2.7091, 2.8377,\n",
      "         2.7059],\n",
      "        [2.6948, 2.4699, 2.7141, 2.9192, 2.6649, 2.8837, 1.4110, 2.7135, 2.4951,\n",
      "         2.5593],\n",
      "        [2.7428, 2.5757, 2.7854, 2.8189, 2.7297, 2.7164, 2.7106, 1.3886, 2.5619,\n",
      "         2.5139],\n",
      "        [2.5881, 2.3878, 2.7745, 2.7406, 2.7236, 2.8404, 2.5578, 2.6629, 1.3044,\n",
      "         2.3747],\n",
      "        [2.5971, 2.2644, 2.8401, 2.7362, 2.6806, 2.7179, 2.5752, 2.5408, 2.5460,\n",
      "         1.3207]], device='cuda:0', grad_fn=<PowBackward0>)\n",
      "returns a uniform measure of cardinality:  10\n",
      "returns a uniform measure of cardinality:  10\n",
      "the transport map is  tensor([[0.1000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0000, 0.1000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0000, 0.0000, 0.1000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.1000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.1000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.1000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.1000, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.1000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.1000,\n",
      "         0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.1000]], device='cuda:0')\n",
      "Ratio of trace to the matrix sum:  tensor(1., device='cuda:0')\n",
      "Here, trace is 9.99755859375 and matrix sum is 9.99755859375 \n",
      "this is past correction for weight mode\n",
      "Shape of aligned wt is  torch.Size([10, 512])\n",
      "Shape of fc_layer0_weight_data is  torch.Size([10, 512])\n",
      "using independent method\n",
      "in _make_layers [Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), AvgPool2d(kernel_size=1, stride=1, padding=0)]\n",
      "Relu Inplace is  False\n",
      "model parameters are \n",
      " [torch.Size([64, 3, 3, 3]), torch.Size([128, 64, 3, 3]), torch.Size([256, 128, 3, 3]), torch.Size([256, 256, 3, 3]), torch.Size([512, 256, 3, 3]), torch.Size([512, 512, 3, 3]), torch.Size([512, 512, 3, 3]), torch.Size([512, 512, 3, 3]), torch.Size([10, 512])]\n",
      "\n",
      "--------- Testing in global mode ---------\n",
      "size of test_loader dataset:  10000\n",
      "\n",
      "Test set: Avg. loss: 0.0023, Accuracy: 776/10000 (8%)\n",
      "\n",
      "len of model parameters and avg aligned layers is  9 9\n",
      "len of model_state_dict is  9\n",
      "len of param_list is  9\n",
      "\n",
      "--------- Testing in global mode ---------\n",
      "size of test_loader dataset:  10000\n",
      "\n",
      "Test set: Avg. loss: 0.0012, Accuracy: 8607/10000 (86%)\n",
      "\n",
      "===============================================\n",
      "===============================================\n",
      "After Ensemble: 86.07\n",
      "===============================================\n",
      "===============================================\n",
      "Timer ends\n",
      "Time taken for geometric ensembling is 3.543765887618065 seconds\n",
      "------- Prediction based ensembling -------\n",
      "\n",
      "Test set: Avg. loss: 0.0003, Accuracy: 9120/10000 (91%)\n",
      "\n",
      "------- Naive ensembling of weights -------\n",
      "[torch.Size([64, 3, 3, 3]), torch.Size([64, 3, 3, 3])]\n",
      "torch.Size([64, 3, 3, 3])\n",
      "[torch.Size([128, 64, 3, 3]), torch.Size([128, 64, 3, 3])]\n",
      "torch.Size([128, 64, 3, 3])\n",
      "[torch.Size([256, 128, 3, 3]), torch.Size([256, 128, 3, 3])]\n",
      "torch.Size([256, 128, 3, 3])\n",
      "[torch.Size([256, 256, 3, 3]), torch.Size([256, 256, 3, 3])]\n",
      "torch.Size([256, 256, 3, 3])\n",
      "[torch.Size([512, 256, 3, 3]), torch.Size([512, 256, 3, 3])]\n",
      "torch.Size([512, 256, 3, 3])\n",
      "[torch.Size([512, 512, 3, 3]), torch.Size([512, 512, 3, 3])]\n",
      "torch.Size([512, 512, 3, 3])\n",
      "[torch.Size([512, 512, 3, 3]), torch.Size([512, 512, 3, 3])]\n",
      "torch.Size([512, 512, 3, 3])\n",
      "[torch.Size([512, 512, 3, 3]), torch.Size([512, 512, 3, 3])]\n",
      "torch.Size([512, 512, 3, 3])\n",
      "[torch.Size([10, 512]), torch.Size([10, 512])]\n",
      "torch.Size([10, 512])\n",
      "in _make_layers [Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), AvgPool2d(kernel_size=1, stride=1, padding=0)]\n",
      "Relu Inplace is  False\n",
      "model parameters are \n",
      " [torch.Size([64, 3, 3, 3]), torch.Size([128, 64, 3, 3]), torch.Size([256, 128, 3, 3]), torch.Size([256, 256, 3, 3]), torch.Size([512, 256, 3, 3]), torch.Size([512, 512, 3, 3]), torch.Size([512, 512, 3, 3]), torch.Size([512, 512, 3, 3]), torch.Size([10, 512])]\n",
      "\n",
      "--------- Testing in global mode ---------\n",
      "size of test_loader dataset:  10000\n",
      "\n",
      "Test set: Avg. loss: 0.0023, Accuracy: 1066/10000 (11%)\n",
      "\n",
      "\n",
      "--------- Testing in global mode ---------\n",
      "size of test_loader dataset:  10000\n",
      "\n",
      "Test set: Avg. loss: 0.0023, Accuracy: 1430/10000 (14%)\n",
      "\n",
      "-------- Retraining the models ---------\n",
      "Retraining model :  model_0\n",
      "num_epochs---------------+++))))))+++ 300\n",
      "num_epochs---------------+++))))))+++ 300\n",
      "lr is  0.05\n",
      "number of epochs would be  30\n",
      "num_epochs-------------- 30\n",
      "Epoch 000\n",
      "/root/miniconda3/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:129: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\"Detected call of `lr_scheduler.step()` before `optimizer.step()`. \"\n",
      "/root/miniconda3/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:154: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "accuracy: {'epoch': 0, 'value': 0.46289999999999987} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 0, 'value': 1.4781726442313183} ({'split': 'train'})\n",
      "accuracy: {'epoch': 0, 'value': 0.6501000344753266} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 0, 'value': 1.1087595582008363} ({'split': 'test'})\n",
      "Epoch 001\n",
      "accuracy: {'epoch': 1, 'value': 0.7291599999999998} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 1, 'value': 0.826286238250733} ({'split': 'train'})\n",
      "accuracy: {'epoch': 1, 'value': 0.7320000410079956} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 1, 'value': 0.8250816404819489} ({'split': 'test'})\n",
      "Epoch 002\n",
      "accuracy: {'epoch': 2, 'value': 0.7880999999999994} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 2, 'value': 0.6528088059806817} ({'split': 'train'})\n",
      "accuracy: {'epoch': 2, 'value': 0.7668000400066376} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 2, 'value': 0.730934739112854} ({'split': 'test'})\n",
      "Epoch 003\n",
      "accuracy: {'epoch': 3, 'value': 0.8120399999999987} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 3, 'value': 0.5733953863716125} ({'split': 'train'})\n",
      "accuracy: {'epoch': 3, 'value': 0.7851000368595124} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 3, 'value': 0.6722977876663209} ({'split': 'test'})\n",
      "Epoch 004\n",
      "accuracy: {'epoch': 4, 'value': 0.8281799999999998} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 4, 'value': 0.522081580562592} ({'split': 'train'})\n",
      "accuracy: {'epoch': 4, 'value': 0.742600041627884} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 4, 'value': 0.7877674579620361} ({'split': 'test'})\n",
      "Epoch 005\n",
      "accuracy: {'epoch': 5, 'value': 0.8413599999999998} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 5, 'value': 0.4835167716407773} ({'split': 'train'})\n",
      "accuracy: {'epoch': 5, 'value': 0.7761000335216522} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 5, 'value': 0.6785974979400635} ({'split': 'test'})\n",
      "Epoch 006\n",
      "accuracy: {'epoch': 6, 'value': 0.8492999999999998} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 6, 'value': 0.45949079609870924} ({'split': 'train'})\n",
      "accuracy: {'epoch': 6, 'value': 0.7769000351428985} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 6, 'value': 0.7717680156230928} ({'split': 'test'})\n",
      "Epoch 007\n",
      "accuracy: {'epoch': 7, 'value': 0.8543799999999994} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 7, 'value': 0.44271955781936656} ({'split': 'train'})\n",
      "accuracy: {'epoch': 7, 'value': 0.7910000324249268} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 7, 'value': 0.6960731267929077} ({'split': 'test'})\n",
      "Epoch 008\n",
      "accuracy: {'epoch': 8, 'value': 0.8610800000000001} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 8, 'value': 0.42570056202888484} ({'split': 'train'})\n",
      "accuracy: {'epoch': 8, 'value': 0.7890000283718108} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 8, 'value': 0.6952049136161804} ({'split': 'test'})\n",
      "Epoch 009\n",
      "accuracy: {'epoch': 9, 'value': 0.8681000000000014} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 9, 'value': 0.4033397150611873} ({'split': 'train'})\n",
      "accuracy: {'epoch': 9, 'value': 0.7845000445842742} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 9, 'value': 0.7178995668888092} ({'split': 'test'})\n",
      "Epoch 010\n",
      "accuracy: {'epoch': 10, 'value': 0.8682400000000006} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 10, 'value': 0.4090823437023163} ({'split': 'train'})\n",
      "accuracy: {'epoch': 10, 'value': 0.7613000392913818} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 10, 'value': 0.767329216003418} ({'split': 'test'})\n",
      "Epoch 011\n",
      "accuracy: {'epoch': 11, 'value': 0.8749000000000005} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 11, 'value': 0.38218344170808766} ({'split': 'train'})\n",
      "accuracy: {'epoch': 11, 'value': 0.789300036430359} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 11, 'value': 0.7229805827140808} ({'split': 'test'})\n",
      "Epoch 012\n",
      "accuracy: {'epoch': 12, 'value': 0.8726600000000002} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 12, 'value': 0.3849793776178355} ({'split': 'train'})\n",
      "accuracy: {'epoch': 12, 'value': 0.7916000425815582} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 12, 'value': 0.6692538261413574} ({'split': 'test'})\n",
      "Epoch 013\n",
      "accuracy: {'epoch': 13, 'value': 0.8781799999999991} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 13, 'value': 0.3750991787767409} ({'split': 'train'})\n",
      "accuracy: {'epoch': 13, 'value': 0.7958000361919403} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 13, 'value': 0.643027800321579} ({'split': 'test'})\n",
      "Epoch 014\n",
      "accuracy: {'epoch': 14, 'value': 0.8838600000000002} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 14, 'value': 0.3563724224233628} ({'split': 'train'})\n",
      "accuracy: {'epoch': 14, 'value': 0.7645000338554382} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 14, 'value': 0.8039357185363771} ({'split': 'test'})\n",
      "Epoch 015\n",
      "accuracy: {'epoch': 15, 'value': 0.8795399999999995} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 15, 'value': 0.3723861938476566} ({'split': 'train'})\n",
      "accuracy: {'epoch': 15, 'value': 0.8037000477313996} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 15, 'value': 0.6595152020454408} ({'split': 'test'})\n",
      "Epoch 016\n",
      "accuracy: {'epoch': 16, 'value': 0.8880200000000007} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 16, 'value': 0.349146549606323} ({'split': 'train'})\n",
      "accuracy: {'epoch': 16, 'value': 0.7815000355243683} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 16, 'value': 0.7728685617446899} ({'split': 'test'})\n",
      "Epoch 017\n",
      "accuracy: {'epoch': 17, 'value': 0.8816200000000005} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 17, 'value': 0.3663059703636166} ({'split': 'train'})\n",
      "accuracy: {'epoch': 17, 'value': 0.810200035572052} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 17, 'value': 0.648629641532898} ({'split': 'test'})\n",
      "Epoch 018\n",
      "accuracy: {'epoch': 18, 'value': 0.8779999999999991} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 18, 'value': 0.37434383252620684} ({'split': 'train'})\n",
      "accuracy: {'epoch': 18, 'value': 0.7528000354766846} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 18, 'value': 0.8281574130058289} ({'split': 'test'})\n",
      "Epoch 019\n",
      "^C\n",
      "Traceback (most recent call last):\n",
      "  File \"main.py\", line 351, in <module>\n",
      "    _, best_retrain_acc = routines.retrain_models(args, [*original_models, geometric_model, naive_model], retrain_loader, test_loader, config, tensorboard_obj=tensorboard_obj, initial_acc=initial_acc, nicks=nicks)\n",
      "  File \"/root/autodl-tmp/OTfusion_pruning/otfusion/routines.py\", line 394, in retrain_models\n",
      "    retrained_network, acc = cifar_train.get_retrained_model(args, retrain_loader, test_loader, old_networks[i], config, output_root_dir, tensorboard_obj=tensorboard_obj, nick=nick, start_acc=initial_acc[i])\n",
      "  File \"/root/autodl-tmp/OTfusion_pruning/otfusion/./cifar/train.py\", line 333, in get_retrained_model\n",
      "    best_acc,model = main(config, output_dir, args.gpu_id, pretrained_model=old_network, pretrained_dataset=(train_loader, test_loader), tensorboard_obj=tensorboard_obj,return_model=True)\n",
      "  File \"/root/autodl-tmp/OTfusion_pruning/otfusion/./cifar/train.py\", line 67, in main\n",
      "    for batch_x, batch_y in training_loader:\n",
      "  File \"/root/miniconda3/lib/python3.8/site-packages/torch/utils/data/dataloader.py\", line 521, in __next__\n",
      "    data = self._next_data()\n",
      "  File \"/root/miniconda3/lib/python3.8/site-packages/torch/utils/data/dataloader.py\", line 561, in _next_data\n",
      "    data = self._dataset_fetcher.fetch(index)  # may raise StopIteration\n",
      "  File \"/root/miniconda3/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py\", line 49, in fetch\n",
      "    data = [self.dataset[idx] for idx in possibly_batched_index]\n",
      "  File \"/root/miniconda3/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py\", line 49, in <listcomp>\n",
      "    data = [self.dataset[idx] for idx in possibly_batched_index]\n",
      "  File \"/root/miniconda3/lib/python3.8/site-packages/torchvision/datasets/cifar.py\", line 121, in __getitem__\n",
      "    img = self.transform(img)\n",
      "  File \"/root/miniconda3/lib/python3.8/site-packages/torchvision/transforms/transforms.py\", line 61, in __call__\n",
      "    img = t(img)\n",
      "  File \"/root/miniconda3/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1102, in _call_impl\n",
      "    return forward_call(*input, **kwargs)\n",
      "  File \"/root/miniconda3/lib/python3.8/site-packages/torchvision/transforms/transforms.py\", line 226, in forward\n",
      "    return F.normalize(tensor, self.mean, self.std, self.inplace)\n",
      "  File \"/root/miniconda3/lib/python3.8/site-packages/torchvision/transforms/functional.py\", line 345, in normalize\n",
      "    if (std == 0).any():\n",
      "KeyboardInterrupt\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m~/miniconda3/lib/python3.8/site-packages/IPython/utils/_process_posix.py\u001b[0m in \u001b[0;36msystem\u001b[0;34m(self, cmd)\u001b[0m\n\u001b[1;32m    161\u001b[0m                 \u001b[0;31m# know whether we've finished (if we matched EOF) or not\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 162\u001b[0;31m                 \u001b[0mres_idx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mchild\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexpect_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpatterns\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_timeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    163\u001b[0m                 \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchild\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbefore\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mout_size\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'replace'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m''\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.8/site-packages/pexpect/spawnbase.py\u001b[0m in \u001b[0;36mexpect_list\u001b[0;34m(self, pattern_list, timeout, searchwindowsize, async_, **kw)\u001b[0m\n\u001b[1;32m    371\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 372\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mexp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexpect_loop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    373\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.8/site-packages/pexpect/expect.py\u001b[0m in \u001b[0;36mexpect_loop\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    168\u001b[0m                 \u001b[0;31m# Still have time left, so read more data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 169\u001b[0;31m                 \u001b[0mincoming\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mspawn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_nonblocking\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mspawn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmaxread\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    170\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mspawn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdelayafterread\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.8/site-packages/pexpect/pty_spawn.py\u001b[0m in \u001b[0;36mread_nonblocking\u001b[0;34m(self, size, timeout)\u001b[0m\n\u001b[1;32m    499\u001b[0m         \u001b[0;31m# (possibly timeout=None), we call select() with a timeout.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 500\u001b[0;31m         \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mselect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    501\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mspawn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_nonblocking\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.8/site-packages/pexpect/pty_spawn.py\u001b[0m in \u001b[0;36mselect\u001b[0;34m(timeout)\u001b[0m\n\u001b[1;32m    449\u001b[0m             \u001b[0;32mdef\u001b[0m \u001b[0mselect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 450\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mselect_ignore_interrupts\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchild_fd\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    451\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.8/site-packages/pexpect/utils.py\u001b[0m in \u001b[0;36mselect_ignore_interrupts\u001b[0;34m(iwtd, owtd, ewtd, timeout)\u001b[0m\n\u001b[1;32m    142\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 143\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mselect\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miwtd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mowtd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mewtd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    144\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mInterruptedError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: ",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_9719/719175872.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msystem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'python main.py --gpu-id 0 --model-name vgg11_nobias --n-epochs 10 --save-result-file sample.csv  --sweep-name exp_sample --correction --ground-metric euclidean --weight-stats  --geom-ensemble-type wts --ground-metric-normalize none --sweep-id 90  --ckpt-type best --dataset Cifar10 --ground-metric-eff --recheck-cifar --activation-seed {activation_seed}  --prelu-acts --past-correction --not-squared --exact --learning-rate 0.0001 --momentum 0.5 --batch-size-train 64 --experiment-dir {experiment_dir} --prunint-rate {prunint_rate} --run-mode {run_mode} --to-download --load-model-dir {load_model_dir} --retrain 30'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/miniconda3/lib/python3.8/site-packages/ipykernel/zmqshell.py\u001b[0m in \u001b[0;36msystem_piped\u001b[0;34m(self, cmd)\u001b[0m\n\u001b[1;32m    635\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muser_ns\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'_exit_code'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msystem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcmd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    636\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 637\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muser_ns\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'_exit_code'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msystem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvar_expand\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcmd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdepth\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    638\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    639\u001b[0m     \u001b[0;31m# Ensure new system_piped implementation is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.8/site-packages/IPython/utils/_process_posix.py\u001b[0m in \u001b[0;36msystem\u001b[0;34m(self, cmd)\u001b[0m\n\u001b[1;32m    184\u001b[0m             \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    185\u001b[0m                 \u001b[0;31m# Ensure the subprocess really is terminated\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 186\u001b[0;31m                 \u001b[0mchild\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mterminate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mforce\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    187\u001b[0m         \u001b[0;31m# add isalive check, to ensure exitstatus is set:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    188\u001b[0m         \u001b[0mchild\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misalive\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.8/site-packages/pexpect/pty_spawn.py\u001b[0m in \u001b[0;36mterminate\u001b[0;34m(self, force)\u001b[0m\n\u001b[1;32m    653\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mforce\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    654\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkill\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msignal\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSIGKILL\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 655\u001b[0;31m                 \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdelayafterterminate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    656\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misalive\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    657\u001b[0m                     \u001b[0;32mreturn\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a78f1c0-85a7-4629-ba27-fcbf65e981c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "activation_seed=21\n",
    "experiment_dir=\"experiment1\"\n",
    "prunint_rate=0.2\n",
    "run_mode='benchmark'\n",
    "load_model_dir='./cifar_models'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c5b102d6-1a16-4de0-88d5-d56ced2e8d4a",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------- Setting up parameters -------\n",
      "dumping parameters at  /root/autodl-tmp/OTfusion_pruning/otfusion/exp_sample/configurations\n",
      "The parameters are: \n",
      " Namespace(act_bug=False, act_num_samples=100, activation_histograms=False, activation_mode=None, activation_seed=21, alpha=0.7, baseroot='/root/autodl-tmp/OTfusion_pruning/otfusion', batch_size_test=1000, batch_size_train=64, center_acts=False, choice='0 2 4 6 8', cifar_style_data=False, ckpt_type='best', clip_gm=False, clip_max=5, clip_min=0, config_dir='/root/autodl-tmp/OTfusion_pruning/otfusion/exp_sample/configurations', config_file=None, correction=True, csv_dir='/root/autodl-tmp/OTfusion_pruning/otfusion/exp_sample/csv', dataset='Cifar10', debug=False, deprecated=None, deterministic=False, diff_init=False, disable_bias=True, dist_epochs=60, dist_normalize=False, dump_final_models=False, dump_model=False, enable_dropout=False, ensemble_step=0.5, eval_aligned=True, exact=True, exp_name='exp_2023-09-16_10-25-05_499081', experiment_dir='experiment1', geom_ensemble_type='wts', gpu_id=0, gromov=False, gromov_loss='square_loss', ground_metric='euclidean', ground_metric_eff=True, ground_metric_normalize='none', handle_skips=False, importance=None, learning_rate=0.004, load_model_dir='./cifar_models', load_models='', log_interval=100, model_name='vgg11_nobias', momentum=0.5, n_epochs=0, no_random_trainloaders=False, normalize_acts=False, normalize_wts=False, not_squared=True, num_hidden_nodes=400, num_hidden_nodes1=400, num_hidden_nodes2=200, num_hidden_nodes3=100, num_hidden_nodes4=50, num_models=2, options_type='generic', partial_reshape=False, partition_dataloader=-1, partition_type='labels', past_correction=True, personal_class_idx=9, personal_split_frac=0.1, pool_acts=False, pool_relu=False, prediction_wts=False, prelu_acts=True, print_distances=False, proper_marginals=False, prunint_rate=0.2, recheck_acc=False, recheck_cifar=True, reg=0.01, reg_m=0.001, reinit_trainloaders=False, result_dir='/root/autodl-tmp/OTfusion_pruning/otfusion/exp_sample/results', retrain=30, retrain_avg_only=False, retrain_geometric_only=False, retrain_lr_decay=-1, retrain_lr_decay_epochs=None, retrain_lr_decay_factor=None, retrain_seed=-1, rootdir='/root/autodl-tmp/OTfusion_pruning/otfusion/exp_sample', run_mode='benchmark', same_model=-1, save_result_file='sample.csv', second_model_name=None, sinkhorn_type='normal', skip_last_layer=False, skip_last_layer_type='average', skip_personal_idx=False, skip_retrain=-1, softmax_temperature=1, standardize_acts=False, sweep_id=90, sweep_name='exp_sample', temperature=20, tensorboard=False, tensorboard_root='./tensorboard', timestamp='2023-09-16_10-25-05_499081', tmap_stats=False, to_download=True, transform_acts=False, unbalanced=False, update_acts=False, weight_stats=True, width_ratio=1)\n",
      "refactored get_config\n",
      "------- Obtain dataloaders -------\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "------- Training independent models -------\n",
      "QWERTY: Enter Cifar 1\n",
      "number of epochs would b>>>>>>>e  300\n",
      "=======================================================\n",
      "=======================================================\n",
      "=======================================================\n",
      "Print the config {'dataset': 'Cifar10', 'model': 'vgg11_nobias', 'optimizer': 'SGD', 'optimizer_decay_at_epochs': [30, 60, 90, 120, 150, 180, 210, 240, 270], 'optimizer_decay_with_factor': 2.0, 'optimizer_learning_rate': 0.05, 'optimizer_momentum': 0.9, 'optimizer_weight_decay': 0.0005, 'batch_size': 128, 'num_epochs': 300, 'seed': 42}\n",
      "in _make_layers [Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), AvgPool2d(kernel_size=1, stride=1, padding=0)]\n",
      "Relu Inplace is  False\n",
      "model parameters are \n",
      " [torch.Size([64, 3, 3, 3]), torch.Size([128, 64, 3, 3]), torch.Size([256, 128, 3, 3]), torch.Size([256, 256, 3, 3]), torch.Size([512, 256, 3, 3]), torch.Size([512, 512, 3, 3]), torch.Size([512, 512, 3, 3]), torch.Size([512, 512, 3, 3]), torch.Size([10, 512])]\n",
      "Loading model at path ./cifar_models/model_0/best.checkpoint which had accuracy 0.9030999821424489 and at epoch 127\n",
      "QWERTY: Enter Cifar 1\n",
      "number of epochs would b>>>>>>>e  300\n",
      "=======================================================\n",
      "=======================================================\n",
      "=======================================================\n",
      "Print the config {'dataset': 'Cifar10', 'model': 'vgg11_nobias', 'optimizer': 'SGD', 'optimizer_decay_at_epochs': [30, 60, 90, 120, 150, 180, 210, 240, 270], 'optimizer_decay_with_factor': 2.0, 'optimizer_learning_rate': 0.05, 'optimizer_momentum': 0.9, 'optimizer_weight_decay': 0.0005, 'batch_size': 128, 'num_epochs': 300, 'seed': 42}\n",
      "in _make_layers [Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), AvgPool2d(kernel_size=1, stride=1, padding=0)]\n",
      "Relu Inplace is  False\n",
      "model parameters are \n",
      " [torch.Size([64, 3, 3, 3]), torch.Size([128, 64, 3, 3]), torch.Size([256, 128, 3, 3]), torch.Size([256, 256, 3, 3]), torch.Size([512, 256, 3, 3]), torch.Size([512, 512, 3, 3]), torch.Size([512, 512, 3, 3]), torch.Size([512, 512, 3, 3]), torch.Size([10, 512])]\n",
      "Loading model at path ./cifar_models/model_1/best.checkpoint which had accuracy 0.9049999803304669 and at epoch 134\n",
      "num_epochs---------------++++++ 300\n",
      "========================================\n",
      "========================================\n",
      "TEST:args.learning_rate,args.momentum 0.004 0.5\n",
      "========================================\n",
      "========================================\n",
      "========================================\n",
      "\n",
      "--------- Testing in global mode ---------\n",
      "size of test_loader dataset:  10000\n",
      "\n",
      "Test set: Avg. loss: 0.0003, Accuracy: 9019/10000 (90%)\n",
      "\n",
      "========================================\n",
      "========================================\n",
      "TEST:args.learning_rate,args.momentum 0.004 0.5\n",
      "========================================\n",
      "========================================\n",
      "========================================\n",
      "\n",
      "--------- Testing in global mode ---------\n",
      "size of test_loader dataset:  10000\n",
      "\n",
      "Test set: Avg. loss: 0.0003, Accuracy: 9046/10000 (90%)\n",
      "\n",
      "models [VGG(\n",
      "  (features): Sequential(\n",
      "    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    (1): ReLU()\n",
      "    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (3): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    (4): ReLU()\n",
      "    (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (6): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    (7): ReLU()\n",
      "    (8): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    (9): ReLU()\n",
      "    (10): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (11): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    (12): ReLU()\n",
      "    (13): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    (14): ReLU()\n",
      "    (15): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (16): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    (17): ReLU()\n",
      "    (18): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    (19): ReLU()\n",
      "    (20): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (21): AvgPool2d(kernel_size=1, stride=1, padding=0)\n",
      "  )\n",
      "  (classifier): Linear(in_features=512, out_features=10, bias=False)\n",
      "), VGG(\n",
      "  (features): Sequential(\n",
      "    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    (1): ReLU()\n",
      "    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (3): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    (4): ReLU()\n",
      "    (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (6): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    (7): ReLU()\n",
      "    (8): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    (9): ReLU()\n",
      "    (10): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (11): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    (12): ReLU()\n",
      "    (13): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    (14): ReLU()\n",
      "    (15): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (16): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    (17): ReLU()\n",
      "    (18): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    (19): ReLU()\n",
      "    (20): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (21): AvgPool2d(kernel_size=1, stride=1, padding=0)\n",
      "  )\n",
      "  (classifier): Linear(in_features=512, out_features=10, bias=False)\n",
      ")]\n",
      "Apply Unstructured L1 Pruning Globally (all conv layers)\n",
      "Apply Unstructured L1 Pruning Globally (all conv layers)\n",
      "layer features.0.weight has #params  1728\n",
      "layer features.3.weight has #params  73728\n",
      "layer features.6.weight has #params  294912\n",
      "layer features.8.weight has #params  589824\n",
      "layer features.11.weight has #params  1179648\n",
      "layer features.13.weight has #params  2359296\n",
      "layer features.16.weight has #params  2359296\n",
      "layer features.18.weight has #params  2359296\n",
      "layer classifier.weight has #params  5120\n",
      "Activation Timer start\n",
      "Activation Timer ends\n",
      "------- Geometric Ensembling -------\n",
      "Timer start\n",
      "Previous layer shape is  None\n",
      "Processing the coordinates to form ground_metric\n",
      "dont leave off the squaring of the ground metric\n",
      "returns a uniform measure of cardinality:  64\n",
      "returns a uniform measure of cardinality:  64\n",
      "the transport map is  tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')\n",
      "Ratio of trace to the matrix sum:  tensor(0.0469, device='cuda:0')\n",
      "Here, trace is 3.0 and matrix sum is 64.0 \n",
      "this is past correction for weight mode\n",
      "Shape of aligned wt is  torch.Size([64, 3, 9])\n",
      "Shape of fc_layer0_weight_data is  torch.Size([64, 3, 9])\n",
      "in _make_layers [Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), AvgPool2d(kernel_size=1, stride=1, padding=0)]\n",
      "Relu Inplace is  False\n",
      "model parameters are \n",
      " [torch.Size([64, 3, 3, 3]), torch.Size([128, 64, 3, 3]), torch.Size([256, 128, 3, 3]), torch.Size([256, 256, 3, 3]), torch.Size([512, 256, 3, 3]), torch.Size([512, 512, 3, 3]), torch.Size([512, 512, 3, 3]), torch.Size([512, 512, 3, 3]), torch.Size([10, 512])]\n",
      "len of model_state_dict is  9\n",
      "len of new_params is  1\n",
      "updated parameters for layer  features.0.weight\n",
      "\n",
      "--------- Testing in global mode ---------\n",
      "size of test_loader dataset:  10000\n",
      "\n",
      "Test set: Avg. loss: 0.0027, Accuracy: 1249/10000 (12%)\n",
      "\n",
      "accuracy after update is  12.49\n",
      "For layer idx 0, accuracy of the updated model is 12.49\n",
      "Previous layer shape is  torch.Size([64, 3, 3, 3])\n",
      "shape of layer: model 0 torch.Size([128, 64, 9])\n",
      "shape of layer: model 1 torch.Size([128, 64, 9])\n",
      "shape of previous transport map torch.Size([64, 64])\n",
      "Processing the coordinates to form ground_metric\n",
      "dont leave off the squaring of the ground metric\n",
      "returns a uniform measure of cardinality:  128\n",
      "returns a uniform measure of cardinality:  128\n",
      "the transport map is  tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        ...,\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0078],\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],\n",
      "       device='cuda:0')\n",
      "Ratio of trace to the matrix sum:  tensor(0.0078, device='cuda:0')\n",
      "Here, trace is 1.0 and matrix sum is 128.0 \n",
      "this is past correction for weight mode\n",
      "Shape of aligned wt is  torch.Size([128, 64, 9])\n",
      "Shape of fc_layer0_weight_data is  torch.Size([128, 64, 9])\n",
      "in _make_layers [Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), AvgPool2d(kernel_size=1, stride=1, padding=0)]\n",
      "Relu Inplace is  False\n",
      "model parameters are \n",
      " [torch.Size([64, 3, 3, 3]), torch.Size([128, 64, 3, 3]), torch.Size([256, 128, 3, 3]), torch.Size([256, 256, 3, 3]), torch.Size([512, 256, 3, 3]), torch.Size([512, 512, 3, 3]), torch.Size([512, 512, 3, 3]), torch.Size([512, 512, 3, 3]), torch.Size([10, 512])]\n",
      "len of model_state_dict is  9\n",
      "len of new_params is  2\n",
      "updated parameters for layer  features.0.weight\n",
      "updated parameters for layer  features.3.weight\n",
      "\n",
      "--------- Testing in global mode ---------\n",
      "size of test_loader dataset:  10000\n",
      "\n",
      "Test set: Avg. loss: 0.0024, Accuracy: 1036/10000 (10%)\n",
      "\n",
      "accuracy after update is  10.36\n",
      "For layer idx 1, accuracy of the updated model is 10.36\n",
      "Previous layer shape is  torch.Size([128, 64, 3, 3])\n",
      "shape of layer: model 0 torch.Size([256, 128, 9])\n",
      "shape of layer: model 1 torch.Size([256, 128, 9])\n",
      "shape of previous transport map torch.Size([128, 128])\n",
      "Processing the coordinates to form ground_metric\n",
      "dont leave off the squaring of the ground metric\n",
      "returns a uniform measure of cardinality:  256\n",
      "returns a uniform measure of cardinality:  256\n",
      "the transport map is  tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')\n",
      "Ratio of trace to the matrix sum:  tensor(0., device='cuda:0')\n",
      "Here, trace is 0.0 and matrix sum is 256.0 \n",
      "this is past correction for weight mode\n",
      "Shape of aligned wt is  torch.Size([256, 128, 9])\n",
      "Shape of fc_layer0_weight_data is  torch.Size([256, 128, 9])\n",
      "in _make_layers [Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), AvgPool2d(kernel_size=1, stride=1, padding=0)]\n",
      "Relu Inplace is  False\n",
      "model parameters are \n",
      " [torch.Size([64, 3, 3, 3]), torch.Size([128, 64, 3, 3]), torch.Size([256, 128, 3, 3]), torch.Size([256, 256, 3, 3]), torch.Size([512, 256, 3, 3]), torch.Size([512, 512, 3, 3]), torch.Size([512, 512, 3, 3]), torch.Size([512, 512, 3, 3]), torch.Size([10, 512])]\n",
      "len of model_state_dict is  9\n",
      "len of new_params is  3\n",
      "updated parameters for layer  features.0.weight\n",
      "updated parameters for layer  features.3.weight\n",
      "updated parameters for layer  features.6.weight\n",
      "\n",
      "--------- Testing in global mode ---------\n",
      "size of test_loader dataset:  10000\n",
      "\n",
      "Test set: Avg. loss: 0.0023, Accuracy: 1146/10000 (11%)\n",
      "\n",
      "accuracy after update is  11.46\n",
      "For layer idx 2, accuracy of the updated model is 11.46\n",
      "Previous layer shape is  torch.Size([256, 128, 3, 3])\n",
      "shape of layer: model 0 torch.Size([256, 256, 9])\n",
      "shape of layer: model 1 torch.Size([256, 256, 9])\n",
      "shape of previous transport map torch.Size([256, 256])\n",
      "Processing the coordinates to form ground_metric\n",
      "dont leave off the squaring of the ground metric\n",
      "returns a uniform measure of cardinality:  256\n",
      "returns a uniform measure of cardinality:  256\n",
      "the transport map is  tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        ...,\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.0039, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],\n",
      "       device='cuda:0')\n",
      "Ratio of trace to the matrix sum:  tensor(0.0078, device='cuda:0')\n",
      "Here, trace is 2.0 and matrix sum is 256.0 \n",
      "this is past correction for weight mode\n",
      "Shape of aligned wt is  torch.Size([256, 256, 9])\n",
      "Shape of fc_layer0_weight_data is  torch.Size([256, 256, 9])\n",
      "in _make_layers [Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), AvgPool2d(kernel_size=1, stride=1, padding=0)]\n",
      "Relu Inplace is  False\n",
      "model parameters are \n",
      " [torch.Size([64, 3, 3, 3]), torch.Size([128, 64, 3, 3]), torch.Size([256, 128, 3, 3]), torch.Size([256, 256, 3, 3]), torch.Size([512, 256, 3, 3]), torch.Size([512, 512, 3, 3]), torch.Size([512, 512, 3, 3]), torch.Size([512, 512, 3, 3]), torch.Size([10, 512])]\n",
      "len of model_state_dict is  9\n",
      "len of new_params is  4\n",
      "updated parameters for layer  features.0.weight\n",
      "updated parameters for layer  features.3.weight\n",
      "updated parameters for layer  features.6.weight\n",
      "updated parameters for layer  features.8.weight\n",
      "\n",
      "--------- Testing in global mode ---------\n",
      "size of test_loader dataset:  10000\n",
      "\n",
      "Test set: Avg. loss: 0.0024, Accuracy: 1249/10000 (12%)\n",
      "\n",
      "accuracy after update is  12.49\n",
      "For layer idx 3, accuracy of the updated model is 12.49\n",
      "Previous layer shape is  torch.Size([256, 256, 3, 3])\n",
      "shape of layer: model 0 torch.Size([512, 256, 9])\n",
      "shape of layer: model 1 torch.Size([512, 256, 9])\n",
      "shape of previous transport map torch.Size([256, 256])\n",
      "Processing the coordinates to form ground_metric\n",
      "dont leave off the squaring of the ground metric\n",
      "returns a uniform measure of cardinality:  512\n",
      "returns a uniform measure of cardinality:  512\n",
      "the transport map is  tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')\n",
      "Ratio of trace to the matrix sum:  tensor(0.0039, device='cuda:0')\n",
      "Here, trace is 2.0 and matrix sum is 512.0 \n",
      "this is past correction for weight mode\n",
      "Shape of aligned wt is  torch.Size([512, 256, 9])\n",
      "Shape of fc_layer0_weight_data is  torch.Size([512, 256, 9])\n",
      "in _make_layers [Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), AvgPool2d(kernel_size=1, stride=1, padding=0)]\n",
      "Relu Inplace is  False\n",
      "model parameters are \n",
      " [torch.Size([64, 3, 3, 3]), torch.Size([128, 64, 3, 3]), torch.Size([256, 128, 3, 3]), torch.Size([256, 256, 3, 3]), torch.Size([512, 256, 3, 3]), torch.Size([512, 512, 3, 3]), torch.Size([512, 512, 3, 3]), torch.Size([512, 512, 3, 3]), torch.Size([10, 512])]\n",
      "len of model_state_dict is  9\n",
      "len of new_params is  5\n",
      "updated parameters for layer  features.0.weight\n",
      "updated parameters for layer  features.3.weight\n",
      "updated parameters for layer  features.6.weight\n",
      "updated parameters for layer  features.8.weight\n",
      "updated parameters for layer  features.11.weight\n",
      "\n",
      "--------- Testing in global mode ---------\n",
      "size of test_loader dataset:  10000\n",
      "\n",
      "Test set: Avg. loss: 0.0027, Accuracy: 903/10000 (9%)\n",
      "\n",
      "accuracy after update is  9.03\n",
      "For layer idx 4, accuracy of the updated model is 9.03\n",
      "Previous layer shape is  torch.Size([512, 256, 3, 3])\n",
      "shape of layer: model 0 torch.Size([512, 512, 9])\n",
      "shape of layer: model 1 torch.Size([512, 512, 9])\n",
      "shape of previous transport map torch.Size([512, 512])\n",
      "Processing the coordinates to form ground_metric\n",
      "dont leave off the squaring of the ground metric\n",
      "returns a uniform measure of cardinality:  512\n",
      "returns a uniform measure of cardinality:  512\n",
      "the transport map is  tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')\n",
      "Ratio of trace to the matrix sum:  tensor(0.0020, device='cuda:0')\n",
      "Here, trace is 1.0 and matrix sum is 512.0 \n",
      "this is past correction for weight mode\n",
      "Shape of aligned wt is  torch.Size([512, 512, 9])\n",
      "Shape of fc_layer0_weight_data is  torch.Size([512, 512, 9])\n",
      "in _make_layers [Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), AvgPool2d(kernel_size=1, stride=1, padding=0)]\n",
      "Relu Inplace is  False\n",
      "model parameters are \n",
      " [torch.Size([64, 3, 3, 3]), torch.Size([128, 64, 3, 3]), torch.Size([256, 128, 3, 3]), torch.Size([256, 256, 3, 3]), torch.Size([512, 256, 3, 3]), torch.Size([512, 512, 3, 3]), torch.Size([512, 512, 3, 3]), torch.Size([512, 512, 3, 3]), torch.Size([10, 512])]\n",
      "len of model_state_dict is  9\n",
      "len of new_params is  6\n",
      "updated parameters for layer  features.0.weight\n",
      "updated parameters for layer  features.3.weight\n",
      "updated parameters for layer  features.6.weight\n",
      "updated parameters for layer  features.8.weight\n",
      "updated parameters for layer  features.11.weight\n",
      "updated parameters for layer  features.13.weight\n",
      "\n",
      "--------- Testing in global mode ---------\n",
      "size of test_loader dataset:  10000\n",
      "\n",
      "Test set: Avg. loss: 0.0024, Accuracy: 1410/10000 (14%)\n",
      "\n",
      "accuracy after update is  14.1\n",
      "For layer idx 5, accuracy of the updated model is 14.1\n",
      "Previous layer shape is  torch.Size([512, 512, 3, 3])\n",
      "shape of layer: model 0 torch.Size([512, 512, 9])\n",
      "shape of layer: model 1 torch.Size([512, 512, 9])\n",
      "shape of previous transport map torch.Size([512, 512])\n",
      "Processing the coordinates to form ground_metric\n",
      "dont leave off the squaring of the ground metric\n",
      "returns a uniform measure of cardinality:  512\n",
      "returns a uniform measure of cardinality:  512\n",
      "the transport map is  tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')\n",
      "Ratio of trace to the matrix sum:  tensor(0.0020, device='cuda:0')\n",
      "Here, trace is 1.0 and matrix sum is 512.0 \n",
      "this is past correction for weight mode\n",
      "Shape of aligned wt is  torch.Size([512, 512, 9])\n",
      "Shape of fc_layer0_weight_data is  torch.Size([512, 512, 9])\n",
      "in _make_layers [Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), AvgPool2d(kernel_size=1, stride=1, padding=0)]\n",
      "Relu Inplace is  False\n",
      "model parameters are \n",
      " [torch.Size([64, 3, 3, 3]), torch.Size([128, 64, 3, 3]), torch.Size([256, 128, 3, 3]), torch.Size([256, 256, 3, 3]), torch.Size([512, 256, 3, 3]), torch.Size([512, 512, 3, 3]), torch.Size([512, 512, 3, 3]), torch.Size([512, 512, 3, 3]), torch.Size([10, 512])]\n",
      "len of model_state_dict is  9\n",
      "len of new_params is  7\n",
      "updated parameters for layer  features.0.weight\n",
      "updated parameters for layer  features.3.weight\n",
      "updated parameters for layer  features.6.weight\n",
      "updated parameters for layer  features.8.weight\n",
      "updated parameters for layer  features.11.weight\n",
      "updated parameters for layer  features.13.weight\n",
      "updated parameters for layer  features.16.weight\n",
      "\n",
      "--------- Testing in global mode ---------\n",
      "size of test_loader dataset:  10000\n",
      "\n",
      "Test set: Avg. loss: 0.0022, Accuracy: 2217/10000 (22%)\n",
      "\n",
      "accuracy after update is  22.17\n",
      "For layer idx 6, accuracy of the updated model is 22.17\n",
      "Previous layer shape is  torch.Size([512, 512, 3, 3])\n",
      "shape of layer: model 0 torch.Size([512, 512, 9])\n",
      "shape of layer: model 1 torch.Size([512, 512, 9])\n",
      "shape of previous transport map torch.Size([512, 512])\n",
      "Processing the coordinates to form ground_metric\n",
      "dont leave off the squaring of the ground metric\n",
      "returns a uniform measure of cardinality:  512\n",
      "returns a uniform measure of cardinality:  512\n",
      "the transport map is  tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        [0.0020, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        ...,\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],\n",
      "       device='cuda:0')\n",
      "Ratio of trace to the matrix sum:  tensor(0., device='cuda:0')\n",
      "Here, trace is 0.0 and matrix sum is 512.0 \n",
      "this is past correction for weight mode\n",
      "Shape of aligned wt is  torch.Size([512, 512, 9])\n",
      "Shape of fc_layer0_weight_data is  torch.Size([512, 512, 9])\n",
      "in _make_layers [Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), AvgPool2d(kernel_size=1, stride=1, padding=0)]\n",
      "Relu Inplace is  False\n",
      "model parameters are \n",
      " [torch.Size([64, 3, 3, 3]), torch.Size([128, 64, 3, 3]), torch.Size([256, 128, 3, 3]), torch.Size([256, 256, 3, 3]), torch.Size([512, 256, 3, 3]), torch.Size([512, 512, 3, 3]), torch.Size([512, 512, 3, 3]), torch.Size([512, 512, 3, 3]), torch.Size([10, 512])]\n",
      "len of model_state_dict is  9\n",
      "len of new_params is  8\n",
      "updated parameters for layer  features.0.weight\n",
      "updated parameters for layer  features.3.weight\n",
      "updated parameters for layer  features.6.weight\n",
      "updated parameters for layer  features.8.weight\n",
      "updated parameters for layer  features.11.weight\n",
      "updated parameters for layer  features.13.weight\n",
      "updated parameters for layer  features.16.weight\n",
      "updated parameters for layer  features.18.weight\n",
      "\n",
      "--------- Testing in global mode ---------\n",
      "size of test_loader dataset:  10000\n",
      "\n",
      "Test set: Avg. loss: 0.0024, Accuracy: 582/10000 (6%)\n",
      "\n",
      "accuracy after update is  5.82\n",
      "For layer idx 7, accuracy of the updated model is 5.82\n",
      "Previous layer shape is  torch.Size([512, 512, 3, 3])\n",
      "shape of layer: model 0 torch.Size([10, 512])\n",
      "shape of layer: model 1 torch.Size([10, 512])\n",
      "shape of previous transport map torch.Size([512, 512])\n",
      "Processing the coordinates to form ground_metric\n",
      "dont leave off the squaring of the ground metric\n",
      "ground metric is  tensor([[1.4241, 2.4872, 2.7804, 2.8719, 2.7374, 2.8799, 2.7276, 2.6342, 2.5641,\n",
      "         2.5922],\n",
      "        [2.4889, 1.3253, 2.6732, 2.5914, 2.7584, 2.5501, 2.4174, 2.5215, 2.3048,\n",
      "         2.4101],\n",
      "        [2.7600, 2.6711, 1.3832, 2.9465, 2.9065, 2.8701, 2.7276, 2.8385, 2.7485,\n",
      "         2.7866],\n",
      "        [2.8352, 2.6791, 2.8919, 1.5430, 2.8605, 2.9159, 2.7574, 2.7980, 2.8164,\n",
      "         2.6354],\n",
      "        [2.7551, 2.5960, 2.8635, 2.8450, 1.5042, 2.7447, 2.7853, 2.6811, 2.7031,\n",
      "         2.7294],\n",
      "        [2.9190, 2.5396, 2.9469, 2.8416, 2.7272, 1.5502, 2.7781, 2.7303, 2.7977,\n",
      "         2.6697],\n",
      "        [2.6778, 2.4457, 2.7190, 2.9111, 2.6488, 2.8526, 1.3931, 2.6890, 2.4981,\n",
      "         2.5560],\n",
      "        [2.7394, 2.5496, 2.7799, 2.7903, 2.7151, 2.6590, 2.7190, 1.3924, 2.5583,\n",
      "         2.5121],\n",
      "        [2.5972, 2.3660, 2.7693, 2.6872, 2.7026, 2.8280, 2.5325, 2.6366, 1.2999,\n",
      "         2.4041],\n",
      "        [2.5918, 2.2730, 2.7816, 2.7576, 2.6697, 2.7273, 2.5482, 2.5290, 2.5207,\n",
      "         1.2878]], device='cuda:0', grad_fn=<PowBackward0>)\n",
      "returns a uniform measure of cardinality:  10\n",
      "returns a uniform measure of cardinality:  10\n",
      "the transport map is  tensor([[0.1000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0000, 0.1000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0000, 0.0000, 0.1000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.1000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.1000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.1000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.1000, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.1000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.1000,\n",
      "         0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.1000]], device='cuda:0')\n",
      "Ratio of trace to the matrix sum:  tensor(1., device='cuda:0')\n",
      "Here, trace is 9.99755859375 and matrix sum is 9.99755859375 \n",
      "this is past correction for weight mode\n",
      "Shape of aligned wt is  torch.Size([10, 512])\n",
      "Shape of fc_layer0_weight_data is  torch.Size([10, 512])\n",
      "in _make_layers [Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), AvgPool2d(kernel_size=1, stride=1, padding=0)]\n",
      "Relu Inplace is  False\n",
      "model parameters are \n",
      " [torch.Size([64, 3, 3, 3]), torch.Size([128, 64, 3, 3]), torch.Size([256, 128, 3, 3]), torch.Size([256, 256, 3, 3]), torch.Size([512, 256, 3, 3]), torch.Size([512, 512, 3, 3]), torch.Size([512, 512, 3, 3]), torch.Size([512, 512, 3, 3]), torch.Size([10, 512])]\n",
      "len of model_state_dict is  9\n",
      "len of new_params is  9\n",
      "updated parameters for layer  features.0.weight\n",
      "updated parameters for layer  features.3.weight\n",
      "updated parameters for layer  features.6.weight\n",
      "updated parameters for layer  features.8.weight\n",
      "updated parameters for layer  features.11.weight\n",
      "updated parameters for layer  features.13.weight\n",
      "updated parameters for layer  features.16.weight\n",
      "updated parameters for layer  features.18.weight\n",
      "updated parameters for layer  classifier.weight\n",
      "\n",
      "--------- Testing in global mode ---------\n",
      "size of test_loader dataset:  10000\n",
      "\n",
      "Test set: Avg. loss: 0.0003, Accuracy: 9019/10000 (90%)\n",
      "\n",
      "accuracy after update is  90.19\n",
      "For layer idx 8, accuracy of the updated model is 90.19\n",
      "using independent method\n",
      "in _make_layers [Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), AvgPool2d(kernel_size=1, stride=1, padding=0)]\n",
      "Relu Inplace is  False\n",
      "model parameters are \n",
      " [torch.Size([64, 3, 3, 3]), torch.Size([128, 64, 3, 3]), torch.Size([256, 128, 3, 3]), torch.Size([256, 256, 3, 3]), torch.Size([512, 256, 3, 3]), torch.Size([512, 512, 3, 3]), torch.Size([512, 512, 3, 3]), torch.Size([512, 512, 3, 3]), torch.Size([10, 512])]\n",
      "\n",
      "--------- Testing in global mode ---------\n",
      "size of test_loader dataset:  10000\n",
      "\n",
      "Test set: Avg. loss: 0.0023, Accuracy: 1091/10000 (11%)\n",
      "\n",
      "len of model parameters and avg aligned layers is  9 9\n",
      "len of model_state_dict is  9\n",
      "len of param_list is  9\n",
      "\n",
      "--------- Testing in global mode ---------\n",
      "size of test_loader dataset:  10000\n",
      "\n",
      "Test set: Avg. loss: 0.0015, Accuracy: 8544/10000 (85%)\n",
      "\n",
      "===============================================\n",
      "===============================================\n",
      "After Ensemble: 85.44\n",
      "===============================================\n",
      "===============================================\n",
      "Timer ends\n",
      "Time taken for geometric ensembling is 35.47849554568529 seconds\n",
      "------- Prediction based ensembling -------\n",
      "\n",
      "Test set: Avg. loss: 0.0003, Accuracy: 9133/10000 (91%)\n",
      "\n",
      "------- Naive ensembling of weights -------\n",
      "[torch.Size([64, 3, 3, 3]), torch.Size([64, 3, 3, 3])]\n",
      "torch.Size([64, 3, 3, 3])\n",
      "[torch.Size([128, 64, 3, 3]), torch.Size([128, 64, 3, 3])]\n",
      "torch.Size([128, 64, 3, 3])\n",
      "[torch.Size([256, 128, 3, 3]), torch.Size([256, 128, 3, 3])]\n",
      "torch.Size([256, 128, 3, 3])\n",
      "[torch.Size([256, 256, 3, 3]), torch.Size([256, 256, 3, 3])]\n",
      "torch.Size([256, 256, 3, 3])\n",
      "[torch.Size([512, 256, 3, 3]), torch.Size([512, 256, 3, 3])]\n",
      "torch.Size([512, 256, 3, 3])\n",
      "[torch.Size([512, 512, 3, 3]), torch.Size([512, 512, 3, 3])]\n",
      "torch.Size([512, 512, 3, 3])\n",
      "[torch.Size([512, 512, 3, 3]), torch.Size([512, 512, 3, 3])]\n",
      "torch.Size([512, 512, 3, 3])\n",
      "[torch.Size([512, 512, 3, 3]), torch.Size([512, 512, 3, 3])]\n",
      "torch.Size([512, 512, 3, 3])\n",
      "[torch.Size([10, 512]), torch.Size([10, 512])]\n",
      "torch.Size([10, 512])\n",
      "in _make_layers [Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), AvgPool2d(kernel_size=1, stride=1, padding=0)]\n",
      "Relu Inplace is  False\n",
      "model parameters are \n",
      " [torch.Size([64, 3, 3, 3]), torch.Size([128, 64, 3, 3]), torch.Size([256, 128, 3, 3]), torch.Size([256, 256, 3, 3]), torch.Size([512, 256, 3, 3]), torch.Size([512, 512, 3, 3]), torch.Size([512, 512, 3, 3]), torch.Size([512, 512, 3, 3]), torch.Size([10, 512])]\n",
      "\n",
      "--------- Testing in global mode ---------\n",
      "size of test_loader dataset:  10000\n",
      "\n",
      "Test set: Avg. loss: 0.0023, Accuracy: 1003/10000 (10%)\n",
      "\n",
      "\n",
      "--------- Testing in global mode ---------\n",
      "size of test_loader dataset:  10000\n",
      "\n",
      "Test set: Avg. loss: 0.0023, Accuracy: 1689/10000 (17%)\n",
      "\n",
      "-------- Retraining the models ---------\n",
      "Retraining model :  model_0\n",
      "num_epochs---------------+++))))))+++ 300\n",
      "num_epochs---------------+++))))))+++ 300\n",
      "lr is  0.05\n",
      "number of epochs would be  30\n",
      "num_epochs-------------- 30\n",
      "Epoch 000\n",
      "/root/miniconda3/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:129: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\"Detected call of `lr_scheduler.step()` before `optimizer.step()`. \"\n",
      "/root/miniconda3/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:154: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "accuracy: {'epoch': 0, 'value': 0.4336000000000002} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 0, 'value': 1.5597181462955472} ({'split': 'train'})\n",
      "accuracy: {'epoch': 0, 'value': 0.6713000297546385} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 0, 'value': 0.9801202356815338} ({'split': 'test'})\n",
      "Epoch 001\n",
      "accuracy: {'epoch': 1, 'value': 0.7297600000000003} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 1, 'value': 0.8241118642234808} ({'split': 'train'})\n",
      "accuracy: {'epoch': 1, 'value': 0.7458000421524047} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 1, 'value': 0.7581176578998565} ({'split': 'test'})\n",
      "Epoch 002\n",
      "accuracy: {'epoch': 2, 'value': 0.7813599999999998} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 2, 'value': 0.6696514800357815} ({'split': 'train'})\n",
      "accuracy: {'epoch': 2, 'value': 0.7658000349998475} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 2, 'value': 0.7249000668525696} ({'split': 'test'})\n",
      "Epoch 003\n",
      "accuracy: {'epoch': 3, 'value': 0.8145399999999998} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 3, 'value': 0.564621856822967} ({'split': 'train'})\n",
      "accuracy: {'epoch': 3, 'value': 0.7531000256538392} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 3, 'value': 0.7701649069786072} ({'split': 'test'})\n",
      "Epoch 004\n",
      "accuracy: {'epoch': 4, 'value': 0.8280599999999995} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 4, 'value': 0.5265779199981689} ({'split': 'train'})\n",
      "accuracy: {'epoch': 4, 'value': 0.7901000320911407} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 4, 'value': 0.6598710715770723} ({'split': 'test'})\n",
      "Epoch 005\n",
      "accuracy: {'epoch': 5, 'value': 0.8370799999999992} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 5, 'value': 0.49813019565582267} ({'split': 'train'})\n",
      "accuracy: {'epoch': 5, 'value': 0.7936000287532806} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 5, 'value': 0.6691171824932098} ({'split': 'test'})\n",
      "Epoch 006\n",
      "accuracy: {'epoch': 6, 'value': 0.84734} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 6, 'value': 0.46337060040473904} ({'split': 'train'})\n",
      "accuracy: {'epoch': 6, 'value': 0.7907000362873078} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 6, 'value': 0.6734202086925507} ({'split': 'test'})\n",
      "Epoch 007\n",
      "accuracy: {'epoch': 7, 'value': 0.8553199999999999} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 7, 'value': 0.441009681873322} ({'split': 'train'})\n",
      "accuracy: {'epoch': 7, 'value': 0.7926000356674195} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 7, 'value': 0.7215842723846435} ({'split': 'test'})\n",
      "Epoch 008\n",
      "accuracy: {'epoch': 8, 'value': 0.8580200000000007} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 8, 'value': 0.43672133791446677} ({'split': 'train'})\n",
      "accuracy: {'epoch': 8, 'value': 0.7659000396728515} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 8, 'value': 0.7722597718238831} ({'split': 'test'})\n",
      "Epoch 009\n",
      "accuracy: {'epoch': 9, 'value': 0.8680199999999998} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 9, 'value': 0.4041635980033878} ({'split': 'train'})\n",
      "accuracy: {'epoch': 9, 'value': 0.787600040435791} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 9, 'value': 0.6870992124080658} ({'split': 'test'})\n",
      "Epoch 010\n",
      "accuracy: {'epoch': 10, 'value': 0.8661799999999993} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 10, 'value': 0.4053042185401919} ({'split': 'train'})\n",
      "accuracy: {'epoch': 10, 'value': 0.7779000461101532} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 10, 'value': 0.7810189187526704} ({'split': 'test'})\n",
      "Epoch 011\n",
      "accuracy: {'epoch': 11, 'value': 0.8683600000000004} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 11, 'value': 0.4106298659515381} ({'split': 'train'})\n",
      "accuracy: {'epoch': 11, 'value': 0.78340003490448} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 11, 'value': 0.7214223980903626} ({'split': 'test'})\n",
      "Epoch 012\n",
      "accuracy: {'epoch': 12, 'value': 0.86858} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 12, 'value': 0.4060838037776946} ({'split': 'train'})\n",
      "accuracy: {'epoch': 12, 'value': 0.7826000392436981} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 12, 'value': 0.7811884343624116} ({'split': 'test'})\n",
      "Epoch 013\n",
      "accuracy: {'epoch': 13, 'value': 0.8745399999999994} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 13, 'value': 0.38643022662639603} ({'split': 'train'})\n",
      "accuracy: {'epoch': 13, 'value': 0.7889000356197357} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 13, 'value': 0.7206459224224089} ({'split': 'test'})\n",
      "Epoch 014\n",
      "accuracy: {'epoch': 14, 'value': 0.8773999999999998} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 14, 'value': 0.378996676464081} ({'split': 'train'})\n",
      "accuracy: {'epoch': 14, 'value': 0.7685000360012055} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 14, 'value': 0.8538445293903351} ({'split': 'test'})\n",
      "Epoch 015\n",
      "accuracy: {'epoch': 15, 'value': 0.8744800000000001} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 15, 'value': 0.38972430214881854} ({'split': 'train'})\n",
      "accuracy: {'epoch': 15, 'value': 0.7941000342369079} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 15, 'value': 0.6818698287010193} ({'split': 'test'})\n",
      "Epoch 016\n",
      "accuracy: {'epoch': 16, 'value': 0.885179999999999} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 16, 'value': 0.3622493941354754} ({'split': 'train'})\n",
      "accuracy: {'epoch': 16, 'value': 0.796500039100647} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 16, 'value': 0.781831818819046} ({'split': 'test'})\n",
      "Epoch 017\n",
      "accuracy: {'epoch': 17, 'value': 0.8827199999999997} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 17, 'value': 0.36708273889780063} ({'split': 'train'})\n",
      "accuracy: {'epoch': 17, 'value': 0.7708000421524047} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 17, 'value': 0.7427750289440155} ({'split': 'test'})\n",
      "Epoch 018\n",
      "accuracy: {'epoch': 18, 'value': 0.8781000000000007} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 18, 'value': 0.3761559832382206} ({'split': 'train'})\n",
      "accuracy: {'epoch': 18, 'value': 0.7759000360965729} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 18, 'value': 0.7987822651863099} ({'split': 'test'})\n",
      "Epoch 019\n",
      "accuracy: {'epoch': 19, 'value': 0.8807000000000008} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 19, 'value': 0.37083543067932184} ({'split': 'train'})\n",
      "accuracy: {'epoch': 19, 'value': 0.7736000418663025} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 19, 'value': 0.7551369607448578} ({'split': 'test'})\n",
      "Epoch 020\n",
      "accuracy: {'epoch': 20, 'value': 0.8820399999999996} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 20, 'value': 0.361744907941818} ({'split': 'train'})\n",
      "accuracy: {'epoch': 20, 'value': 0.7601000308990478} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 20, 'value': 0.8091262161731719} ({'split': 'test'})\n",
      "Epoch 021\n",
      "accuracy: {'epoch': 21, 'value': 0.8845800000000003} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 21, 'value': 0.35935356058120765} ({'split': 'train'})\n",
      "accuracy: {'epoch': 21, 'value': 0.7866000413894653} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 21, 'value': 0.7499563694000244} ({'split': 'test'})\n",
      "Epoch 022\n",
      "accuracy: {'epoch': 22, 'value': 0.8894200000000007} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 22, 'value': 0.3403456424617768} ({'split': 'train'})\n",
      "accuracy: {'epoch': 22, 'value': 0.7830000340938569} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 22, 'value': 0.7549613058567046} ({'split': 'test'})\n",
      "Epoch 023\n",
      "accuracy: {'epoch': 25, 'value': 0.8904999999999996} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 25, 'value': 0.34193298570632985} ({'split': 'train'})\n",
      "accuracy: {'epoch': 25, 'value': 0.7815000414848328} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 25, 'value': 0.7715516805648803} ({'split': 'test'})\n",
      "Epoch 026\n",
      "accuracy: {'epoch': 26, 'value': 0.8886600000000013} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 26, 'value': 0.3446659360599519} ({'split': 'train'})\n",
      "accuracy: {'epoch': 26, 'value': 0.7733000338077545} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 26, 'value': 0.7918766796588899} ({'split': 'test'})\n",
      "Epoch 027\n",
      "accuracy: {'epoch': 27, 'value': 0.8841799999999997} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 27, 'value': 0.35458068172454826} ({'split': 'train'})\n",
      "accuracy: {'epoch': 27, 'value': 0.8018000304698943} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 27, 'value': 0.6729396283626556} ({'split': 'test'})\n",
      "Epoch 028\n",
      "accuracy: {'epoch': 28, 'value': 0.8891599999999995} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 28, 'value': 0.3437848457622529} ({'split': 'train'})\n",
      "accuracy: {'epoch': 28, 'value': 0.7695000410079956} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 28, 'value': 0.8653207838535308} ({'split': 'test'})\n",
      "Epoch 029\n",
      "accuracy: {'epoch': 29, 'value': 0.8875800000000001} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 29, 'value': 0.35262730140209186} ({'split': 'train'})\n",
      "accuracy: {'epoch': 29, 'value': 0.7907000243663789} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 29, 'value': 0.7512350857257842} ({'split': 'test'})\n",
      "mean_test_accuracy <cifar_utils.accumulators.Mean object at 0x7f2aad59b3d0>\n",
      "QWERTY: Enter Cifar 2, acc 0.8018000304698943\n",
      "Retraining model :  model_1\n",
      "num_epochs---------------+++))))))+++ 30\n",
      "num_epochs---------------+++))))))+++ 30\n",
      "lr is  0.05\n",
      "number of epochs would be  30\n",
      "num_epochs-------------- 30\n",
      "Epoch 000\n",
      "accuracy: {'epoch': 0, 'value': 0.4562399999999997} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 0, 'value': 1.496698154916764} ({'split': 'train'})\n",
      "accuracy: {'epoch': 0, 'value': 0.7115000307559967} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 0, 'value': 0.8756510436534881} ({'split': 'test'})\n",
      "Epoch 001\n",
      "accuracy: {'epoch': 1, 'value': 0.7396399999999997} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 1, 'value': 0.8022125393104566} ({'split': 'train'})\n",
      "accuracy: {'epoch': 1, 'value': 0.761500036716461} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 1, 'value': 0.7302771985530853} ({'split': 'test'})\n",
      "Epoch 002\n",
      "accuracy: {'epoch': 2, 'value': 0.7912800000000003} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 2, 'value': 0.6435718333625792} ({'split': 'train'})\n",
      "accuracy: {'epoch': 2, 'value': 0.7750000417232513} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 2, 'value': 0.7061725318431854} ({'split': 'test'})\n",
      "Epoch 003\n",
      "accuracy: {'epoch': 3, 'value': 0.8162999999999998} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 3, 'value': 0.5576388818359374} ({'split': 'train'})\n",
      "accuracy: {'epoch': 3, 'value': 0.7696000397205353} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 3, 'value': 0.7479754745960236} ({'split': 'test'})\n",
      "Epoch 004\n",
      "accuracy: {'epoch': 4, 'value': 0.8327800000000001} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 4, 'value': 0.5099289708137514} ({'split': 'train'})\n",
      "accuracy: {'epoch': 4, 'value': 0.7567000329494477} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 4, 'value': 0.7603942573070526} ({'split': 'test'})\n",
      "Epoch 005\n",
      "accuracy: {'epoch': 5, 'value': 0.8438000000000007} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 5, 'value': 0.4749817955398563} ({'split': 'train'})\n",
      "accuracy: {'epoch': 5, 'value': 0.7863000333309174} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 5, 'value': 0.6923391103744505} ({'split': 'test'})\n",
      "Epoch 006\n",
      "accuracy: {'epoch': 6, 'value': 0.8524400000000002} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 6, 'value': 0.45001534381866426} ({'split': 'train'})\n",
      "accuracy: {'epoch': 6, 'value': 0.7624000370502473} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 6, 'value': 0.821131807565689} ({'split': 'test'})\n",
      "Epoch 007\n",
      "accuracy: {'epoch': 7, 'value': 0.8577799999999999} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 7, 'value': 0.4336183850860595} ({'split': 'train'})\n",
      "accuracy: {'epoch': 7, 'value': 0.8062000453472138} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 7, 'value': 0.6474069118499756} ({'split': 'test'})\n",
      "Epoch 008\n",
      "accuracy: {'epoch': 8, 'value': 0.8606999999999989} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 8, 'value': 0.4213998666000364} ({'split': 'train'})\n",
      "accuracy: {'epoch': 8, 'value': 0.8039000332355499} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 8, 'value': 0.6299012899398804} ({'split': 'test'})\n",
      "Epoch 009\n",
      "accuracy: {'epoch': 9, 'value': 0.8705800000000001} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 9, 'value': 0.39901255009412734} ({'split': 'train'})\n",
      "accuracy: {'epoch': 9, 'value': 0.7828000247478485} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 9, 'value': 0.7423131942749024} ({'split': 'test'})\n",
      "Epoch 010\n",
      "accuracy: {'epoch': 10, 'value': 0.8684999999999999} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 10, 'value': 0.40051394992291944} ({'split': 'train'})\n",
      "accuracy: {'epoch': 10, 'value': 0.78790003657341} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 10, 'value': 0.7405853033065797} ({'split': 'test'})\n",
      "Epoch 011\n",
      "accuracy: {'epoch': 11, 'value': 0.8790399999999987} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 11, 'value': 0.37526750086784366} ({'split': 'train'})\n",
      "accuracy: {'epoch': 11, 'value': 0.7529000401496887} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 11, 'value': 0.8844749212265015} ({'split': 'test'})\n",
      "Epoch 012\n",
      "accuracy: {'epoch': 12, 'value': 0.8727799999999999} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 12, 'value': 0.3940035326480871} ({'split': 'train'})\n",
      "accuracy: {'epoch': 12, 'value': 0.7744000375270844} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 12, 'value': 0.7863071262836456} ({'split': 'test'})\n",
      "Epoch 013\n",
      "accuracy: {'epoch': 13, 'value': 0.8805999999999999} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 13, 'value': 0.3762404103851319} ({'split': 'train'})\n",
      "accuracy: {'epoch': 13, 'value': 0.7987000286579132} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 13, 'value': 0.6668142974376678} ({'split': 'test'})\n",
      "Epoch 014\n",
      "accuracy: {'epoch': 14, 'value': 0.8776199999999988} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 14, 'value': 0.3790101517868039} ({'split': 'train'})\n",
      "accuracy: {'epoch': 14, 'value': 0.7832000434398652} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 14, 'value': 0.7833970129489899} ({'split': 'test'})\n",
      "Epoch 015\n",
      "accuracy: {'epoch': 15, 'value': 0.8863400000000005} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 15, 'value': 0.3520174220085142} ({'split': 'train'})\n",
      "accuracy: {'epoch': 15, 'value': 0.8015000402927398} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 15, 'value': 0.6719992339611054} ({'split': 'test'})\n",
      "Epoch 016\n",
      "accuracy: {'epoch': 16, 'value': 0.8879400000000001} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 16, 'value': 0.3468151648044589} ({'split': 'train'})\n",
      "accuracy: {'epoch': 16, 'value': 0.8029000401496886} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 16, 'value': 0.7086418151855468} ({'split': 'test'})\n",
      "Epoch 017\n",
      "accuracy: {'epoch': 17, 'value': 0.8865199999999999} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 17, 'value': 0.3528199974060061} ({'split': 'train'})\n",
      "accuracy: {'epoch': 17, 'value': 0.7965000391006469} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 17, 'value': 0.6700470209121704} ({'split': 'test'})\n",
      "Epoch 018\n",
      "accuracy: {'epoch': 18, 'value': 0.8836800000000002} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 18, 'value': 0.36027796088695474} ({'split': 'train'})\n",
      "accuracy: {'epoch': 18, 'value': 0.7464000344276428} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 18, 'value': 0.9345724999904633} ({'split': 'test'})\n",
      "Epoch 019\n",
      "accuracy: {'epoch': 19, 'value': 0.8822200000000003} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 19, 'value': 0.3631704073953623} ({'split': 'train'})\n",
      "accuracy: {'epoch': 19, 'value': 0.7701000392436981} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 19, 'value': 0.8051477551460267} ({'split': 'test'})\n",
      "Epoch 020\n",
      "accuracy: {'epoch': 20, 'value': 0.8887999999999988} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 20, 'value': 0.3430306144428252} ({'split': 'train'})\n",
      "accuracy: {'epoch': 20, 'value': 0.7840000450611114} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 20, 'value': 0.727930200099945} ({'split': 'test'})\n",
      "Epoch 021\n",
      "accuracy: {'epoch': 21, 'value': 0.8862800000000005} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 21, 'value': 0.35263646491050715} ({'split': 'train'})\n",
      "accuracy: {'epoch': 21, 'value': 0.7920000433921814} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 21, 'value': 0.7039123952388763} ({'split': 'test'})\n",
      "Epoch 022\n",
      "accuracy: {'epoch': 22, 'value': 0.8949599999999996} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 22, 'value': 0.32281504945755} ({'split': 'train'})\n",
      "accuracy: {'epoch': 22, 'value': 0.7661000311374664} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 22, 'value': 0.736232316493988} ({'split': 'test'})\n",
      "Epoch 023\n",
      "accuracy: {'epoch': 23, 'value': 0.8886800000000004} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 23, 'value': 0.34536652940750107} ({'split': 'train'})\n",
      "accuracy: {'epoch': 23, 'value': 0.7937000453472137} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 23, 'value': 0.7506037533283233} ({'split': 'test'})\n",
      "Epoch 024\n",
      "accuracy: {'epoch': 24, 'value': 0.8912799999999999} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 24, 'value': 0.33646515978813146} ({'split': 'train'})\n",
      "accuracy: {'epoch': 24, 'value': 0.7760000348091126} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 24, 'value': 0.800137460231781} ({'split': 'test'})\n",
      "Epoch 025\n",
      "accuracy: {'epoch': 25, 'value': 0.891459999999999} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 25, 'value': 0.34031870148658755} ({'split': 'train'})\n",
      "accuracy: {'epoch': 25, 'value': 0.7586000263690948} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 25, 'value': 0.8300036609172821} ({'split': 'test'})\n",
      "Epoch 026\n",
      "accuracy: {'epoch': 26, 'value': 0.8915000000000001} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 26, 'value': 0.33868253534316983} ({'split': 'train'})\n",
      "accuracy: {'epoch': 26, 'value': 0.7952000439167023} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 26, 'value': 0.7199432194232941} ({'split': 'test'})\n",
      "Epoch 027\n",
      "accuracy: {'epoch': 27, 'value': 0.8945200000000003} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 27, 'value': 0.32929059752464246} ({'split': 'train'})\n",
      "accuracy: {'epoch': 27, 'value': 0.7878000378608704} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 27, 'value': 0.7372381269931794} ({'split': 'test'})\n",
      "Epoch 028\n",
      "accuracy: {'epoch': 28, 'value': 0.8939799999999997} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 28, 'value': 0.3328656266784667} ({'split': 'train'})\n",
      "accuracy: {'epoch': 28, 'value': 0.7779000461101531} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 28, 'value': 0.7272949159145355} ({'split': 'test'})\n",
      "Epoch 029\n",
      "accuracy: {'epoch': 29, 'value': 0.8935199999999992} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 29, 'value': 0.33136307847022994} ({'split': 'train'})\n",
      "accuracy: {'epoch': 29, 'value': 0.7899000406265259} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 29, 'value': 0.8140536367893219} ({'split': 'test'})\n",
      "mean_test_accuracy <cifar_utils.accumulators.Mean object at 0x7f2aad59b490>\n",
      "QWERTY: Enter Cifar 2, acc 0.8062000453472138\n",
      "Retraining model :  geometric\n",
      "num_epochs---------------+++))))))+++ 30\n",
      "num_epochs---------------+++))))))+++ 30\n",
      "lr is  0.05\n",
      "number of epochs would be  30\n",
      "num_epochs-------------- 30\n",
      "Epoch 000\n",
      "accuracy: {'epoch': 0, 'value': 0.10548000000000002} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 0, 'value': 2.3047519077301026} ({'split': 'train'})\n",
      "accuracy: {'epoch': 0, 'value': 0.10000000447034836} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 0, 'value': 2.3025848865509033} ({'split': 'test'})\n",
      "Epoch 001\n",
      "accuracy: {'epoch': 1, 'value': 0.10002000000000001} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 1, 'value': 2.302584886703491} ({'split': 'train'})\n",
      "accuracy: {'epoch': 1, 'value': 0.10000000521540642} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 1, 'value': 2.3025848865509033} ({'split': 'test'})\n",
      "Epoch 002\n",
      "accuracy: {'epoch': 2, 'value': 0.09999999999999995} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 2, 'value': 2.302584886703491} ({'split': 'train'})\n",
      "accuracy: {'epoch': 2, 'value': 0.10000000447034835} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 2, 'value': 2.3025848865509033} ({'split': 'test'})\n",
      "Epoch 003\n",
      "accuracy: {'epoch': 3, 'value': 0.09999999999999995} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 3, 'value': 2.302584886703491} ({'split': 'train'})\n",
      "accuracy: {'epoch': 3, 'value': 0.1000000037252903} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 3, 'value': 2.3025848865509033} ({'split': 'test'})\n",
      "Epoch 004\n",
      "accuracy: {'epoch': 4, 'value': 0.09999999999999994} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 4, 'value': 2.302584886703491} ({'split': 'train'})\n",
      "accuracy: {'epoch': 4, 'value': 0.1000000037252903} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 4, 'value': 2.3025848865509033} ({'split': 'test'})\n",
      "Epoch 005\n",
      "accuracy: {'epoch': 5, 'value': 0.10000000000000009} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 5, 'value': 2.302584886703491} ({'split': 'train'})\n",
      "accuracy: {'epoch': 5, 'value': 0.10000000447034836} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 5, 'value': 2.3025848865509033} ({'split': 'test'})\n",
      "Epoch 006\n",
      "accuracy: {'epoch': 6, 'value': 0.09999999999999999} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 6, 'value': 2.302584886703491} ({'split': 'train'})\n",
      "accuracy: {'epoch': 6, 'value': 0.10000000447034835} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 6, 'value': 2.3025848865509033} ({'split': 'test'})\n",
      "Epoch 007\n",
      "accuracy: {'epoch': 7, 'value': 0.09999999999999991} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 7, 'value': 2.302584886703491} ({'split': 'train'})\n",
      "accuracy: {'epoch': 7, 'value': 0.1000000037252903} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 7, 'value': 2.3025848865509033} ({'split': 'test'})\n",
      "Epoch 008\n",
      "accuracy: {'epoch': 8, 'value': 0.09999999999999995} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 8, 'value': 2.302584886703491} ({'split': 'train'})\n",
      "accuracy: {'epoch': 8, 'value': 0.1000000037252903} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 8, 'value': 2.3025848865509033} ({'split': 'test'})\n",
      "Epoch 009\n",
      "accuracy: {'epoch': 9, 'value': 0.09999999999999996} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 9, 'value': 2.302584886703491} ({'split': 'train'})\n",
      "accuracy: {'epoch': 9, 'value': 0.10000000298023225} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 9, 'value': 2.3025848865509033} ({'split': 'test'})\n",
      "Epoch 010\n",
      "accuracy: {'epoch': 10, 'value': 0.09999999999999995} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 10, 'value': 2.302584886703491} ({'split': 'train'})\n",
      "accuracy: {'epoch': 10, 'value': 0.10000000447034836} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 10, 'value': 2.3025848865509033} ({'split': 'test'})\n",
      "Epoch 011\n",
      "accuracy: {'epoch': 11, 'value': 0.10000000000000003} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 11, 'value': 2.302584886703491} ({'split': 'train'})\n",
      "accuracy: {'epoch': 11, 'value': 0.10000000596046449} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 11, 'value': 2.3025848865509033} ({'split': 'test'})\n",
      "Epoch 012\n",
      "accuracy: {'epoch': 12, 'value': 0.10000000000000006} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 12, 'value': 2.302584886703491} ({'split': 'train'})\n",
      "accuracy: {'epoch': 12, 'value': 0.10000000521540643} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 12, 'value': 2.3025848865509033} ({'split': 'test'})\n",
      "Epoch 013\n",
      "accuracy: {'epoch': 13, 'value': 0.09999999999999999} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 13, 'value': 2.302584886703491} ({'split': 'train'})\n",
      "accuracy: {'epoch': 13, 'value': 0.10000000447034837} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 13, 'value': 2.3025848865509033} ({'split': 'test'})\n",
      "Epoch 014\n",
      "accuracy: {'epoch': 14, 'value': 0.09999999999999998} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 14, 'value': 2.302584886703491} ({'split': 'train'})\n",
      "accuracy: {'epoch': 14, 'value': 0.10000000447034836} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 14, 'value': 2.3025848865509033} ({'split': 'test'})\n",
      "Epoch 015\n",
      "accuracy: {'epoch': 15, 'value': 0.09999999999999994} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 15, 'value': 2.302584886703491} ({'split': 'train'})\n",
      "accuracy: {'epoch': 15, 'value': 0.10000000670552253} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 15, 'value': 2.3025848865509033} ({'split': 'test'})\n",
      "Epoch 016\n",
      "accuracy: {'epoch': 16, 'value': 0.10000000000000009} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 16, 'value': 2.302584886703491} ({'split': 'train'})\n",
      "accuracy: {'epoch': 16, 'value': 0.10000000447034836} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 16, 'value': 2.3025848865509033} ({'split': 'test'})\n",
      "Epoch 017\n",
      "accuracy: {'epoch': 17, 'value': 0.10000000000000002} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 17, 'value': 2.302584886703491} ({'split': 'train'})\n",
      "accuracy: {'epoch': 17, 'value': 0.10000000521540642} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 17, 'value': 2.3025848865509033} ({'split': 'test'})\n",
      "Epoch 018\n",
      "accuracy: {'epoch': 18, 'value': 0.09999999999999996} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 18, 'value': 2.302584886703491} ({'split': 'train'})\n",
      "accuracy: {'epoch': 18, 'value': 0.10000000521540642} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 18, 'value': 2.3025848865509033} ({'split': 'test'})\n",
      "Epoch 019\n",
      "accuracy: {'epoch': 19, 'value': 0.1000000000000001} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 19, 'value': 2.302584886703491} ({'split': 'train'})\n",
      "accuracy: {'epoch': 19, 'value': 0.10000000447034836} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 19, 'value': 2.3025848865509033} ({'split': 'test'})\n",
      "Epoch 020\n",
      "accuracy: {'epoch': 20, 'value': 0.1} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 20, 'value': 2.302584886703491} ({'split': 'train'})\n",
      "accuracy: {'epoch': 20, 'value': 0.10000000447034836} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 20, 'value': 2.3025848865509033} ({'split': 'test'})\n",
      "Epoch 021\n",
      "accuracy: {'epoch': 21, 'value': 0.10000000000000002} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 21, 'value': 2.302584886703491} ({'split': 'train'})\n",
      "accuracy: {'epoch': 21, 'value': 0.10000000596046447} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 21, 'value': 2.3025848865509033} ({'split': 'test'})\n",
      "Epoch 022\n",
      "accuracy: {'epoch': 22, 'value': 0.09999999999999998} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 22, 'value': 2.302584886703491} ({'split': 'train'})\n",
      "accuracy: {'epoch': 22, 'value': 0.10000000447034836} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 22, 'value': 2.3025848865509033} ({'split': 'test'})\n",
      "Epoch 023\n",
      "accuracy: {'epoch': 23, 'value': 0.10000000000000013} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 23, 'value': 2.302584886703491} ({'split': 'train'})\n",
      "accuracy: {'epoch': 23, 'value': 0.10000000521540642} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 23, 'value': 2.3025848865509033} ({'split': 'test'})\n",
      "Epoch 024\n",
      "accuracy: {'epoch': 24, 'value': 0.10000000000000003} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 24, 'value': 2.302584886703491} ({'split': 'train'})\n",
      "accuracy: {'epoch': 24, 'value': 0.10000000521540642} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 24, 'value': 2.3025848865509033} ({'split': 'test'})\n",
      "Epoch 025\n",
      "accuracy: {'epoch': 25, 'value': 0.10000000000000005} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 25, 'value': 2.302584886703491} ({'split': 'train'})\n",
      "accuracy: {'epoch': 25, 'value': 0.10000000521540642} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 25, 'value': 2.3025848865509033} ({'split': 'test'})\n",
      "Epoch 026\n",
      "accuracy: {'epoch': 26, 'value': 0.10000000000000002} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 26, 'value': 2.302584886703491} ({'split': 'train'})\n",
      "accuracy: {'epoch': 26, 'value': 0.1000000037252903} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 26, 'value': 2.3025848865509033} ({'split': 'test'})\n",
      "Epoch 027\n",
      "accuracy: {'epoch': 27, 'value': 0.09999999999999995} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 27, 'value': 2.302584886703491} ({'split': 'train'})\n",
      "accuracy: {'epoch': 27, 'value': 0.10000000447034836} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 27, 'value': 2.3025848865509033} ({'split': 'test'})\n",
      "Epoch 028\n",
      "accuracy: {'epoch': 28, 'value': 0.1} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 28, 'value': 2.302584886703491} ({'split': 'train'})\n",
      "accuracy: {'epoch': 28, 'value': 0.10000000447034836} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 28, 'value': 2.3025848865509033} ({'split': 'test'})\n",
      "Epoch 029\n",
      "accuracy: {'epoch': 29, 'value': 0.10000000000000012} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 29, 'value': 2.302584886703491} ({'split': 'train'})\n",
      "accuracy: {'epoch': 29, 'value': 0.10000000596046449} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 29, 'value': 2.3025848865509033} ({'split': 'test'})\n",
      "mean_test_accuracy <cifar_utils.accumulators.Mean object at 0x7f2aad58bd60>\n",
      "QWERTY: Enter Cifar 2, acc 0.10000000670552253\n",
      "Retraining model :  naive_averaging\n",
      "num_epochs---------------+++))))))+++ 30\n",
      "num_epochs---------------+++))))))+++ 30\n",
      "lr is  0.05\n",
      "number of epochs would be  30\n",
      "num_epochs-------------- 30\n",
      "Epoch 000\n",
      "accuracy: {'epoch': 0, 'value': 0.1818000000000001} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 0, 'value': 2.1084258403396605} ({'split': 'train'})\n",
      "accuracy: {'epoch': 0, 'value': 0.29000001251697544} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 0, 'value': 2.00372257232666} ({'split': 'test'})\n",
      "Epoch 001\n",
      "accuracy: {'epoch': 1, 'value': 0.41307999999999967} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 1, 'value': 1.5593513153457639} ({'split': 'train'})\n",
      "accuracy: {'epoch': 1, 'value': 0.4804000228643417} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 1, 'value': 1.436147403717041} ({'split': 'test'})\n",
      "Epoch 002\n",
      "accuracy: {'epoch': 2, 'value': 0.5633599999999995} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 2, 'value': 1.2410577666091918} ({'split': 'train'})\n",
      "accuracy: {'epoch': 2, 'value': 0.6057000219821931} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 2, 'value': 1.1310455203056335} ({'split': 'test'})\n",
      "Epoch 003\n",
      "accuracy: {'epoch': 3, 'value': 0.6448999999999999} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 3, 'value': 1.0355456029510495} ({'split': 'train'})\n",
      "accuracy: {'epoch': 3, 'value': 0.6595000207424163} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 3, 'value': 1.0092097103595734} ({'split': 'test'})\n",
      "Epoch 004\n",
      "accuracy: {'epoch': 4, 'value': 0.6929599999999994} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 4, 'value': 0.9104726698303223} ({'split': 'train'})\n",
      "accuracy: {'epoch': 4, 'value': 0.6862000286579132} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 4, 'value': 0.9560130357742309} ({'split': 'test'})\n",
      "Epoch 005\n",
      "accuracy: {'epoch': 5, 'value': 0.7254400000000005} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 5, 'value': 0.8230020975494388} ({'split': 'train'})\n",
      "accuracy: {'epoch': 5, 'value': 0.7016000390052795} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 5, 'value': 0.931545615196228} ({'split': 'test'})\n",
      "Epoch 006\n",
      "accuracy: {'epoch': 6, 'value': 0.7560800000000003} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 6, 'value': 0.7380233353424076} ({'split': 'train'})\n",
      "accuracy: {'epoch': 6, 'value': 0.7365000367164611} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 6, 'value': 0.7930045664310454} ({'split': 'test'})\n",
      "Epoch 007\n",
      "accuracy: {'epoch': 7, 'value': 0.7677000000000005} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 7, 'value': 0.7053903690338134} ({'split': 'train'})\n",
      "accuracy: {'epoch': 7, 'value': 0.7321000277996064} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 7, 'value': 0.8166541755199432} ({'split': 'test'})\n",
      "Epoch 008\n",
      "accuracy: {'epoch': 8, 'value': 0.7808999999999993} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 8, 'value': 0.670510867290497} ({'split': 'train'})\n",
      "accuracy: {'epoch': 8, 'value': 0.7476000308990479} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 8, 'value': 0.7851340174674988} ({'split': 'test'})\n",
      "Epoch 009\n",
      "accuracy: {'epoch': 9, 'value': 0.7931599999999998} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 9, 'value': 0.6288634703350067} ({'split': 'train'})\n",
      "accuracy: {'epoch': 9, 'value': 0.714900028705597} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 9, 'value': 0.8830189943313599} ({'split': 'test'})\n",
      "Epoch 010\n",
      "accuracy: {'epoch': 10, 'value': 0.8060800000000004} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 10, 'value': 0.5915442943668356} ({'split': 'train'})\n",
      "accuracy: {'epoch': 10, 'value': 0.7544000327587128} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 10, 'value': 0.7752367436885833} ({'split': 'test'})\n",
      "Epoch 011\n",
      "accuracy: {'epoch': 11, 'value': 0.8147400000000006} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 11, 'value': 0.5666357011795046} ({'split': 'train'})\n",
      "accuracy: {'epoch': 11, 'value': 0.7673000276088714} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 11, 'value': 0.7452292919158935} ({'split': 'test'})\n",
      "Epoch 012\n",
      "accuracy: {'epoch': 12, 'value': 0.8215399999999993} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 12, 'value': 0.5385940451908102} ({'split': 'train'})\n",
      "accuracy: {'epoch': 12, 'value': 0.7758000373840332} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 12, 'value': 0.7684906601905823} ({'split': 'test'})\n",
      "Epoch 013\n",
      "accuracy: {'epoch': 13, 'value': 0.8263999999999997} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 13, 'value': 0.5294094088745124} ({'split': 'train'})\n",
      "accuracy: {'epoch': 13, 'value': 0.7405000329017639} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 13, 'value': 0.8507984519004823} ({'split': 'test'})\n",
      "Epoch 014\n",
      "accuracy: {'epoch': 14, 'value': 0.8327999999999992} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 14, 'value': 0.5077233056163789} ({'split': 'train'})\n",
      "accuracy: {'epoch': 14, 'value': 0.7593000471591949} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 14, 'value': 0.8864212810993195} ({'split': 'test'})\n",
      "Epoch 015\n",
      "accuracy: {'epoch': 15, 'value': 0.8390800000000007} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 15, 'value': 0.49282721412658687} ({'split': 'train'})\n",
      "accuracy: {'epoch': 15, 'value': 0.7870000302791595} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 15, 'value': 0.6905732035636902} ({'split': 'test'})\n",
      "Epoch 016\n",
      "accuracy: {'epoch': 16, 'value': 0.8408599999999995} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 16, 'value': 0.48485248830795274} ({'split': 'train'})\n",
      "accuracy: {'epoch': 16, 'value': 0.7647000312805176} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 16, 'value': 0.7677149832248688} ({'split': 'test'})\n",
      "Epoch 017\n",
      "accuracy: {'epoch': 17, 'value': 0.8466000000000005} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 17, 'value': 0.46630572265625014} ({'split': 'train'})\n",
      "accuracy: {'epoch': 17, 'value': 0.7615000307559967} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 17, 'value': 0.774495267868042} ({'split': 'test'})\n",
      "Epoch 018\n",
      "accuracy: {'epoch': 18, 'value': 0.8548000000000004} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 18, 'value': 0.44244194842338586} ({'split': 'train'})\n",
      "accuracy: {'epoch': 18, 'value': 0.7836000323295593} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 18, 'value': 0.7346784889698028} ({'split': 'test'})\n",
      "Epoch 019\n",
      "accuracy: {'epoch': 19, 'value': 0.8513599999999995} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 19, 'value': 0.4528655918788907} ({'split': 'train'})\n",
      "accuracy: {'epoch': 19, 'value': 0.7727000296115876} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 19, 'value': 0.7576559960842133} ({'split': 'test'})\n",
      "Epoch 020\n",
      "accuracy: {'epoch': 20, 'value': 0.8535000000000004} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 20, 'value': 0.4416997969341278} ({'split': 'train'})\n",
      "accuracy: {'epoch': 20, 'value': 0.7704000413417816} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 20, 'value': 0.7616524457931518} ({'split': 'test'})\n",
      "Epoch 021\n",
      "accuracy: {'epoch': 21, 'value': 0.8582400000000001} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 21, 'value': 0.43410323319435107} ({'split': 'train'})\n",
      "accuracy: {'epoch': 21, 'value': 0.7851000368595124} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 21, 'value': 0.7218056201934814} ({'split': 'test'})\n",
      "Epoch 022\n",
      "accuracy: {'epoch': 22, 'value': 0.8590800000000013} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 22, 'value': 0.4304526620197294} ({'split': 'train'})\n",
      "accuracy: {'epoch': 22, 'value': 0.7629000306129455} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 22, 'value': 0.7964264631271363} ({'split': 'test'})\n",
      "Epoch 023\n",
      "accuracy: {'epoch': 23, 'value': 0.8570999999999996} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 23, 'value': 0.43874537193298346} ({'split': 'train'})\n",
      "accuracy: {'epoch': 23, 'value': 0.7834000408649444} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 23, 'value': 0.7295995056629181} ({'split': 'test'})\n",
      "Epoch 024\n",
      "accuracy: {'epoch': 24, 'value': 0.8617} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 24, 'value': 0.4195485930061342} ({'split': 'train'})\n",
      "accuracy: {'epoch': 24, 'value': 0.7871000409126282} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 24, 'value': 0.7368532538414002} ({'split': 'test'})\n",
      "Epoch 025\n",
      "accuracy: {'epoch': 25, 'value': 0.8661000000000009} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 25, 'value': 0.41145368385314945} ({'split': 'train'})\n",
      "accuracy: {'epoch': 25, 'value': 0.7763000369071961} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 25, 'value': 0.7468551576137542} ({'split': 'test'})\n",
      "Epoch 026\n",
      "accuracy: {'epoch': 26, 'value': 0.8684400000000007} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 26, 'value': 0.4110174777984616} ({'split': 'train'})\n",
      "accuracy: {'epoch': 26, 'value': 0.7855000376701355} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 26, 'value': 0.7419838845729827} ({'split': 'test'})\n",
      "Epoch 027\n",
      "accuracy: {'epoch': 27, 'value': 0.8673} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 27, 'value': 0.4019548893642424} ({'split': 'train'})\n",
      "accuracy: {'epoch': 27, 'value': 0.7728000342845918} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 27, 'value': 0.8169683396816254} ({'split': 'test'})\n",
      "Epoch 028\n",
      "accuracy: {'epoch': 28, 'value': 0.8700799999999992} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 28, 'value': 0.40044418381691016} ({'split': 'train'})\n",
      "accuracy: {'epoch': 28, 'value': 0.7898000478744507} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 28, 'value': 0.7550821602344513} ({'split': 'test'})\n",
      "Epoch 029\n",
      "accuracy: {'epoch': 29, 'value': 0.8686200000000005} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 29, 'value': 0.4080447007179266} ({'split': 'train'})\n",
      "accuracy: {'epoch': 29, 'value': 0.7714000344276428} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 29, 'value': 0.8107502162456512} ({'split': 'test'})\n",
      "mean_test_accuracy <cifar_utils.accumulators.Mean object at 0x7f2aad5a0820>\n",
      "QWERTY: Enter Cifar 2, acc 0.7898000478744507\n",
      "----- Saved results at sample.csv ------\n",
      "{'exp_name': 'exp_2023-09-16_10-25-05_499081', 'model0_acc': 90.30999821424489, 'model1_acc': 90.4999980330467, 'geometric_acc': 85.44, 'prediction_acc': 91.33, 'naive_acc': 16.89, 'geometric_gain': -5.059998033046696, 'geometric_gain_%': -5.591158169085267, 'prediction_gain': 0.8300019669533043, 'prediction_gain_%': 0.917129265185424, 'relative_loss_wrt_prediction': 6.508287434270691, 'model0_aligned': 90.19, 'geometric_time': 35.47849554568529, 'retrain_geometric_best': 10.000000670552254, 'retrain_naive_best': 78.98000478744507, 'retrain_model0_best': 80.18000304698944, 'retrain_model1_best': 80.62000453472137, 'retrain_epochs': 30}\n",
      "FYI: the parameters were: \n",
      " Namespace(act_bug=False, act_num_samples=100, activation_histograms=False, activation_mode=None, activation_seed=21, activation_time=2.627447247505188e-05, alpha=0.7, baseroot='/root/autodl-tmp/OTfusion_pruning/otfusion', batch_size_test=1000, batch_size_train=64, center_acts=False, choice='0 2 4 6 8', cifar_style_data=False, ckpt_type='best', clip_gm=False, clip_max=5, clip_min=0, config={'dataset': 'Cifar10', 'model': 'vgg11_nobias', 'optimizer': 'SGD', 'optimizer_decay_at_epochs': [30, 60, 90, 120, 150, 180, 210, 240, 270], 'optimizer_decay_with_factor': 2.0, 'optimizer_learning_rate': 0.05, 'optimizer_momentum': 0.9, 'optimizer_weight_decay': 0.0005, 'batch_size': 128, 'num_epochs': 30, 'seed': 42, 'nick': 'naive_averaging', 'start_acc': 16.89}, config_dir='/root/autodl-tmp/OTfusion_pruning/otfusion/exp_sample/configurations', config_file=None, correction=True, csv_dir='/root/autodl-tmp/OTfusion_pruning/otfusion/exp_sample/csv', dataset='Cifar10', debug=False, deprecated=None, deterministic=False, diff_init=False, disable_bias=True, dist_epochs=60, dist_normalize=False, dump_final_models=False, dump_model=False, enable_dropout=False, ensemble_step=0.5, eval_aligned=True, exact=True, exp_name='exp_2023-09-16_10-25-05_499081', experiment_dir='experiment1', geom_ensemble_type='wts', geometric_time=35.47849554568529, gpu_id=0, gromov=False, gromov_loss='square_loss', ground_metric='euclidean', ground_metric_eff=True, ground_metric_normalize='none', handle_skips=False, importance=None, learning_rate=0.004, load_model_dir='./cifar_models', load_models='', log_interval=100, model0_aligned_acc=90.19, model0_aligned_acc_layer_0=12.49, model0_aligned_acc_layer_1=10.36, model0_aligned_acc_layer_2=11.46, model0_aligned_acc_layer_3=12.49, model0_aligned_acc_layer_4=9.03, model0_aligned_acc_layer_5=14.1, model0_aligned_acc_layer_6=22.17, model0_aligned_acc_layer_7=5.82, model0_aligned_acc_layer_8=90.19, model_name='vgg11_nobias', momentum=0.5, n_epochs=0, no_random_trainloaders=False, normalize_acts=False, normalize_wts=False, not_squared=True, num_hidden_nodes=400, num_hidden_nodes1=400, num_hidden_nodes2=200, num_hidden_nodes3=100, num_hidden_nodes4=50, num_models=2, options_type='generic', params_geometric=9222848, params_model_0=9222848, params_model_1=9222848, partial_reshape=False, partition_dataloader=-1, partition_type='labels', past_correction=True, personal_class_idx=9, personal_split_frac=0.1, pool_acts=False, pool_relu=False, prediction_wts=False, prelu_acts=True, print_distances=False, proper_marginals=False, prunint_rate=0.2, recheck_acc=False, recheck_cifar=True, reg=0.01, reg_m=0.001, reinit_trainloaders=False, result_dir='/root/autodl-tmp/OTfusion_pruning/otfusion/exp_sample/results', retrain=30, retrain_avg_only=False, retrain_geometric_best=0.10000000670552253, retrain_geometric_only=False, retrain_lr_decay=-1, retrain_lr_decay_epochs=None, retrain_lr_decay_factor=None, retrain_model0_best=0.8018000304698943, retrain_model1_best=0.8062000453472138, retrain_naive_best=0.7898000478744507, retrain_seed=-1, rootdir='/root/autodl-tmp/OTfusion_pruning/otfusion/exp_sample', run_mode='benchmark', same_model=-1, save_result_file='sample.csv', second_config={'dataset': 'Cifar10', 'model': 'vgg11_nobias', 'optimizer': 'SGD', 'optimizer_decay_at_epochs': [30, 60, 90, 120, 150, 180, 210, 240, 270], 'optimizer_decay_with_factor': 2.0, 'optimizer_learning_rate': 0.05, 'optimizer_momentum': 0.9, 'optimizer_weight_decay': 0.0005, 'batch_size': 128, 'num_epochs': 30, 'seed': 42, 'nick': 'naive_averaging', 'start_acc': 16.89}, second_model_name=None, sinkhorn_type='normal', skip_last_layer=False, skip_last_layer_type='average', skip_personal_idx=False, skip_retrain=-1, softmax_temperature=1, standardize_acts=False, sweep_id=90, sweep_name='exp_sample', temperature=20, tensorboard=False, tensorboard_root='./tensorboard', timestamp='2023-09-16_10-25-05_499081', tmap_stats=False, to_download=True, transform_acts=False, unbalanced=False, update_acts=False, weight_stats=True, width_ratio=1, **{'trace_sum_ratio_classifier.weight': 1.0, 'trace_sum_ratio_features.0.weight': 0.046875, 'trace_sum_ratio_features.11.weight': 0.00390625, 'trace_sum_ratio_features.13.weight': 0.001953125, 'trace_sum_ratio_features.16.weight': 0.001953125, 'trace_sum_ratio_features.18.weight': 0.0, 'trace_sum_ratio_features.3.weight': 0.0078125, 'trace_sum_ratio_features.6.weight': 0.0, 'trace_sum_ratio_features.8.weight': 0.0078125})\n"
     ]
    }
   ],
   "source": [
    "!python main.py --gpu-id 0 --model-name vgg11_nobias --n-epochs 0 --save-result-file sample.csv \\\n",
    "--sweep-name exp_sample --correction --ground-metric euclidean --weight-stats \\\n",
    "--geom-ensemble-type wts --ground-metric-normalize none --sweep-id 90 \\\n",
    "--ckpt-type best --dataset Cifar10 --ground-metric-eff --recheck-cifar --activation-seed {activation_seed} \\\n",
    "--prelu-acts --past-correction --not-squared --exact\\\n",
    "--learning-rate 0.004\\\n",
    "--momentum 0.5\\\n",
    "--batch-size-train 64 --experiment-dir {experiment_dir}\\\n",
    "--prunint-rate {prunint_rate}\\\n",
    "--run-mode {run_mode}\\\n",
    "--to-download\\\n",
    "--load-model-dir {load_model_dir}\\\n",
    "--retrain 30\\\n",
    "--eval-aligned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c4db3b58-04ae-4bdf-b853-68b86e28644a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#剪枝实验1:prune without finetune\n",
    "activation_seed=21\n",
    "experiment_dir=\"experiment1\"\n",
    "prunint_rate=0.2\n",
    "run_mode='prune'\n",
    "load_model_dir='./cifar_models'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "acf10f3e-1c4a-40b3-85af-c6fc57512703",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------- Setting up parameters -------\n",
      "dumping parameters at  /root/autodl-tmp/OTfusion_pruning/otfusion/exp_sample/configurations\n",
      "The parameters are: \n",
      " Namespace(act_bug=False, act_num_samples=100, activation_histograms=False, activation_mode=None, activation_seed=21, alpha=0.7, baseroot='/root/autodl-tmp/OTfusion_pruning/otfusion', batch_size_test=1000, batch_size_train=64, center_acts=False, choice='0 2 4 6 8', cifar_style_data=False, ckpt_type='best', clip_gm=False, clip_max=5, clip_min=0, config_dir='/root/autodl-tmp/OTfusion_pruning/otfusion/exp_sample/configurations', config_file=None, correction=True, csv_dir='/root/autodl-tmp/OTfusion_pruning/otfusion/exp_sample/csv', dataset='Cifar10', debug=False, deprecated=None, deterministic=False, diff_init=False, disable_bias=True, dist_epochs=60, dist_normalize=False, dump_final_models=False, dump_model=False, enable_dropout=False, ensemble_step=0.5, eval_aligned=False, exact=True, exp_name='exp_2023-09-16_14-53-20_717630', experiment_dir='experiment1', geom_ensemble_type='wts', gpu_id=0, gromov=False, gromov_loss='square_loss', ground_metric='euclidean', ground_metric_eff=True, ground_metric_normalize='none', handle_skips=False, importance=None, learning_rate=0.08, load_model_dir='./cifar_models', load_models='', log_interval=100, model_name='vgg11_nobias', momentum=0.5, n_epochs=90, no_random_trainloaders=False, normalize_acts=False, normalize_wts=False, not_squared=True, num_hidden_nodes=400, num_hidden_nodes1=400, num_hidden_nodes2=200, num_hidden_nodes3=100, num_hidden_nodes4=50, num_models=2, options_type='generic', partial_reshape=False, partition_dataloader=-1, partition_type='labels', past_correction=True, personal_class_idx=9, personal_split_frac=0.1, pool_acts=False, pool_relu=False, prediction_wts=False, prelu_acts=True, print_distances=False, proper_marginals=False, prunint_rate=0.2, recheck_acc=False, recheck_cifar=True, reg=0.01, reg_m=0.001, reinit_trainloaders=False, result_dir='/root/autodl-tmp/OTfusion_pruning/otfusion/exp_sample/results', retrain=30, retrain_avg_only=False, retrain_geometric_only=False, retrain_lr_decay=-1, retrain_lr_decay_epochs=None, retrain_lr_decay_factor=None, retrain_seed=-1, rootdir='/root/autodl-tmp/OTfusion_pruning/otfusion/exp_sample', run_mode='prune', same_model=-1, save_result_file='sample.csv', second_model_name=None, sinkhorn_type='normal', skip_last_layer=False, skip_last_layer_type='average', skip_personal_idx=False, skip_retrain=-1, softmax_temperature=1, standardize_acts=False, sweep_id=90, sweep_name='exp_sample', temperature=20, tensorboard=False, tensorboard_root='./tensorboard', timestamp='2023-09-16_14-53-20_717630', tmap_stats=False, to_download=True, transform_acts=False, unbalanced=False, update_acts=False, weight_stats=True, width_ratio=1)\n",
      "refactored get_config\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "in _make_layers [Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), AvgPool2d(kernel_size=1, stride=1, padding=0)]\n",
      "Relu Inplace is  False\n",
      "model parameters are \n",
      " [torch.Size([64, 3, 3, 3]), torch.Size([128, 64, 3, 3]), torch.Size([256, 128, 3, 3]), torch.Size([256, 256, 3, 3]), torch.Size([512, 256, 3, 3]), torch.Size([512, 512, 3, 3]), torch.Size([512, 512, 3, 3]), torch.Size([512, 512, 3, 3]), torch.Size([10, 512])]\n",
      "in _make_layers [Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), AvgPool2d(kernel_size=1, stride=1, padding=0)]\n",
      "Relu Inplace is  False\n",
      "model parameters are \n",
      " [torch.Size([64, 3, 3, 3]), torch.Size([128, 64, 3, 3]), torch.Size([256, 128, 3, 3]), torch.Size([256, 256, 3, 3]), torch.Size([512, 256, 3, 3]), torch.Size([512, 512, 3, 3]), torch.Size([512, 512, 3, 3]), torch.Size([512, 512, 3, 3]), torch.Size([10, 512])]\n",
      "in _make_layers [Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), AvgPool2d(kernel_size=1, stride=1, padding=0)]\n",
      "Relu Inplace is  False\n",
      "model parameters are \n",
      " [torch.Size([64, 3, 3, 3]), torch.Size([128, 64, 3, 3]), torch.Size([256, 128, 3, 3]), torch.Size([256, 256, 3, 3]), torch.Size([512, 256, 3, 3]), torch.Size([512, 512, 3, 3]), torch.Size([512, 512, 3, 3]), torch.Size([512, 512, 3, 3]), torch.Size([10, 512])]\n",
      "in _make_layers [Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), AvgPool2d(kernel_size=1, stride=1, padding=0)]\n",
      "Relu Inplace is  False\n",
      "model parameters are \n",
      " [torch.Size([64, 3, 3, 3]), torch.Size([128, 64, 3, 3]), torch.Size([256, 128, 3, 3]), torch.Size([256, 256, 3, 3]), torch.Size([512, 256, 3, 3]), torch.Size([512, 512, 3, 3]), torch.Size([512, 512, 3, 3]), torch.Size([512, 512, 3, 3]), torch.Size([10, 512])]\n",
      "Pruning with custom mask (all conv layers)\n",
      "Can not find [features.0.weight] in mask_dict\n",
      "Can not find [features.3.weight] in mask_dict\n",
      "Can not find [features.6.weight] in mask_dict\n",
      "Can not find [features.8.weight] in mask_dict\n",
      "Can not find [features.11.weight] in mask_dict\n",
      "Can not find [features.13.weight] in mask_dict\n",
      "Can not find [features.16.weight] in mask_dict\n",
      "Can not find [features.18.weight] in mask_dict\n",
      "\n",
      "--------- Testing in global mode ---------\n",
      "size of test_loader dataset:  10000\n",
      "\n",
      "Test set: Avg. loss: 0.0003, Accuracy: 9019/10000 (90%)\n",
      "\n",
      "Pruning with custom mask (all conv layers)\n",
      "Can not find [features.0.weight] in mask_dict\n",
      "Can not find [features.3.weight] in mask_dict\n",
      "Can not find [features.6.weight] in mask_dict\n",
      "Can not find [features.8.weight] in mask_dict\n",
      "Can not find [features.11.weight] in mask_dict\n",
      "Can not find [features.13.weight] in mask_dict\n",
      "Can not find [features.16.weight] in mask_dict\n",
      "Can not find [features.18.weight] in mask_dict\n",
      "\n",
      "--------- Testing in global mode ---------\n",
      "size of test_loader dataset:  10000\n",
      "\n",
      "Test set: Avg. loss: 0.0003, Accuracy: 9046/10000 (90%)\n",
      "\n",
      "layer features.0.weight has #params  1728\n",
      "layer features.3.weight has #params  73728\n",
      "layer features.6.weight has #params  294912\n",
      "layer features.8.weight has #params  589824\n",
      "layer features.11.weight has #params  1179648\n",
      "layer features.13.weight has #params  2359296\n",
      "layer features.16.weight has #params  2359296\n",
      "layer features.18.weight has #params  2359296\n",
      "layer classifier.weight has #params  5120\n",
      "Activation Timer start\n",
      "Activation Timer ends\n",
      "------- Geometric Ensembling -------\n",
      "Timer start\n",
      "Previous layer shape is  None\n",
      "Processing the coordinates to form ground_metric\n",
      "dont leave off the squaring of the ground metric\n",
      "returns a uniform measure of cardinality:  64\n",
      "returns a uniform measure of cardinality:  64\n",
      "the transport map is  tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')\n",
      "Ratio of trace to the matrix sum:  tensor(0.0469, device='cuda:0')\n",
      "Here, trace is 3.0 and matrix sum is 64.0 \n",
      "this is past correction for weight mode\n",
      "Shape of aligned wt is  torch.Size([64, 3, 9])\n",
      "Shape of fc_layer0_weight_data is  torch.Size([64, 3, 9])\n",
      "Previous layer shape is  torch.Size([64, 3, 3, 3])\n",
      "shape of layer: model 0 torch.Size([128, 64, 9])\n",
      "shape of layer: model 1 torch.Size([128, 64, 9])\n",
      "shape of previous transport map torch.Size([64, 64])\n",
      "Processing the coordinates to form ground_metric\n",
      "dont leave off the squaring of the ground metric\n",
      "returns a uniform measure of cardinality:  128\n",
      "returns a uniform measure of cardinality:  128\n",
      "the transport map is  tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        ...,\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0078],\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],\n",
      "       device='cuda:0')\n",
      "Ratio of trace to the matrix sum:  tensor(0.0078, device='cuda:0')\n",
      "Here, trace is 1.0 and matrix sum is 128.0 \n",
      "this is past correction for weight mode\n",
      "Shape of aligned wt is  torch.Size([128, 64, 9])\n",
      "Shape of fc_layer0_weight_data is  torch.Size([128, 64, 9])\n",
      "Previous layer shape is  torch.Size([128, 64, 3, 3])\n",
      "shape of layer: model 0 torch.Size([256, 128, 9])\n",
      "shape of layer: model 1 torch.Size([256, 128, 9])\n",
      "shape of previous transport map torch.Size([128, 128])\n",
      "Processing the coordinates to form ground_metric\n",
      "dont leave off the squaring of the ground metric\n",
      "returns a uniform measure of cardinality:  256\n",
      "returns a uniform measure of cardinality:  256\n",
      "the transport map is  tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')\n",
      "Ratio of trace to the matrix sum:  tensor(0., device='cuda:0')\n",
      "Here, trace is 0.0 and matrix sum is 256.0 \n",
      "this is past correction for weight mode\n",
      "Shape of aligned wt is  torch.Size([256, 128, 9])\n",
      "Shape of fc_layer0_weight_data is  torch.Size([256, 128, 9])\n",
      "Previous layer shape is  torch.Size([256, 128, 3, 3])\n",
      "shape of layer: model 0 torch.Size([256, 256, 9])\n",
      "shape of layer: model 1 torch.Size([256, 256, 9])\n",
      "shape of previous transport map torch.Size([256, 256])\n",
      "Processing the coordinates to form ground_metric\n",
      "dont leave off the squaring of the ground metric\n",
      "returns a uniform measure of cardinality:  256\n",
      "returns a uniform measure of cardinality:  256\n",
      "the transport map is  tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        ...,\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.0039, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],\n",
      "       device='cuda:0')\n",
      "Ratio of trace to the matrix sum:  tensor(0.0078, device='cuda:0')\n",
      "Here, trace is 2.0 and matrix sum is 256.0 \n",
      "this is past correction for weight mode\n",
      "Shape of aligned wt is  torch.Size([256, 256, 9])\n",
      "Shape of fc_layer0_weight_data is  torch.Size([256, 256, 9])\n",
      "Previous layer shape is  torch.Size([256, 256, 3, 3])\n",
      "shape of layer: model 0 torch.Size([512, 256, 9])\n",
      "shape of layer: model 1 torch.Size([512, 256, 9])\n",
      "shape of previous transport map torch.Size([256, 256])\n",
      "Processing the coordinates to form ground_metric\n",
      "dont leave off the squaring of the ground metric\n",
      "returns a uniform measure of cardinality:  512\n",
      "returns a uniform measure of cardinality:  512\n",
      "the transport map is  tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')\n",
      "Ratio of trace to the matrix sum:  tensor(0.0039, device='cuda:0')\n",
      "Here, trace is 2.0 and matrix sum is 512.0 \n",
      "this is past correction for weight mode\n",
      "Shape of aligned wt is  torch.Size([512, 256, 9])\n",
      "Shape of fc_layer0_weight_data is  torch.Size([512, 256, 9])\n",
      "Previous layer shape is  torch.Size([512, 256, 3, 3])\n",
      "shape of layer: model 0 torch.Size([512, 512, 9])\n",
      "shape of layer: model 1 torch.Size([512, 512, 9])\n",
      "shape of previous transport map torch.Size([512, 512])\n",
      "Processing the coordinates to form ground_metric\n",
      "dont leave off the squaring of the ground metric\n",
      "returns a uniform measure of cardinality:  512\n",
      "returns a uniform measure of cardinality:  512\n",
      "the transport map is  tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')\n",
      "Ratio of trace to the matrix sum:  tensor(0.0020, device='cuda:0')\n",
      "Here, trace is 1.0 and matrix sum is 512.0 \n",
      "this is past correction for weight mode\n",
      "Shape of aligned wt is  torch.Size([512, 512, 9])\n",
      "Shape of fc_layer0_weight_data is  torch.Size([512, 512, 9])\n",
      "Previous layer shape is  torch.Size([512, 512, 3, 3])\n",
      "shape of layer: model 0 torch.Size([512, 512, 9])\n",
      "shape of layer: model 1 torch.Size([512, 512, 9])\n",
      "shape of previous transport map torch.Size([512, 512])\n",
      "Processing the coordinates to form ground_metric\n",
      "dont leave off the squaring of the ground metric\n",
      "returns a uniform measure of cardinality:  512\n",
      "returns a uniform measure of cardinality:  512\n",
      "the transport map is  tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')\n",
      "Ratio of trace to the matrix sum:  tensor(0.0020, device='cuda:0')\n",
      "Here, trace is 1.0 and matrix sum is 512.0 \n",
      "this is past correction for weight mode\n",
      "Shape of aligned wt is  torch.Size([512, 512, 9])\n",
      "Shape of fc_layer0_weight_data is  torch.Size([512, 512, 9])\n",
      "Previous layer shape is  torch.Size([512, 512, 3, 3])\n",
      "shape of layer: model 0 torch.Size([512, 512, 9])\n",
      "shape of layer: model 1 torch.Size([512, 512, 9])\n",
      "shape of previous transport map torch.Size([512, 512])\n",
      "Processing the coordinates to form ground_metric\n",
      "dont leave off the squaring of the ground metric\n",
      "returns a uniform measure of cardinality:  512\n",
      "returns a uniform measure of cardinality:  512\n",
      "the transport map is  tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        [0.0020, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        ...,\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],\n",
      "       device='cuda:0')\n",
      "Ratio of trace to the matrix sum:  tensor(0., device='cuda:0')\n",
      "Here, trace is 0.0 and matrix sum is 512.0 \n",
      "this is past correction for weight mode\n",
      "Shape of aligned wt is  torch.Size([512, 512, 9])\n",
      "Shape of fc_layer0_weight_data is  torch.Size([512, 512, 9])\n",
      "Previous layer shape is  torch.Size([512, 512, 3, 3])\n",
      "shape of layer: model 0 torch.Size([10, 512])\n",
      "shape of layer: model 1 torch.Size([10, 512])\n",
      "shape of previous transport map torch.Size([512, 512])\n",
      "Processing the coordinates to form ground_metric\n",
      "dont leave off the squaring of the ground metric\n",
      "ground metric is  tensor([[1.4241, 2.4872, 2.7804, 2.8719, 2.7374, 2.8799, 2.7276, 2.6342, 2.5641,\n",
      "         2.5922],\n",
      "        [2.4889, 1.3253, 2.6732, 2.5914, 2.7584, 2.5501, 2.4174, 2.5215, 2.3048,\n",
      "         2.4101],\n",
      "        [2.7600, 2.6711, 1.3832, 2.9465, 2.9065, 2.8701, 2.7276, 2.8385, 2.7485,\n",
      "         2.7866],\n",
      "        [2.8352, 2.6791, 2.8919, 1.5430, 2.8605, 2.9159, 2.7574, 2.7980, 2.8164,\n",
      "         2.6354],\n",
      "        [2.7551, 2.5960, 2.8635, 2.8450, 1.5042, 2.7447, 2.7853, 2.6811, 2.7031,\n",
      "         2.7294],\n",
      "        [2.9190, 2.5396, 2.9469, 2.8416, 2.7272, 1.5502, 2.7781, 2.7303, 2.7977,\n",
      "         2.6697],\n",
      "        [2.6778, 2.4457, 2.7190, 2.9111, 2.6488, 2.8526, 1.3931, 2.6890, 2.4981,\n",
      "         2.5560],\n",
      "        [2.7394, 2.5496, 2.7799, 2.7903, 2.7151, 2.6590, 2.7190, 1.3924, 2.5583,\n",
      "         2.5121],\n",
      "        [2.5972, 2.3660, 2.7693, 2.6872, 2.7026, 2.8280, 2.5325, 2.6366, 1.2999,\n",
      "         2.4041],\n",
      "        [2.5918, 2.2730, 2.7816, 2.7576, 2.6697, 2.7273, 2.5482, 2.5290, 2.5207,\n",
      "         1.2878]], device='cuda:0', grad_fn=<PowBackward0>)\n",
      "returns a uniform measure of cardinality:  10\n",
      "returns a uniform measure of cardinality:  10\n",
      "the transport map is  tensor([[0.1000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0000, 0.1000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0000, 0.0000, 0.1000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.1000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.1000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.1000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.1000, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.1000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.1000,\n",
      "         0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.1000]], device='cuda:0')\n",
      "Ratio of trace to the matrix sum:  tensor(1., device='cuda:0')\n",
      "Here, trace is 9.99755859375 and matrix sum is 9.99755859375 \n",
      "this is past correction for weight mode\n",
      "Shape of aligned wt is  torch.Size([10, 512])\n",
      "Shape of fc_layer0_weight_data is  torch.Size([10, 512])\n",
      "using independent method\n",
      "in _make_layers [Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), AvgPool2d(kernel_size=1, stride=1, padding=0)]\n",
      "Relu Inplace is  False\n",
      "model parameters are \n",
      " [torch.Size([64, 3, 3, 3]), torch.Size([128, 64, 3, 3]), torch.Size([256, 128, 3, 3]), torch.Size([256, 256, 3, 3]), torch.Size([512, 256, 3, 3]), torch.Size([512, 512, 3, 3]), torch.Size([512, 512, 3, 3]), torch.Size([512, 512, 3, 3]), torch.Size([10, 512])]\n",
      "\n",
      "--------- Testing in global mode ---------\n",
      "size of test_loader dataset:  10000\n",
      "\n",
      "Test set: Avg. loss: 0.0023, Accuracy: 1019/10000 (10%)\n",
      "\n",
      "len of model parameters and avg aligned layers is  9 9\n",
      "len of model_state_dict is  9\n",
      "len of param_list is  9\n",
      "\n",
      "--------- Testing in global mode ---------\n",
      "size of test_loader dataset:  10000\n",
      "\n",
      "Test set: Avg. loss: 0.0015, Accuracy: 8545/10000 (85%)\n",
      "\n",
      "===============================================\n",
      "===============================================\n",
      "After Ensemble: 85.45\n",
      "===============================================\n",
      "===============================================\n",
      "Timer ends\n",
      "Time taken for geometric ensembling is 3.620155781507492 seconds\n",
      "------- Prediction based ensembling -------\n",
      "\n",
      "Test set: Avg. loss: 0.0003, Accuracy: 9133/10000 (91%)\n",
      "\n",
      "------- Naive ensembling of weights -------\n",
      "[torch.Size([64, 3, 3, 3]), torch.Size([64, 3, 3, 3])]\n",
      "torch.Size([64, 3, 3, 3])\n",
      "[torch.Size([128, 64, 3, 3]), torch.Size([128, 64, 3, 3])]\n",
      "torch.Size([128, 64, 3, 3])\n",
      "[torch.Size([256, 128, 3, 3]), torch.Size([256, 128, 3, 3])]\n",
      "torch.Size([256, 128, 3, 3])\n",
      "[torch.Size([256, 256, 3, 3]), torch.Size([256, 256, 3, 3])]\n",
      "torch.Size([256, 256, 3, 3])\n",
      "[torch.Size([512, 256, 3, 3]), torch.Size([512, 256, 3, 3])]\n",
      "torch.Size([512, 256, 3, 3])\n",
      "[torch.Size([512, 512, 3, 3]), torch.Size([512, 512, 3, 3])]\n",
      "torch.Size([512, 512, 3, 3])\n",
      "[torch.Size([512, 512, 3, 3]), torch.Size([512, 512, 3, 3])]\n",
      "torch.Size([512, 512, 3, 3])\n",
      "[torch.Size([512, 512, 3, 3]), torch.Size([512, 512, 3, 3])]\n",
      "torch.Size([512, 512, 3, 3])\n",
      "[torch.Size([10, 512]), torch.Size([10, 512])]\n",
      "torch.Size([10, 512])\n",
      "in _make_layers [Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), AvgPool2d(kernel_size=1, stride=1, padding=0)]\n",
      "Relu Inplace is  False\n",
      "model parameters are \n",
      " [torch.Size([64, 3, 3, 3]), torch.Size([128, 64, 3, 3]), torch.Size([256, 128, 3, 3]), torch.Size([256, 256, 3, 3]), torch.Size([512, 256, 3, 3]), torch.Size([512, 512, 3, 3]), torch.Size([512, 512, 3, 3]), torch.Size([512, 512, 3, 3]), torch.Size([10, 512])]\n",
      "\n",
      "--------- Testing in global mode ---------\n",
      "size of test_loader dataset:  10000\n",
      "\n",
      "Test set: Avg. loss: 0.0023, Accuracy: 1143/10000 (11%)\n",
      "\n",
      "\n",
      "--------- Testing in global mode ---------\n",
      "size of test_loader dataset:  10000\n",
      "\n",
      "Test set: Avg. loss: 0.0023, Accuracy: 1689/10000 (17%)\n",
      "\n",
      "-------- Retraining the models ---------\n",
      "Retraining model :  model_0\n",
      "num_epochs---------------+++))))))+++ 300\n",
      "num_epochs---------------+++))))))+++ 300\n",
      "lr is  0.05\n",
      "number of epochs would be  30\n",
      "num_epochs-------------- 30\n",
      "Epoch 000\n",
      "/root/miniconda3/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:129: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\"Detected call of `lr_scheduler.step()` before `optimizer.step()`. \"\n",
      "/root/miniconda3/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:154: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "accuracy: {'epoch': 0, 'value': 0.5033000000000002} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 0, 'value': 1.4018298149490362} ({'split': 'train'})\n",
      "accuracy: {'epoch': 0, 'value': 0.7034000277519227} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 0, 'value': 0.9205841004848481} ({'split': 'test'})\n",
      "Epoch 001\n",
      "^C\n",
      "Traceback (most recent call last):\n",
      "  File \"main.py\", line 338, in <module>\n",
      "    _, best_retrain_acc = routines.retrain_models(args, [*original_models, geometric_model, naive_model], retrain_loader, test_loader, config, tensorboard_obj=tensorboard_obj, initial_acc=initial_acc, nicks=nicks)\n",
      "  File \"/root/autodl-tmp/OTfusion_pruning/otfusion/routines.py\", line 392, in retrain_models\n",
      "    retrained_network, acc = cifar_train.get_retrained_model(args, retrain_loader, test_loader, old_networks[i], config, output_root_dir, tensorboard_obj=tensorboard_obj, nick=nick, start_acc=initial_acc[i])\n",
      "  File \"/root/autodl-tmp/OTfusion_pruning/otfusion/./cifar/train.py\", line 333, in get_retrained_model\n",
      "    best_acc,model = main(config, output_dir, args.gpu_id, pretrained_model=old_network, pretrained_dataset=(train_loader, test_loader), tensorboard_obj=tensorboard_obj,return_model=True)\n",
      "  File \"/root/autodl-tmp/OTfusion_pruning/otfusion/./cifar/train.py\", line 75, in main\n",
      "    loss.backward()\n",
      "  File \"/root/miniconda3/lib/python3.8/site-packages/torch/_tensor.py\", line 307, in backward\n",
      "    torch.autograd.backward(self, gradient, retain_graph, create_graph, inputs=inputs)\n",
      "  File \"/root/miniconda3/lib/python3.8/site-packages/torch/autograd/__init__.py\", line 154, in backward\n",
      "    Variable._execution_engine.run_backward(\n",
      "KeyboardInterrupt\n"
     ]
    }
   ],
   "source": [
    "!python main.py --gpu-id 0 --model-name vgg11_nobias --n-epochs 90 --save-result-file sample.csv \\\n",
    "--sweep-name exp_sample --correction --ground-metric euclidean --weight-stats \\\n",
    "--geom-ensemble-type wts --ground-metric-normalize none --sweep-id 90 \\\n",
    "--ckpt-type best --dataset Cifar10 --ground-metric-eff --recheck-cifar --activation-seed {activation_seed} \\\n",
    "--prelu-acts --past-correction --not-squared --exact\\\n",
    "--learning-rate 0.08\\\n",
    "--momentum 0.5\\\n",
    "--batch-size-train 64 --experiment-dir {experiment_dir}\\\n",
    "--prunint-rate {prunint_rate}\\\n",
    "--run-mode {run_mode}\\\n",
    "--to-download\\\n",
    "--load-model-dir {load_model_dir}\\\n",
    "--retrain 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f467a3e4-8ba5-45f2-b0e9-8c80dd5fe3e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#剪枝实验2:prune finetune\n",
    "activation_seed=21\n",
    "experiment_dir=\"experiment1\"\n",
    "prunint_rate=0.2\n",
    "run_mode='finetune'\n",
    "load_model_dir='./cifar_models'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6e7e97b-c2e3-453d-8c13-88d3c93fad93",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python main.py --gpu-id 0 --model-name vgg11_nobias --n-epochs 90 --save-result-file sample.csv \\\n",
    "--sweep-name exp_sample --correction --ground-metric euclidean --weight-stats \\\n",
    "--geom-ensemble-type wts --ground-metric-normalize none --sweep-id 90 \\\n",
    "--ckpt-type best --dataset Cifar10 --ground-metric-eff --recheck-cifar --activation-seed {activation_seed} \\\n",
    "--prelu-acts --past-correction --not-squared --exact\\\n",
    "--learning-rate 0.08\\\n",
    "--momentum 0.5\\\n",
    "--batch-size-train 64 --experiment-dir {experiment_dir}\\\n",
    "--prunint-rate {prunint_rate}\\\n",
    "--run-mode {run_mode}\\\n",
    "--to-download\\\n",
    "--load-model-dir {load_model_dir}\\\n",
    "--retrain 30"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
