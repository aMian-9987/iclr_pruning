{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9548f487-ef4e-48bc-b034-06d3a5938736",
   "metadata": {},
   "outputs": [],
   "source": [
    "activation_seed=2147\n",
    "experiment_dir=\"experiment10\"\n",
    "prunint_rate=0.2\n",
    "run_mode='benchmark'\n",
    "load_model_dir='./resnet_models'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f6e80ead-659e-47bc-bf87-d87c13b16229",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------- Setting up parameters -------\n",
      "dumping parameters at  /root/autodl-tmp/OTfusion_pruning/otfusion/exp_sample/configurations\n",
      "The parameters are: \n",
      " Namespace(act_bug=False, act_num_samples=400, activation_histograms=True, activation_mode='raw', activation_seed=2147, alpha=0.7, baseroot='/root/autodl-tmp/OTfusion_pruning/otfusion', batch_size_test=1000, batch_size_train=64, center_acts=False, choice='0 2 4 6 8', cifar_style_data=False, ckpt_type='best', clip_gm=False, clip_max=5, clip_min=0, config_dir='/root/autodl-tmp/OTfusion_pruning/otfusion/exp_sample/configurations', config_file=None, correction=True, csv_dir='/root/autodl-tmp/OTfusion_pruning/otfusion/exp_sample/csv', dataset='Cifar10', debug=False, deprecated=None, deterministic=False, diff_init=False, disable_bias=True, dist_epochs=60, dist_normalize=False, dump_final_models=False, dump_model=False, enable_dropout=False, ensemble_step=0.5, eval_aligned=True, exact=True, exp_name='exp_2023-09-17_17-33-48_891426', experiment_dir='experiment10', finetunetimes_benchmark=0, geom_ensemble_type='acts', gpu_id=0, gromov=False, gromov_loss='square_loss', ground_metric='euclidean', ground_metric_eff=False, ground_metric_normalize='none', handle_skips=True, importance=None, learning_rate=0.004, load_model_dir='./resnet_models', load_models='./resnet_models/', log_interval=100, model_name='resnet18_nobias_nobn', momentum=0.5, n_epochs=0, no_random_trainloaders=False, normalize_acts=False, normalize_wts=False, not_squared=True, num_hidden_nodes=400, num_hidden_nodes1=400, num_hidden_nodes2=200, num_hidden_nodes3=100, num_hidden_nodes4=50, num_models=2, options_type='generic', partial_reshape=False, partition_dataloader=-1, partition_type='labels', past_correction=True, personal_class_idx=9, personal_split_frac=0.1, pool_acts=False, pool_relu=False, prediction_wts=False, prelu_acts=True, print_distances=False, proper_marginals=False, prune_times=4, prunint_rate=0.2, recheck_acc=True, recheck_cifar=False, reg=0.01, reg_m=0.001, reinit_trainloaders=False, result_dir='/root/autodl-tmp/OTfusion_pruning/otfusion/exp_sample/results', retrain=30, retrain_avg_only=False, retrain_geometric_only=False, retrain_lr_decay=-1, retrain_lr_decay_epochs=None, retrain_lr_decay_factor=None, retrain_seed=-1, rootdir='/root/autodl-tmp/OTfusion_pruning/otfusion/exp_sample', run_mode='benchmark', same_model=-1, save_result_file='sample.csv', second_model_name=None, sinkhorn_type='normal', skip_last_layer=False, skip_last_layer_type='average', skip_personal_idx=False, skip_retrain=-1, softmax_temperature=1, standardize_acts=False, sweep_id=21, sweep_name='exp_sample', temperature=20, tensorboard=False, tensorboard_root='./tensorboard', timestamp='2023-09-17_17-33-48_891426', tmap_stats=False, to_download=True, transform_acts=False, unbalanced=False, update_acts=False, weight_stats=True, width_ratio=1)\n",
      "refactored get_config\n",
      "------- Obtain dataloaders -------\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "------- Training independent models -------\n",
      "QWERTY: Enter Cifar 1\n",
      "number of epochs would b>>>>>>>e  300\n",
      "model parameters are \n",
      " [torch.Size([64, 3, 3, 3]), torch.Size([64, 64, 3, 3]), torch.Size([64, 64, 3, 3]), torch.Size([64, 64, 3, 3]), torch.Size([64, 64, 3, 3]), torch.Size([128, 64, 3, 3]), torch.Size([128, 128, 3, 3]), torch.Size([128, 64, 1, 1]), torch.Size([128, 128, 3, 3]), torch.Size([128, 128, 3, 3]), torch.Size([256, 128, 3, 3]), torch.Size([256, 256, 3, 3]), torch.Size([256, 128, 1, 1]), torch.Size([256, 256, 3, 3]), torch.Size([256, 256, 3, 3]), torch.Size([512, 256, 3, 3]), torch.Size([512, 512, 3, 3]), torch.Size([512, 256, 1, 1]), torch.Size([512, 512, 3, 3]), torch.Size([512, 512, 3, 3]), torch.Size([10, 512])]\n",
      "Loading model at path ./resnet_models/model_0/best.checkpoint which had accuracy 0.9310999816656113 and at epoch 181\n",
      "\n",
      "--------- Testing in global mode ---------\n",
      "size of test_loader dataset:  10000\n",
      "\n",
      "Test set: Avg. loss: 0.0002, Accuracy: 9268/10000 (93%)\n",
      "\n",
      "He says accury is  93.10999816656113 but actually 92.68\n",
      "QWERTY: Enter Cifar 1\n",
      "number of epochs would b>>>>>>>e  300\n",
      "model parameters are \n",
      " [torch.Size([64, 3, 3, 3]), torch.Size([64, 64, 3, 3]), torch.Size([64, 64, 3, 3]), torch.Size([64, 64, 3, 3]), torch.Size([64, 64, 3, 3]), torch.Size([128, 64, 3, 3]), torch.Size([128, 128, 3, 3]), torch.Size([128, 64, 1, 1]), torch.Size([128, 128, 3, 3]), torch.Size([128, 128, 3, 3]), torch.Size([256, 128, 3, 3]), torch.Size([256, 256, 3, 3]), torch.Size([256, 128, 1, 1]), torch.Size([256, 256, 3, 3]), torch.Size([256, 256, 3, 3]), torch.Size([512, 256, 3, 3]), torch.Size([512, 512, 3, 3]), torch.Size([512, 256, 1, 1]), torch.Size([512, 512, 3, 3]), torch.Size([512, 512, 3, 3]), torch.Size([10, 512])]\n",
      "Loading model at path ./resnet_models/model_1/best.checkpoint which had accuracy 0.9319999837875367 and at epoch 205\n",
      "\n",
      "--------- Testing in global mode ---------\n",
      "size of test_loader dataset:  10000\n",
      "\n",
      "Test set: Avg. loss: 0.0002, Accuracy: 9285/10000 (93%)\n",
      "\n",
      "He says accury is  93.19999837875366 but actually 92.85\n",
      "========================================\n",
      "========================================\n",
      "TEST:args.learning_rate,args.momentum 0.004 0.5\n",
      "========================================\n",
      "========================================\n",
      "========================================\n",
      "\n",
      "--------- Testing in global mode ---------\n",
      "size of test_loader dataset:  10000\n",
      "\n",
      "Test set: Avg. loss: 0.0002, Accuracy: 9268/10000 (93%)\n",
      "\n",
      "========================================\n",
      "========================================\n",
      "TEST:args.learning_rate,args.momentum 0.004 0.5\n",
      "========================================\n",
      "========================================\n",
      "========================================\n",
      "\n",
      "--------- Testing in global mode ---------\n",
      "size of test_loader dataset:  10000\n",
      "\n",
      "Test set: Avg. loss: 0.0002, Accuracy: 9284/10000 (93%)\n",
      "\n",
      "===============================================\n",
      "===============================================\n",
      "before Ensemble,acc: [92.68, 92.85] loss [0.0002285910040140152, 0.00023178379237651824]\n",
      "===============================================\n",
      "===============================================\n",
      "check zero rate for model 0\n",
      "odict_keys(['conv1.weight', 'layer1.0.conv1.weight', 'layer1.0.conv2.weight', 'layer1.1.conv1.weight', 'layer1.1.conv2.weight', 'layer2.0.conv1.weight', 'layer2.0.conv2.weight', 'layer2.0.shortcut.0.weight', 'layer2.1.conv1.weight', 'layer2.1.conv2.weight', 'layer3.0.conv1.weight', 'layer3.0.conv2.weight', 'layer3.0.shortcut.0.weight', 'layer3.1.conv1.weight', 'layer3.1.conv2.weight', 'layer4.0.conv1.weight', 'layer4.0.conv2.weight', 'layer4.0.shortcut.0.weight', 'layer4.1.conv1.weight', 'layer4.1.conv2.weight', 'linear.weight'])\n",
      "the ratio of zero for conv1.weight\n",
      "tensor(0., device='cuda:0')\n",
      "the ratio of zero for layer1.0.conv1.weight\n",
      "tensor(0., device='cuda:0')\n",
      "the ratio of zero for layer1.0.conv2.weight\n",
      "tensor(0., device='cuda:0')\n",
      "the ratio of zero for layer1.1.conv1.weight\n",
      "tensor(0., device='cuda:0')\n",
      "the ratio of zero for layer1.1.conv2.weight\n",
      "tensor(0., device='cuda:0')\n",
      "the ratio of zero for layer2.0.conv1.weight\n",
      "tensor(0., device='cuda:0')\n",
      "the ratio of zero for layer2.0.conv2.weight\n",
      "tensor(0., device='cuda:0')\n",
      "the ratio of zero for layer2.0.shortcut.0.weight\n",
      "tensor(0., device='cuda:0')\n",
      "the ratio of zero for layer2.1.conv1.weight\n",
      "tensor(0., device='cuda:0')\n",
      "the ratio of zero for layer2.1.conv2.weight\n",
      "tensor(0., device='cuda:0')\n",
      "the ratio of zero for layer3.0.conv1.weight\n",
      "tensor(0., device='cuda:0')\n",
      "the ratio of zero for layer3.0.conv2.weight\n",
      "tensor(0., device='cuda:0')\n",
      "the ratio of zero for layer3.0.shortcut.0.weight\n",
      "tensor(0., device='cuda:0')\n",
      "the ratio of zero for layer3.1.conv1.weight\n",
      "tensor(0., device='cuda:0')\n",
      "the ratio of zero for layer3.1.conv2.weight\n",
      "tensor(0., device='cuda:0')\n",
      "the ratio of zero for layer4.0.conv1.weight\n",
      "tensor(0., device='cuda:0')\n",
      "the ratio of zero for layer4.0.conv2.weight\n",
      "tensor(0., device='cuda:0')\n",
      "the ratio of zero for layer4.0.shortcut.0.weight\n",
      "tensor(0., device='cuda:0')\n",
      "the ratio of zero for layer4.1.conv1.weight\n",
      "tensor(0., device='cuda:0')\n",
      "the ratio of zero for layer4.1.conv2.weight\n",
      "tensor(0., device='cuda:0')\n",
      "the ratio of zero for linear.weight\n",
      "tensor(0., device='cuda:0')\n",
      "check zero rate for model 1\n",
      "odict_keys(['conv1.weight', 'layer1.0.conv1.weight', 'layer1.0.conv2.weight', 'layer1.1.conv1.weight', 'layer1.1.conv2.weight', 'layer2.0.conv1.weight', 'layer2.0.conv2.weight', 'layer2.0.shortcut.0.weight', 'layer2.1.conv1.weight', 'layer2.1.conv2.weight', 'layer3.0.conv1.weight', 'layer3.0.conv2.weight', 'layer3.0.shortcut.0.weight', 'layer3.1.conv1.weight', 'layer3.1.conv2.weight', 'layer4.0.conv1.weight', 'layer4.0.conv2.weight', 'layer4.0.shortcut.0.weight', 'layer4.1.conv1.weight', 'layer4.1.conv2.weight', 'linear.weight'])\n",
      "the ratio of zero for conv1.weight\n",
      "tensor(0., device='cuda:0')\n",
      "the ratio of zero for layer1.0.conv1.weight\n",
      "tensor(0., device='cuda:0')\n",
      "the ratio of zero for layer1.0.conv2.weight\n",
      "tensor(0., device='cuda:0')\n",
      "the ratio of zero for layer1.1.conv1.weight\n",
      "tensor(0., device='cuda:0')\n",
      "the ratio of zero for layer1.1.conv2.weight\n",
      "tensor(0., device='cuda:0')\n",
      "the ratio of zero for layer2.0.conv1.weight\n",
      "tensor(0., device='cuda:0')\n",
      "the ratio of zero for layer2.0.conv2.weight\n",
      "tensor(0., device='cuda:0')\n",
      "the ratio of zero for layer2.0.shortcut.0.weight\n",
      "tensor(0., device='cuda:0')\n",
      "the ratio of zero for layer2.1.conv1.weight\n",
      "tensor(0., device='cuda:0')\n",
      "the ratio of zero for layer2.1.conv2.weight\n",
      "tensor(0., device='cuda:0')\n",
      "the ratio of zero for layer3.0.conv1.weight\n",
      "tensor(0., device='cuda:0')\n",
      "the ratio of zero for layer3.0.conv2.weight\n",
      "tensor(0., device='cuda:0')\n",
      "the ratio of zero for layer3.0.shortcut.0.weight\n",
      "tensor(0., device='cuda:0')\n",
      "the ratio of zero for layer3.1.conv1.weight\n",
      "tensor(0., device='cuda:0')\n",
      "the ratio of zero for layer3.1.conv2.weight\n",
      "tensor(0., device='cuda:0')\n",
      "the ratio of zero for layer4.0.conv1.weight\n",
      "tensor(0., device='cuda:0')\n",
      "the ratio of zero for layer4.0.conv2.weight\n",
      "tensor(0., device='cuda:0')\n",
      "the ratio of zero for layer4.0.shortcut.0.weight\n",
      "tensor(0., device='cuda:0')\n",
      "the ratio of zero for layer4.1.conv1.weight\n",
      "tensor(0., device='cuda:0')\n",
      "the ratio of zero for layer4.1.conv2.weight\n",
      "tensor(0., device='cuda:0')\n",
      "the ratio of zero for linear.weight\n",
      "tensor(0., device='cuda:0')\n",
      "layer conv1.weight has #params  1728\n",
      "layer layer1.0.conv1.weight has #params  36864\n",
      "layer layer1.0.conv2.weight has #params  36864\n",
      "layer layer1.1.conv1.weight has #params  36864\n",
      "layer layer1.1.conv2.weight has #params  36864\n",
      "layer layer2.0.conv1.weight has #params  73728\n",
      "layer layer2.0.conv2.weight has #params  147456\n",
      "layer layer2.0.shortcut.0.weight has #params  8192\n",
      "layer layer2.1.conv1.weight has #params  147456\n",
      "layer layer2.1.conv2.weight has #params  147456\n",
      "layer layer3.0.conv1.weight has #params  294912\n",
      "layer layer3.0.conv2.weight has #params  589824\n",
      "layer layer3.0.shortcut.0.weight has #params  32768\n",
      "layer layer3.1.conv1.weight has #params  589824\n",
      "layer layer3.1.conv2.weight has #params  589824\n",
      "layer layer4.0.conv1.weight has #params  1179648\n",
      "layer layer4.0.conv2.weight has #params  2359296\n",
      "layer layer4.0.shortcut.0.weight has #params  131072\n",
      "layer layer4.1.conv1.weight has #params  2359296\n",
      "layer layer4.1.conv2.weight has #params  2359296\n",
      "layer linear.weight has #params  5120\n",
      "Activation Timer start\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "excluded\n",
      "set forward hook for layer named:  conv1\n",
      "this was continued,  bn1\n",
      "this was continued,  layer1\n",
      "this was continued,  layer1.0\n",
      "set forward hook for layer named:  layer1.0.conv1\n",
      "this was continued,  layer1.0.bn1\n",
      "set forward hook for layer named:  layer1.0.conv2\n",
      "this was continued,  layer1.0.shortcut\n",
      "this was continued,  layer1.1\n",
      "set forward hook for layer named:  layer1.1.conv1\n",
      "this was continued,  layer1.1.bn1\n",
      "set forward hook for layer named:  layer1.1.conv2\n",
      "this was continued,  layer1.1.shortcut\n",
      "this was continued,  layer2\n",
      "this was continued,  layer2.0\n",
      "set forward hook for layer named:  layer2.0.conv1\n",
      "this was continued,  layer2.0.bn1\n",
      "set forward hook for layer named:  layer2.0.conv2\n",
      "this was continued,  layer2.0.shortcut\n",
      "set forward hook for layer named:  layer2.0.shortcut.0\n",
      "this was continued,  layer2.0.shortcut.1\n",
      "this was continued,  layer2.1\n",
      "set forward hook for layer named:  layer2.1.conv1\n",
      "this was continued,  layer2.1.bn1\n",
      "set forward hook for layer named:  layer2.1.conv2\n",
      "this was continued,  layer2.1.shortcut\n",
      "this was continued,  layer3\n",
      "this was continued,  layer3.0\n",
      "set forward hook for layer named:  layer3.0.conv1\n",
      "this was continued,  layer3.0.bn1\n",
      "set forward hook for layer named:  layer3.0.conv2\n",
      "this was continued,  layer3.0.shortcut\n",
      "set forward hook for layer named:  layer3.0.shortcut.0\n",
      "this was continued,  layer3.0.shortcut.1\n",
      "this was continued,  layer3.1\n",
      "set forward hook for layer named:  layer3.1.conv1\n",
      "this was continued,  layer3.1.bn1\n",
      "set forward hook for layer named:  layer3.1.conv2\n",
      "this was continued,  layer3.1.shortcut\n",
      "this was continued,  layer4\n",
      "this was continued,  layer4.0\n",
      "set forward hook for layer named:  layer4.0.conv1\n",
      "this was continued,  layer4.0.bn1\n",
      "set forward hook for layer named:  layer4.0.conv2\n",
      "this was continued,  layer4.0.shortcut\n",
      "set forward hook for layer named:  layer4.0.shortcut.0\n",
      "this was continued,  layer4.0.shortcut.1\n",
      "this was continued,  layer4.1\n",
      "set forward hook for layer named:  layer4.1.conv1\n",
      "this was continued,  layer4.1.bn1\n",
      "set forward hook for layer named:  layer4.1.conv2\n",
      "this was continued,  layer4.1.shortcut\n",
      "set forward hook for layer named:  linear\n",
      "excluded\n",
      "set forward hook for layer named:  conv1\n",
      "this was continued,  bn1\n",
      "this was continued,  layer1\n",
      "this was continued,  layer1.0\n",
      "set forward hook for layer named:  layer1.0.conv1\n",
      "this was continued,  layer1.0.bn1\n",
      "set forward hook for layer named:  layer1.0.conv2\n",
      "this was continued,  layer1.0.shortcut\n",
      "this was continued,  layer1.1\n",
      "set forward hook for layer named:  layer1.1.conv1\n",
      "this was continued,  layer1.1.bn1\n",
      "set forward hook for layer named:  layer1.1.conv2\n",
      "this was continued,  layer1.1.shortcut\n",
      "this was continued,  layer2\n",
      "this was continued,  layer2.0\n",
      "set forward hook for layer named:  layer2.0.conv1\n",
      "this was continued,  layer2.0.bn1\n",
      "set forward hook for layer named:  layer2.0.conv2\n",
      "this was continued,  layer2.0.shortcut\n",
      "set forward hook for layer named:  layer2.0.shortcut.0\n",
      "this was continued,  layer2.0.shortcut.1\n",
      "this was continued,  layer2.1\n",
      "set forward hook for layer named:  layer2.1.conv1\n",
      "this was continued,  layer2.1.bn1\n",
      "set forward hook for layer named:  layer2.1.conv2\n",
      "this was continued,  layer2.1.shortcut\n",
      "this was continued,  layer3\n",
      "this was continued,  layer3.0\n",
      "set forward hook for layer named:  layer3.0.conv1\n",
      "this was continued,  layer3.0.bn1\n",
      "set forward hook for layer named:  layer3.0.conv2\n",
      "this was continued,  layer3.0.shortcut\n",
      "set forward hook for layer named:  layer3.0.shortcut.0\n",
      "this was continued,  layer3.0.shortcut.1\n",
      "this was continued,  layer3.1\n",
      "set forward hook for layer named:  layer3.1.conv1\n",
      "this was continued,  layer3.1.bn1\n",
      "set forward hook for layer named:  layer3.1.conv2\n",
      "this was continued,  layer3.1.shortcut\n",
      "this was continued,  layer4\n",
      "this was continued,  layer4.0\n",
      "set forward hook for layer named:  layer4.0.conv1\n",
      "this was continued,  layer4.0.bn1\n",
      "set forward hook for layer named:  layer4.0.conv2\n",
      "this was continued,  layer4.0.shortcut\n",
      "set forward hook for layer named:  layer4.0.shortcut.0\n",
      "this was continued,  layer4.0.shortcut.1\n",
      "this was continued,  layer4.1\n",
      "set forward hook for layer named:  layer4.1.conv1\n",
      "this was continued,  layer4.1.bn1\n",
      "set forward hook for layer named:  layer4.1.conv2\n",
      "this was continued,  layer4.1.shortcut\n",
      "set forward hook for layer named:  linear\n",
      "num_personal_idx  41\n",
      "model_name is  resnet18_nobias_nobn\n",
      "***********\n",
      "min of act: -19.086545944213867, max: 19.853519439697266, mean: -0.0023665931075811386\n",
      "activations for idx 0 at layer conv1 have the following shape  torch.Size([400, 1, 64, 32, 32])\n",
      "-----------\n",
      "***********\n",
      "min of act: -48.929534912109375, max: 17.019357681274414, mean: -2.2665793895721436\n",
      "activations for idx 0 at layer layer1.0.conv1 have the following shape  torch.Size([400, 1, 64, 32, 32])\n",
      "-----------\n",
      "***********\n",
      "min of act: -11.439618110656738, max: 21.495359420776367, mean: 0.1820448487997055\n",
      "activations for idx 0 at layer layer1.0.conv2 have the following shape  torch.Size([400, 1, 64, 32, 32])\n",
      "-----------\n",
      "***********\n",
      "min of act: -33.63824462890625, max: 17.936914443969727, mean: -2.7412588596343994\n",
      "activations for idx 0 at layer layer1.1.conv1 have the following shape  torch.Size([400, 1, 64, 32, 32])\n",
      "-----------\n",
      "***********\n",
      "min of act: -10.153985023498535, max: 22.90478515625, mean: 0.15087001025676727\n",
      "activations for idx 0 at layer layer1.1.conv2 have the following shape  torch.Size([400, 1, 64, 32, 32])\n",
      "-----------\n",
      "***********\n",
      "min of act: -46.37905502319336, max: 52.616729736328125, mean: -1.2972335815429688\n",
      "activations for idx 0 at layer layer2.0.conv1 have the following shape  torch.Size([400, 1, 128, 16, 16])\n",
      "-----------\n",
      "***********\n",
      "min of act: -76.94646453857422, max: 68.23684692382812, mean: -1.6091291904449463\n",
      "activations for idx 0 at layer layer2.0.conv2 have the following shape  torch.Size([400, 1, 128, 16, 16])\n",
      "-----------\n",
      "***********\n",
      "min of act: -10.064547538757324, max: 22.929990768432617, mean: 0.09822580218315125\n",
      "activations for idx 0 at layer layer2.0.shortcut.0 have the following shape  torch.Size([400, 1, 128, 16, 16])\n",
      "-----------\n",
      "***********\n",
      "min of act: -111.32418060302734, max: 74.19022369384766, mean: -9.59034252166748\n",
      "activations for idx 0 at layer layer2.1.conv1 have the following shape  torch.Size([400, 1, 128, 16, 16])\n",
      "-----------\n",
      "***********\n",
      "min of act: -40.925804138183594, max: 58.684444427490234, mean: -0.023186303675174713\n",
      "activations for idx 0 at layer layer2.1.conv2 have the following shape  torch.Size([400, 1, 128, 16, 16])\n",
      "-----------\n",
      "***********\n",
      "min of act: -83.38603973388672, max: 82.2189712524414, mean: -5.565643310546875\n",
      "activations for idx 0 at layer layer3.0.conv1 have the following shape  torch.Size([400, 1, 256, 8, 8])\n",
      "-----------\n",
      "***********\n",
      "min of act: -101.85784149169922, max: 157.4051055908203, mean: -6.628462791442871\n",
      "activations for idx 0 at layer layer3.0.conv2 have the following shape  torch.Size([400, 1, 256, 8, 8])\n",
      "-----------\n",
      "***********\n",
      "min of act: -23.5888671875, max: 42.69386291503906, mean: -0.6083037257194519\n",
      "activations for idx 0 at layer layer3.0.shortcut.0 have the following shape  torch.Size([400, 1, 256, 8, 8])\n",
      "-----------\n",
      "***********\n",
      "min of act: -146.8773193359375, max: 163.03623962402344, mean: -14.000837326049805\n",
      "activations for idx 0 at layer layer3.1.conv1 have the following shape  torch.Size([400, 1, 256, 8, 8])\n",
      "-----------\n",
      "***********\n",
      "min of act: -94.09503173828125, max: 160.44760131835938, mean: -3.43271803855896\n",
      "activations for idx 0 at layer layer3.1.conv2 have the following shape  torch.Size([400, 1, 256, 8, 8])\n",
      "-----------\n",
      "***********\n",
      "min of act: -81.45551300048828, max: 131.25865173339844, mean: -5.984751224517822\n",
      "activations for idx 0 at layer layer4.0.conv1 have the following shape  torch.Size([400, 1, 512, 4, 4])\n",
      "-----------\n",
      "***********\n",
      "min of act: -111.28056335449219, max: 141.5394287109375, mean: -5.962480545043945\n",
      "activations for idx 0 at layer layer4.0.conv2 have the following shape  torch.Size([400, 1, 512, 4, 4])\n",
      "-----------\n",
      "***********\n",
      "min of act: -26.10564422607422, max: 35.45812225341797, mean: -1.7793729305267334\n",
      "activations for idx 0 at layer layer4.0.shortcut.0 have the following shape  torch.Size([400, 1, 512, 4, 4])\n",
      "-----------\n",
      "***********\n",
      "min of act: -103.46488952636719, max: 77.01327514648438, mean: -2.530442714691162\n",
      "activations for idx 0 at layer layer4.1.conv1 have the following shape  torch.Size([400, 1, 512, 4, 4])\n",
      "-----------\n",
      "***********\n",
      "min of act: -51.94965744018555, max: 136.22222900390625, mean: -1.5852023363113403\n",
      "activations for idx 0 at layer layer4.1.conv2 have the following shape  torch.Size([400, 1, 512, 4, 4])\n",
      "-----------\n",
      "***********\n",
      "min of act: -25.368488311767578, max: 102.02352905273438, mean: 0.0036567803472280502\n",
      "activations for idx 0 at layer linear have the following shape  torch.Size([400, 1, 10])\n",
      "-----------\n",
      "***********\n",
      "min of act: -19.12042999267578, max: 18.02828025817871, mean: -0.007675784640014172\n",
      "activations for idx 1 at layer conv1 have the following shape  torch.Size([400, 1, 64, 32, 32])\n",
      "-----------\n",
      "***********\n",
      "min of act: -41.92988586425781, max: 16.90180015563965, mean: -2.355785608291626\n",
      "activations for idx 1 at layer layer1.0.conv1 have the following shape  torch.Size([400, 1, 64, 32, 32])\n",
      "-----------\n",
      "***********\n",
      "min of act: -11.020833969116211, max: 23.247459411621094, mean: 0.16562393307685852\n",
      "activations for idx 1 at layer layer1.0.conv2 have the following shape  torch.Size([400, 1, 64, 32, 32])\n",
      "-----------\n",
      "***********\n",
      "min of act: -36.56871032714844, max: 20.832712173461914, mean: -2.620500326156616\n",
      "activations for idx 1 at layer layer1.1.conv1 have the following shape  torch.Size([400, 1, 64, 32, 32])\n",
      "-----------\n",
      "***********\n",
      "min of act: -10.991537094116211, max: 22.472055435180664, mean: 0.09660456329584122\n",
      "activations for idx 1 at layer layer1.1.conv2 have the following shape  torch.Size([400, 1, 64, 32, 32])\n",
      "-----------\n",
      "***********\n",
      "min of act: -48.368507385253906, max: 45.55572509765625, mean: -1.056715726852417\n",
      "activations for idx 1 at layer layer2.0.conv1 have the following shape  torch.Size([400, 1, 128, 16, 16])\n",
      "-----------\n",
      "***********\n",
      "min of act: -65.96369171142578, max: 77.6406478881836, mean: -1.5260908603668213\n",
      "activations for idx 1 at layer layer2.0.conv2 have the following shape  torch.Size([400, 1, 128, 16, 16])\n",
      "-----------\n",
      "***********\n",
      "min of act: -7.917287826538086, max: 18.54184913635254, mean: 0.11855992674827576\n",
      "activations for idx 1 at layer layer2.0.shortcut.0 have the following shape  torch.Size([400, 1, 128, 16, 16])\n",
      "-----------\n",
      "***********\n",
      "min of act: -120.63600158691406, max: 60.203834533691406, mean: -9.583484649658203\n",
      "activations for idx 1 at layer layer2.1.conv1 have the following shape  torch.Size([400, 1, 128, 16, 16])\n",
      "-----------\n",
      "***********\n",
      "min of act: -30.084056854248047, max: 69.73750305175781, mean: 0.05549104884266853\n",
      "activations for idx 1 at layer layer2.1.conv2 have the following shape  torch.Size([400, 1, 128, 16, 16])\n",
      "-----------\n",
      "***********\n",
      "min of act: -99.42365264892578, max: 96.97014617919922, mean: -5.1881818771362305\n",
      "activations for idx 1 at layer layer3.0.conv1 have the following shape  torch.Size([400, 1, 256, 8, 8])\n",
      "-----------\n",
      "***********\n",
      "min of act: -108.48775482177734, max: 152.23928833007812, mean: -6.021147727966309\n",
      "activations for idx 1 at layer layer3.0.conv2 have the following shape  torch.Size([400, 1, 256, 8, 8])\n",
      "-----------\n",
      "***********\n",
      "min of act: -31.133634567260742, max: 71.25508880615234, mean: -0.5211047530174255\n",
      "activations for idx 1 at layer layer3.0.shortcut.0 have the following shape  torch.Size([400, 1, 256, 8, 8])\n",
      "-----------\n",
      "***********\n",
      "min of act: -167.1964874267578, max: 171.9534454345703, mean: -14.35699462890625\n",
      "activations for idx 1 at layer layer3.1.conv1 have the following shape  torch.Size([400, 1, 256, 8, 8])\n",
      "-----------\n",
      "***********\n",
      "min of act: -84.13597106933594, max: 147.03546142578125, mean: -2.9847781658172607\n",
      "activations for idx 1 at layer layer3.1.conv2 have the following shape  torch.Size([400, 1, 256, 8, 8])\n",
      "-----------\n",
      "***********\n",
      "min of act: -76.46736907958984, max: 97.3463134765625, mean: -6.200111389160156\n",
      "activations for idx 1 at layer layer4.0.conv1 have the following shape  torch.Size([400, 1, 512, 4, 4])\n",
      "-----------\n",
      "***********\n",
      "min of act: -115.49440002441406, max: 109.72575378417969, mean: -6.788937091827393\n",
      "activations for idx 1 at layer layer4.0.conv2 have the following shape  torch.Size([400, 1, 512, 4, 4])\n",
      "-----------\n",
      "***********\n",
      "min of act: -31.710533142089844, max: 40.35109329223633, mean: -2.1102445125579834\n",
      "activations for idx 1 at layer layer4.0.shortcut.0 have the following shape  torch.Size([400, 1, 512, 4, 4])\n",
      "-----------\n",
      "***********\n",
      "min of act: -104.48495483398438, max: 67.14689636230469, mean: -2.6165051460266113\n",
      "activations for idx 1 at layer layer4.1.conv1 have the following shape  torch.Size([400, 1, 512, 4, 4])\n",
      "-----------\n",
      "***********\n",
      "min of act: -50.79534149169922, max: 110.20286560058594, mean: -1.6844558715820312\n",
      "activations for idx 1 at layer layer4.1.conv2 have the following shape  torch.Size([400, 1, 512, 4, 4])\n",
      "-----------\n",
      "***********\n",
      "min of act: -28.984180450439453, max: 93.10883331298828, mean: -0.013390034437179565\n",
      "activations for idx 1 at layer linear have the following shape  torch.Size([400, 1, 10])\n",
      "-----------\n",
      "Activation Timer ends\n",
      "------- Geometric Ensembling -------\n",
      "Timer start\n",
      "\n",
      "--------------- At layer index 0 ------------- \n",
      " \n",
      "Previous layer shape is  None\n",
      "let's see the difference in layer names conv1 conv1\n",
      "torch.Size([400, 1, 64, 32, 32]) shape of activations generally\n",
      "reorder_dim is  [1, 2, 3, 0]\n",
      "In layer conv1.weight: getting activation distance statistics\n",
      "Statistics of the distance from neurons of layer 1 (averaged across nodes of layer 0): \n",
      "\n",
      "Max : 28.138473510742188, Mean : 9.856670379638672, Min : 2.5233349800109863, Std: 3.9626688957214355\n",
      "returns a uniform measure of cardinality:  64\n",
      "returns a uniform measure of cardinality:  64\n",
      "Refactored ground metric calc\n",
      "inside refactored\n",
      "Processing the coordinates to form ground_metric\n",
      "dont leave off the squaring of the ground metric\n",
      "# of ground metric features is  409600\n",
      "# of ground metric features in 0 is   409600\n",
      "# of ground metric features in 1 is   409600\n",
      "ground metric (m0) is  tensor([[1192.0941, 1162.6614, 1016.0740,  ...,  821.1477, 1024.5602,\n",
      "         1366.9434],\n",
      "        [ 779.3542, 1049.8641,  978.2101,  ...,  879.2721,  931.5432,\n",
      "         1187.7019],\n",
      "        [1268.3306, 1454.4122,  747.5684,  ..., 1033.8036, 1206.7462,\n",
      "         1420.9146],\n",
      "        ...,\n",
      "        [ 714.2711, 1063.7609, 1112.5272,  ...,  917.2332,  845.6914,\n",
      "          794.6495],\n",
      "        [1395.8130, 1663.5249, 1401.4392,  ..., 1286.7491, 1530.5651,\n",
      "         1228.1877],\n",
      "        [1232.7340, 1172.8323,  812.7419,  ...,  792.7755, 1178.0835,\n",
      "         1426.1866]], device='cuda:0')\n",
      "shape of T_var is  torch.Size([64, 64])\n",
      "T_var before correction  tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')\n",
      "marginals are  tensor([[63.9996, 63.9996, 63.9996,  ..., 63.9996, 63.9996, 63.9996],\n",
      "        [63.9996, 63.9996, 63.9996,  ..., 63.9996, 63.9996, 63.9996],\n",
      "        [63.9996, 63.9996, 63.9996,  ..., 63.9996, 63.9996, 63.9996],\n",
      "        ...,\n",
      "        [63.9996, 63.9996, 63.9996,  ..., 63.9996, 63.9996, 63.9996],\n",
      "        [63.9996, 63.9996, 63.9996,  ..., 63.9996, 63.9996, 63.9996],\n",
      "        [63.9996, 63.9996, 63.9996,  ..., 63.9996, 63.9996, 63.9996]],\n",
      "       device='cuda:0')\n",
      "T_var after correction  tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')\n",
      "T_var stats: max 0.9999935626983643, min 0.0, mean 0.015624899417161942, std 0.12403393536806107 \n",
      "Ratio of trace to the matrix sum:  tensor(0.0625, device='cuda:0')\n",
      "Here, trace is 3.999974250793457 and matrix sum is 63.99958801269531 \n",
      "Shape of aligned wt is  torch.Size([64, 3, 9])\n",
      "Shape of fc_layer0_weight_data is  torch.Size([64, 3, 9])\n",
      "model parameters are \n",
      " [torch.Size([64, 3, 3, 3]), torch.Size([64, 64, 3, 3]), torch.Size([64, 64, 3, 3]), torch.Size([64, 64, 3, 3]), torch.Size([64, 64, 3, 3]), torch.Size([128, 64, 3, 3]), torch.Size([128, 128, 3, 3]), torch.Size([128, 64, 1, 1]), torch.Size([128, 128, 3, 3]), torch.Size([128, 128, 3, 3]), torch.Size([256, 128, 3, 3]), torch.Size([256, 256, 3, 3]), torch.Size([256, 128, 1, 1]), torch.Size([256, 256, 3, 3]), torch.Size([256, 256, 3, 3]), torch.Size([512, 256, 3, 3]), torch.Size([512, 512, 3, 3]), torch.Size([512, 256, 1, 1]), torch.Size([512, 512, 3, 3]), torch.Size([512, 512, 3, 3]), torch.Size([10, 512])]\n",
      "len of model_state_dict is  21\n",
      "len of new_params is  1\n",
      "updated parameters for layer  conv1.weight\n",
      "\n",
      "--------- Testing in global mode ---------\n",
      "size of test_loader dataset:  10000\n",
      "\n",
      "Test set: Avg. loss: 0.0024, Accuracy: 1264/10000 (13%)\n",
      "\n",
      "accuracy after update is  12.64\n",
      "For layer idx 0, accuracy of the updated model is 12.64\n",
      "\n",
      "--------------- At layer index 1 ------------- \n",
      " \n",
      "Previous layer shape is  torch.Size([64, 3, 3, 3])\n",
      "let's see the difference in layer names layer1.0.conv1 layer1.0.conv1\n",
      "torch.Size([400, 1, 64, 32, 32]) shape of activations generally\n",
      "reorder_dim is  [1, 2, 3, 0]\n",
      "In layer layer1.0.conv1.weight: getting activation distance statistics\n",
      "Statistics of the distance from neurons of layer 1 (averaged across nodes of layer 0): \n",
      "\n",
      "Max : 49.132652282714844, Mean : 17.79385757446289, Min : 4.579073429107666, Std: 7.077404022216797\n",
      "shape of layer: model 0 torch.Size([64, 64, 9])\n",
      "shape of layer: model 1 torch.Size([64, 64, 9])\n",
      "shape of activations: model 0 torch.Size([64, 32, 32, 400])\n",
      "shape of activations: model 1 torch.Size([64, 32, 32, 400])\n",
      "shape of previous transport map torch.Size([64, 64])\n",
      "doing nothing for skips\n",
      "returns a uniform measure of cardinality:  64\n",
      "returns a uniform measure of cardinality:  64\n",
      "Refactored ground metric calc\n",
      "inside refactored\n",
      "Processing the coordinates to form ground_metric\n",
      "dont leave off the squaring of the ground metric\n",
      "# of ground metric features is  409600\n",
      "# of ground metric features in 0 is   409600\n",
      "# of ground metric features in 1 is   409600\n",
      "ground metric (m0) is  tensor([[1757.1388, 2513.6006, 2298.3735,  ..., 1646.9622, 2555.3079,\n",
      "         2591.5278],\n",
      "        [1918.7620, 2708.3970, 2201.6116,  ..., 2115.9744, 2814.0813,\n",
      "         2867.8032],\n",
      "        [ 474.0726, 2653.6750, 2060.0515,  ..., 2417.2537, 3005.2847,\n",
      "         1535.7290],\n",
      "        ...,\n",
      "        [2483.6990, 2179.0090, 2517.5652,  ..., 2473.1633, 2448.5896,\n",
      "         2727.1040],\n",
      "        [ 804.8188, 3226.1526, 2408.9346,  ..., 2949.7366, 3614.6921,\n",
      "         1722.5566],\n",
      "        [2727.1853, 2188.2979, 2188.4761,  ..., 2526.1165, 2707.8552,\n",
      "         2980.0571]], device='cuda:0')\n",
      "shape of T_var is  torch.Size([64, 64])\n",
      "T_var before correction  tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0156, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        ...,\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],\n",
      "       device='cuda:0')\n",
      "marginals are  tensor([[63.9996, 63.9996, 63.9996,  ..., 63.9996, 63.9996, 63.9996],\n",
      "        [63.9996, 63.9996, 63.9996,  ..., 63.9996, 63.9996, 63.9996],\n",
      "        [63.9996, 63.9996, 63.9996,  ..., 63.9996, 63.9996, 63.9996],\n",
      "        ...,\n",
      "        [63.9996, 63.9996, 63.9996,  ..., 63.9996, 63.9996, 63.9996],\n",
      "        [63.9996, 63.9996, 63.9996,  ..., 63.9996, 63.9996, 63.9996],\n",
      "        [63.9996, 63.9996, 63.9996,  ..., 63.9996, 63.9996, 63.9996]],\n",
      "       device='cuda:0')\n",
      "T_var after correction  tensor([[0.0000, 0.0000, 0.0000,  ..., 1.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        ...,\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],\n",
      "       device='cuda:0')\n",
      "T_var stats: max 0.9999935626983643, min 0.0, mean 0.015624899417161942, std 0.12403393536806107 \n",
      "Ratio of trace to the matrix sum:  tensor(0.0156, device='cuda:0')\n",
      "Here, trace is 0.9999935626983643 and matrix sum is 63.99958801269531 \n",
      "Shape of aligned wt is  torch.Size([64, 64, 9])\n",
      "Shape of fc_layer0_weight_data is  torch.Size([64, 64, 9])\n",
      "model parameters are \n",
      " [torch.Size([64, 3, 3, 3]), torch.Size([64, 64, 3, 3]), torch.Size([64, 64, 3, 3]), torch.Size([64, 64, 3, 3]), torch.Size([64, 64, 3, 3]), torch.Size([128, 64, 3, 3]), torch.Size([128, 128, 3, 3]), torch.Size([128, 64, 1, 1]), torch.Size([128, 128, 3, 3]), torch.Size([128, 128, 3, 3]), torch.Size([256, 128, 3, 3]), torch.Size([256, 256, 3, 3]), torch.Size([256, 128, 1, 1]), torch.Size([256, 256, 3, 3]), torch.Size([256, 256, 3, 3]), torch.Size([512, 256, 3, 3]), torch.Size([512, 512, 3, 3]), torch.Size([512, 256, 1, 1]), torch.Size([512, 512, 3, 3]), torch.Size([512, 512, 3, 3]), torch.Size([10, 512])]\n",
      "len of model_state_dict is  21\n",
      "len of new_params is  2\n",
      "updated parameters for layer  conv1.weight\n",
      "updated parameters for layer  layer1.0.conv1.weight\n",
      "\n",
      "--------- Testing in global mode ---------\n",
      "size of test_loader dataset:  10000\n",
      "\n",
      "Test set: Avg. loss: 0.0023, Accuracy: 1548/10000 (15%)\n",
      "\n",
      "accuracy after update is  15.48\n",
      "For layer idx 1, accuracy of the updated model is 15.48\n",
      "\n",
      "--------------- At layer index 2 ------------- \n",
      " \n",
      "Previous layer shape is  torch.Size([64, 64, 3, 3])\n",
      "let's see the difference in layer names layer1.0.conv2 layer1.0.conv2\n",
      "torch.Size([400, 1, 64, 32, 32]) shape of activations generally\n",
      "reorder_dim is  [1, 2, 3, 0]\n",
      "In layer layer1.0.conv2.weight: getting activation distance statistics\n",
      "Statistics of the distance from neurons of layer 1 (averaged across nodes of layer 0): \n",
      "\n",
      "Max : 23.676546096801758, Mean : 7.7395501136779785, Min : 1.7403491735458374, Std: 3.2677221298217773\n",
      "shape of layer: model 0 torch.Size([64, 64, 9])\n",
      "shape of layer: model 1 torch.Size([64, 64, 9])\n",
      "shape of activations: model 0 torch.Size([64, 32, 32, 400])\n",
      "shape of activations: model 1 torch.Size([64, 32, 32, 400])\n",
      "shape of previous transport map torch.Size([64, 64])\n",
      "doing nothing for skips\n",
      "returns a uniform measure of cardinality:  64\n",
      "returns a uniform measure of cardinality:  64\n",
      "Refactored ground metric calc\n",
      "inside refactored\n",
      "Processing the coordinates to form ground_metric\n",
      "dont leave off the squaring of the ground metric\n",
      "# of ground metric features is  409600\n",
      "# of ground metric features in 0 is   409600\n",
      "# of ground metric features in 1 is   409600\n",
      "ground metric (m0) is  tensor([[1108.2616,  834.4162,  824.2452,  ...,  781.8707,  846.3913,\n",
      "         1171.2523],\n",
      "        [ 895.9232,  780.4227,  800.2548,  ...,  776.8232,  844.7159,\n",
      "         1093.5690],\n",
      "        [1125.4194,  945.8459,  592.7088,  ...,  870.3105,  950.8710,\n",
      "         1121.1326],\n",
      "        ...,\n",
      "        [ 668.3545,  810.0167,  891.3011,  ...,  898.4471,  758.2879,\n",
      "          676.9951],\n",
      "        [1196.1418, 1096.1798, 1028.7982,  ...,  967.1706, 1068.0906,\n",
      "         1087.0918],\n",
      "        [1043.0776,  781.1746,  618.2307,  ...,  651.3107,  952.9897,\n",
      "         1173.5334]], device='cuda:0')\n",
      "shape of T_var is  torch.Size([64, 64])\n",
      "T_var before correction  tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')\n",
      "marginals are  tensor([[63.9996, 63.9996, 63.9996,  ..., 63.9996, 63.9996, 63.9996],\n",
      "        [63.9996, 63.9996, 63.9996,  ..., 63.9996, 63.9996, 63.9996],\n",
      "        [63.9996, 63.9996, 63.9996,  ..., 63.9996, 63.9996, 63.9996],\n",
      "        ...,\n",
      "        [63.9996, 63.9996, 63.9996,  ..., 63.9996, 63.9996, 63.9996],\n",
      "        [63.9996, 63.9996, 63.9996,  ..., 63.9996, 63.9996, 63.9996],\n",
      "        [63.9996, 63.9996, 63.9996,  ..., 63.9996, 63.9996, 63.9996]],\n",
      "       device='cuda:0')\n",
      "T_var after correction  tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')\n",
      "T_var stats: max 0.9999935626983643, min 0.0, mean 0.015624899417161942, std 0.12403393536806107 \n",
      "Ratio of trace to the matrix sum:  tensor(0.0312, device='cuda:0')\n",
      "Here, trace is 1.9999871253967285 and matrix sum is 63.99958801269531 \n",
      "Shape of aligned wt is  torch.Size([64, 64, 9])\n",
      "Shape of fc_layer0_weight_data is  torch.Size([64, 64, 9])\n",
      "model parameters are \n",
      " [torch.Size([64, 3, 3, 3]), torch.Size([64, 64, 3, 3]), torch.Size([64, 64, 3, 3]), torch.Size([64, 64, 3, 3]), torch.Size([64, 64, 3, 3]), torch.Size([128, 64, 3, 3]), torch.Size([128, 128, 3, 3]), torch.Size([128, 64, 1, 1]), torch.Size([128, 128, 3, 3]), torch.Size([128, 128, 3, 3]), torch.Size([256, 128, 3, 3]), torch.Size([256, 256, 3, 3]), torch.Size([256, 128, 1, 1]), torch.Size([256, 256, 3, 3]), torch.Size([256, 256, 3, 3]), torch.Size([512, 256, 3, 3]), torch.Size([512, 512, 3, 3]), torch.Size([512, 256, 1, 1]), torch.Size([512, 512, 3, 3]), torch.Size([512, 512, 3, 3]), torch.Size([10, 512])]\n",
      "len of model_state_dict is  21\n",
      "len of new_params is  3\n",
      "updated parameters for layer  conv1.weight\n",
      "updated parameters for layer  layer1.0.conv1.weight\n",
      "updated parameters for layer  layer1.0.conv2.weight\n",
      "\n",
      "--------- Testing in global mode ---------\n",
      "size of test_loader dataset:  10000\n",
      "\n",
      "Test set: Avg. loss: 0.0024, Accuracy: 1330/10000 (13%)\n",
      "\n",
      "accuracy after update is  13.3\n",
      "For layer idx 2, accuracy of the updated model is 13.3\n",
      "\n",
      "--------------- At layer index 3 ------------- \n",
      " \n",
      "Previous layer shape is  torch.Size([64, 64, 3, 3])\n",
      "let's see the difference in layer names layer1.1.conv1 layer1.1.conv1\n",
      "torch.Size([400, 1, 64, 32, 32]) shape of activations generally\n",
      "reorder_dim is  [1, 2, 3, 0]\n",
      "In layer layer1.1.conv1.weight: getting activation distance statistics\n",
      "Statistics of the distance from neurons of layer 1 (averaged across nodes of layer 0): \n",
      "\n",
      "Max : 42.12384033203125, Mean : 16.265609741210938, Min : 4.490899085998535, Std: 5.880727767944336\n",
      "shape of layer: model 0 torch.Size([64, 64, 9])\n",
      "shape of layer: model 1 torch.Size([64, 64, 9])\n",
      "shape of activations: model 0 torch.Size([64, 32, 32, 400])\n",
      "shape of activations: model 1 torch.Size([64, 32, 32, 400])\n",
      "shape of previous transport map torch.Size([64, 64])\n",
      "doing nothing for skips\n",
      "returns a uniform measure of cardinality:  64\n",
      "returns a uniform measure of cardinality:  64\n",
      "Refactored ground metric calc\n",
      "inside refactored\n",
      "Processing the coordinates to form ground_metric\n",
      "dont leave off the squaring of the ground metric\n",
      "# of ground metric features is  409600\n",
      "# of ground metric features in 0 is   409600\n",
      "# of ground metric features in 1 is   409600\n",
      "ground metric (m0) is  tensor([[1501.3851, 1971.2820, 1398.2450,  ..., 1734.0000, 2059.3062,\n",
      "         1883.1416],\n",
      "        [2152.8523, 2598.2065, 1873.3074,  ..., 2595.9932, 1978.6632,\n",
      "         1555.9241],\n",
      "        [2689.5835, 3257.6277, 2028.0374,  ..., 2626.2922, 1973.4865,\n",
      "         1930.4736],\n",
      "        ...,\n",
      "        [1693.6190, 2144.1179, 1474.0256,  ..., 2240.9810, 2001.5625,\n",
      "         1832.8422],\n",
      "        [1524.5302, 2215.2957, 1151.1871,  ..., 1827.0419, 1884.5089,\n",
      "         1975.4673],\n",
      "        [2039.5710, 2643.1072, 1761.2001,  ..., 2310.4709, 1122.1516,\n",
      "         2102.0132]], device='cuda:0')\n",
      "shape of T_var is  torch.Size([64, 64])\n",
      "T_var before correction  tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        ...,\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0156, 0.0000]],\n",
      "       device='cuda:0')\n",
      "marginals are  tensor([[63.9996, 63.9996, 63.9996,  ..., 63.9996, 63.9996, 63.9996],\n",
      "        [63.9996, 63.9996, 63.9996,  ..., 63.9996, 63.9996, 63.9996],\n",
      "        [63.9996, 63.9996, 63.9996,  ..., 63.9996, 63.9996, 63.9996],\n",
      "        ...,\n",
      "        [63.9996, 63.9996, 63.9996,  ..., 63.9996, 63.9996, 63.9996],\n",
      "        [63.9996, 63.9996, 63.9996,  ..., 63.9996, 63.9996, 63.9996],\n",
      "        [63.9996, 63.9996, 63.9996,  ..., 63.9996, 63.9996, 63.9996]],\n",
      "       device='cuda:0')\n",
      "T_var after correction  tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        ...,\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 1.0000, 0.0000]],\n",
      "       device='cuda:0')\n",
      "T_var stats: max 0.9999935626983643, min 0.0, mean 0.015624899417161942, std 0.12403393536806107 \n",
      "Ratio of trace to the matrix sum:  tensor(0.0156, device='cuda:0')\n",
      "Here, trace is 0.9999935626983643 and matrix sum is 63.99958801269531 \n",
      "Shape of aligned wt is  torch.Size([64, 64, 9])\n",
      "Shape of fc_layer0_weight_data is  torch.Size([64, 64, 9])\n",
      "model parameters are \n",
      " [torch.Size([64, 3, 3, 3]), torch.Size([64, 64, 3, 3]), torch.Size([64, 64, 3, 3]), torch.Size([64, 64, 3, 3]), torch.Size([64, 64, 3, 3]), torch.Size([128, 64, 3, 3]), torch.Size([128, 128, 3, 3]), torch.Size([128, 64, 1, 1]), torch.Size([128, 128, 3, 3]), torch.Size([128, 128, 3, 3]), torch.Size([256, 128, 3, 3]), torch.Size([256, 256, 3, 3]), torch.Size([256, 128, 1, 1]), torch.Size([256, 256, 3, 3]), torch.Size([256, 256, 3, 3]), torch.Size([512, 256, 3, 3]), torch.Size([512, 512, 3, 3]), torch.Size([512, 256, 1, 1]), torch.Size([512, 512, 3, 3]), torch.Size([512, 512, 3, 3]), torch.Size([10, 512])]\n",
      "len of model_state_dict is  21\n",
      "len of new_params is  4\n",
      "updated parameters for layer  conv1.weight\n",
      "updated parameters for layer  layer1.0.conv1.weight\n",
      "updated parameters for layer  layer1.0.conv2.weight\n",
      "updated parameters for layer  layer1.1.conv1.weight\n",
      "\n",
      "--------- Testing in global mode ---------\n",
      "size of test_loader dataset:  10000\n",
      "\n",
      "Test set: Avg. loss: 0.0024, Accuracy: 1416/10000 (14%)\n",
      "\n",
      "accuracy after update is  14.16\n",
      "For layer idx 3, accuracy of the updated model is 14.16\n",
      "\n",
      "--------------- At layer index 4 ------------- \n",
      " \n",
      "Previous layer shape is  torch.Size([64, 64, 3, 3])\n",
      "let's see the difference in layer names layer1.1.conv2 layer1.1.conv2\n",
      "torch.Size([400, 1, 64, 32, 32]) shape of activations generally\n",
      "reorder_dim is  [1, 2, 3, 0]\n",
      "In layer layer1.1.conv2.weight: getting activation distance statistics\n",
      "Statistics of the distance from neurons of layer 1 (averaged across nodes of layer 0): \n",
      "\n",
      "Max : 22.886850357055664, Mean : 7.329019069671631, Min : 1.6463813781738281, Std: 3.2565555572509766\n",
      "shape of layer: model 0 torch.Size([64, 64, 9])\n",
      "shape of layer: model 1 torch.Size([64, 64, 9])\n",
      "shape of activations: model 0 torch.Size([64, 32, 32, 400])\n",
      "shape of activations: model 1 torch.Size([64, 32, 32, 400])\n",
      "shape of previous transport map torch.Size([64, 64])\n",
      "doing nothing for skips\n",
      "returns a uniform measure of cardinality:  64\n",
      "returns a uniform measure of cardinality:  64\n",
      "Refactored ground metric calc\n",
      "inside refactored\n",
      "Processing the coordinates to form ground_metric\n",
      "dont leave off the squaring of the ground metric\n",
      "# of ground metric features is  409600\n",
      "# of ground metric features in 0 is   409600\n",
      "# of ground metric features in 1 is   409600\n",
      "ground metric (m0) is  tensor([[1010.6139,  701.9634,  690.4545,  ...,  621.8702,  742.7314,\n",
      "         1195.5529],\n",
      "        [ 938.8429,  751.3388,  728.1429,  ...,  718.1390,  822.2685,\n",
      "         1172.9178],\n",
      "        [1059.5896,  886.0253,  544.9119,  ...,  735.1557,  865.0385,\n",
      "         1175.6145],\n",
      "        ...,\n",
      "        [ 675.5916,  804.7971,  793.3608,  ...,  763.3981,  775.1302,\n",
      "          834.7479],\n",
      "        [1095.3995,  952.0324,  917.2091,  ...,  841.7198,  974.1612,\n",
      "         1160.1299],\n",
      "        [ 900.5968,  677.4550,  619.7835,  ...,  671.6517,  735.7722,\n",
      "         1034.2971]], device='cuda:0')\n",
      "shape of T_var is  torch.Size([64, 64])\n",
      "T_var before correction  tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')\n",
      "marginals are  tensor([[63.9996, 63.9996, 63.9996,  ..., 63.9996, 63.9996, 63.9996],\n",
      "        [63.9996, 63.9996, 63.9996,  ..., 63.9996, 63.9996, 63.9996],\n",
      "        [63.9996, 63.9996, 63.9996,  ..., 63.9996, 63.9996, 63.9996],\n",
      "        ...,\n",
      "        [63.9996, 63.9996, 63.9996,  ..., 63.9996, 63.9996, 63.9996],\n",
      "        [63.9996, 63.9996, 63.9996,  ..., 63.9996, 63.9996, 63.9996],\n",
      "        [63.9996, 63.9996, 63.9996,  ..., 63.9996, 63.9996, 63.9996]],\n",
      "       device='cuda:0')\n",
      "T_var after correction  tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')\n",
      "T_var stats: max 0.9999935626983643, min 0.0, mean 0.015624899417161942, std 0.12403393536806107 \n",
      "Ratio of trace to the matrix sum:  tensor(0.0469, device='cuda:0')\n",
      "Here, trace is 2.9999806880950928 and matrix sum is 63.99958801269531 \n",
      "Shape of aligned wt is  torch.Size([64, 64, 9])\n",
      "Shape of fc_layer0_weight_data is  torch.Size([64, 64, 9])\n",
      "model parameters are \n",
      " [torch.Size([64, 3, 3, 3]), torch.Size([64, 64, 3, 3]), torch.Size([64, 64, 3, 3]), torch.Size([64, 64, 3, 3]), torch.Size([64, 64, 3, 3]), torch.Size([128, 64, 3, 3]), torch.Size([128, 128, 3, 3]), torch.Size([128, 64, 1, 1]), torch.Size([128, 128, 3, 3]), torch.Size([128, 128, 3, 3]), torch.Size([256, 128, 3, 3]), torch.Size([256, 256, 3, 3]), torch.Size([256, 128, 1, 1]), torch.Size([256, 256, 3, 3]), torch.Size([256, 256, 3, 3]), torch.Size([512, 256, 3, 3]), torch.Size([512, 512, 3, 3]), torch.Size([512, 256, 1, 1]), torch.Size([512, 512, 3, 3]), torch.Size([512, 512, 3, 3]), torch.Size([10, 512])]\n",
      "len of model_state_dict is  21\n",
      "len of new_params is  5\n",
      "updated parameters for layer  conv1.weight\n",
      "updated parameters for layer  layer1.0.conv1.weight\n",
      "updated parameters for layer  layer1.0.conv2.weight\n",
      "updated parameters for layer  layer1.1.conv1.weight\n",
      "updated parameters for layer  layer1.1.conv2.weight\n",
      "\n",
      "--------- Testing in global mode ---------\n",
      "size of test_loader dataset:  10000\n",
      "\n",
      "Test set: Avg. loss: 0.0024, Accuracy: 1296/10000 (13%)\n",
      "\n",
      "accuracy after update is  12.96\n",
      "For layer idx 4, accuracy of the updated model is 12.96\n",
      "\n",
      "--------------- At layer index 5 ------------- \n",
      " \n",
      "Previous layer shape is  torch.Size([64, 64, 3, 3])\n",
      "let's see the difference in layer names layer2.0.conv1 layer2.0.conv1\n",
      "torch.Size([400, 1, 128, 16, 16]) shape of activations generally\n",
      "reorder_dim is  [1, 2, 3, 0]\n",
      "In layer layer2.0.conv1.weight: getting activation distance statistics\n",
      "Statistics of the distance from neurons of layer 1 (averaged across nodes of layer 0): \n",
      "\n",
      "Max : 41.045616149902344, Mean : 13.9276123046875, Min : 3.2573728561401367, Std: 5.771397590637207\n",
      "shape of layer: model 0 torch.Size([128, 64, 9])\n",
      "shape of layer: model 1 torch.Size([128, 64, 9])\n",
      "shape of activations: model 0 torch.Size([128, 16, 16, 400])\n",
      "shape of activations: model 1 torch.Size([128, 16, 16, 400])\n",
      "shape of previous transport map torch.Size([64, 64])\n",
      "saved skip T_var at layer 5 with shape torch.Size([128, 64, 3, 3])\n",
      "shape of previous transport map now is torch.Size([64, 64])\n",
      "returns a uniform measure of cardinality:  128\n",
      "returns a uniform measure of cardinality:  128\n",
      "Refactored ground metric calc\n",
      "inside refactored\n",
      "Processing the coordinates to form ground_metric\n",
      "dont leave off the squaring of the ground metric\n",
      "# of ground metric features is  102400\n",
      "# of ground metric features in 0 is   102400\n",
      "# of ground metric features in 1 is   102400\n",
      "ground metric (m0) is  tensor([[1539.1848, 1409.8213, 1090.9116,  ..., 1056.4088, 1100.4639,\n",
      "         1186.4526],\n",
      "        [1269.8988, 1243.9910, 1008.7574,  ..., 1071.6194, 1001.8035,\n",
      "         1062.2991],\n",
      "        [1354.3138, 1305.7042, 1115.4666,  ..., 1341.5242, 1293.7098,\n",
      "         1223.9669],\n",
      "        ...,\n",
      "        [1266.3922, 1153.1797,  975.1634,  ...,  890.2763,  704.7229,\n",
      "          903.9075],\n",
      "        [1688.8588, 1316.4324,  949.5436,  ..., 1052.5319, 1279.5809,\n",
      "         1333.2335],\n",
      "        [1618.8230, 1361.1147, 1037.3171,  ..., 1277.4004, 1467.0978,\n",
      "         1324.5092]], device='cuda:0')\n",
      "shape of T_var is  torch.Size([128, 128])\n",
      "T_var before correction  tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')\n",
      "marginals are  tensor([[127.9984, 127.9984, 127.9984,  ..., 127.9984, 127.9984, 127.9984],\n",
      "        [127.9984, 127.9984, 127.9984,  ..., 127.9984, 127.9984, 127.9984],\n",
      "        [127.9984, 127.9984, 127.9984,  ..., 127.9984, 127.9984, 127.9984],\n",
      "        ...,\n",
      "        [127.9984, 127.9984, 127.9984,  ..., 127.9984, 127.9984, 127.9984],\n",
      "        [127.9984, 127.9984, 127.9984,  ..., 127.9984, 127.9984, 127.9984],\n",
      "        [127.9984, 127.9984, 127.9984,  ..., 127.9984, 127.9984, 127.9984]],\n",
      "       device='cuda:0')\n",
      "T_var after correction  tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')\n",
      "T_var stats: max 0.9999872446060181, min 0.0, mean 0.007812400348484516, std 0.08804396539926529 \n",
      "Ratio of trace to the matrix sum:  tensor(0.0156, device='cuda:0')\n",
      "Here, trace is 1.9999744892120361 and matrix sum is 127.99836730957031 \n",
      "Shape of aligned wt is  torch.Size([128, 64, 9])\n",
      "Shape of fc_layer0_weight_data is  torch.Size([128, 64, 9])\n",
      "model parameters are \n",
      " [torch.Size([64, 3, 3, 3]), torch.Size([64, 64, 3, 3]), torch.Size([64, 64, 3, 3]), torch.Size([64, 64, 3, 3]), torch.Size([64, 64, 3, 3]), torch.Size([128, 64, 3, 3]), torch.Size([128, 128, 3, 3]), torch.Size([128, 64, 1, 1]), torch.Size([128, 128, 3, 3]), torch.Size([128, 128, 3, 3]), torch.Size([256, 128, 3, 3]), torch.Size([256, 256, 3, 3]), torch.Size([256, 128, 1, 1]), torch.Size([256, 256, 3, 3]), torch.Size([256, 256, 3, 3]), torch.Size([512, 256, 3, 3]), torch.Size([512, 512, 3, 3]), torch.Size([512, 256, 1, 1]), torch.Size([512, 512, 3, 3]), torch.Size([512, 512, 3, 3]), torch.Size([10, 512])]\n",
      "len of model_state_dict is  21\n",
      "len of new_params is  6\n",
      "updated parameters for layer  conv1.weight\n",
      "updated parameters for layer  layer1.0.conv1.weight\n",
      "updated parameters for layer  layer1.0.conv2.weight\n",
      "updated parameters for layer  layer1.1.conv1.weight\n",
      "updated parameters for layer  layer1.1.conv2.weight\n",
      "updated parameters for layer  layer2.0.conv1.weight\n",
      "\n",
      "--------- Testing in global mode ---------\n",
      "size of test_loader dataset:  10000\n",
      "\n",
      "Test set: Avg. loss: 0.0023, Accuracy: 1365/10000 (14%)\n",
      "\n",
      "accuracy after update is  13.65\n",
      "For layer idx 5, accuracy of the updated model is 13.65\n",
      "\n",
      "--------------- At layer index 6 ------------- \n",
      " \n",
      "Previous layer shape is  torch.Size([128, 64, 3, 3])\n",
      "let's see the difference in layer names layer2.0.conv2 layer2.0.conv2\n",
      "torch.Size([400, 1, 128, 16, 16]) shape of activations generally\n",
      "reorder_dim is  [1, 2, 3, 0]\n",
      "In layer layer2.0.conv2.weight: getting activation distance statistics\n",
      "Statistics of the distance from neurons of layer 1 (averaged across nodes of layer 0): \n",
      "\n",
      "Max : 76.16622924804688, Mean : 26.517139434814453, Min : 6.516888618469238, Std: 10.528343200683594\n",
      "shape of layer: model 0 torch.Size([128, 128, 9])\n",
      "shape of layer: model 1 torch.Size([128, 128, 9])\n",
      "shape of activations: model 0 torch.Size([128, 16, 16, 400])\n",
      "shape of activations: model 1 torch.Size([128, 16, 16, 400])\n",
      "shape of previous transport map torch.Size([128, 128])\n",
      "doing nothing for skips\n",
      "returns a uniform measure of cardinality:  128\n",
      "returns a uniform measure of cardinality:  128\n",
      "Refactored ground metric calc\n",
      "inside refactored\n",
      "Processing the coordinates to form ground_metric\n",
      "dont leave off the squaring of the ground metric\n",
      "# of ground metric features is  102400\n",
      "# of ground metric features in 0 is   102400\n",
      "# of ground metric features in 1 is   102400\n",
      "ground metric (m0) is  tensor([[2689.8306, 2092.4341, 2441.6599,  ..., 2413.5852, 1307.2345,\n",
      "         2875.5867],\n",
      "        [2859.2913, 1744.5885, 2711.6370,  ..., 2884.4868, 2335.9438,\n",
      "         2113.7212],\n",
      "        [2688.1228, 2872.1653, 2518.2668,  ..., 2411.7561, 2249.0933,\n",
      "         2927.0706],\n",
      "        ...,\n",
      "        [2548.4531, 1952.5598, 2617.2249,  ..., 2670.9810, 2337.5862,\n",
      "         2332.2051],\n",
      "        [2088.1519, 1989.1541, 2208.3210,  ..., 2325.0930, 2166.6453,\n",
      "         2157.5718],\n",
      "        [2951.2598, 2293.9011, 2720.8291,  ..., 2533.0476, 2443.5750,\n",
      "         2698.4280]], device='cuda:0')\n",
      "shape of T_var is  torch.Size([128, 128])\n",
      "T_var before correction  tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0078, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        ...,\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],\n",
      "       device='cuda:0')\n",
      "marginals are  tensor([[127.9984, 127.9984, 127.9984,  ..., 127.9984, 127.9984, 127.9984],\n",
      "        [127.9984, 127.9984, 127.9984,  ..., 127.9984, 127.9984, 127.9984],\n",
      "        [127.9984, 127.9984, 127.9984,  ..., 127.9984, 127.9984, 127.9984],\n",
      "        ...,\n",
      "        [127.9984, 127.9984, 127.9984,  ..., 127.9984, 127.9984, 127.9984],\n",
      "        [127.9984, 127.9984, 127.9984,  ..., 127.9984, 127.9984, 127.9984],\n",
      "        [127.9984, 127.9984, 127.9984,  ..., 127.9984, 127.9984, 127.9984]],\n",
      "       device='cuda:0')\n",
      "T_var after correction  tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 1.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        ...,\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],\n",
      "       device='cuda:0')\n",
      "T_var stats: max 0.9999872446060181, min 0.0, mean 0.007812400348484516, std 0.08804396539926529 \n",
      "Ratio of trace to the matrix sum:  tensor(0.0234, device='cuda:0')\n",
      "Here, trace is 2.9999618530273438 and matrix sum is 127.99836730957031 \n",
      "Shape of aligned wt is  torch.Size([128, 128, 9])\n",
      "Shape of fc_layer0_weight_data is  torch.Size([128, 128, 9])\n",
      "model parameters are \n",
      " [torch.Size([64, 3, 3, 3]), torch.Size([64, 64, 3, 3]), torch.Size([64, 64, 3, 3]), torch.Size([64, 64, 3, 3]), torch.Size([64, 64, 3, 3]), torch.Size([128, 64, 3, 3]), torch.Size([128, 128, 3, 3]), torch.Size([128, 64, 1, 1]), torch.Size([128, 128, 3, 3]), torch.Size([128, 128, 3, 3]), torch.Size([256, 128, 3, 3]), torch.Size([256, 256, 3, 3]), torch.Size([256, 128, 1, 1]), torch.Size([256, 256, 3, 3]), torch.Size([256, 256, 3, 3]), torch.Size([512, 256, 3, 3]), torch.Size([512, 512, 3, 3]), torch.Size([512, 256, 1, 1]), torch.Size([512, 512, 3, 3]), torch.Size([512, 512, 3, 3]), torch.Size([10, 512])]\n",
      "len of model_state_dict is  21\n",
      "len of new_params is  7\n",
      "updated parameters for layer  conv1.weight\n",
      "updated parameters for layer  layer1.0.conv1.weight\n",
      "updated parameters for layer  layer1.0.conv2.weight\n",
      "updated parameters for layer  layer1.1.conv1.weight\n",
      "updated parameters for layer  layer1.1.conv2.weight\n",
      "updated parameters for layer  layer2.0.conv1.weight\n",
      "updated parameters for layer  layer2.0.conv2.weight\n",
      "\n",
      "--------- Testing in global mode ---------\n",
      "size of test_loader dataset:  10000\n",
      "\n",
      "Test set: Avg. loss: 0.0025, Accuracy: 1096/10000 (11%)\n",
      "\n",
      "accuracy after update is  10.96\n",
      "For layer idx 6, accuracy of the updated model is 10.96\n",
      "\n",
      "--------------- At layer index 7 ------------- \n",
      " \n",
      "Previous layer shape is  torch.Size([128, 128, 3, 3])\n",
      "let's see the difference in layer names layer2.0.shortcut.0 layer2.0.shortcut.0\n",
      "torch.Size([400, 1, 128, 16, 16]) shape of activations generally\n",
      "reorder_dim is  [1, 2, 3, 0]\n",
      "In layer layer2.0.shortcut.0.weight: getting activation distance statistics\n",
      "Statistics of the distance from neurons of layer 1 (averaged across nodes of layer 0): \n",
      "\n",
      "Max : 11.020503044128418, Mean : 3.600231885910034, Min : 0.8292902708053589, Std: 1.5321660041809082\n",
      "shape of layer: model 0 torch.Size([128, 64, 1])\n",
      "shape of layer: model 1 torch.Size([128, 64, 1])\n",
      "shape of activations: model 0 torch.Size([128, 16, 16, 400])\n",
      "shape of activations: model 1 torch.Size([128, 16, 16, 400])\n",
      "shape of previous transport map torch.Size([128, 128])\n",
      "utilizing skip T_var saved from layer layer 5 with shape torch.Size([64, 64])\n",
      "shape of previous transport map now is torch.Size([64, 64])\n",
      "returns a uniform measure of cardinality:  128\n",
      "returns a uniform measure of cardinality:  128\n",
      "Refactored ground metric calc\n",
      "inside refactored\n",
      "Processing the coordinates to form ground_metric\n",
      "dont leave off the squaring of the ground metric\n",
      "# of ground metric features is  102400\n",
      "# of ground metric features in 0 is   102400\n",
      "# of ground metric features in 1 is   102400\n",
      "ground metric (m0) is  tensor([[305.0612, 276.8852, 404.2863,  ..., 375.2061, 175.6771, 541.3282],\n",
      "        [385.7270, 197.8650, 394.9281,  ..., 567.3016, 367.7976, 397.6922],\n",
      "        [346.8623, 370.5832, 448.2150,  ..., 247.1132, 173.7475, 623.1581],\n",
      "        ...,\n",
      "        [341.9699, 231.8637, 423.7338,  ..., 481.9994, 313.7203, 457.5811],\n",
      "        [202.5097, 224.7916, 294.4124,  ..., 434.6928, 313.4581, 268.9748],\n",
      "        [288.2688, 251.0616, 371.7166,  ..., 316.4862, 236.8714, 485.1107]],\n",
      "       device='cuda:0')\n",
      "shape of T_var is  torch.Size([128, 128])\n",
      "T_var before correction  tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0078, 0.0000],\n",
      "        ...,\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],\n",
      "       device='cuda:0')\n",
      "marginals are  tensor([[127.9984, 127.9984, 127.9984,  ..., 127.9984, 127.9984, 127.9984],\n",
      "        [127.9984, 127.9984, 127.9984,  ..., 127.9984, 127.9984, 127.9984],\n",
      "        [127.9984, 127.9984, 127.9984,  ..., 127.9984, 127.9984, 127.9984],\n",
      "        ...,\n",
      "        [127.9984, 127.9984, 127.9984,  ..., 127.9984, 127.9984, 127.9984],\n",
      "        [127.9984, 127.9984, 127.9984,  ..., 127.9984, 127.9984, 127.9984],\n",
      "        [127.9984, 127.9984, 127.9984,  ..., 127.9984, 127.9984, 127.9984]],\n",
      "       device='cuda:0')\n",
      "T_var after correction  tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 1.0000, 0.0000],\n",
      "        ...,\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],\n",
      "       device='cuda:0')\n",
      "T_var stats: max 0.9999872446060181, min 0.0, mean 0.007812400348484516, std 0.08804396539926529 \n",
      "Ratio of trace to the matrix sum:  tensor(0.0234, device='cuda:0')\n",
      "Here, trace is 2.9999618530273438 and matrix sum is 127.99836730957031 \n",
      "Shape of aligned wt is  torch.Size([128, 64, 1])\n",
      "Shape of fc_layer0_weight_data is  torch.Size([128, 64, 1])\n",
      "model parameters are \n",
      " [torch.Size([64, 3, 3, 3]), torch.Size([64, 64, 3, 3]), torch.Size([64, 64, 3, 3]), torch.Size([64, 64, 3, 3]), torch.Size([64, 64, 3, 3]), torch.Size([128, 64, 3, 3]), torch.Size([128, 128, 3, 3]), torch.Size([128, 64, 1, 1]), torch.Size([128, 128, 3, 3]), torch.Size([128, 128, 3, 3]), torch.Size([256, 128, 3, 3]), torch.Size([256, 256, 3, 3]), torch.Size([256, 128, 1, 1]), torch.Size([256, 256, 3, 3]), torch.Size([256, 256, 3, 3]), torch.Size([512, 256, 3, 3]), torch.Size([512, 512, 3, 3]), torch.Size([512, 256, 1, 1]), torch.Size([512, 512, 3, 3]), torch.Size([512, 512, 3, 3]), torch.Size([10, 512])]\n",
      "len of model_state_dict is  21\n",
      "len of new_params is  8\n",
      "updated parameters for layer  conv1.weight\n",
      "updated parameters for layer  layer1.0.conv1.weight\n",
      "updated parameters for layer  layer1.0.conv2.weight\n",
      "updated parameters for layer  layer1.1.conv1.weight\n",
      "updated parameters for layer  layer1.1.conv2.weight\n",
      "updated parameters for layer  layer2.0.conv1.weight\n",
      "updated parameters for layer  layer2.0.conv2.weight\n",
      "updated parameters for layer  layer2.0.shortcut.0.weight\n",
      "\n",
      "--------- Testing in global mode ---------\n",
      "size of test_loader dataset:  10000\n",
      "\n",
      "Test set: Avg. loss: 0.0024, Accuracy: 1072/10000 (11%)\n",
      "\n",
      "accuracy after update is  10.72\n",
      "For layer idx 7, accuracy of the updated model is 10.72\n",
      "\n",
      "--------------- At layer index 8 ------------- \n",
      " \n",
      "Previous layer shape is  torch.Size([128, 64, 1, 1])\n",
      "let's see the difference in layer names layer2.1.conv1 layer2.1.conv1\n",
      "torch.Size([400, 1, 128, 16, 16]) shape of activations generally\n",
      "reorder_dim is  [1, 2, 3, 0]\n",
      "In layer layer2.1.conv1.weight: getting activation distance statistics\n",
      "Statistics of the distance from neurons of layer 1 (averaged across nodes of layer 0): \n",
      "\n",
      "Max : 97.8304443359375, Mean : 33.51240158081055, Min : 8.846086502075195, Std: 13.141454696655273\n",
      "shape of layer: model 0 torch.Size([128, 128, 9])\n",
      "shape of layer: model 1 torch.Size([128, 128, 9])\n",
      "shape of activations: model 0 torch.Size([128, 16, 16, 400])\n",
      "shape of activations: model 1 torch.Size([128, 16, 16, 400])\n",
      "shape of previous transport map torch.Size([128, 128])\n",
      "averaging multiple T_var's\n",
      "returns a uniform measure of cardinality:  128\n",
      "returns a uniform measure of cardinality:  128\n",
      "Refactored ground metric calc\n",
      "inside refactored\n",
      "Processing the coordinates to form ground_metric\n",
      "dont leave off the squaring of the ground metric\n",
      "# of ground metric features is  102400\n",
      "# of ground metric features in 0 is   102400\n",
      "# of ground metric features in 1 is   102400\n",
      "ground metric (m0) is  tensor([[3409.9248, 3413.5042, 3572.5427,  ..., 2859.8467, 2836.5510,\n",
      "         3486.9871],\n",
      "        [2863.1494, 2676.8247, 3575.3210,  ..., 2637.7954, 2163.0435,\n",
      "         2927.2551],\n",
      "        [3104.0371, 2453.4155, 3224.0403,  ..., 1892.2585, 2457.4600,\n",
      "         2894.7344],\n",
      "        ...,\n",
      "        [3499.5386, 2243.3076, 3833.1206,  ..., 2462.1929, 2190.9189,\n",
      "         2886.2285],\n",
      "        [3385.7546, 3081.1875, 4012.0386,  ..., 2757.1931, 2903.9377,\n",
      "         3505.2556],\n",
      "        [2935.0203, 3232.3125, 3181.1172,  ..., 3087.0479, 2796.1643,\n",
      "         3029.7002]], device='cuda:0')\n",
      "shape of T_var is  torch.Size([128, 128])\n",
      "T_var before correction  tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')\n",
      "marginals are  tensor([[127.9984, 127.9984, 127.9984,  ..., 127.9984, 127.9984, 127.9984],\n",
      "        [127.9984, 127.9984, 127.9984,  ..., 127.9984, 127.9984, 127.9984],\n",
      "        [127.9984, 127.9984, 127.9984,  ..., 127.9984, 127.9984, 127.9984],\n",
      "        ...,\n",
      "        [127.9984, 127.9984, 127.9984,  ..., 127.9984, 127.9984, 127.9984],\n",
      "        [127.9984, 127.9984, 127.9984,  ..., 127.9984, 127.9984, 127.9984],\n",
      "        [127.9984, 127.9984, 127.9984,  ..., 127.9984, 127.9984, 127.9984]],\n",
      "       device='cuda:0')\n",
      "T_var after correction  tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')\n",
      "T_var stats: max 0.9999872446060181, min 0.0, mean 0.007812400348484516, std 0.08804396539926529 \n",
      "Ratio of trace to the matrix sum:  tensor(0.0156, device='cuda:0')\n",
      "Here, trace is 1.9999744892120361 and matrix sum is 127.99836730957031 \n",
      "Shape of aligned wt is  torch.Size([128, 128, 9])\n",
      "Shape of fc_layer0_weight_data is  torch.Size([128, 128, 9])\n",
      "model parameters are \n",
      " [torch.Size([64, 3, 3, 3]), torch.Size([64, 64, 3, 3]), torch.Size([64, 64, 3, 3]), torch.Size([64, 64, 3, 3]), torch.Size([64, 64, 3, 3]), torch.Size([128, 64, 3, 3]), torch.Size([128, 128, 3, 3]), torch.Size([128, 64, 1, 1]), torch.Size([128, 128, 3, 3]), torch.Size([128, 128, 3, 3]), torch.Size([256, 128, 3, 3]), torch.Size([256, 256, 3, 3]), torch.Size([256, 128, 1, 1]), torch.Size([256, 256, 3, 3]), torch.Size([256, 256, 3, 3]), torch.Size([512, 256, 3, 3]), torch.Size([512, 512, 3, 3]), torch.Size([512, 256, 1, 1]), torch.Size([512, 512, 3, 3]), torch.Size([512, 512, 3, 3]), torch.Size([10, 512])]\n",
      "len of model_state_dict is  21\n",
      "len of new_params is  9\n",
      "updated parameters for layer  conv1.weight\n",
      "updated parameters for layer  layer1.0.conv1.weight\n",
      "updated parameters for layer  layer1.0.conv2.weight\n",
      "updated parameters for layer  layer1.1.conv1.weight\n",
      "updated parameters for layer  layer1.1.conv2.weight\n",
      "updated parameters for layer  layer2.0.conv1.weight\n",
      "updated parameters for layer  layer2.0.conv2.weight\n",
      "updated parameters for layer  layer2.0.shortcut.0.weight\n",
      "updated parameters for layer  layer2.1.conv1.weight\n",
      "\n",
      "--------- Testing in global mode ---------\n",
      "size of test_loader dataset:  10000\n",
      "\n",
      "Test set: Avg. loss: 0.0024, Accuracy: 1087/10000 (11%)\n",
      "\n",
      "accuracy after update is  10.87\n",
      "For layer idx 8, accuracy of the updated model is 10.87\n",
      "\n",
      "--------------- At layer index 9 ------------- \n",
      " \n",
      "Previous layer shape is  torch.Size([128, 128, 3, 3])\n",
      "let's see the difference in layer names layer2.1.conv2 layer2.1.conv2\n",
      "torch.Size([400, 1, 128, 16, 16]) shape of activations generally\n",
      "reorder_dim is  [1, 2, 3, 0]\n",
      "In layer layer2.1.conv2.weight: getting activation distance statistics\n",
      "Statistics of the distance from neurons of layer 1 (averaged across nodes of layer 0): \n",
      "\n",
      "Max : 51.83403778076172, Mean : 14.631948471069336, Min : 2.461434841156006, Std: 7.396355628967285\n",
      "shape of layer: model 0 torch.Size([128, 128, 9])\n",
      "shape of layer: model 1 torch.Size([128, 128, 9])\n",
      "shape of activations: model 0 torch.Size([128, 16, 16, 400])\n",
      "shape of activations: model 1 torch.Size([128, 16, 16, 400])\n",
      "shape of previous transport map torch.Size([128, 128])\n",
      "doing nothing for skips\n",
      "returns a uniform measure of cardinality:  128\n",
      "returns a uniform measure of cardinality:  128\n",
      "Refactored ground metric calc\n",
      "inside refactored\n",
      "Processing the coordinates to form ground_metric\n",
      "dont leave off the squaring of the ground metric\n",
      "# of ground metric features is  102400\n",
      "# of ground metric features in 0 is   102400\n",
      "# of ground metric features in 1 is   102400\n",
      "ground metric (m0) is  tensor([[1470.3453, 1265.4148, 1555.4110,  ..., 1342.0127, 1056.3451,\n",
      "         1630.3616],\n",
      "        [1658.1862, 1185.7130, 1633.3276,  ..., 1492.4503, 1474.3381,\n",
      "         1385.9513],\n",
      "        [1319.4102, 1453.0148, 1311.0508,  ..., 1236.1337, 1132.9913,\n",
      "         1505.6823],\n",
      "        ...,\n",
      "        [1468.7457, 1403.6041, 1571.7750,  ..., 1450.4109, 1347.8342,\n",
      "         1532.0530],\n",
      "        [1237.3391, 1307.0703, 1361.5669,  ..., 1175.2968, 1148.0856,\n",
      "         1297.0483],\n",
      "        [1474.9006, 1479.6936, 1531.9194,  ..., 1308.3978, 1295.5806,\n",
      "         1594.6633]], device='cuda:0')\n",
      "shape of T_var is  torch.Size([128, 128])\n",
      "T_var before correction  tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')\n",
      "marginals are  tensor([[127.9984, 127.9984, 127.9984,  ..., 127.9984, 127.9984, 127.9984],\n",
      "        [127.9984, 127.9984, 127.9984,  ..., 127.9984, 127.9984, 127.9984],\n",
      "        [127.9984, 127.9984, 127.9984,  ..., 127.9984, 127.9984, 127.9984],\n",
      "        ...,\n",
      "        [127.9984, 127.9984, 127.9984,  ..., 127.9984, 127.9984, 127.9984],\n",
      "        [127.9984, 127.9984, 127.9984,  ..., 127.9984, 127.9984, 127.9984],\n",
      "        [127.9984, 127.9984, 127.9984,  ..., 127.9984, 127.9984, 127.9984]],\n",
      "       device='cuda:0')\n",
      "T_var after correction  tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')\n",
      "T_var stats: max 0.9999872446060181, min 0.0, mean 0.007812400348484516, std 0.08804396539926529 \n",
      "Ratio of trace to the matrix sum:  tensor(0.0156, device='cuda:0')\n",
      "Here, trace is 1.9999744892120361 and matrix sum is 127.99836730957031 \n",
      "Shape of aligned wt is  torch.Size([128, 128, 9])\n",
      "Shape of fc_layer0_weight_data is  torch.Size([128, 128, 9])\n",
      "model parameters are \n",
      " [torch.Size([64, 3, 3, 3]), torch.Size([64, 64, 3, 3]), torch.Size([64, 64, 3, 3]), torch.Size([64, 64, 3, 3]), torch.Size([64, 64, 3, 3]), torch.Size([128, 64, 3, 3]), torch.Size([128, 128, 3, 3]), torch.Size([128, 64, 1, 1]), torch.Size([128, 128, 3, 3]), torch.Size([128, 128, 3, 3]), torch.Size([256, 128, 3, 3]), torch.Size([256, 256, 3, 3]), torch.Size([256, 128, 1, 1]), torch.Size([256, 256, 3, 3]), torch.Size([256, 256, 3, 3]), torch.Size([512, 256, 3, 3]), torch.Size([512, 512, 3, 3]), torch.Size([512, 256, 1, 1]), torch.Size([512, 512, 3, 3]), torch.Size([512, 512, 3, 3]), torch.Size([10, 512])]\n",
      "len of model_state_dict is  21\n",
      "len of new_params is  10\n",
      "updated parameters for layer  conv1.weight\n",
      "updated parameters for layer  layer1.0.conv1.weight\n",
      "updated parameters for layer  layer1.0.conv2.weight\n",
      "updated parameters for layer  layer1.1.conv1.weight\n",
      "updated parameters for layer  layer1.1.conv2.weight\n",
      "updated parameters for layer  layer2.0.conv1.weight\n",
      "updated parameters for layer  layer2.0.conv2.weight\n",
      "updated parameters for layer  layer2.0.shortcut.0.weight\n",
      "updated parameters for layer  layer2.1.conv1.weight\n",
      "updated parameters for layer  layer2.1.conv2.weight\n",
      "\n",
      "--------- Testing in global mode ---------\n",
      "size of test_loader dataset:  10000\n",
      "\n",
      "Test set: Avg. loss: 0.0024, Accuracy: 1080/10000 (11%)\n",
      "\n",
      "accuracy after update is  10.8\n",
      "For layer idx 9, accuracy of the updated model is 10.8\n",
      "\n",
      "--------------- At layer index 10 ------------- \n",
      " \n",
      "Previous layer shape is  torch.Size([128, 128, 3, 3])\n",
      "let's see the difference in layer names layer3.0.conv1 layer3.0.conv1\n",
      "torch.Size([400, 1, 256, 8, 8]) shape of activations generally\n",
      "reorder_dim is  [1, 2, 3, 0]\n",
      "In layer layer3.0.conv1.weight: getting activation distance statistics\n",
      "Statistics of the distance from neurons of layer 1 (averaged across nodes of layer 0): \n",
      "\n",
      "Max : 74.27424621582031, Mean : 24.848613739013672, Min : 5.638044357299805, Std: 10.49411392211914\n",
      "shape of layer: model 0 torch.Size([256, 128, 9])\n",
      "shape of layer: model 1 torch.Size([256, 128, 9])\n",
      "shape of activations: model 0 torch.Size([256, 8, 8, 400])\n",
      "shape of activations: model 1 torch.Size([256, 8, 8, 400])\n",
      "shape of previous transport map torch.Size([128, 128])\n",
      "saved skip T_var at layer 10 with shape torch.Size([256, 128, 3, 3])\n",
      "shape of previous transport map now is torch.Size([128, 128])\n",
      "returns a uniform measure of cardinality:  256\n",
      "returns a uniform measure of cardinality:  256\n",
      "Refactored ground metric calc\n",
      "inside refactored\n",
      "Processing the coordinates to form ground_metric\n",
      "dont leave off the squaring of the ground metric\n",
      "# of ground metric features is  25600\n",
      "# of ground metric features in 0 is   25600\n",
      "# of ground metric features in 1 is   25600\n",
      "ground metric (m0) is  tensor([[1570.0496, 1696.0758, 1464.2113,  ..., 1625.4736, 1620.5135,\n",
      "         1569.5142],\n",
      "        [1429.0084, 1652.9474, 1272.1696,  ..., 1662.9103, 1696.2484,\n",
      "         1522.5836],\n",
      "        [1640.1317, 1356.5063, 1517.9454,  ..., 1429.0134, 1743.0629,\n",
      "         1710.0702],\n",
      "        ...,\n",
      "        [1229.5813, 1809.7738, 1646.3840,  ..., 1684.2657, 1417.6357,\n",
      "         1505.5638],\n",
      "        [1751.7072, 1560.4574, 1786.3085,  ..., 1793.3055, 1728.0748,\n",
      "         1769.3514],\n",
      "        [1776.3253, 1441.9470, 1367.0649,  ..., 1452.1100, 1862.9229,\n",
      "         1746.1027]], device='cuda:0')\n",
      "shape of T_var is  torch.Size([256, 256])\n",
      "T_var before correction  tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')\n",
      "marginals are  tensor([[255.9934, 255.9934, 255.9934,  ..., 255.9934, 255.9934, 255.9934],\n",
      "        [255.9934, 255.9934, 255.9934,  ..., 255.9934, 255.9934, 255.9934],\n",
      "        [255.9934, 255.9934, 255.9934,  ..., 255.9934, 255.9934, 255.9934],\n",
      "        ...,\n",
      "        [255.9934, 255.9934, 255.9934,  ..., 255.9934, 255.9934, 255.9934],\n",
      "        [255.9934, 255.9934, 255.9934,  ..., 255.9934, 255.9934, 255.9934],\n",
      "        [255.9934, 255.9934, 255.9934,  ..., 255.9934, 255.9934, 255.9934]],\n",
      "       device='cuda:0')\n",
      "T_var after correction  tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')\n",
      "T_var stats: max 0.9999743700027466, min 0.0, mean 0.003906149882823229, std 0.06237669289112091 \n",
      "Ratio of trace to the matrix sum:  tensor(0.0156, device='cuda:0')\n",
      "Here, trace is 3.9998974800109863 and matrix sum is 255.99343872070312 \n",
      "Shape of aligned wt is  torch.Size([256, 128, 9])\n",
      "Shape of fc_layer0_weight_data is  torch.Size([256, 128, 9])\n",
      "model parameters are \n",
      " [torch.Size([64, 3, 3, 3]), torch.Size([64, 64, 3, 3]), torch.Size([64, 64, 3, 3]), torch.Size([64, 64, 3, 3]), torch.Size([64, 64, 3, 3]), torch.Size([128, 64, 3, 3]), torch.Size([128, 128, 3, 3]), torch.Size([128, 64, 1, 1]), torch.Size([128, 128, 3, 3]), torch.Size([128, 128, 3, 3]), torch.Size([256, 128, 3, 3]), torch.Size([256, 256, 3, 3]), torch.Size([256, 128, 1, 1]), torch.Size([256, 256, 3, 3]), torch.Size([256, 256, 3, 3]), torch.Size([512, 256, 3, 3]), torch.Size([512, 512, 3, 3]), torch.Size([512, 256, 1, 1]), torch.Size([512, 512, 3, 3]), torch.Size([512, 512, 3, 3]), torch.Size([10, 512])]\n",
      "len of model_state_dict is  21\n",
      "len of new_params is  11\n",
      "updated parameters for layer  conv1.weight\n",
      "updated parameters for layer  layer1.0.conv1.weight\n",
      "updated parameters for layer  layer1.0.conv2.weight\n",
      "updated parameters for layer  layer1.1.conv1.weight\n",
      "updated parameters for layer  layer1.1.conv2.weight\n",
      "updated parameters for layer  layer2.0.conv1.weight\n",
      "updated parameters for layer  layer2.0.conv2.weight\n",
      "updated parameters for layer  layer2.0.shortcut.0.weight\n",
      "updated parameters for layer  layer2.1.conv1.weight\n",
      "updated parameters for layer  layer2.1.conv2.weight\n",
      "updated parameters for layer  layer3.0.conv1.weight\n",
      "\n",
      "--------- Testing in global mode ---------\n",
      "size of test_loader dataset:  10000\n",
      "\n",
      "Test set: Avg. loss: 0.0023, Accuracy: 1194/10000 (12%)\n",
      "\n",
      "accuracy after update is  11.94\n",
      "For layer idx 10, accuracy of the updated model is 11.94\n",
      "\n",
      "--------------- At layer index 11 ------------- \n",
      " \n",
      "Previous layer shape is  torch.Size([256, 128, 3, 3])\n",
      "let's see the difference in layer names layer3.0.conv2 layer3.0.conv2\n",
      "torch.Size([400, 1, 256, 8, 8]) shape of activations generally\n",
      "reorder_dim is  [1, 2, 3, 0]\n",
      "In layer layer3.0.conv2.weight: getting activation distance statistics\n",
      "Statistics of the distance from neurons of layer 1 (averaged across nodes of layer 0): \n",
      "\n",
      "Max : 102.69164276123047, Mean : 33.88176727294922, Min : 7.511137962341309, Std: 14.876443862915039\n",
      "shape of layer: model 0 torch.Size([256, 256, 9])\n",
      "shape of layer: model 1 torch.Size([256, 256, 9])\n",
      "shape of activations: model 0 torch.Size([256, 8, 8, 400])\n",
      "shape of activations: model 1 torch.Size([256, 8, 8, 400])\n",
      "shape of previous transport map torch.Size([256, 256])\n",
      "doing nothing for skips\n",
      "returns a uniform measure of cardinality:  256\n",
      "returns a uniform measure of cardinality:  256\n",
      "Refactored ground metric calc\n",
      "inside refactored\n",
      "Processing the coordinates to form ground_metric\n",
      "dont leave off the squaring of the ground metric\n",
      "# of ground metric features is  25600\n",
      "# of ground metric features in 0 is   25600\n",
      "# of ground metric features in 1 is   25600\n",
      "ground metric (m0) is  tensor([[2128.9270, 1903.5760, 1461.1394,  ..., 1547.5396, 1923.2927,\n",
      "         2039.9850],\n",
      "        [2237.7056, 2013.9417, 1879.5272,  ..., 1724.3981, 1927.5223,\n",
      "         2014.4081],\n",
      "        [2237.9880, 1998.5420, 2483.6331,  ..., 2252.2205, 2311.8157,\n",
      "         2867.0178],\n",
      "        ...,\n",
      "        [2317.9001, 1745.0690, 1369.4651,  ..., 1034.6487, 1549.8406,\n",
      "         1657.1597],\n",
      "        [2677.0012, 2159.2542, 1801.0477,  ..., 1597.7311, 1932.9708,\n",
      "         1930.2035],\n",
      "        [2845.9001, 2278.3232, 2456.7517,  ..., 2206.5366, 2621.4761,\n",
      "         2629.0723]], device='cuda:0')\n",
      "shape of T_var is  torch.Size([256, 256])\n",
      "T_var before correction  tensor([[0.0000, 0.0000, 0.0039,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        ...,\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],\n",
      "       device='cuda:0')\n",
      "marginals are  tensor([[255.9934, 255.9934, 255.9934,  ..., 255.9934, 255.9934, 255.9934],\n",
      "        [255.9934, 255.9934, 255.9934,  ..., 255.9934, 255.9934, 255.9934],\n",
      "        [255.9934, 255.9934, 255.9934,  ..., 255.9934, 255.9934, 255.9934],\n",
      "        ...,\n",
      "        [255.9934, 255.9934, 255.9934,  ..., 255.9934, 255.9934, 255.9934],\n",
      "        [255.9934, 255.9934, 255.9934,  ..., 255.9934, 255.9934, 255.9934],\n",
      "        [255.9934, 255.9934, 255.9934,  ..., 255.9934, 255.9934, 255.9934]],\n",
      "       device='cuda:0')\n",
      "T_var after correction  tensor([[0.0000, 0.0000, 1.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        ...,\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],\n",
      "       device='cuda:0')\n",
      "T_var stats: max 0.9999743700027466, min 0.0, mean 0.003906149882823229, std 0.06237669289112091 \n",
      "Ratio of trace to the matrix sum:  tensor(0.0078, device='cuda:0')\n",
      "Here, trace is 1.9999487400054932 and matrix sum is 255.99343872070312 \n",
      "Shape of aligned wt is  torch.Size([256, 256, 9])\n",
      "Shape of fc_layer0_weight_data is  torch.Size([256, 256, 9])\n",
      "model parameters are \n",
      " [torch.Size([64, 3, 3, 3]), torch.Size([64, 64, 3, 3]), torch.Size([64, 64, 3, 3]), torch.Size([64, 64, 3, 3]), torch.Size([64, 64, 3, 3]), torch.Size([128, 64, 3, 3]), torch.Size([128, 128, 3, 3]), torch.Size([128, 64, 1, 1]), torch.Size([128, 128, 3, 3]), torch.Size([128, 128, 3, 3]), torch.Size([256, 128, 3, 3]), torch.Size([256, 256, 3, 3]), torch.Size([256, 128, 1, 1]), torch.Size([256, 256, 3, 3]), torch.Size([256, 256, 3, 3]), torch.Size([512, 256, 3, 3]), torch.Size([512, 512, 3, 3]), torch.Size([512, 256, 1, 1]), torch.Size([512, 512, 3, 3]), torch.Size([512, 512, 3, 3]), torch.Size([10, 512])]\n",
      "len of model_state_dict is  21\n",
      "len of new_params is  12\n",
      "updated parameters for layer  conv1.weight\n",
      "updated parameters for layer  layer1.0.conv1.weight\n",
      "updated parameters for layer  layer1.0.conv2.weight\n",
      "updated parameters for layer  layer1.1.conv1.weight\n",
      "updated parameters for layer  layer1.1.conv2.weight\n",
      "updated parameters for layer  layer2.0.conv1.weight\n",
      "updated parameters for layer  layer2.0.conv2.weight\n",
      "updated parameters for layer  layer2.0.shortcut.0.weight\n",
      "updated parameters for layer  layer2.1.conv1.weight\n",
      "updated parameters for layer  layer2.1.conv2.weight\n",
      "updated parameters for layer  layer3.0.conv1.weight\n",
      "updated parameters for layer  layer3.0.conv2.weight\n",
      "\n",
      "--------- Testing in global mode ---------\n",
      "size of test_loader dataset:  10000\n",
      "\n",
      "Test set: Avg. loss: 0.0024, Accuracy: 1085/10000 (11%)\n",
      "\n",
      "accuracy after update is  10.85\n",
      "For layer idx 11, accuracy of the updated model is 10.85\n",
      "\n",
      "--------------- At layer index 12 ------------- \n",
      " \n",
      "Previous layer shape is  torch.Size([256, 256, 3, 3])\n",
      "let's see the difference in layer names layer3.0.shortcut.0 layer3.0.shortcut.0\n",
      "torch.Size([400, 1, 256, 8, 8]) shape of activations generally\n",
      "reorder_dim is  [1, 2, 3, 0]\n",
      "In layer layer3.0.shortcut.0.weight: getting activation distance statistics\n",
      "Statistics of the distance from neurons of layer 1 (averaged across nodes of layer 0): \n",
      "\n",
      "Max : 25.974735260009766, Mean : 8.905929565429688, Min : 1.9324649572372437, Std: 3.6388635635375977\n",
      "shape of layer: model 0 torch.Size([256, 128, 1])\n",
      "shape of layer: model 1 torch.Size([256, 128, 1])\n",
      "shape of activations: model 0 torch.Size([256, 8, 8, 400])\n",
      "shape of activations: model 1 torch.Size([256, 8, 8, 400])\n",
      "shape of previous transport map torch.Size([256, 256])\n",
      "utilizing skip T_var saved from layer layer 10 with shape torch.Size([128, 128])\n",
      "shape of previous transport map now is torch.Size([128, 128])\n",
      "returns a uniform measure of cardinality:  256\n",
      "returns a uniform measure of cardinality:  256\n",
      "Refactored ground metric calc\n",
      "inside refactored\n",
      "Processing the coordinates to form ground_metric\n",
      "dont leave off the squaring of the ground metric\n",
      "# of ground metric features is  25600\n",
      "# of ground metric features in 0 is   25600\n",
      "# of ground metric features in 1 is   25600\n",
      "ground metric (m0) is  tensor([[435.7055, 505.0486, 314.0035,  ..., 391.1060, 363.1125, 588.7825],\n",
      "        [603.3903, 475.8788, 438.1085,  ..., 288.9998, 453.1526, 441.4623],\n",
      "        [487.4855, 380.9051, 534.1444,  ..., 503.0339, 585.7407, 882.2692],\n",
      "        ...,\n",
      "        [523.3444, 290.2693, 267.4396,  ..., 170.1447, 371.3849, 479.9949],\n",
      "        [684.7943, 511.3703, 472.8501,  ..., 315.7245, 532.7597, 419.7451],\n",
      "        [754.6938, 425.7928, 562.8787,  ..., 409.6473, 688.7233, 664.6825]],\n",
      "       device='cuda:0')\n",
      "shape of T_var is  torch.Size([256, 256])\n",
      "T_var before correction  tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')\n",
      "marginals are  tensor([[255.9934, 255.9934, 255.9934,  ..., 255.9934, 255.9934, 255.9934],\n",
      "        [255.9934, 255.9934, 255.9934,  ..., 255.9934, 255.9934, 255.9934],\n",
      "        [255.9934, 255.9934, 255.9934,  ..., 255.9934, 255.9934, 255.9934],\n",
      "        ...,\n",
      "        [255.9934, 255.9934, 255.9934,  ..., 255.9934, 255.9934, 255.9934],\n",
      "        [255.9934, 255.9934, 255.9934,  ..., 255.9934, 255.9934, 255.9934],\n",
      "        [255.9934, 255.9934, 255.9934,  ..., 255.9934, 255.9934, 255.9934]],\n",
      "       device='cuda:0')\n",
      "T_var after correction  tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')\n",
      "T_var stats: max 0.9999743700027466, min 0.0, mean 0.003906149882823229, std 0.06237669289112091 \n",
      "Ratio of trace to the matrix sum:  tensor(0.0039, device='cuda:0')\n",
      "Here, trace is 0.9999743700027466 and matrix sum is 255.99343872070312 \n",
      "Shape of aligned wt is  torch.Size([256, 128, 1])\n",
      "Shape of fc_layer0_weight_data is  torch.Size([256, 128, 1])\n",
      "model parameters are \n",
      " [torch.Size([64, 3, 3, 3]), torch.Size([64, 64, 3, 3]), torch.Size([64, 64, 3, 3]), torch.Size([64, 64, 3, 3]), torch.Size([64, 64, 3, 3]), torch.Size([128, 64, 3, 3]), torch.Size([128, 128, 3, 3]), torch.Size([128, 64, 1, 1]), torch.Size([128, 128, 3, 3]), torch.Size([128, 128, 3, 3]), torch.Size([256, 128, 3, 3]), torch.Size([256, 256, 3, 3]), torch.Size([256, 128, 1, 1]), torch.Size([256, 256, 3, 3]), torch.Size([256, 256, 3, 3]), torch.Size([512, 256, 3, 3]), torch.Size([512, 512, 3, 3]), torch.Size([512, 256, 1, 1]), torch.Size([512, 512, 3, 3]), torch.Size([512, 512, 3, 3]), torch.Size([10, 512])]\n",
      "len of model_state_dict is  21\n",
      "len of new_params is  13\n",
      "updated parameters for layer  conv1.weight\n",
      "updated parameters for layer  layer1.0.conv1.weight\n",
      "updated parameters for layer  layer1.0.conv2.weight\n",
      "updated parameters for layer  layer1.1.conv1.weight\n",
      "updated parameters for layer  layer1.1.conv2.weight\n",
      "updated parameters for layer  layer2.0.conv1.weight\n",
      "updated parameters for layer  layer2.0.conv2.weight\n",
      "updated parameters for layer  layer2.0.shortcut.0.weight\n",
      "updated parameters for layer  layer2.1.conv1.weight\n",
      "updated parameters for layer  layer2.1.conv2.weight\n",
      "updated parameters for layer  layer3.0.conv1.weight\n",
      "updated parameters for layer  layer3.0.conv2.weight\n",
      "updated parameters for layer  layer3.0.shortcut.0.weight\n",
      "\n",
      "--------- Testing in global mode ---------\n",
      "size of test_loader dataset:  10000\n",
      "\n",
      "Test set: Avg. loss: 0.0023, Accuracy: 1238/10000 (12%)\n",
      "\n",
      "accuracy after update is  12.38\n",
      "For layer idx 12, accuracy of the updated model is 12.38\n",
      "\n",
      "--------------- At layer index 13 ------------- \n",
      " \n",
      "Previous layer shape is  torch.Size([256, 128, 1, 1])\n",
      "let's see the difference in layer names layer3.1.conv1 layer3.1.conv1\n",
      "torch.Size([400, 1, 256, 8, 8]) shape of activations generally\n",
      "reorder_dim is  [1, 2, 3, 0]\n",
      "In layer layer3.1.conv1.weight: getting activation distance statistics\n",
      "Statistics of the distance from neurons of layer 1 (averaged across nodes of layer 0): \n",
      "\n",
      "Max : 119.32659912109375, Mean : 38.3370361328125, Min : 8.578753471374512, Std: 16.990859985351562\n",
      "shape of layer: model 0 torch.Size([256, 256, 9])\n",
      "shape of layer: model 1 torch.Size([256, 256, 9])\n",
      "shape of activations: model 0 torch.Size([256, 8, 8, 400])\n",
      "shape of activations: model 1 torch.Size([256, 8, 8, 400])\n",
      "shape of previous transport map torch.Size([256, 256])\n",
      "averaging multiple T_var's\n",
      "returns a uniform measure of cardinality:  256\n",
      "returns a uniform measure of cardinality:  256\n",
      "Refactored ground metric calc\n",
      "inside refactored\n",
      "Processing the coordinates to form ground_metric\n",
      "dont leave off the squaring of the ground metric\n",
      "# of ground metric features is  25600\n",
      "# of ground metric features in 0 is   25600\n",
      "# of ground metric features in 1 is   25600\n",
      "ground metric (m0) is  tensor([[2101.4258, 1838.3823, 2603.7278,  ..., 2484.5452, 2873.9739,\n",
      "         2374.1560],\n",
      "        [2337.3848, 2133.6245, 2465.2949,  ..., 2559.1631, 2819.6299,\n",
      "         2230.9526],\n",
      "        [2017.7037, 1972.0002, 2318.1848,  ..., 2366.7727, 2714.7808,\n",
      "         2276.7227],\n",
      "        ...,\n",
      "        [2300.6692, 2300.5991, 2689.2622,  ..., 2383.2434, 2914.1372,\n",
      "         2531.6763],\n",
      "        [2569.5002, 2552.7253, 2537.3945,  ..., 2205.3452, 2813.7922,\n",
      "         2334.5425],\n",
      "        [2007.5781, 1894.4872, 2260.8145,  ..., 2448.8003, 2745.2131,\n",
      "         2223.8081]], device='cuda:0')\n",
      "shape of T_var is  torch.Size([256, 256])\n",
      "T_var before correction  tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')\n",
      "marginals are  tensor([[255.9934, 255.9934, 255.9934,  ..., 255.9934, 255.9934, 255.9934],\n",
      "        [255.9934, 255.9934, 255.9934,  ..., 255.9934, 255.9934, 255.9934],\n",
      "        [255.9934, 255.9934, 255.9934,  ..., 255.9934, 255.9934, 255.9934],\n",
      "        ...,\n",
      "        [255.9934, 255.9934, 255.9934,  ..., 255.9934, 255.9934, 255.9934],\n",
      "        [255.9934, 255.9934, 255.9934,  ..., 255.9934, 255.9934, 255.9934],\n",
      "        [255.9934, 255.9934, 255.9934,  ..., 255.9934, 255.9934, 255.9934]],\n",
      "       device='cuda:0')\n",
      "T_var after correction  tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')\n",
      "T_var stats: max 0.9999743700027466, min 0.0, mean 0.003906149882823229, std 0.06237669289112091 \n",
      "Ratio of trace to the matrix sum:  tensor(0., device='cuda:0')\n",
      "Here, trace is 0.0 and matrix sum is 255.99343872070312 \n",
      "Shape of aligned wt is  torch.Size([256, 256, 9])\n",
      "Shape of fc_layer0_weight_data is  torch.Size([256, 256, 9])\n",
      "model parameters are \n",
      " [torch.Size([64, 3, 3, 3]), torch.Size([64, 64, 3, 3]), torch.Size([64, 64, 3, 3]), torch.Size([64, 64, 3, 3]), torch.Size([64, 64, 3, 3]), torch.Size([128, 64, 3, 3]), torch.Size([128, 128, 3, 3]), torch.Size([128, 64, 1, 1]), torch.Size([128, 128, 3, 3]), torch.Size([128, 128, 3, 3]), torch.Size([256, 128, 3, 3]), torch.Size([256, 256, 3, 3]), torch.Size([256, 128, 1, 1]), torch.Size([256, 256, 3, 3]), torch.Size([256, 256, 3, 3]), torch.Size([512, 256, 3, 3]), torch.Size([512, 512, 3, 3]), torch.Size([512, 256, 1, 1]), torch.Size([512, 512, 3, 3]), torch.Size([512, 512, 3, 3]), torch.Size([10, 512])]\n",
      "len of model_state_dict is  21\n",
      "len of new_params is  14\n",
      "updated parameters for layer  conv1.weight\n",
      "updated parameters for layer  layer1.0.conv1.weight\n",
      "updated parameters for layer  layer1.0.conv2.weight\n",
      "updated parameters for layer  layer1.1.conv1.weight\n",
      "updated parameters for layer  layer1.1.conv2.weight\n",
      "updated parameters for layer  layer2.0.conv1.weight\n",
      "updated parameters for layer  layer2.0.conv2.weight\n",
      "updated parameters for layer  layer2.0.shortcut.0.weight\n",
      "updated parameters for layer  layer2.1.conv1.weight\n",
      "updated parameters for layer  layer2.1.conv2.weight\n",
      "updated parameters for layer  layer3.0.conv1.weight\n",
      "updated parameters for layer  layer3.0.conv2.weight\n",
      "updated parameters for layer  layer3.0.shortcut.0.weight\n",
      "updated parameters for layer  layer3.1.conv1.weight\n",
      "\n",
      "--------- Testing in global mode ---------\n",
      "size of test_loader dataset:  10000\n",
      "\n",
      "Test set: Avg. loss: 0.0023, Accuracy: 1261/10000 (13%)\n",
      "\n",
      "accuracy after update is  12.61\n",
      "For layer idx 13, accuracy of the updated model is 12.61\n",
      "\n",
      "--------------- At layer index 14 ------------- \n",
      " \n",
      "Previous layer shape is  torch.Size([256, 256, 3, 3])\n",
      "let's see the difference in layer names layer3.1.conv2 layer3.1.conv2\n",
      "torch.Size([400, 1, 256, 8, 8]) shape of activations generally\n",
      "reorder_dim is  [1, 2, 3, 0]\n",
      "In layer layer3.1.conv2.weight: getting activation distance statistics\n",
      "Statistics of the distance from neurons of layer 1 (averaged across nodes of layer 0): \n",
      "\n",
      "Max : 76.56063079833984, Mean : 19.054277420043945, Min : 3.181194543838501, Std: 10.810230255126953\n",
      "shape of layer: model 0 torch.Size([256, 256, 9])\n",
      "shape of layer: model 1 torch.Size([256, 256, 9])\n",
      "shape of activations: model 0 torch.Size([256, 8, 8, 400])\n",
      "shape of activations: model 1 torch.Size([256, 8, 8, 400])\n",
      "shape of previous transport map torch.Size([256, 256])\n",
      "doing nothing for skips\n",
      "returns a uniform measure of cardinality:  256\n",
      "returns a uniform measure of cardinality:  256\n",
      "Refactored ground metric calc\n",
      "inside refactored\n",
      "Processing the coordinates to form ground_metric\n",
      "dont leave off the squaring of the ground metric\n",
      "# of ground metric features is  25600\n",
      "# of ground metric features in 0 is   25600\n",
      "# of ground metric features in 1 is   25600\n",
      "ground metric (m0) is  tensor([[1254.5562, 1302.0526, 1028.5027,  ..., 1142.5444, 1258.8884,\n",
      "         1361.9332],\n",
      "        [1284.7727, 1336.4730, 1121.7169,  ..., 1032.0897, 1177.7096,\n",
      "         1307.4197],\n",
      "        [1365.0793, 1209.9919, 1161.6266,  ..., 1083.8580, 1255.5369,\n",
      "         1453.6943],\n",
      "        ...,\n",
      "        [1272.5203, 1113.9951,  903.5585,  ...,  677.6348,  935.3311,\n",
      "         1085.8307],\n",
      "        [1520.4742, 1426.6165, 1220.4939,  ..., 1202.0603, 1290.6261,\n",
      "         1386.2227],\n",
      "        [1400.6702, 1308.5248, 1176.4117,  ..., 1089.2819, 1291.3824,\n",
      "         1359.6178]], device='cuda:0')\n",
      "shape of T_var is  torch.Size([256, 256])\n",
      "T_var before correction  tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')\n",
      "marginals are  tensor([[255.9934, 255.9934, 255.9934,  ..., 255.9934, 255.9934, 255.9934],\n",
      "        [255.9934, 255.9934, 255.9934,  ..., 255.9934, 255.9934, 255.9934],\n",
      "        [255.9934, 255.9934, 255.9934,  ..., 255.9934, 255.9934, 255.9934],\n",
      "        ...,\n",
      "        [255.9934, 255.9934, 255.9934,  ..., 255.9934, 255.9934, 255.9934],\n",
      "        [255.9934, 255.9934, 255.9934,  ..., 255.9934, 255.9934, 255.9934],\n",
      "        [255.9934, 255.9934, 255.9934,  ..., 255.9934, 255.9934, 255.9934]],\n",
      "       device='cuda:0')\n",
      "T_var after correction  tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')\n",
      "T_var stats: max 0.9999743700027466, min 0.0, mean 0.003906149882823229, std 0.06237669289112091 \n",
      "Ratio of trace to the matrix sum:  tensor(0.0039, device='cuda:0')\n",
      "Here, trace is 0.9999743700027466 and matrix sum is 255.99343872070312 \n",
      "Shape of aligned wt is  torch.Size([256, 256, 9])\n",
      "Shape of fc_layer0_weight_data is  torch.Size([256, 256, 9])\n",
      "model parameters are \n",
      " [torch.Size([64, 3, 3, 3]), torch.Size([64, 64, 3, 3]), torch.Size([64, 64, 3, 3]), torch.Size([64, 64, 3, 3]), torch.Size([64, 64, 3, 3]), torch.Size([128, 64, 3, 3]), torch.Size([128, 128, 3, 3]), torch.Size([128, 64, 1, 1]), torch.Size([128, 128, 3, 3]), torch.Size([128, 128, 3, 3]), torch.Size([256, 128, 3, 3]), torch.Size([256, 256, 3, 3]), torch.Size([256, 128, 1, 1]), torch.Size([256, 256, 3, 3]), torch.Size([256, 256, 3, 3]), torch.Size([512, 256, 3, 3]), torch.Size([512, 512, 3, 3]), torch.Size([512, 256, 1, 1]), torch.Size([512, 512, 3, 3]), torch.Size([512, 512, 3, 3]), torch.Size([10, 512])]\n",
      "len of model_state_dict is  21\n",
      "len of new_params is  15\n",
      "updated parameters for layer  conv1.weight\n",
      "updated parameters for layer  layer1.0.conv1.weight\n",
      "updated parameters for layer  layer1.0.conv2.weight\n",
      "updated parameters for layer  layer1.1.conv1.weight\n",
      "updated parameters for layer  layer1.1.conv2.weight\n",
      "updated parameters for layer  layer2.0.conv1.weight\n",
      "updated parameters for layer  layer2.0.conv2.weight\n",
      "updated parameters for layer  layer2.0.shortcut.0.weight\n",
      "updated parameters for layer  layer2.1.conv1.weight\n",
      "updated parameters for layer  layer2.1.conv2.weight\n",
      "updated parameters for layer  layer3.0.conv1.weight\n",
      "updated parameters for layer  layer3.0.conv2.weight\n",
      "updated parameters for layer  layer3.0.shortcut.0.weight\n",
      "updated parameters for layer  layer3.1.conv1.weight\n",
      "updated parameters for layer  layer3.1.conv2.weight\n",
      "\n",
      "--------- Testing in global mode ---------\n",
      "size of test_loader dataset:  10000\n",
      "\n",
      "Test set: Avg. loss: 0.0023, Accuracy: 1260/10000 (13%)\n",
      "\n",
      "accuracy after update is  12.6\n",
      "For layer idx 14, accuracy of the updated model is 12.6\n",
      "\n",
      "--------------- At layer index 15 ------------- \n",
      " \n",
      "Previous layer shape is  torch.Size([256, 256, 3, 3])\n",
      "let's see the difference in layer names layer4.0.conv1 layer4.0.conv1\n",
      "torch.Size([400, 1, 512, 4, 4]) shape of activations generally\n",
      "reorder_dim is  [1, 2, 3, 0]\n",
      "In layer layer4.0.conv1.weight: getting activation distance statistics\n",
      "Statistics of the distance from neurons of layer 1 (averaged across nodes of layer 0): \n",
      "\n",
      "Max : 59.191490173339844, Mean : 16.275943756103516, Min : 2.32735013961792, Std: 8.42201042175293\n",
      "shape of layer: model 0 torch.Size([512, 256, 9])\n",
      "shape of layer: model 1 torch.Size([512, 256, 9])\n",
      "shape of activations: model 0 torch.Size([512, 4, 4, 400])\n",
      "shape of activations: model 1 torch.Size([512, 4, 4, 400])\n",
      "shape of previous transport map torch.Size([256, 256])\n",
      "saved skip T_var at layer 15 with shape torch.Size([512, 256, 3, 3])\n",
      "shape of previous transport map now is torch.Size([256, 256])\n",
      "returns a uniform measure of cardinality:  512\n",
      "returns a uniform measure of cardinality:  512\n",
      "Refactored ground metric calc\n",
      "inside refactored\n",
      "Processing the coordinates to form ground_metric\n",
      "dont leave off the squaring of the ground metric\n",
      "# of ground metric features is  6400\n",
      "# of ground metric features in 0 is   6400\n",
      "# of ground metric features in 1 is   6400\n",
      "ground metric (m0) is  tensor([[626.0355, 668.0406, 665.6281,  ..., 502.9333, 649.5323, 672.8596],\n",
      "        [603.7901, 595.8591, 752.8376,  ..., 641.7186, 604.2195, 647.7964],\n",
      "        [686.9192, 757.7859, 725.4408,  ..., 667.6447, 659.1479, 686.1840],\n",
      "        ...,\n",
      "        [676.1075, 635.7678, 848.6725,  ..., 713.9682, 727.7301, 695.8904],\n",
      "        [757.8631, 789.2197, 807.9872,  ..., 629.5906, 722.1960, 727.5311],\n",
      "        [671.2762, 669.1202, 730.5822,  ..., 568.4310, 638.7506, 697.8833]],\n",
      "       device='cuda:0')\n",
      "shape of T_var is  torch.Size([512, 512])\n",
      "T_var before correction  tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')\n",
      "marginals are  tensor([[511.9738, 511.9738, 511.9738,  ..., 511.9738, 511.9738, 511.9738],\n",
      "        [511.9738, 511.9738, 511.9738,  ..., 511.9738, 511.9738, 511.9738],\n",
      "        [511.9738, 511.9738, 511.9738,  ..., 511.9738, 511.9738, 511.9738],\n",
      "        ...,\n",
      "        [511.9738, 511.9738, 511.9738,  ..., 511.9738, 511.9738, 511.9738],\n",
      "        [511.9738, 511.9738, 511.9738,  ..., 511.9738, 511.9738, 511.9738],\n",
      "        [511.9738, 511.9738, 511.9738,  ..., 511.9738, 511.9738, 511.9738]],\n",
      "       device='cuda:0')\n",
      "T_var after correction  tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')\n",
      "T_var stats: max 0.9999488592147827, min 0.0, mean 0.0019530251156538725, std 0.044148821383714676 \n",
      "Ratio of trace to the matrix sum:  tensor(0.0020, device='cuda:0')\n",
      "Here, trace is 0.9999488592147827 and matrix sum is 511.97381591796875 \n",
      "Shape of aligned wt is  torch.Size([512, 256, 9])\n",
      "Shape of fc_layer0_weight_data is  torch.Size([512, 256, 9])\n",
      "model parameters are \n",
      " [torch.Size([64, 3, 3, 3]), torch.Size([64, 64, 3, 3]), torch.Size([64, 64, 3, 3]), torch.Size([64, 64, 3, 3]), torch.Size([64, 64, 3, 3]), torch.Size([128, 64, 3, 3]), torch.Size([128, 128, 3, 3]), torch.Size([128, 64, 1, 1]), torch.Size([128, 128, 3, 3]), torch.Size([128, 128, 3, 3]), torch.Size([256, 128, 3, 3]), torch.Size([256, 256, 3, 3]), torch.Size([256, 128, 1, 1]), torch.Size([256, 256, 3, 3]), torch.Size([256, 256, 3, 3]), torch.Size([512, 256, 3, 3]), torch.Size([512, 512, 3, 3]), torch.Size([512, 256, 1, 1]), torch.Size([512, 512, 3, 3]), torch.Size([512, 512, 3, 3]), torch.Size([10, 512])]\n",
      "len of model_state_dict is  21\n",
      "len of new_params is  16\n",
      "updated parameters for layer  conv1.weight\n",
      "updated parameters for layer  layer1.0.conv1.weight\n",
      "updated parameters for layer  layer1.0.conv2.weight\n",
      "updated parameters for layer  layer1.1.conv1.weight\n",
      "updated parameters for layer  layer1.1.conv2.weight\n",
      "updated parameters for layer  layer2.0.conv1.weight\n",
      "updated parameters for layer  layer2.0.conv2.weight\n",
      "updated parameters for layer  layer2.0.shortcut.0.weight\n",
      "updated parameters for layer  layer2.1.conv1.weight\n",
      "updated parameters for layer  layer2.1.conv2.weight\n",
      "updated parameters for layer  layer3.0.conv1.weight\n",
      "updated parameters for layer  layer3.0.conv2.weight\n",
      "updated parameters for layer  layer3.0.shortcut.0.weight\n",
      "updated parameters for layer  layer3.1.conv1.weight\n",
      "updated parameters for layer  layer3.1.conv2.weight\n",
      "updated parameters for layer  layer4.0.conv1.weight\n",
      "\n",
      "--------- Testing in global mode ---------\n",
      "size of test_loader dataset:  10000\n",
      "\n",
      "Test set: Avg. loss: 0.0023, Accuracy: 1027/10000 (10%)\n",
      "\n",
      "accuracy after update is  10.27\n",
      "For layer idx 15, accuracy of the updated model is 10.27\n",
      "\n",
      "--------------- At layer index 16 ------------- \n",
      " \n",
      "Previous layer shape is  torch.Size([512, 256, 3, 3])\n",
      "let's see the difference in layer names layer4.0.conv2 layer4.0.conv2\n",
      "torch.Size([400, 1, 512, 4, 4]) shape of activations generally\n",
      "reorder_dim is  [1, 2, 3, 0]\n",
      "In layer layer4.0.conv2.weight: getting activation distance statistics\n",
      "Statistics of the distance from neurons of layer 1 (averaged across nodes of layer 0): \n",
      "\n",
      "Max : 63.363433837890625, Mean : 16.411827087402344, Min : 2.2971460819244385, Std: 8.879426956176758\n",
      "shape of layer: model 0 torch.Size([512, 512, 9])\n",
      "shape of layer: model 1 torch.Size([512, 512, 9])\n",
      "shape of activations: model 0 torch.Size([512, 4, 4, 400])\n",
      "shape of activations: model 1 torch.Size([512, 4, 4, 400])\n",
      "shape of previous transport map torch.Size([512, 512])\n",
      "doing nothing for skips\n",
      "returns a uniform measure of cardinality:  512\n",
      "returns a uniform measure of cardinality:  512\n",
      "Refactored ground metric calc\n",
      "inside refactored\n",
      "Processing the coordinates to form ground_metric\n",
      "dont leave off the squaring of the ground metric\n",
      "# of ground metric features is  6400\n",
      "# of ground metric features in 0 is   6400\n",
      "# of ground metric features in 1 is   6400\n",
      "ground metric (m0) is  tensor([[ 506.1026,  914.8215,  462.8824,  ...,  917.4948,  702.6364,\n",
      "          709.1425],\n",
      "        [ 616.4424,  746.9993,  677.0173,  ...,  933.9543,  709.3979,\n",
      "          713.9149],\n",
      "        [ 676.2862,  882.7742,  709.5919,  ...,  984.2590,  962.9390,\n",
      "          763.1322],\n",
      "        ...,\n",
      "        [ 770.7283,  899.1275,  760.7206,  ...,  976.2028,  768.3853,\n",
      "          800.0747],\n",
      "        [ 797.6489,  957.0354,  776.2086,  ..., 1012.1277,  813.6610,\n",
      "          868.6823],\n",
      "        [ 769.8860,  896.8557,  796.5077,  ...,  991.2401,  899.1191,\n",
      "          850.6819]], device='cuda:0')\n",
      "shape of T_var is  torch.Size([512, 512])\n",
      "T_var before correction  tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')\n",
      "marginals are  tensor([[511.9738, 511.9738, 511.9738,  ..., 511.9738, 511.9738, 511.9738],\n",
      "        [511.9738, 511.9738, 511.9738,  ..., 511.9738, 511.9738, 511.9738],\n",
      "        [511.9738, 511.9738, 511.9738,  ..., 511.9738, 511.9738, 511.9738],\n",
      "        ...,\n",
      "        [511.9738, 511.9738, 511.9738,  ..., 511.9738, 511.9738, 511.9738],\n",
      "        [511.9738, 511.9738, 511.9738,  ..., 511.9738, 511.9738, 511.9738],\n",
      "        [511.9738, 511.9738, 511.9738,  ..., 511.9738, 511.9738, 511.9738]],\n",
      "       device='cuda:0')\n",
      "T_var after correction  tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')\n",
      "T_var stats: max 0.9999488592147827, min 0.0, mean 0.0019530251156538725, std 0.044148821383714676 \n",
      "Ratio of trace to the matrix sum:  tensor(0.0020, device='cuda:0')\n",
      "Here, trace is 0.9999488592147827 and matrix sum is 511.97381591796875 \n",
      "Shape of aligned wt is  torch.Size([512, 512, 9])\n",
      "Shape of fc_layer0_weight_data is  torch.Size([512, 512, 9])\n",
      "model parameters are \n",
      " [torch.Size([64, 3, 3, 3]), torch.Size([64, 64, 3, 3]), torch.Size([64, 64, 3, 3]), torch.Size([64, 64, 3, 3]), torch.Size([64, 64, 3, 3]), torch.Size([128, 64, 3, 3]), torch.Size([128, 128, 3, 3]), torch.Size([128, 64, 1, 1]), torch.Size([128, 128, 3, 3]), torch.Size([128, 128, 3, 3]), torch.Size([256, 128, 3, 3]), torch.Size([256, 256, 3, 3]), torch.Size([256, 128, 1, 1]), torch.Size([256, 256, 3, 3]), torch.Size([256, 256, 3, 3]), torch.Size([512, 256, 3, 3]), torch.Size([512, 512, 3, 3]), torch.Size([512, 256, 1, 1]), torch.Size([512, 512, 3, 3]), torch.Size([512, 512, 3, 3]), torch.Size([10, 512])]\n",
      "len of model_state_dict is  21\n",
      "len of new_params is  17\n",
      "updated parameters for layer  conv1.weight\n",
      "updated parameters for layer  layer1.0.conv1.weight\n",
      "updated parameters for layer  layer1.0.conv2.weight\n",
      "updated parameters for layer  layer1.1.conv1.weight\n",
      "updated parameters for layer  layer1.1.conv2.weight\n",
      "updated parameters for layer  layer2.0.conv1.weight\n",
      "updated parameters for layer  layer2.0.conv2.weight\n",
      "updated parameters for layer  layer2.0.shortcut.0.weight\n",
      "updated parameters for layer  layer2.1.conv1.weight\n",
      "updated parameters for layer  layer2.1.conv2.weight\n",
      "updated parameters for layer  layer3.0.conv1.weight\n",
      "updated parameters for layer  layer3.0.conv2.weight\n",
      "updated parameters for layer  layer3.0.shortcut.0.weight\n",
      "updated parameters for layer  layer3.1.conv1.weight\n",
      "updated parameters for layer  layer3.1.conv2.weight\n",
      "updated parameters for layer  layer4.0.conv1.weight\n",
      "updated parameters for layer  layer4.0.conv2.weight\n",
      "\n",
      "--------- Testing in global mode ---------\n",
      "size of test_loader dataset:  10000\n",
      "\n",
      "Test set: Avg. loss: 0.0024, Accuracy: 887/10000 (9%)\n",
      "\n",
      "accuracy after update is  8.87\n",
      "For layer idx 16, accuracy of the updated model is 8.87\n",
      "\n",
      "--------------- At layer index 17 ------------- \n",
      " \n",
      "Previous layer shape is  torch.Size([512, 512, 3, 3])\n",
      "let's see the difference in layer names layer4.0.shortcut.0 layer4.0.shortcut.0\n",
      "torch.Size([400, 1, 512, 4, 4]) shape of activations generally\n",
      "reorder_dim is  [1, 2, 3, 0]\n",
      "In layer layer4.0.shortcut.0.weight: getting activation distance statistics\n",
      "Statistics of the distance from neurons of layer 1 (averaged across nodes of layer 0): \n",
      "\n",
      "Max : 17.27553367614746, Mean : 5.380155563354492, Min : 0.8679923415184021, Std: 2.5304813385009766\n",
      "shape of layer: model 0 torch.Size([512, 256, 1])\n",
      "shape of layer: model 1 torch.Size([512, 256, 1])\n",
      "shape of activations: model 0 torch.Size([512, 4, 4, 400])\n",
      "shape of activations: model 1 torch.Size([512, 4, 4, 400])\n",
      "shape of previous transport map torch.Size([512, 512])\n",
      "utilizing skip T_var saved from layer layer 15 with shape torch.Size([256, 256])\n",
      "shape of previous transport map now is torch.Size([256, 256])\n",
      "returns a uniform measure of cardinality:  512\n",
      "returns a uniform measure of cardinality:  512\n",
      "Refactored ground metric calc\n",
      "inside refactored\n",
      "Processing the coordinates to form ground_metric\n",
      "dont leave off the squaring of the ground metric\n",
      "# of ground metric features is  6400\n",
      "# of ground metric features in 0 is   6400\n",
      "# of ground metric features in 1 is   6400\n",
      "ground metric (m0) is  tensor([[147.0593, 328.2797, 151.3626,  ..., 342.5892, 228.4982, 186.3041],\n",
      "        [217.6147, 204.4168, 234.2082,  ..., 226.2246, 204.1296, 256.1272],\n",
      "        [203.3338, 289.7830, 225.5855,  ..., 287.4108, 228.6938, 228.9592],\n",
      "        ...,\n",
      "        [183.8987, 253.6003, 190.2079,  ..., 271.8763, 203.9173, 225.7293],\n",
      "        [204.7959, 272.9883, 196.0376,  ..., 300.1330, 214.9562, 257.3027],\n",
      "        [217.0164, 260.6195, 227.2967,  ..., 263.3675, 241.4893, 263.5172]],\n",
      "       device='cuda:0')\n",
      "shape of T_var is  torch.Size([512, 512])\n",
      "T_var before correction  tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')\n",
      "marginals are  tensor([[511.9738, 511.9738, 511.9738,  ..., 511.9738, 511.9738, 511.9738],\n",
      "        [511.9738, 511.9738, 511.9738,  ..., 511.9738, 511.9738, 511.9738],\n",
      "        [511.9738, 511.9738, 511.9738,  ..., 511.9738, 511.9738, 511.9738],\n",
      "        ...,\n",
      "        [511.9738, 511.9738, 511.9738,  ..., 511.9738, 511.9738, 511.9738],\n",
      "        [511.9738, 511.9738, 511.9738,  ..., 511.9738, 511.9738, 511.9738],\n",
      "        [511.9738, 511.9738, 511.9738,  ..., 511.9738, 511.9738, 511.9738]],\n",
      "       device='cuda:0')\n",
      "T_var after correction  tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')\n",
      "T_var stats: max 0.9999488592147827, min 0.0, mean 0.0019530251156538725, std 0.044148821383714676 \n",
      "Ratio of trace to the matrix sum:  tensor(0.0020, device='cuda:0')\n",
      "Here, trace is 0.9999488592147827 and matrix sum is 511.97381591796875 \n",
      "Shape of aligned wt is  torch.Size([512, 256, 1])\n",
      "Shape of fc_layer0_weight_data is  torch.Size([512, 256, 1])\n",
      "model parameters are \n",
      " [torch.Size([64, 3, 3, 3]), torch.Size([64, 64, 3, 3]), torch.Size([64, 64, 3, 3]), torch.Size([64, 64, 3, 3]), torch.Size([64, 64, 3, 3]), torch.Size([128, 64, 3, 3]), torch.Size([128, 128, 3, 3]), torch.Size([128, 64, 1, 1]), torch.Size([128, 128, 3, 3]), torch.Size([128, 128, 3, 3]), torch.Size([256, 128, 3, 3]), torch.Size([256, 256, 3, 3]), torch.Size([256, 128, 1, 1]), torch.Size([256, 256, 3, 3]), torch.Size([256, 256, 3, 3]), torch.Size([512, 256, 3, 3]), torch.Size([512, 512, 3, 3]), torch.Size([512, 256, 1, 1]), torch.Size([512, 512, 3, 3]), torch.Size([512, 512, 3, 3]), torch.Size([10, 512])]\n",
      "len of model_state_dict is  21\n",
      "len of new_params is  18\n",
      "updated parameters for layer  conv1.weight\n",
      "updated parameters for layer  layer1.0.conv1.weight\n",
      "updated parameters for layer  layer1.0.conv2.weight\n",
      "updated parameters for layer  layer1.1.conv1.weight\n",
      "updated parameters for layer  layer1.1.conv2.weight\n",
      "updated parameters for layer  layer2.0.conv1.weight\n",
      "updated parameters for layer  layer2.0.conv2.weight\n",
      "updated parameters for layer  layer2.0.shortcut.0.weight\n",
      "updated parameters for layer  layer2.1.conv1.weight\n",
      "updated parameters for layer  layer2.1.conv2.weight\n",
      "updated parameters for layer  layer3.0.conv1.weight\n",
      "updated parameters for layer  layer3.0.conv2.weight\n",
      "updated parameters for layer  layer3.0.shortcut.0.weight\n",
      "updated parameters for layer  layer3.1.conv1.weight\n",
      "updated parameters for layer  layer3.1.conv2.weight\n",
      "updated parameters for layer  layer4.0.conv1.weight\n",
      "updated parameters for layer  layer4.0.conv2.weight\n",
      "updated parameters for layer  layer4.0.shortcut.0.weight\n",
      "\n",
      "--------- Testing in global mode ---------\n",
      "size of test_loader dataset:  10000\n",
      "\n",
      "Test set: Avg. loss: 0.0023, Accuracy: 853/10000 (9%)\n",
      "\n",
      "accuracy after update is  8.53\n",
      "For layer idx 17, accuracy of the updated model is 8.53\n",
      "\n",
      "--------------- At layer index 18 ------------- \n",
      " \n",
      "Previous layer shape is  torch.Size([512, 256, 1, 1])\n",
      "let's see the difference in layer names layer4.1.conv1 layer4.1.conv1\n",
      "torch.Size([400, 1, 512, 4, 4]) shape of activations generally\n",
      "reorder_dim is  [1, 2, 3, 0]\n",
      "In layer layer4.1.conv1.weight: getting activation distance statistics\n",
      "Statistics of the distance from neurons of layer 1 (averaged across nodes of layer 0): \n",
      "\n",
      "Max : 36.36698913574219, Mean : 8.272093772888184, Min : 1.0039401054382324, Std: 5.100873947143555\n",
      "shape of layer: model 0 torch.Size([512, 512, 9])\n",
      "shape of layer: model 1 torch.Size([512, 512, 9])\n",
      "shape of activations: model 0 torch.Size([512, 4, 4, 400])\n",
      "shape of activations: model 1 torch.Size([512, 4, 4, 400])\n",
      "shape of previous transport map torch.Size([512, 512])\n",
      "averaging multiple T_var's\n",
      "returns a uniform measure of cardinality:  512\n",
      "returns a uniform measure of cardinality:  512\n",
      "Refactored ground metric calc\n",
      "inside refactored\n",
      "Processing the coordinates to form ground_metric\n",
      "dont leave off the squaring of the ground metric\n",
      "# of ground metric features is  6400\n",
      "# of ground metric features in 0 is   6400\n",
      "# of ground metric features in 1 is   6400\n",
      "ground metric (m0) is  tensor([[316.3335, 103.0240, 324.8353,  ..., 237.5609, 357.3593, 558.6335],\n",
      "        [365.5013, 445.1589, 375.2972,  ..., 438.8097, 408.2382, 552.5544],\n",
      "        [350.2751, 273.9625, 351.3872,  ..., 290.2983, 400.9707, 534.8622],\n",
      "        ...,\n",
      "        [428.7861, 436.1027, 414.9620,  ..., 482.0325, 497.0441, 579.0605],\n",
      "        [357.9507, 354.2211, 332.5125,  ..., 345.9354, 392.6111, 455.7034],\n",
      "        [293.7960, 274.3161, 312.2804,  ..., 290.5285, 359.9109, 513.5147]],\n",
      "       device='cuda:0')\n",
      "shape of T_var is  torch.Size([512, 512])\n",
      "T_var before correction  tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')\n",
      "marginals are  tensor([[511.9738, 511.9738, 511.9738,  ..., 511.9738, 511.9738, 511.9738],\n",
      "        [511.9738, 511.9738, 511.9738,  ..., 511.9738, 511.9738, 511.9738],\n",
      "        [511.9738, 511.9738, 511.9738,  ..., 511.9738, 511.9738, 511.9738],\n",
      "        ...,\n",
      "        [511.9738, 511.9738, 511.9738,  ..., 511.9738, 511.9738, 511.9738],\n",
      "        [511.9738, 511.9738, 511.9738,  ..., 511.9738, 511.9738, 511.9738],\n",
      "        [511.9738, 511.9738, 511.9738,  ..., 511.9738, 511.9738, 511.9738]],\n",
      "       device='cuda:0')\n",
      "T_var after correction  tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')\n",
      "T_var stats: max 0.9999488592147827, min 0.0, mean 0.0019530251156538725, std 0.044148821383714676 \n",
      "Ratio of trace to the matrix sum:  tensor(0., device='cuda:0')\n",
      "Here, trace is 0.0 and matrix sum is 511.97381591796875 \n",
      "Shape of aligned wt is  torch.Size([512, 512, 9])\n",
      "Shape of fc_layer0_weight_data is  torch.Size([512, 512, 9])\n",
      "model parameters are \n",
      " [torch.Size([64, 3, 3, 3]), torch.Size([64, 64, 3, 3]), torch.Size([64, 64, 3, 3]), torch.Size([64, 64, 3, 3]), torch.Size([64, 64, 3, 3]), torch.Size([128, 64, 3, 3]), torch.Size([128, 128, 3, 3]), torch.Size([128, 64, 1, 1]), torch.Size([128, 128, 3, 3]), torch.Size([128, 128, 3, 3]), torch.Size([256, 128, 3, 3]), torch.Size([256, 256, 3, 3]), torch.Size([256, 128, 1, 1]), torch.Size([256, 256, 3, 3]), torch.Size([256, 256, 3, 3]), torch.Size([512, 256, 3, 3]), torch.Size([512, 512, 3, 3]), torch.Size([512, 256, 1, 1]), torch.Size([512, 512, 3, 3]), torch.Size([512, 512, 3, 3]), torch.Size([10, 512])]\n",
      "len of model_state_dict is  21\n",
      "len of new_params is  19\n",
      "updated parameters for layer  conv1.weight\n",
      "updated parameters for layer  layer1.0.conv1.weight\n",
      "updated parameters for layer  layer1.0.conv2.weight\n",
      "updated parameters for layer  layer1.1.conv1.weight\n",
      "updated parameters for layer  layer1.1.conv2.weight\n",
      "updated parameters for layer  layer2.0.conv1.weight\n",
      "updated parameters for layer  layer2.0.conv2.weight\n",
      "updated parameters for layer  layer2.0.shortcut.0.weight\n",
      "updated parameters for layer  layer2.1.conv1.weight\n",
      "updated parameters for layer  layer2.1.conv2.weight\n",
      "updated parameters for layer  layer3.0.conv1.weight\n",
      "updated parameters for layer  layer3.0.conv2.weight\n",
      "updated parameters for layer  layer3.0.shortcut.0.weight\n",
      "updated parameters for layer  layer3.1.conv1.weight\n",
      "updated parameters for layer  layer3.1.conv2.weight\n",
      "updated parameters for layer  layer4.0.conv1.weight\n",
      "updated parameters for layer  layer4.0.conv2.weight\n",
      "updated parameters for layer  layer4.0.shortcut.0.weight\n",
      "updated parameters for layer  layer4.1.conv1.weight\n",
      "\n",
      "--------- Testing in global mode ---------\n",
      "size of test_loader dataset:  10000\n",
      "\n",
      "Test set: Avg. loss: 0.0024, Accuracy: 855/10000 (9%)\n",
      "\n",
      "accuracy after update is  8.55\n",
      "For layer idx 18, accuracy of the updated model is 8.55\n",
      "\n",
      "--------------- At layer index 19 ------------- \n",
      " \n",
      "Previous layer shape is  torch.Size([512, 512, 3, 3])\n",
      "let's see the difference in layer names layer4.1.conv2 layer4.1.conv2\n",
      "torch.Size([400, 1, 512, 4, 4]) shape of activations generally\n",
      "reorder_dim is  [1, 2, 3, 0]\n",
      "In layer layer4.1.conv2.weight: getting activation distance statistics\n",
      "Statistics of the distance from neurons of layer 1 (averaged across nodes of layer 0): \n",
      "\n",
      "Max : 44.656890869140625, Mean : 6.8264570236206055, Min : 0.5516976118087769, Std: 5.924533843994141\n",
      "shape of layer: model 0 torch.Size([512, 512, 9])\n",
      "shape of layer: model 1 torch.Size([512, 512, 9])\n",
      "shape of activations: model 0 torch.Size([512, 4, 4, 400])\n",
      "shape of activations: model 1 torch.Size([512, 4, 4, 400])\n",
      "shape of previous transport map torch.Size([512, 512])\n",
      "doing nothing for skips\n",
      "returns a uniform measure of cardinality:  512\n",
      "returns a uniform measure of cardinality:  512\n",
      "Refactored ground metric calc\n",
      "inside refactored\n",
      "Processing the coordinates to form ground_metric\n",
      "dont leave off the squaring of the ground metric\n",
      "# of ground metric features is  6400\n",
      "# of ground metric features in 0 is   6400\n",
      "# of ground metric features in 1 is   6400\n",
      "ground metric (m0) is  tensor([[241.9051, 362.3036, 263.0335,  ..., 502.0368, 310.7578, 277.0370],\n",
      "        [280.4156, 297.4754, 329.5185,  ..., 507.9861, 360.2754, 348.1075],\n",
      "        [312.5384, 402.7995, 302.4356,  ..., 544.0206, 410.4257, 334.9333],\n",
      "        ...,\n",
      "        [362.0023, 441.4859, 355.7368,  ..., 548.0283, 417.9586, 424.9020],\n",
      "        [312.3874, 401.4399, 344.9959,  ..., 528.4473, 390.1169, 399.6970],\n",
      "        [341.1722, 389.1925, 376.4375,  ..., 534.4396, 417.6031, 403.0433]],\n",
      "       device='cuda:0')\n",
      "shape of T_var is  torch.Size([512, 512])\n",
      "T_var before correction  tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')\n",
      "marginals are  tensor([[511.9738, 511.9738, 511.9738,  ..., 511.9738, 511.9738, 511.9738],\n",
      "        [511.9738, 511.9738, 511.9738,  ..., 511.9738, 511.9738, 511.9738],\n",
      "        [511.9738, 511.9738, 511.9738,  ..., 511.9738, 511.9738, 511.9738],\n",
      "        ...,\n",
      "        [511.9738, 511.9738, 511.9738,  ..., 511.9738, 511.9738, 511.9738],\n",
      "        [511.9738, 511.9738, 511.9738,  ..., 511.9738, 511.9738, 511.9738],\n",
      "        [511.9738, 511.9738, 511.9738,  ..., 511.9738, 511.9738, 511.9738]],\n",
      "       device='cuda:0')\n",
      "T_var after correction  tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')\n",
      "T_var stats: max 0.9999488592147827, min 0.0, mean 0.0019530251156538725, std 0.044148821383714676 \n",
      "Ratio of trace to the matrix sum:  tensor(0.0020, device='cuda:0')\n",
      "Here, trace is 0.9999488592147827 and matrix sum is 511.97381591796875 \n",
      "Shape of aligned wt is  torch.Size([512, 512, 9])\n",
      "Shape of fc_layer0_weight_data is  torch.Size([512, 512, 9])\n",
      "model parameters are \n",
      " [torch.Size([64, 3, 3, 3]), torch.Size([64, 64, 3, 3]), torch.Size([64, 64, 3, 3]), torch.Size([64, 64, 3, 3]), torch.Size([64, 64, 3, 3]), torch.Size([128, 64, 3, 3]), torch.Size([128, 128, 3, 3]), torch.Size([128, 64, 1, 1]), torch.Size([128, 128, 3, 3]), torch.Size([128, 128, 3, 3]), torch.Size([256, 128, 3, 3]), torch.Size([256, 256, 3, 3]), torch.Size([256, 128, 1, 1]), torch.Size([256, 256, 3, 3]), torch.Size([256, 256, 3, 3]), torch.Size([512, 256, 3, 3]), torch.Size([512, 512, 3, 3]), torch.Size([512, 256, 1, 1]), torch.Size([512, 512, 3, 3]), torch.Size([512, 512, 3, 3]), torch.Size([10, 512])]\n",
      "len of model_state_dict is  21\n",
      "len of new_params is  20\n",
      "updated parameters for layer  conv1.weight\n",
      "updated parameters for layer  layer1.0.conv1.weight\n",
      "updated parameters for layer  layer1.0.conv2.weight\n",
      "updated parameters for layer  layer1.1.conv1.weight\n",
      "updated parameters for layer  layer1.1.conv2.weight\n",
      "updated parameters for layer  layer2.0.conv1.weight\n",
      "updated parameters for layer  layer2.0.conv2.weight\n",
      "updated parameters for layer  layer2.0.shortcut.0.weight\n",
      "updated parameters for layer  layer2.1.conv1.weight\n",
      "updated parameters for layer  layer2.1.conv2.weight\n",
      "updated parameters for layer  layer3.0.conv1.weight\n",
      "updated parameters for layer  layer3.0.conv2.weight\n",
      "updated parameters for layer  layer3.0.shortcut.0.weight\n",
      "updated parameters for layer  layer3.1.conv1.weight\n",
      "updated parameters for layer  layer3.1.conv2.weight\n",
      "updated parameters for layer  layer4.0.conv1.weight\n",
      "updated parameters for layer  layer4.0.conv2.weight\n",
      "updated parameters for layer  layer4.0.shortcut.0.weight\n",
      "updated parameters for layer  layer4.1.conv1.weight\n",
      "updated parameters for layer  layer4.1.conv2.weight\n",
      "\n",
      "--------- Testing in global mode ---------\n",
      "size of test_loader dataset:  10000\n",
      "\n",
      "Test set: Avg. loss: 0.0024, Accuracy: 853/10000 (9%)\n",
      "\n",
      "accuracy after update is  8.53\n",
      "For layer idx 19, accuracy of the updated model is 8.53\n",
      "\n",
      "--------------- At layer index 20 ------------- \n",
      " \n",
      "Previous layer shape is  torch.Size([512, 512, 3, 3])\n",
      "let's see the difference in layer names linear linear\n",
      "torch.Size([400, 1, 10]) shape of activations generally\n",
      "In layer linear.weight: getting activation distance statistics\n",
      "Statistics of the distance from neurons of layer 1 (averaged across nodes of layer 0): \n",
      "\n",
      "Max : 446.5838928222656, Mean : 333.5245666503906, Min : 71.3207015991211, Std: 105.8348388671875\n",
      "shape of layer: model 0 torch.Size([10, 512])\n",
      "shape of layer: model 1 torch.Size([10, 512])\n",
      "shape of activations: model 0 torch.Size([10, 400])\n",
      "shape of activations: model 1 torch.Size([10, 400])\n",
      "shape of previous transport map torch.Size([512, 512])\n",
      "returns a uniform measure of cardinality:  10\n",
      "returns a uniform measure of cardinality:  10\n",
      "Refactored ground metric calc\n",
      "inside refactored\n",
      "Processing the coordinates to form ground_metric\n",
      "dont leave off the squaring of the ground metric\n",
      "# of ground metric features in 0 is   400\n",
      "# of ground metric features in 1 is   400\n",
      "ground metric (m0) is  tensor([[ 65.3095, 348.6325, 305.8385, 328.5684, 369.9477, 375.7669, 357.8004,\n",
      "         440.6782, 295.4938, 323.0192],\n",
      "        [335.2177,  64.1312, 416.1236, 376.7540, 434.5971, 420.4138, 387.3246,\n",
      "         472.9048, 323.3000, 270.8060],\n",
      "        [298.3986, 402.3407,  76.6125, 262.2491, 298.5297, 296.1998, 277.2967,\n",
      "         384.3573, 362.0182, 368.2161],\n",
      "        [330.0822, 381.5046, 268.5572,  76.5712, 315.8429, 261.8405, 282.1046,\n",
      "         412.8726, 366.2175, 360.9109],\n",
      "        [374.7378, 446.1231, 288.3049, 305.4601,  71.2870, 291.1384, 347.6939,\n",
      "         345.7160, 415.7847, 435.0886],\n",
      "        [365.2484, 408.3029, 282.2446, 233.7799, 284.8190,  76.2855, 314.4114,\n",
      "         329.4008, 389.6039, 401.8809],\n",
      "        [349.5398, 377.5303, 280.7869, 282.1130, 351.6187, 333.8308,  72.7221,\n",
      "         445.5439, 369.4525, 395.3549],\n",
      "        [452.1899, 501.2928, 392.9823, 416.6744, 352.2268, 330.5045, 458.8931,\n",
      "          68.9441, 468.8822, 471.9706],\n",
      "        [304.4298, 362.0076, 387.0581, 369.5578, 417.4204, 410.2924, 385.7323,\n",
      "         469.5416,  74.6683, 337.2657],\n",
      "        [327.1704, 288.3439, 393.5165, 355.3315, 425.2010, 412.0782, 401.1440,\n",
      "         466.2383, 323.0656,  66.6757]], device='cuda:0')\n",
      "shape of T_var is  torch.Size([10, 10])\n",
      "T_var before correction  tensor([[0.1000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0000, 0.1000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0000, 0.0000, 0.1000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.1000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.1000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.1000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.1000, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.1000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.1000,\n",
      "         0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.1000]], device='cuda:0')\n",
      "marginals are  tensor([[10.0024, 10.0024, 10.0024, 10.0024, 10.0024, 10.0024, 10.0024, 10.0024,\n",
      "         10.0024, 10.0024],\n",
      "        [10.0024, 10.0024, 10.0024, 10.0024, 10.0024, 10.0024, 10.0024, 10.0024,\n",
      "         10.0024, 10.0024],\n",
      "        [10.0024, 10.0024, 10.0024, 10.0024, 10.0024, 10.0024, 10.0024, 10.0024,\n",
      "         10.0024, 10.0024],\n",
      "        [10.0024, 10.0024, 10.0024, 10.0024, 10.0024, 10.0024, 10.0024, 10.0024,\n",
      "         10.0024, 10.0024],\n",
      "        [10.0024, 10.0024, 10.0024, 10.0024, 10.0024, 10.0024, 10.0024, 10.0024,\n",
      "         10.0024, 10.0024],\n",
      "        [10.0024, 10.0024, 10.0024, 10.0024, 10.0024, 10.0024, 10.0024, 10.0024,\n",
      "         10.0024, 10.0024],\n",
      "        [10.0024, 10.0024, 10.0024, 10.0024, 10.0024, 10.0024, 10.0024, 10.0024,\n",
      "         10.0024, 10.0024],\n",
      "        [10.0024, 10.0024, 10.0024, 10.0024, 10.0024, 10.0024, 10.0024, 10.0024,\n",
      "         10.0024, 10.0024],\n",
      "        [10.0024, 10.0024, 10.0024, 10.0024, 10.0024, 10.0024, 10.0024, 10.0024,\n",
      "         10.0024, 10.0024],\n",
      "        [10.0024, 10.0024, 10.0024, 10.0024, 10.0024, 10.0024, 10.0024, 10.0024,\n",
      "         10.0024, 10.0024]], device='cuda:0')\n",
      "T_var after correction  tensor([[1.0002, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0000, 1.0002, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0000, 0.0000, 1.0002, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 1.0002, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 1.0002, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 1.0002, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 1.0002, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 1.0002, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 1.0002,\n",
      "         0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         1.0002]], device='cuda:0')\n",
      "T_var stats: max 1.0002431869506836, min 0.0, mean 0.10002431273460388, std 0.3015846312046051 \n",
      "Ratio of trace to the matrix sum:  tensor(1., device='cuda:0')\n",
      "Here, trace is 10.002431869506836 and matrix sum is 10.002431869506836 \n",
      "Shape of aligned wt is  torch.Size([10, 512])\n",
      "Shape of fc_layer0_weight_data is  torch.Size([10, 512])\n",
      "model parameters are \n",
      " [torch.Size([64, 3, 3, 3]), torch.Size([64, 64, 3, 3]), torch.Size([64, 64, 3, 3]), torch.Size([64, 64, 3, 3]), torch.Size([64, 64, 3, 3]), torch.Size([128, 64, 3, 3]), torch.Size([128, 128, 3, 3]), torch.Size([128, 64, 1, 1]), torch.Size([128, 128, 3, 3]), torch.Size([128, 128, 3, 3]), torch.Size([256, 128, 3, 3]), torch.Size([256, 256, 3, 3]), torch.Size([256, 128, 1, 1]), torch.Size([256, 256, 3, 3]), torch.Size([256, 256, 3, 3]), torch.Size([512, 256, 3, 3]), torch.Size([512, 512, 3, 3]), torch.Size([512, 256, 1, 1]), torch.Size([512, 512, 3, 3]), torch.Size([512, 512, 3, 3]), torch.Size([10, 512])]\n",
      "len of model_state_dict is  21\n",
      "len of new_params is  21\n",
      "updated parameters for layer  conv1.weight\n",
      "updated parameters for layer  layer1.0.conv1.weight\n",
      "updated parameters for layer  layer1.0.conv2.weight\n",
      "updated parameters for layer  layer1.1.conv1.weight\n",
      "updated parameters for layer  layer1.1.conv2.weight\n",
      "updated parameters for layer  layer2.0.conv1.weight\n",
      "updated parameters for layer  layer2.0.conv2.weight\n",
      "updated parameters for layer  layer2.0.shortcut.0.weight\n",
      "updated parameters for layer  layer2.1.conv1.weight\n",
      "updated parameters for layer  layer2.1.conv2.weight\n",
      "updated parameters for layer  layer3.0.conv1.weight\n",
      "updated parameters for layer  layer3.0.conv2.weight\n",
      "updated parameters for layer  layer3.0.shortcut.0.weight\n",
      "updated parameters for layer  layer3.1.conv1.weight\n",
      "updated parameters for layer  layer3.1.conv2.weight\n",
      "updated parameters for layer  layer4.0.conv1.weight\n",
      "updated parameters for layer  layer4.0.conv2.weight\n",
      "updated parameters for layer  layer4.0.shortcut.0.weight\n",
      "updated parameters for layer  layer4.1.conv1.weight\n",
      "updated parameters for layer  layer4.1.conv2.weight\n",
      "updated parameters for layer  linear.weight\n",
      "\n",
      "--------- Testing in global mode ---------\n",
      "size of test_loader dataset:  10000\n",
      "\n",
      "Test set: Avg. loss: 0.0017, Accuracy: 6115/10000 (61%)\n",
      "\n",
      "accuracy after update is  61.15\n",
      "For layer idx 20, accuracy of the updated model is 61.15\n",
      "using independent method\n",
      "model parameters are \n",
      " [torch.Size([64, 3, 3, 3]), torch.Size([64, 64, 3, 3]), torch.Size([64, 64, 3, 3]), torch.Size([64, 64, 3, 3]), torch.Size([64, 64, 3, 3]), torch.Size([128, 64, 3, 3]), torch.Size([128, 128, 3, 3]), torch.Size([128, 64, 1, 1]), torch.Size([128, 128, 3, 3]), torch.Size([128, 128, 3, 3]), torch.Size([256, 128, 3, 3]), torch.Size([256, 256, 3, 3]), torch.Size([256, 128, 1, 1]), torch.Size([256, 256, 3, 3]), torch.Size([256, 256, 3, 3]), torch.Size([512, 256, 3, 3]), torch.Size([512, 512, 3, 3]), torch.Size([512, 256, 1, 1]), torch.Size([512, 512, 3, 3]), torch.Size([512, 512, 3, 3]), torch.Size([10, 512])]\n",
      "\n",
      "--------- Testing in global mode ---------\n",
      "size of test_loader dataset:  10000\n",
      "\n",
      "Test set: Avg. loss: 0.0023, Accuracy: 996/10000 (10%)\n",
      "\n",
      "len of model parameters and avg aligned layers is  21 21\n",
      "len of model_state_dict is  21\n",
      "len of param_list is  21\n",
      "\n",
      "--------- Testing in global mode ---------\n",
      "size of test_loader dataset:  10000\n",
      "\n",
      "Test set: Avg. loss: 0.0016, Accuracy: 6937/10000 (69%)\n",
      "\n",
      "===============================================\n",
      "===============================================\n",
      "After Ensemble,acc: 69.37 loss 0.0016472686409950256\n",
      "===============================================\n",
      "===============================================\n",
      "Timer ends\n",
      "Time taken for geometric ensembling is 134.14353914558887 seconds\n",
      "------- Prediction based ensembling -------\n",
      "\n",
      "Test set: Avg. loss: 0.0002, Accuracy: 9375/10000 (94%)\n",
      "\n",
      "------- Naive ensembling of weights -------\n",
      "[torch.Size([64, 3, 3, 3]), torch.Size([64, 3, 3, 3])]\n",
      "torch.Size([64, 3, 3, 3])\n",
      "[torch.Size([64, 64, 3, 3]), torch.Size([64, 64, 3, 3])]\n",
      "torch.Size([64, 64, 3, 3])\n",
      "[torch.Size([64, 64, 3, 3]), torch.Size([64, 64, 3, 3])]\n",
      "torch.Size([64, 64, 3, 3])\n",
      "[torch.Size([64, 64, 3, 3]), torch.Size([64, 64, 3, 3])]\n",
      "torch.Size([64, 64, 3, 3])\n",
      "[torch.Size([64, 64, 3, 3]), torch.Size([64, 64, 3, 3])]\n",
      "torch.Size([64, 64, 3, 3])\n",
      "[torch.Size([128, 64, 3, 3]), torch.Size([128, 64, 3, 3])]\n",
      "torch.Size([128, 64, 3, 3])\n",
      "[torch.Size([128, 128, 3, 3]), torch.Size([128, 128, 3, 3])]\n",
      "torch.Size([128, 128, 3, 3])\n",
      "[torch.Size([128, 64, 1, 1]), torch.Size([128, 64, 1, 1])]\n",
      "torch.Size([128, 64, 1, 1])\n",
      "[torch.Size([128, 128, 3, 3]), torch.Size([128, 128, 3, 3])]\n",
      "torch.Size([128, 128, 3, 3])\n",
      "[torch.Size([128, 128, 3, 3]), torch.Size([128, 128, 3, 3])]\n",
      "torch.Size([128, 128, 3, 3])\n",
      "[torch.Size([256, 128, 3, 3]), torch.Size([256, 128, 3, 3])]\n",
      "torch.Size([256, 128, 3, 3])\n",
      "[torch.Size([256, 256, 3, 3]), torch.Size([256, 256, 3, 3])]\n",
      "torch.Size([256, 256, 3, 3])\n",
      "[torch.Size([256, 128, 1, 1]), torch.Size([256, 128, 1, 1])]\n",
      "torch.Size([256, 128, 1, 1])\n",
      "[torch.Size([256, 256, 3, 3]), torch.Size([256, 256, 3, 3])]\n",
      "torch.Size([256, 256, 3, 3])\n",
      "[torch.Size([256, 256, 3, 3]), torch.Size([256, 256, 3, 3])]\n",
      "torch.Size([256, 256, 3, 3])\n",
      "[torch.Size([512, 256, 3, 3]), torch.Size([512, 256, 3, 3])]\n",
      "torch.Size([512, 256, 3, 3])\n",
      "[torch.Size([512, 512, 3, 3]), torch.Size([512, 512, 3, 3])]\n",
      "torch.Size([512, 512, 3, 3])\n",
      "[torch.Size([512, 256, 1, 1]), torch.Size([512, 256, 1, 1])]\n",
      "torch.Size([512, 256, 1, 1])\n",
      "[torch.Size([512, 512, 3, 3]), torch.Size([512, 512, 3, 3])]\n",
      "torch.Size([512, 512, 3, 3])\n",
      "[torch.Size([512, 512, 3, 3]), torch.Size([512, 512, 3, 3])]\n",
      "torch.Size([512, 512, 3, 3])\n",
      "[torch.Size([10, 512]), torch.Size([10, 512])]\n",
      "torch.Size([10, 512])\n",
      "model parameters are \n",
      " [torch.Size([64, 3, 3, 3]), torch.Size([64, 64, 3, 3]), torch.Size([64, 64, 3, 3]), torch.Size([64, 64, 3, 3]), torch.Size([64, 64, 3, 3]), torch.Size([128, 64, 3, 3]), torch.Size([128, 128, 3, 3]), torch.Size([128, 64, 1, 1]), torch.Size([128, 128, 3, 3]), torch.Size([128, 128, 3, 3]), torch.Size([256, 128, 3, 3]), torch.Size([256, 256, 3, 3]), torch.Size([256, 128, 1, 1]), torch.Size([256, 256, 3, 3]), torch.Size([256, 256, 3, 3]), torch.Size([512, 256, 3, 3]), torch.Size([512, 512, 3, 3]), torch.Size([512, 256, 1, 1]), torch.Size([512, 512, 3, 3]), torch.Size([512, 512, 3, 3]), torch.Size([10, 512])]\n",
      "\n",
      "--------- Testing in global mode ---------\n",
      "size of test_loader dataset:  10000\n",
      "\n",
      "Test set: Avg. loss: 0.0023, Accuracy: 997/10000 (10%)\n",
      "\n",
      "\n",
      "--------- Testing in global mode ---------\n",
      "size of test_loader dataset:  10000\n",
      "\n",
      "Test set: Avg. loss: 0.0023, Accuracy: 1882/10000 (19%)\n",
      "\n",
      "-------- Retraining the models ---------\n",
      "Retraining model :  model_0\n",
      "num_epochs---------------+++))))))+++ 300\n",
      "num_epochs---------------+++))))))+++ 300\n",
      "lr is  0.1\n",
      "number of epochs would be  30\n",
      "num_epochs-------------- 30\n",
      "Epoch 000\n",
      "/root/miniconda3/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:129: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\"Detected call of `lr_scheduler.step()` before `optimizer.step()`. \"\n",
      "/root/miniconda3/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:154: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "accuracy: {'epoch': 0, 'value': 0.8158599999999998} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 0, 'value': 0.5655392816902691} ({'split': 'train'})\n",
      "accuracy: {'epoch': 0, 'value': 0.7764000296592712} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 0, 'value': 0.81679967045784} ({'split': 'test'})\n",
      "Epoch 001\n",
      "accuracy: {'epoch': 1, 'value': 0.8484800000000001} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 1, 'value': 0.4552057174682619} ({'split': 'train'})\n",
      "accuracy: {'epoch': 1, 'value': 0.7950000345706938} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 1, 'value': 0.6488913238048554} ({'split': 'test'})\n",
      "Epoch 002\n",
      "accuracy: {'epoch': 2, 'value': 0.8708599999999999} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 2, 'value': 0.3941639728355409} ({'split': 'train'})\n",
      "accuracy: {'epoch': 2, 'value': 0.792100042104721} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 2, 'value': 0.7455640435218811} ({'split': 'test'})\n",
      "Epoch 003\n",
      "accuracy: {'epoch': 3, 'value': 0.8720199999999998} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 3, 'value': 0.39146831775665275} ({'split': 'train'})\n",
      "accuracy: {'epoch': 3, 'value': 0.7908000469207762} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 3, 'value': 0.6828693628311157} ({'split': 'test'})\n",
      "Epoch 004\n",
      "accuracy: {'epoch': 4, 'value': 0.8725599999999998} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 4, 'value': 0.39087280989646933} ({'split': 'train'})\n",
      "accuracy: {'epoch': 4, 'value': 0.7808000326156617} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 4, 'value': 0.6836070358753205} ({'split': 'test'})\n",
      "Epoch 005\n",
      "accuracy: {'epoch': 5, 'value': 0.8757000000000001} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 5, 'value': 0.38397136266708337} ({'split': 'train'})\n",
      "accuracy: {'epoch': 5, 'value': 0.8025000393390656} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 5, 'value': 0.6389950454235076} ({'split': 'test'})\n",
      "Epoch 006\n",
      "accuracy: {'epoch': 6, 'value': 0.8755200000000005} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 6, 'value': 0.38382136834144576} ({'split': 'train'})\n",
      "accuracy: {'epoch': 6, 'value': 0.7672000348567963} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 6, 'value': 0.8218771934509277} ({'split': 'test'})\n",
      "Epoch 007\n",
      "accuracy: {'epoch': 7, 'value': 0.8596600000000009} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 7, 'value': 0.4344207754993437} ({'split': 'train'})\n",
      "accuracy: {'epoch': 7, 'value': 0.7710000336170196} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 7, 'value': 0.8064086377620696} ({'split': 'test'})\n",
      "Epoch 008\n",
      "^C\n",
      "Traceback (most recent call last):\n",
      "  File \"main.py\", line 460, in <module>\n",
      "    _, best_retrain_acc = routines.retrain_models(args, [*original_models, geometric_model, naive_model], retrain_loader, test_loader, config, tensorboard_obj=tensorboard_obj, initial_acc=initial_acc, nicks=nicks)\n",
      "  File \"/root/autodl-tmp/OTfusion_pruning/otfusion/routines.py\", line 394, in retrain_models\n",
      "    retrained_network, acc = cifar_train.get_retrained_model(args, retrain_loader, test_loader, old_networks[i], config, output_root_dir, tensorboard_obj=tensorboard_obj, nick=nick, start_acc=initial_acc[i])\n",
      "  File \"/root/autodl-tmp/OTfusion_pruning/otfusion/./cifar/train.py\", line 333, in get_retrained_model\n",
      "    best_acc,model = main(config, output_dir, args.gpu_id, pretrained_model=old_network, pretrained_dataset=(train_loader, test_loader), tensorboard_obj=tensorboard_obj,return_model=True)\n",
      "  File \"/root/autodl-tmp/OTfusion_pruning/otfusion/./cifar/train.py\", line 75, in main\n",
      "    loss.backward()\n",
      "  File \"/root/miniconda3/lib/python3.8/site-packages/torch/_tensor.py\", line 307, in backward\n",
      "    torch.autograd.backward(self, gradient, retain_graph, create_graph, inputs=inputs)\n",
      "  File \"/root/miniconda3/lib/python3.8/site-packages/torch/autograd/__init__.py\", line 154, in backward\n",
      "    Variable._execution_engine.run_backward(\n",
      "KeyboardInterrupt\n"
     ]
    }
   ],
   "source": [
    "!python main.py --gpu-id 0 --model-name resnet18_nobias_nobn --n-epochs 0 --save-result-file sample.csv \\\n",
    "--sweep-name exp_sample --exact --correction --ground-metric euclidean --weight-stats \\\n",
    "--activation-histograms --activation-mode raw --geom-ensemble-type acts --sweep-id 21 \\\n",
    "--act-num-samples 400 --ground-metric-normalize none --activation-seed {activation_seed} --prelu-acts --recheck-acc \\\n",
    "--load-models ./resnet_models/ --ckpt-type best --past-correction --not-squared  --dataset Cifar10 \\\n",
    "--handle-skips\\\n",
    "--learning-rate 0.004\\\n",
    "--momentum 0.5\\\n",
    "--batch-size-train 64 --experiment-dir {experiment_dir}\\\n",
    "--prunint-rate {prunint_rate}\\\n",
    "--run-mode {run_mode}\\\n",
    "--load-model-dir {load_model_dir}\\\n",
    "--to-download\\\n",
    "--retrain 30\\\n",
    "--eval-aligned\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ec765092-4eca-4996-bc03-1996d88f4dd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#实验3: prune many times\n",
    "#剪枝实验3:prune times without finetune\n",
    "activation_seed=21\n",
    "experiment_dir=\"experiment10\"\n",
    "prunint_rate=0.4\n",
    "run_mode='prune_times'\n",
    "load_model_dir='./resnet_models'\n",
    "#68.04"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2261e23e-80e7-4852-b3d3-2e373666f7fc",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------- Setting up parameters -------\n",
      "dumping parameters at  /root/autodl-tmp/OTfusion_pruning/otfusion/exp_sample/configurations\n",
      "The parameters are: \n",
      " Namespace(act_bug=False, act_num_samples=200, activation_histograms=True, activation_mode='raw', activation_seed=21, alpha=0.7, baseroot='/root/autodl-tmp/OTfusion_pruning/otfusion', batch_size_test=1000, batch_size_train=64, center_acts=False, choice='0 2 4 6 8', cifar_style_data=False, ckpt_type='best', clip_gm=False, clip_max=5, clip_min=0, config_dir='/root/autodl-tmp/OTfusion_pruning/otfusion/exp_sample/configurations', config_file=None, correction=True, csv_dir='/root/autodl-tmp/OTfusion_pruning/otfusion/exp_sample/csv', dataset='Cifar10', debug=False, deprecated=None, deterministic=False, diff_init=False, disable_bias=True, dist_epochs=60, dist_normalize=False, dump_final_models=False, dump_model=False, enable_dropout=False, ensemble_step=0.5, eval_aligned=True, exact=True, exp_name='exp_2023-09-17_16-42-46_966336', experiment_dir='experiment10', finetunetimes_benchmark=0, geom_ensemble_type='acts', gpu_id=0, gromov=False, gromov_loss='square_loss', ground_metric='euclidean', ground_metric_eff=False, ground_metric_normalize='none', handle_skips=True, importance=None, learning_rate=0.004, load_model_dir='./resnet_models', load_models='./resnet_models/', log_interval=100, model_name='resnet18_nobias_nobn', momentum=0.5, n_epochs=10, no_random_trainloaders=False, normalize_acts=False, normalize_wts=False, not_squared=True, num_hidden_nodes=400, num_hidden_nodes1=400, num_hidden_nodes2=200, num_hidden_nodes3=100, num_hidden_nodes4=50, num_models=2, options_type='generic', partial_reshape=False, partition_dataloader=-1, partition_type='labels', past_correction=True, personal_class_idx=9, personal_split_frac=0.1, pool_acts=False, pool_relu=False, prediction_wts=False, prelu_acts=True, print_distances=False, proper_marginals=False, prune_times=1, prunint_rate=0.4, recheck_acc=True, recheck_cifar=False, reg=0.01, reg_m=0.001, reinit_trainloaders=False, result_dir='/root/autodl-tmp/OTfusion_pruning/otfusion/exp_sample/results', retrain=30, retrain_avg_only=False, retrain_geometric_only=False, retrain_lr_decay=-1, retrain_lr_decay_epochs=None, retrain_lr_decay_factor=None, retrain_seed=-1, rootdir='/root/autodl-tmp/OTfusion_pruning/otfusion/exp_sample', run_mode='prune_times', same_model=-1, save_result_file='sample.csv', second_model_name=None, sinkhorn_type='normal', skip_last_layer=False, skip_last_layer_type='average', skip_personal_idx=False, skip_retrain=-1, softmax_temperature=1, standardize_acts=False, sweep_id=21, sweep_name='exp_sample', temperature=20, tensorboard=False, tensorboard_root='./tensorboard', timestamp='2023-09-17_16-42-46_966336', tmap_stats=False, to_download=True, transform_acts=False, unbalanced=False, update_acts=False, weight_stats=True, width_ratio=1)\n",
      "refactored get_config\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "model parameters are \n",
      " [torch.Size([64, 3, 3, 3]), torch.Size([64, 64, 3, 3]), torch.Size([64, 64, 3, 3]), torch.Size([64, 64, 3, 3]), torch.Size([64, 64, 3, 3]), torch.Size([128, 64, 3, 3]), torch.Size([128, 128, 3, 3]), torch.Size([128, 64, 1, 1]), torch.Size([128, 128, 3, 3]), torch.Size([128, 128, 3, 3]), torch.Size([256, 128, 3, 3]), torch.Size([256, 256, 3, 3]), torch.Size([256, 128, 1, 1]), torch.Size([256, 256, 3, 3]), torch.Size([256, 256, 3, 3]), torch.Size([512, 256, 3, 3]), torch.Size([512, 512, 3, 3]), torch.Size([512, 256, 1, 1]), torch.Size([512, 512, 3, 3]), torch.Size([512, 512, 3, 3]), torch.Size([10, 512])]\n",
      "model parameters are \n",
      " [torch.Size([64, 3, 3, 3]), torch.Size([64, 64, 3, 3]), torch.Size([64, 64, 3, 3]), torch.Size([64, 64, 3, 3]), torch.Size([64, 64, 3, 3]), torch.Size([128, 64, 3, 3]), torch.Size([128, 128, 3, 3]), torch.Size([128, 64, 1, 1]), torch.Size([128, 128, 3, 3]), torch.Size([128, 128, 3, 3]), torch.Size([256, 128, 3, 3]), torch.Size([256, 256, 3, 3]), torch.Size([256, 128, 1, 1]), torch.Size([256, 256, 3, 3]), torch.Size([256, 256, 3, 3]), torch.Size([512, 256, 3, 3]), torch.Size([512, 512, 3, 3]), torch.Size([512, 256, 1, 1]), torch.Size([512, 512, 3, 3]), torch.Size([512, 512, 3, 3]), torch.Size([10, 512])]\n",
      "model parameters are \n",
      " [torch.Size([64, 3, 3, 3]), torch.Size([64, 64, 3, 3]), torch.Size([64, 64, 3, 3]), torch.Size([64, 64, 3, 3]), torch.Size([64, 64, 3, 3]), torch.Size([128, 64, 3, 3]), torch.Size([128, 128, 3, 3]), torch.Size([128, 64, 1, 1]), torch.Size([128, 128, 3, 3]), torch.Size([128, 128, 3, 3]), torch.Size([256, 128, 3, 3]), torch.Size([256, 256, 3, 3]), torch.Size([256, 128, 1, 1]), torch.Size([256, 256, 3, 3]), torch.Size([256, 256, 3, 3]), torch.Size([512, 256, 3, 3]), torch.Size([512, 512, 3, 3]), torch.Size([512, 256, 1, 1]), torch.Size([512, 512, 3, 3]), torch.Size([512, 512, 3, 3]), torch.Size([10, 512])]\n",
      "model parameters are \n",
      " [torch.Size([64, 3, 3, 3]), torch.Size([64, 64, 3, 3]), torch.Size([64, 64, 3, 3]), torch.Size([64, 64, 3, 3]), torch.Size([64, 64, 3, 3]), torch.Size([128, 64, 3, 3]), torch.Size([128, 128, 3, 3]), torch.Size([128, 64, 1, 1]), torch.Size([128, 128, 3, 3]), torch.Size([128, 128, 3, 3]), torch.Size([256, 128, 3, 3]), torch.Size([256, 256, 3, 3]), torch.Size([256, 128, 1, 1]), torch.Size([256, 256, 3, 3]), torch.Size([256, 256, 3, 3]), torch.Size([512, 256, 3, 3]), torch.Size([512, 512, 3, 3]), torch.Size([512, 256, 1, 1]), torch.Size([512, 512, 3, 3]), torch.Size([512, 512, 3, 3]), torch.Size([10, 512])]\n",
      "enter prune_times, the times is 1,ratio:0.4\n",
      "Train Epoch: 1 [0/50000 (0%)]\tLoss: 0.014191\n",
      "Train Epoch: 1 [6400/50000 (13%)]\tLoss: 0.005840\n",
      "Train Epoch: 1 [12800/50000 (26%)]\tLoss: 0.021307\n",
      "Train Epoch: 1 [19200/50000 (38%)]\tLoss: 0.007613\n",
      "Train Epoch: 1 [25600/50000 (51%)]\tLoss: 0.006376\n",
      "Train Epoch: 1 [32000/50000 (64%)]\tLoss: 0.006387\n",
      "Train Epoch: 1 [38400/50000 (77%)]\tLoss: 0.015469\n",
      "Train Epoch: 1 [44800/50000 (90%)]\tLoss: 0.005406\n",
      "\n",
      "--------- Testing in global mode ---------\n",
      "size of test_loader dataset:  10000\n",
      "\n",
      "Test set: Avg. loss: 0.0002, Accuracy: 9281/10000 (93%)\n",
      "\n",
      "Train Epoch: 2 [0/50000 (0%)]\tLoss: 0.004913\n",
      "Train Epoch: 2 [6400/50000 (13%)]\tLoss: 0.002051\n",
      "Train Epoch: 2 [12800/50000 (26%)]\tLoss: 0.002218\n",
      "Train Epoch: 2 [19200/50000 (38%)]\tLoss: 0.005148\n",
      "Train Epoch: 2 [25600/50000 (51%)]\tLoss: 0.002838\n",
      "Train Epoch: 2 [32000/50000 (64%)]\tLoss: 0.003794\n",
      "Train Epoch: 2 [38400/50000 (77%)]\tLoss: 0.003100\n",
      "Train Epoch: 2 [44800/50000 (90%)]\tLoss: 0.001293\n",
      "\n",
      "--------- Testing in global mode ---------\n",
      "size of test_loader dataset:  10000\n",
      "\n",
      "Test set: Avg. loss: 0.0003, Accuracy: 9289/10000 (93%)\n",
      "\n",
      "Train Epoch: 3 [0/50000 (0%)]\tLoss: 0.002778\n",
      "Train Epoch: 3 [6400/50000 (13%)]\tLoss: 0.002570\n",
      "Train Epoch: 3 [12800/50000 (26%)]\tLoss: 0.002029\n",
      "Train Epoch: 3 [19200/50000 (38%)]\tLoss: 0.004439\n",
      "Train Epoch: 3 [25600/50000 (51%)]\tLoss: 0.002093\n",
      "Train Epoch: 3 [32000/50000 (64%)]\tLoss: 0.001174\n",
      "Train Epoch: 3 [38400/50000 (77%)]\tLoss: 0.008421\n",
      "Train Epoch: 3 [44800/50000 (90%)]\tLoss: 0.001424\n",
      "\n",
      "--------- Testing in global mode ---------\n",
      "size of test_loader dataset:  10000\n",
      "\n",
      "Test set: Avg. loss: 0.0003, Accuracy: 9285/10000 (93%)\n",
      "\n",
      "Train Epoch: 4 [0/50000 (0%)]\tLoss: 0.001972\n",
      "Train Epoch: 4 [6400/50000 (13%)]\tLoss: 0.002693\n",
      "Train Epoch: 4 [12800/50000 (26%)]\tLoss: 0.002113\n",
      "Train Epoch: 4 [19200/50000 (38%)]\tLoss: 0.004430\n",
      "Train Epoch: 4 [25600/50000 (51%)]\tLoss: 0.000988\n",
      "Train Epoch: 4 [32000/50000 (64%)]\tLoss: 0.003185\n",
      "Train Epoch: 4 [38400/50000 (77%)]\tLoss: 0.002270\n",
      "Train Epoch: 4 [44800/50000 (90%)]\tLoss: 0.003198\n",
      "\n",
      "--------- Testing in global mode ---------\n",
      "size of test_loader dataset:  10000\n",
      "\n",
      "Test set: Avg. loss: 0.0003, Accuracy: 9280/10000 (93%)\n",
      "\n",
      "Train Epoch: 5 [0/50000 (0%)]\tLoss: 0.002018\n",
      "Train Epoch: 5 [6400/50000 (13%)]\tLoss: 0.000776\n",
      "Train Epoch: 5 [12800/50000 (26%)]\tLoss: 0.003188\n",
      "Train Epoch: 5 [19200/50000 (38%)]\tLoss: 0.005744\n",
      "Train Epoch: 5 [25600/50000 (51%)]\tLoss: 0.007395\n",
      "Train Epoch: 5 [32000/50000 (64%)]\tLoss: 0.003487\n",
      "Train Epoch: 5 [38400/50000 (77%)]\tLoss: 0.000695\n",
      "Train Epoch: 5 [44800/50000 (90%)]\tLoss: 0.002006\n",
      "\n",
      "--------- Testing in global mode ---------\n",
      "size of test_loader dataset:  10000\n",
      "\n",
      "Test set: Avg. loss: 0.0003, Accuracy: 9288/10000 (93%)\n",
      "\n",
      "Train Epoch: 6 [0/50000 (0%)]\tLoss: 0.005032\n",
      "Train Epoch: 6 [6400/50000 (13%)]\tLoss: 0.001747\n",
      "Train Epoch: 6 [12800/50000 (26%)]\tLoss: 0.001790\n",
      "Train Epoch: 6 [19200/50000 (38%)]\tLoss: 0.002324\n",
      "Train Epoch: 6 [25600/50000 (51%)]\tLoss: 0.000815\n",
      "Train Epoch: 6 [32000/50000 (64%)]\tLoss: 0.001364\n",
      "Train Epoch: 6 [38400/50000 (77%)]\tLoss: 0.000669\n",
      "Train Epoch: 6 [44800/50000 (90%)]\tLoss: 0.002121\n",
      "\n",
      "--------- Testing in global mode ---------\n",
      "size of test_loader dataset:  10000\n",
      "\n",
      "Test set: Avg. loss: 0.0003, Accuracy: 9288/10000 (93%)\n",
      "\n",
      "Train Epoch: 7 [0/50000 (0%)]\tLoss: 0.002623\n",
      "Train Epoch: 7 [6400/50000 (13%)]\tLoss: 0.000585\n",
      "Train Epoch: 7 [12800/50000 (26%)]\tLoss: 0.001300\n",
      "Train Epoch: 7 [19200/50000 (38%)]\tLoss: 0.001209\n",
      "Train Epoch: 7 [25600/50000 (51%)]\tLoss: 0.003606\n",
      "Train Epoch: 7 [32000/50000 (64%)]\tLoss: 0.000140\n",
      "Train Epoch: 7 [38400/50000 (77%)]\tLoss: 0.001779\n",
      "Train Epoch: 7 [44800/50000 (90%)]\tLoss: 0.001033\n",
      "\n",
      "--------- Testing in global mode ---------\n",
      "size of test_loader dataset:  10000\n",
      "\n",
      "Test set: Avg. loss: 0.0003, Accuracy: 9287/10000 (93%)\n",
      "\n",
      "Train Epoch: 8 [0/50000 (0%)]\tLoss: 0.003799\n",
      "Train Epoch: 8 [6400/50000 (13%)]\tLoss: 0.000607\n",
      "Train Epoch: 8 [12800/50000 (26%)]\tLoss: 0.001338\n",
      "Train Epoch: 8 [19200/50000 (38%)]\tLoss: 0.001089\n",
      "Train Epoch: 8 [25600/50000 (51%)]\tLoss: 0.000377\n",
      "Train Epoch: 8 [32000/50000 (64%)]\tLoss: 0.000700\n",
      "Train Epoch: 8 [38400/50000 (77%)]\tLoss: 0.000767\n",
      "Train Epoch: 8 [44800/50000 (90%)]\tLoss: 0.001201\n",
      "\n",
      "--------- Testing in global mode ---------\n",
      "size of test_loader dataset:  10000\n",
      "\n",
      "Test set: Avg. loss: 0.0003, Accuracy: 9289/10000 (93%)\n",
      "\n",
      "Train Epoch: 9 [0/50000 (0%)]\tLoss: 0.000203\n",
      "Train Epoch: 9 [6400/50000 (13%)]\tLoss: 0.000637\n",
      "Train Epoch: 9 [12800/50000 (26%)]\tLoss: 0.003319\n",
      "Train Epoch: 9 [19200/50000 (38%)]\tLoss: 0.000294\n",
      "Train Epoch: 9 [25600/50000 (51%)]\tLoss: 0.000147\n",
      "Train Epoch: 9 [32000/50000 (64%)]\tLoss: 0.003248\n",
      "Train Epoch: 9 [38400/50000 (77%)]\tLoss: 0.001348\n",
      "Train Epoch: 9 [44800/50000 (90%)]\tLoss: 0.000640\n",
      "\n",
      "--------- Testing in global mode ---------\n",
      "size of test_loader dataset:  10000\n",
      "\n",
      "Test set: Avg. loss: 0.0003, Accuracy: 9286/10000 (93%)\n",
      "\n",
      "Train Epoch: 10 [0/50000 (0%)]\tLoss: 0.001014\n",
      "Train Epoch: 10 [6400/50000 (13%)]\tLoss: 0.000981\n",
      "Train Epoch: 10 [12800/50000 (26%)]\tLoss: 0.000650\n",
      "Train Epoch: 10 [19200/50000 (38%)]\tLoss: 0.000232\n",
      "Train Epoch: 10 [25600/50000 (51%)]\tLoss: 0.001628\n",
      "Train Epoch: 10 [32000/50000 (64%)]\tLoss: 0.000510\n",
      "Train Epoch: 10 [38400/50000 (77%)]\tLoss: 0.000557\n",
      "Train Epoch: 10 [44800/50000 (90%)]\tLoss: 0.002283\n",
      "\n",
      "--------- Testing in global mode ---------\n",
      "size of test_loader dataset:  10000\n",
      "\n",
      "Test set: Avg. loss: 0.0003, Accuracy: 9283/10000 (93%)\n",
      "\n",
      "Train Epoch: 1 [0/50000 (0%)]\tLoss: 0.039944\n",
      "Train Epoch: 1 [6400/50000 (13%)]\tLoss: 0.013408\n",
      "Train Epoch: 1 [12800/50000 (26%)]\tLoss: 0.006304\n",
      "Train Epoch: 1 [19200/50000 (38%)]\tLoss: 0.013216\n",
      "Train Epoch: 1 [25600/50000 (51%)]\tLoss: 0.022279\n",
      "Train Epoch: 1 [32000/50000 (64%)]\tLoss: 0.007812\n",
      "Train Epoch: 1 [38400/50000 (77%)]\tLoss: 0.008065\n",
      "Train Epoch: 1 [44800/50000 (90%)]\tLoss: 0.004964\n",
      "\n",
      "--------- Testing in global mode ---------\n",
      "size of test_loader dataset:  10000\n",
      "\n",
      "Test set: Avg. loss: 0.0003, Accuracy: 9299/10000 (93%)\n",
      "\n",
      "Train Epoch: 2 [0/50000 (0%)]\tLoss: 0.015261\n",
      "Train Epoch: 2 [6400/50000 (13%)]\tLoss: 0.002277\n",
      "Train Epoch: 2 [12800/50000 (26%)]\tLoss: 0.003746\n",
      "Train Epoch: 2 [19200/50000 (38%)]\tLoss: 0.007022\n",
      "Train Epoch: 2 [25600/50000 (51%)]\tLoss: 0.002538\n",
      "Train Epoch: 2 [32000/50000 (64%)]\tLoss: 0.003291\n",
      "Train Epoch: 2 [38400/50000 (77%)]\tLoss: 0.006487\n",
      "Train Epoch: 2 [44800/50000 (90%)]\tLoss: 0.002105\n",
      "\n",
      "--------- Testing in global mode ---------\n",
      "size of test_loader dataset:  10000\n",
      "\n",
      "Test set: Avg. loss: 0.0003, Accuracy: 9306/10000 (93%)\n",
      "\n",
      "Train Epoch: 3 [0/50000 (0%)]\tLoss: 0.002422\n",
      "Train Epoch: 3 [6400/50000 (13%)]\tLoss: 0.004363\n",
      "Train Epoch: 3 [12800/50000 (26%)]\tLoss: 0.001563\n",
      "Train Epoch: 3 [19200/50000 (38%)]\tLoss: 0.000735\n",
      "Train Epoch: 3 [25600/50000 (51%)]\tLoss: 0.003873\n",
      "Train Epoch: 3 [32000/50000 (64%)]\tLoss: 0.006208\n",
      "Train Epoch: 3 [38400/50000 (77%)]\tLoss: 0.000486\n",
      "Train Epoch: 3 [44800/50000 (90%)]\tLoss: 0.004057\n",
      "\n",
      "--------- Testing in global mode ---------\n",
      "size of test_loader dataset:  10000\n",
      "\n",
      "Test set: Avg. loss: 0.0003, Accuracy: 9307/10000 (93%)\n",
      "\n",
      "Train Epoch: 4 [0/50000 (0%)]\tLoss: 0.002587\n",
      "Train Epoch: 4 [6400/50000 (13%)]\tLoss: 0.004373\n",
      "Train Epoch: 4 [12800/50000 (26%)]\tLoss: 0.002550\n",
      "Train Epoch: 4 [19200/50000 (38%)]\tLoss: 0.003143\n",
      "Train Epoch: 4 [25600/50000 (51%)]\tLoss: 0.001556\n",
      "Train Epoch: 4 [32000/50000 (64%)]\tLoss: 0.003311\n",
      "Train Epoch: 4 [38400/50000 (77%)]\tLoss: 0.001834\n",
      "Train Epoch: 4 [44800/50000 (90%)]\tLoss: 0.001507\n",
      "\n",
      "--------- Testing in global mode ---------\n",
      "size of test_loader dataset:  10000\n",
      "\n",
      "Test set: Avg. loss: 0.0003, Accuracy: 9305/10000 (93%)\n",
      "\n",
      "Train Epoch: 5 [0/50000 (0%)]\tLoss: 0.004027\n",
      "Train Epoch: 5 [6400/50000 (13%)]\tLoss: 0.003805\n",
      "Train Epoch: 5 [12800/50000 (26%)]\tLoss: 0.001424\n",
      "Train Epoch: 5 [19200/50000 (38%)]\tLoss: 0.000823\n",
      "Train Epoch: 5 [25600/50000 (51%)]\tLoss: 0.002724\n",
      "Train Epoch: 5 [32000/50000 (64%)]\tLoss: 0.002813\n",
      "Train Epoch: 5 [38400/50000 (77%)]\tLoss: 0.005621\n",
      "Train Epoch: 5 [44800/50000 (90%)]\tLoss: 0.001676\n",
      "\n",
      "--------- Testing in global mode ---------\n",
      "size of test_loader dataset:  10000\n",
      "\n",
      "Test set: Avg. loss: 0.0003, Accuracy: 9301/10000 (93%)\n",
      "\n",
      "Train Epoch: 6 [0/50000 (0%)]\tLoss: 0.001252\n",
      "Train Epoch: 6 [6400/50000 (13%)]\tLoss: 0.003361\n",
      "Train Epoch: 6 [12800/50000 (26%)]\tLoss: 0.007477\n",
      "Train Epoch: 6 [19200/50000 (38%)]\tLoss: 0.001505\n",
      "Train Epoch: 6 [25600/50000 (51%)]\tLoss: 0.002062\n",
      "Train Epoch: 6 [32000/50000 (64%)]\tLoss: 0.002039\n",
      "Train Epoch: 6 [38400/50000 (77%)]\tLoss: 0.000880\n",
      "Train Epoch: 6 [44800/50000 (90%)]\tLoss: 0.005699\n",
      "\n",
      "--------- Testing in global mode ---------\n",
      "size of test_loader dataset:  10000\n",
      "\n",
      "Test set: Avg. loss: 0.0003, Accuracy: 9304/10000 (93%)\n",
      "\n",
      "Train Epoch: 7 [0/50000 (0%)]\tLoss: 0.001960\n",
      "Train Epoch: 7 [6400/50000 (13%)]\tLoss: 0.001077\n",
      "Train Epoch: 7 [12800/50000 (26%)]\tLoss: 0.001751\n",
      "Train Epoch: 7 [19200/50000 (38%)]\tLoss: 0.001457\n",
      "Train Epoch: 7 [25600/50000 (51%)]\tLoss: 0.003689\n",
      "Train Epoch: 7 [32000/50000 (64%)]\tLoss: 0.002009\n",
      "Train Epoch: 7 [38400/50000 (77%)]\tLoss: 0.000035\n",
      "Train Epoch: 7 [44800/50000 (90%)]\tLoss: 0.000147\n",
      "\n",
      "--------- Testing in global mode ---------\n",
      "size of test_loader dataset:  10000\n",
      "\n",
      "Test set: Avg. loss: 0.0003, Accuracy: 9302/10000 (93%)\n",
      "\n",
      "Train Epoch: 8 [0/50000 (0%)]\tLoss: 0.002812\n",
      "Train Epoch: 8 [6400/50000 (13%)]\tLoss: 0.001735\n",
      "Train Epoch: 8 [12800/50000 (26%)]\tLoss: 0.002386\n",
      "Train Epoch: 8 [19200/50000 (38%)]\tLoss: 0.001663\n",
      "Train Epoch: 8 [25600/50000 (51%)]\tLoss: 0.000885\n",
      "Train Epoch: 8 [32000/50000 (64%)]\tLoss: 0.000827\n",
      "Train Epoch: 8 [38400/50000 (77%)]\tLoss: 0.002378\n",
      "Train Epoch: 8 [44800/50000 (90%)]\tLoss: 0.001584\n",
      "\n",
      "--------- Testing in global mode ---------\n",
      "size of test_loader dataset:  10000\n",
      "\n",
      "Test set: Avg. loss: 0.0003, Accuracy: 9304/10000 (93%)\n",
      "\n",
      "Train Epoch: 9 [0/50000 (0%)]\tLoss: 0.000710\n",
      "Train Epoch: 9 [6400/50000 (13%)]\tLoss: 0.000293\n",
      "Train Epoch: 9 [12800/50000 (26%)]\tLoss: 0.000943\n",
      "Train Epoch: 9 [19200/50000 (38%)]\tLoss: 0.002452\n",
      "Train Epoch: 9 [25600/50000 (51%)]\tLoss: 0.000984\n",
      "Train Epoch: 9 [32000/50000 (64%)]\tLoss: 0.000249\n",
      "Train Epoch: 9 [38400/50000 (77%)]\tLoss: 0.000482\n",
      "Train Epoch: 9 [44800/50000 (90%)]\tLoss: 0.001008\n",
      "\n",
      "--------- Testing in global mode ---------\n",
      "size of test_loader dataset:  10000\n",
      "\n",
      "Test set: Avg. loss: 0.0003, Accuracy: 9303/10000 (93%)\n",
      "\n",
      "Train Epoch: 10 [0/50000 (0%)]\tLoss: 0.001602\n",
      "Train Epoch: 10 [6400/50000 (13%)]\tLoss: 0.001072\n",
      "Train Epoch: 10 [12800/50000 (26%)]\tLoss: 0.001505\n",
      "Train Epoch: 10 [19200/50000 (38%)]\tLoss: 0.000786\n",
      "Train Epoch: 10 [25600/50000 (51%)]\tLoss: 0.000509\n",
      "Train Epoch: 10 [32000/50000 (64%)]\tLoss: 0.000556\n",
      "Train Epoch: 10 [38400/50000 (77%)]\tLoss: 0.001485\n",
      "Train Epoch: 10 [44800/50000 (90%)]\tLoss: 0.000709\n",
      "\n",
      "--------- Testing in global mode ---------\n",
      "size of test_loader dataset:  10000\n",
      "\n",
      "Test set: Avg. loss: 0.0003, Accuracy: 9299/10000 (93%)\n",
      "\n",
      "in times 0, now cumm_ratio 0.4,accuracies:[92.83, 92.99],losses:[0.00034066676795482635, 0.00034090448021888733]\n",
      "===============================================\n",
      "===============================================\n",
      "before Ensemble,acc: [92.83, 92.99] loss [0.00034066676795482635, 0.00034090448021888733]\n",
      "===============================================\n",
      "===============================================\n",
      "check zero rate for model 0\n",
      "odict_keys(['conv1.weight', 'layer1.0.conv1.weight', 'layer1.0.conv2.weight', 'layer1.1.conv1.weight', 'layer1.1.conv2.weight', 'layer2.0.conv1.weight', 'layer2.0.conv2.weight', 'layer2.0.shortcut.0.weight', 'layer2.1.conv1.weight', 'layer2.1.conv2.weight', 'layer3.0.conv1.weight', 'layer3.0.conv2.weight', 'layer3.0.shortcut.0.weight', 'layer3.1.conv1.weight', 'layer3.1.conv2.weight', 'layer4.0.conv1.weight', 'layer4.0.conv2.weight', 'layer4.0.shortcut.0.weight', 'layer4.1.conv1.weight', 'layer4.1.conv2.weight', 'linear.weight'])\n",
      "the ratio of zero for conv1.weight\n",
      "tensor(0.3999, device='cuda:0')\n",
      "the ratio of zero for layer1.0.conv1.weight\n",
      "tensor(0.4000, device='cuda:0')\n",
      "the ratio of zero for layer1.0.conv2.weight\n",
      "tensor(0.4000, device='cuda:0')\n",
      "the ratio of zero for layer1.1.conv1.weight\n",
      "tensor(0.4000, device='cuda:0')\n",
      "the ratio of zero for layer1.1.conv2.weight\n",
      "tensor(0.4000, device='cuda:0')\n",
      "the ratio of zero for layer2.0.conv1.weight\n",
      "tensor(0.4000, device='cuda:0')\n",
      "the ratio of zero for layer2.0.conv2.weight\n",
      "tensor(0.4000, device='cuda:0')\n",
      "the ratio of zero for layer2.0.shortcut.0.weight\n",
      "tensor(0.4000, device='cuda:0')\n",
      "the ratio of zero for layer2.1.conv1.weight\n",
      "tensor(0.4000, device='cuda:0')\n",
      "the ratio of zero for layer2.1.conv2.weight\n",
      "tensor(0.4000, device='cuda:0')\n",
      "the ratio of zero for layer3.0.conv1.weight\n",
      "tensor(0.4000, device='cuda:0')\n",
      "the ratio of zero for layer3.0.conv2.weight\n",
      "tensor(0.4000, device='cuda:0')\n",
      "the ratio of zero for layer3.0.shortcut.0.weight\n",
      "tensor(0.4000, device='cuda:0')\n",
      "the ratio of zero for layer3.1.conv1.weight\n",
      "tensor(0.4000, device='cuda:0')\n",
      "the ratio of zero for layer3.1.conv2.weight\n",
      "tensor(0.4000, device='cuda:0')\n",
      "the ratio of zero for layer4.0.conv1.weight\n",
      "tensor(0.4000, device='cuda:0')\n",
      "the ratio of zero for layer4.0.conv2.weight\n",
      "tensor(0.4000, device='cuda:0')\n",
      "the ratio of zero for layer4.0.shortcut.0.weight\n",
      "tensor(0.4000, device='cuda:0')\n",
      "the ratio of zero for layer4.1.conv1.weight\n",
      "tensor(0.4000, device='cuda:0')\n",
      "the ratio of zero for layer4.1.conv2.weight\n",
      "tensor(0.4000, device='cuda:0')\n",
      "the ratio of zero for linear.weight\n",
      "tensor(0.4000, device='cuda:0')\n",
      "check zero rate for model 1\n",
      "odict_keys(['conv1.weight', 'layer1.0.conv1.weight', 'layer1.0.conv2.weight', 'layer1.1.conv1.weight', 'layer1.1.conv2.weight', 'layer2.0.conv1.weight', 'layer2.0.conv2.weight', 'layer2.0.shortcut.0.weight', 'layer2.1.conv1.weight', 'layer2.1.conv2.weight', 'layer3.0.conv1.weight', 'layer3.0.conv2.weight', 'layer3.0.shortcut.0.weight', 'layer3.1.conv1.weight', 'layer3.1.conv2.weight', 'layer4.0.conv1.weight', 'layer4.0.conv2.weight', 'layer4.0.shortcut.0.weight', 'layer4.1.conv1.weight', 'layer4.1.conv2.weight', 'linear.weight'])\n",
      "the ratio of zero for conv1.weight\n",
      "tensor(0.3999, device='cuda:0')\n",
      "the ratio of zero for layer1.0.conv1.weight\n",
      "tensor(0.4000, device='cuda:0')\n",
      "the ratio of zero for layer1.0.conv2.weight\n",
      "tensor(0.4000, device='cuda:0')\n",
      "the ratio of zero for layer1.1.conv1.weight\n",
      "tensor(0.4000, device='cuda:0')\n",
      "the ratio of zero for layer1.1.conv2.weight\n",
      "tensor(0.4000, device='cuda:0')\n",
      "the ratio of zero for layer2.0.conv1.weight\n",
      "tensor(0.4000, device='cuda:0')\n",
      "the ratio of zero for layer2.0.conv2.weight\n",
      "tensor(0.4000, device='cuda:0')\n",
      "the ratio of zero for layer2.0.shortcut.0.weight\n",
      "tensor(0.4000, device='cuda:0')\n",
      "the ratio of zero for layer2.1.conv1.weight\n",
      "tensor(0.4000, device='cuda:0')\n",
      "the ratio of zero for layer2.1.conv2.weight\n",
      "tensor(0.4000, device='cuda:0')\n",
      "the ratio of zero for layer3.0.conv1.weight\n",
      "tensor(0.4000, device='cuda:0')\n",
      "the ratio of zero for layer3.0.conv2.weight\n",
      "tensor(0.4000, device='cuda:0')\n",
      "the ratio of zero for layer3.0.shortcut.0.weight\n",
      "tensor(0.4000, device='cuda:0')\n",
      "the ratio of zero for layer3.1.conv1.weight\n",
      "tensor(0.4000, device='cuda:0')\n",
      "the ratio of zero for layer3.1.conv2.weight\n",
      "tensor(0.4000, device='cuda:0')\n",
      "the ratio of zero for layer4.0.conv1.weight\n",
      "tensor(0.4000, device='cuda:0')\n",
      "the ratio of zero for layer4.0.conv2.weight\n",
      "tensor(0.4000, device='cuda:0')\n",
      "the ratio of zero for layer4.0.shortcut.0.weight\n",
      "tensor(0.4000, device='cuda:0')\n",
      "the ratio of zero for layer4.1.conv1.weight\n",
      "tensor(0.4000, device='cuda:0')\n",
      "the ratio of zero for layer4.1.conv2.weight\n",
      "tensor(0.4000, device='cuda:0')\n",
      "the ratio of zero for linear.weight\n",
      "tensor(0.4000, device='cuda:0')\n",
      "layer conv1.weight has #params  1728\n",
      "layer layer1.0.conv1.weight has #params  36864\n",
      "layer layer1.0.conv2.weight has #params  36864\n",
      "layer layer1.1.conv1.weight has #params  36864\n",
      "layer layer1.1.conv2.weight has #params  36864\n",
      "layer layer2.0.conv1.weight has #params  73728\n",
      "layer layer2.0.conv2.weight has #params  147456\n",
      "layer layer2.0.shortcut.0.weight has #params  8192\n",
      "layer layer2.1.conv1.weight has #params  147456\n",
      "layer layer2.1.conv2.weight has #params  147456\n",
      "layer layer3.0.conv1.weight has #params  294912\n",
      "layer layer3.0.conv2.weight has #params  589824\n",
      "layer layer3.0.shortcut.0.weight has #params  32768\n",
      "layer layer3.1.conv1.weight has #params  589824\n",
      "layer layer3.1.conv2.weight has #params  589824\n",
      "layer layer4.0.conv1.weight has #params  1179648\n",
      "layer layer4.0.conv2.weight has #params  2359296\n",
      "layer layer4.0.shortcut.0.weight has #params  131072\n",
      "layer layer4.1.conv1.weight has #params  2359296\n",
      "layer layer4.1.conv2.weight has #params  2359296\n",
      "layer linear.weight has #params  5120\n",
      "Activation Timer start\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "excluded\n",
      "set forward hook for layer named:  conv1\n",
      "this was continued,  bn1\n",
      "this was continued,  layer1\n",
      "this was continued,  layer1.0\n",
      "set forward hook for layer named:  layer1.0.conv1\n",
      "this was continued,  layer1.0.bn1\n",
      "set forward hook for layer named:  layer1.0.conv2\n",
      "this was continued,  layer1.0.shortcut\n",
      "this was continued,  layer1.1\n",
      "set forward hook for layer named:  layer1.1.conv1\n",
      "this was continued,  layer1.1.bn1\n",
      "set forward hook for layer named:  layer1.1.conv2\n",
      "this was continued,  layer1.1.shortcut\n",
      "this was continued,  layer2\n",
      "this was continued,  layer2.0\n",
      "set forward hook for layer named:  layer2.0.conv1\n",
      "this was continued,  layer2.0.bn1\n",
      "set forward hook for layer named:  layer2.0.conv2\n",
      "this was continued,  layer2.0.shortcut\n",
      "set forward hook for layer named:  layer2.0.shortcut.0\n",
      "this was continued,  layer2.0.shortcut.1\n",
      "this was continued,  layer2.1\n",
      "set forward hook for layer named:  layer2.1.conv1\n",
      "this was continued,  layer2.1.bn1\n",
      "set forward hook for layer named:  layer2.1.conv2\n",
      "this was continued,  layer2.1.shortcut\n",
      "this was continued,  layer3\n",
      "this was continued,  layer3.0\n",
      "set forward hook for layer named:  layer3.0.conv1\n",
      "this was continued,  layer3.0.bn1\n",
      "set forward hook for layer named:  layer3.0.conv2\n",
      "this was continued,  layer3.0.shortcut\n",
      "set forward hook for layer named:  layer3.0.shortcut.0\n",
      "this was continued,  layer3.0.shortcut.1\n",
      "this was continued,  layer3.1\n",
      "set forward hook for layer named:  layer3.1.conv1\n",
      "this was continued,  layer3.1.bn1\n",
      "set forward hook for layer named:  layer3.1.conv2\n",
      "this was continued,  layer3.1.shortcut\n",
      "this was continued,  layer4\n",
      "this was continued,  layer4.0\n",
      "set forward hook for layer named:  layer4.0.conv1\n",
      "this was continued,  layer4.0.bn1\n",
      "set forward hook for layer named:  layer4.0.conv2\n",
      "this was continued,  layer4.0.shortcut\n",
      "set forward hook for layer named:  layer4.0.shortcut.0\n",
      "this was continued,  layer4.0.shortcut.1\n",
      "this was continued,  layer4.1\n",
      "set forward hook for layer named:  layer4.1.conv1\n",
      "this was continued,  layer4.1.bn1\n",
      "set forward hook for layer named:  layer4.1.conv2\n",
      "this was continued,  layer4.1.shortcut\n",
      "set forward hook for layer named:  linear\n",
      "excluded\n",
      "set forward hook for layer named:  conv1\n",
      "this was continued,  bn1\n",
      "this was continued,  layer1\n",
      "this was continued,  layer1.0\n",
      "set forward hook for layer named:  layer1.0.conv1\n",
      "this was continued,  layer1.0.bn1\n",
      "set forward hook for layer named:  layer1.0.conv2\n",
      "this was continued,  layer1.0.shortcut\n",
      "this was continued,  layer1.1\n",
      "set forward hook for layer named:  layer1.1.conv1\n",
      "this was continued,  layer1.1.bn1\n",
      "set forward hook for layer named:  layer1.1.conv2\n",
      "this was continued,  layer1.1.shortcut\n",
      "this was continued,  layer2\n",
      "this was continued,  layer2.0\n",
      "set forward hook for layer named:  layer2.0.conv1\n",
      "this was continued,  layer2.0.bn1\n",
      "set forward hook for layer named:  layer2.0.conv2\n",
      "this was continued,  layer2.0.shortcut\n",
      "set forward hook for layer named:  layer2.0.shortcut.0\n",
      "this was continued,  layer2.0.shortcut.1\n",
      "this was continued,  layer2.1\n",
      "set forward hook for layer named:  layer2.1.conv1\n",
      "this was continued,  layer2.1.bn1\n",
      "set forward hook for layer named:  layer2.1.conv2\n",
      "this was continued,  layer2.1.shortcut\n",
      "this was continued,  layer3\n",
      "this was continued,  layer3.0\n",
      "set forward hook for layer named:  layer3.0.conv1\n",
      "this was continued,  layer3.0.bn1\n",
      "set forward hook for layer named:  layer3.0.conv2\n",
      "this was continued,  layer3.0.shortcut\n",
      "set forward hook for layer named:  layer3.0.shortcut.0\n",
      "this was continued,  layer3.0.shortcut.1\n",
      "this was continued,  layer3.1\n",
      "set forward hook for layer named:  layer3.1.conv1\n",
      "this was continued,  layer3.1.bn1\n",
      "set forward hook for layer named:  layer3.1.conv2\n",
      "this was continued,  layer3.1.shortcut\n",
      "this was continued,  layer4\n",
      "this was continued,  layer4.0\n",
      "set forward hook for layer named:  layer4.0.conv1\n",
      "this was continued,  layer4.0.bn1\n",
      "set forward hook for layer named:  layer4.0.conv2\n",
      "this was continued,  layer4.0.shortcut\n",
      "set forward hook for layer named:  layer4.0.shortcut.0\n",
      "this was continued,  layer4.0.shortcut.1\n",
      "this was continued,  layer4.1\n",
      "set forward hook for layer named:  layer4.1.conv1\n",
      "this was continued,  layer4.1.bn1\n",
      "set forward hook for layer named:  layer4.1.conv2\n",
      "this was continued,  layer4.1.shortcut\n",
      "set forward hook for layer named:  linear\n",
      "num_personal_idx  25\n",
      "model_name is  resnet18_nobias_nobn\n",
      "***********\n",
      "min of act: -19.003673553466797, max: 20.05396270751953, mean: -0.010229022242128849\n",
      "activations for idx 0 at layer conv1 have the following shape  torch.Size([200, 1, 64, 32, 32])\n",
      "-----------\n",
      "***********\n",
      "min of act: -44.39855194091797, max: 15.79669189453125, mean: -2.175050973892212\n",
      "activations for idx 0 at layer layer1.0.conv1 have the following shape  torch.Size([200, 1, 64, 32, 32])\n",
      "-----------\n",
      "***********\n",
      "min of act: -10.477140426635742, max: 22.092164993286133, mean: 0.20962199568748474\n",
      "activations for idx 0 at layer layer1.0.conv2 have the following shape  torch.Size([200, 1, 64, 32, 32])\n",
      "-----------\n",
      "***********\n",
      "min of act: -34.51483154296875, max: 18.649398803710938, mean: -2.8068602085113525\n",
      "activations for idx 0 at layer layer1.1.conv1 have the following shape  torch.Size([200, 1, 64, 32, 32])\n",
      "-----------\n",
      "***********\n",
      "min of act: -10.660174369812012, max: 19.857585906982422, mean: 0.22405841946601868\n",
      "activations for idx 0 at layer layer1.1.conv2 have the following shape  torch.Size([200, 1, 64, 32, 32])\n",
      "-----------\n",
      "***********\n",
      "min of act: -46.581974029541016, max: 40.871646881103516, mean: -1.1154792308807373\n",
      "activations for idx 0 at layer layer2.0.conv1 have the following shape  torch.Size([200, 1, 128, 16, 16])\n",
      "-----------\n",
      "***********\n",
      "min of act: -54.26078796386719, max: 61.97058868408203, mean: -1.4441564083099365\n",
      "activations for idx 0 at layer layer2.0.conv2 have the following shape  torch.Size([200, 1, 128, 16, 16])\n",
      "-----------\n",
      "***********\n",
      "min of act: -8.984762191772461, max: 15.808109283447266, mean: 0.12490052729845047\n",
      "activations for idx 0 at layer layer2.0.shortcut.0 have the following shape  torch.Size([200, 1, 128, 16, 16])\n",
      "-----------\n",
      "***********\n",
      "min of act: -92.92515563964844, max: 69.8560791015625, mean: -10.442387580871582\n",
      "activations for idx 0 at layer layer2.1.conv1 have the following shape  torch.Size([200, 1, 128, 16, 16])\n",
      "-----------\n",
      "***********\n",
      "min of act: -30.128543853759766, max: 63.515506744384766, mean: 0.2296648472547531\n",
      "activations for idx 0 at layer layer2.1.conv2 have the following shape  torch.Size([200, 1, 128, 16, 16])\n",
      "-----------\n",
      "***********\n",
      "min of act: -78.1180648803711, max: 103.83104705810547, mean: -5.5252790451049805\n",
      "activations for idx 0 at layer layer3.0.conv1 have the following shape  torch.Size([200, 1, 256, 8, 8])\n",
      "-----------\n",
      "***********\n",
      "min of act: -131.83291625976562, max: 149.67872619628906, mean: -7.013665676116943\n",
      "activations for idx 0 at layer layer3.0.conv2 have the following shape  torch.Size([200, 1, 256, 8, 8])\n",
      "-----------\n",
      "***********\n",
      "min of act: -25.791736602783203, max: 38.14049530029297, mean: -0.5271344780921936\n",
      "activations for idx 0 at layer layer3.0.shortcut.0 have the following shape  torch.Size([200, 1, 256, 8, 8])\n",
      "-----------\n",
      "***********\n",
      "min of act: -174.91326904296875, max: 197.60638427734375, mean: -16.27251625061035\n",
      "activations for idx 0 at layer layer3.1.conv1 have the following shape  torch.Size([200, 1, 256, 8, 8])\n",
      "-----------\n",
      "***********\n",
      "min of act: -81.99774932861328, max: 149.20388793945312, mean: -3.7772135734558105\n",
      "activations for idx 0 at layer layer3.1.conv2 have the following shape  torch.Size([200, 1, 256, 8, 8])\n",
      "-----------\n",
      "***********\n",
      "min of act: -77.75872802734375, max: 128.04855346679688, mean: -6.584503173828125\n",
      "activations for idx 0 at layer layer4.0.conv1 have the following shape  torch.Size([200, 1, 512, 4, 4])\n",
      "-----------\n",
      "***********\n",
      "min of act: -126.59754943847656, max: 162.01608276367188, mean: -6.76527738571167\n",
      "activations for idx 0 at layer layer4.0.conv2 have the following shape  torch.Size([200, 1, 512, 4, 4])\n",
      "-----------\n",
      "***********\n",
      "min of act: -27.381223678588867, max: 35.39390182495117, mean: -1.9078307151794434\n",
      "activations for idx 0 at layer layer4.0.shortcut.0 have the following shape  torch.Size([200, 1, 512, 4, 4])\n",
      "-----------\n",
      "***********\n",
      "min of act: -128.70889282226562, max: 84.36071014404297, mean: -3.4071648120880127\n",
      "activations for idx 0 at layer layer4.1.conv1 have the following shape  torch.Size([200, 1, 512, 4, 4])\n",
      "-----------\n",
      "***********\n",
      "min of act: -63.090274810791016, max: 161.83062744140625, mean: -1.008026123046875\n",
      "activations for idx 0 at layer layer4.1.conv2 have the following shape  torch.Size([200, 1, 512, 4, 4])\n",
      "-----------\n",
      "***********\n",
      "min of act: -55.48170471191406, max: 179.81195068359375, mean: 0.652633011341095\n",
      "activations for idx 0 at layer linear have the following shape  torch.Size([200, 1, 10])\n",
      "-----------\n",
      "***********\n",
      "min of act: -18.38300132751465, max: 17.480871200561523, mean: -0.00913222599774599\n",
      "activations for idx 1 at layer conv1 have the following shape  torch.Size([200, 1, 64, 32, 32])\n",
      "-----------\n",
      "***********\n",
      "min of act: -38.40105056762695, max: 16.341997146606445, mean: -2.198030471801758\n",
      "activations for idx 1 at layer layer1.0.conv1 have the following shape  torch.Size([200, 1, 64, 32, 32])\n",
      "-----------\n",
      "***********\n",
      "min of act: -11.241580963134766, max: 22.69576644897461, mean: 0.1879190057516098\n",
      "activations for idx 1 at layer layer1.0.conv2 have the following shape  torch.Size([200, 1, 64, 32, 32])\n",
      "-----------\n",
      "***********\n",
      "min of act: -36.35603332519531, max: 20.618778228759766, mean: -2.6679680347442627\n",
      "activations for idx 1 at layer layer1.1.conv1 have the following shape  torch.Size([200, 1, 64, 32, 32])\n",
      "-----------\n",
      "***********\n",
      "min of act: -8.261842727661133, max: 22.749881744384766, mean: 0.17290353775024414\n",
      "activations for idx 1 at layer layer1.1.conv2 have the following shape  torch.Size([200, 1, 64, 32, 32])\n",
      "-----------\n",
      "***********\n",
      "min of act: -43.482173919677734, max: 40.27925491333008, mean: -0.9560218453407288\n",
      "activations for idx 1 at layer layer2.0.conv1 have the following shape  torch.Size([200, 1, 128, 16, 16])\n",
      "-----------\n",
      "***********\n",
      "min of act: -60.79599380493164, max: 75.78958129882812, mean: -1.3077856302261353\n",
      "activations for idx 1 at layer layer2.0.conv2 have the following shape  torch.Size([200, 1, 128, 16, 16])\n",
      "-----------\n",
      "***********\n",
      "min of act: -7.357316017150879, max: 20.5065975189209, mean: 0.1470755636692047\n",
      "activations for idx 1 at layer layer2.0.shortcut.0 have the following shape  torch.Size([200, 1, 128, 16, 16])\n",
      "-----------\n",
      "***********\n",
      "min of act: -93.84811401367188, max: 75.72224426269531, mean: -10.611730575561523\n",
      "activations for idx 1 at layer layer2.1.conv1 have the following shape  torch.Size([200, 1, 128, 16, 16])\n",
      "-----------\n",
      "***********\n",
      "min of act: -33.03167724609375, max: 86.64940643310547, mean: 0.3159615695476532\n",
      "activations for idx 1 at layer layer2.1.conv2 have the following shape  torch.Size([200, 1, 128, 16, 16])\n",
      "-----------\n",
      "***********\n",
      "min of act: -88.94628143310547, max: 114.96620178222656, mean: -5.27089262008667\n",
      "activations for idx 1 at layer layer3.0.conv1 have the following shape  torch.Size([200, 1, 256, 8, 8])\n",
      "-----------\n",
      "***********\n",
      "min of act: -113.12664031982422, max: 178.90704345703125, mean: -6.424009799957275\n",
      "activations for idx 1 at layer layer3.0.conv2 have the following shape  torch.Size([200, 1, 256, 8, 8])\n",
      "-----------\n",
      "***********\n",
      "min of act: -25.7744197845459, max: 51.96471405029297, mean: -0.41872215270996094\n",
      "activations for idx 1 at layer layer3.0.shortcut.0 have the following shape  torch.Size([200, 1, 256, 8, 8])\n",
      "-----------\n",
      "***********\n",
      "min of act: -164.76968383789062, max: 213.2523651123047, mean: -16.907377243041992\n",
      "activations for idx 1 at layer layer3.1.conv1 have the following shape  torch.Size([200, 1, 256, 8, 8])\n",
      "-----------\n",
      "***********\n",
      "min of act: -84.30760955810547, max: 185.32073974609375, mean: -3.339142084121704\n",
      "activations for idx 1 at layer layer3.1.conv2 have the following shape  torch.Size([200, 1, 256, 8, 8])\n",
      "-----------\n",
      "***********\n",
      "min of act: -81.30033111572266, max: 152.99346923828125, mean: -6.784125804901123\n",
      "activations for idx 1 at layer layer4.0.conv1 have the following shape  torch.Size([200, 1, 512, 4, 4])\n",
      "-----------\n",
      "***********\n",
      "min of act: -145.43521118164062, max: 148.44161987304688, mean: -7.920382499694824\n",
      "activations for idx 1 at layer layer4.0.conv2 have the following shape  torch.Size([200, 1, 512, 4, 4])\n",
      "-----------\n",
      "***********\n",
      "min of act: -30.10893440246582, max: 34.62908935546875, mean: -2.279965400695801\n",
      "activations for idx 1 at layer layer4.0.shortcut.0 have the following shape  torch.Size([200, 1, 512, 4, 4])\n",
      "-----------\n",
      "***********\n",
      "min of act: -143.47508239746094, max: 124.2194595336914, mean: -3.589510917663574\n",
      "activations for idx 1 at layer layer4.1.conv1 have the following shape  torch.Size([200, 1, 512, 4, 4])\n",
      "-----------\n",
      "***********\n",
      "min of act: -54.07634735107422, max: 167.81387329101562, mean: -1.1214184761047363\n",
      "activations for idx 1 at layer layer4.1.conv2 have the following shape  torch.Size([200, 1, 512, 4, 4])\n",
      "-----------\n",
      "***********\n",
      "min of act: -48.97771072387695, max: 174.87277221679688, mean: 1.3380790948867798\n",
      "activations for idx 1 at layer linear have the following shape  torch.Size([200, 1, 10])\n",
      "-----------\n",
      "Activation Timer ends\n",
      "------- Geometric Ensembling -------\n",
      "Timer start\n",
      "odict_keys(['conv1.weight', 'layer1.0.conv1.weight', 'layer1.0.conv2.weight', 'layer1.1.conv1.weight', 'layer1.1.conv2.weight', 'layer2.0.conv1.weight', 'layer2.0.conv2.weight', 'layer2.0.shortcut.0.weight', 'layer2.1.conv1.weight', 'layer2.1.conv2.weight', 'layer3.0.conv1.weight', 'layer3.0.conv2.weight', 'layer3.0.shortcut.0.weight', 'layer3.1.conv1.weight', 'layer3.1.conv2.weight', 'layer4.0.conv1.weight', 'layer4.0.conv2.weight', 'layer4.0.shortcut.0.weight', 'layer4.1.conv1.weight', 'layer4.1.conv2.weight', 'linear.weight'])\n",
      "the ratio of zero for conv1.weight\n",
      "tensor(0.3999, device='cuda:0')\n",
      "the ratio of zero for layer1.0.conv1.weight\n",
      "tensor(0.4000, device='cuda:0')\n",
      "the ratio of zero for layer1.0.conv2.weight\n",
      "tensor(0.4000, device='cuda:0')\n",
      "the ratio of zero for layer1.1.conv1.weight\n",
      "tensor(0.4000, device='cuda:0')\n",
      "the ratio of zero for layer1.1.conv2.weight\n",
      "tensor(0.4000, device='cuda:0')\n",
      "the ratio of zero for layer2.0.conv1.weight\n",
      "tensor(0.4000, device='cuda:0')\n",
      "the ratio of zero for layer2.0.conv2.weight\n",
      "tensor(0.4000, device='cuda:0')\n",
      "the ratio of zero for layer2.0.shortcut.0.weight\n",
      "tensor(0.4000, device='cuda:0')\n",
      "the ratio of zero for layer2.1.conv1.weight\n",
      "tensor(0.4000, device='cuda:0')\n",
      "the ratio of zero for layer2.1.conv2.weight\n",
      "tensor(0.4000, device='cuda:0')\n",
      "the ratio of zero for layer3.0.conv1.weight\n",
      "tensor(0.4000, device='cuda:0')\n",
      "the ratio of zero for layer3.0.conv2.weight\n",
      "tensor(0.4000, device='cuda:0')\n",
      "the ratio of zero for layer3.0.shortcut.0.weight\n",
      "tensor(0.4000, device='cuda:0')\n",
      "the ratio of zero for layer3.1.conv1.weight\n",
      "tensor(0.4000, device='cuda:0')\n",
      "the ratio of zero for layer3.1.conv2.weight\n",
      "tensor(0.4000, device='cuda:0')\n",
      "the ratio of zero for layer4.0.conv1.weight\n",
      "tensor(0.4000, device='cuda:0')\n",
      "the ratio of zero for layer4.0.conv2.weight\n",
      "tensor(0.4000, device='cuda:0')\n",
      "the ratio of zero for layer4.0.shortcut.0.weight\n",
      "tensor(0.4000, device='cuda:0')\n",
      "the ratio of zero for layer4.1.conv1.weight\n",
      "tensor(0.4000, device='cuda:0')\n",
      "the ratio of zero for layer4.1.conv2.weight\n",
      "tensor(0.4000, device='cuda:0')\n",
      "the ratio of zero for linear.weight\n",
      "tensor(0.4000, device='cuda:0')\n",
      "\n",
      "--------------- At layer index 0 ------------- \n",
      " \n",
      "Previous layer shape is  None\n",
      "let's see the difference in layer names conv1 conv1\n",
      "torch.Size([200, 1, 64, 32, 32]) shape of activations generally\n",
      "reorder_dim is  [1, 2, 3, 0]\n",
      "In layer conv1.weight: getting activation distance statistics\n",
      "Statistics of the distance from neurons of layer 1 (averaged across nodes of layer 0): \n",
      "\n",
      "Max : 26.031343460083008, Mean : 9.810158729553223, Min : 2.7295031547546387, Std: 3.7494614124298096\n",
      "returns a uniform measure of cardinality:  64\n",
      "returns a uniform measure of cardinality:  64\n",
      "Refactored ground metric calc\n",
      "inside refactored\n",
      "Processing the coordinates to form ground_metric\n",
      "dont leave off the squaring of the ground metric\n",
      "# of ground metric features is  204800\n",
      "# of ground metric features in 0 is   204800\n",
      "# of ground metric features in 1 is   204800\n",
      "ground metric (m0) is  tensor([[ 778.9396,  834.0956,  704.3387,  ...,  600.6801,  697.8077,\n",
      "          935.1586],\n",
      "        [ 457.3718,  710.5472,  563.8930,  ...,  507.3390,  547.5653,\n",
      "          771.6165],\n",
      "        [ 876.5671, 1075.5835,  548.8743,  ...,  724.9958,  820.2525,\n",
      "         1000.5515],\n",
      "        ...,\n",
      "        [ 374.3263,  774.4860,  666.8965,  ...,  508.1389,  585.6245,\n",
      "          501.1748],\n",
      "        [ 882.8002, 1116.6929,  891.2694,  ...,  739.8856, 1002.5624,\n",
      "          825.2947],\n",
      "        [ 889.5539,  775.1683,  594.1552,  ...,  574.4146,  656.7846,\n",
      "         1025.1089]], device='cuda:0')\n",
      "shape of T_var is  torch.Size([64, 64])\n",
      "T_var before correction  tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')\n",
      "marginals are  tensor([[63.9996, 63.9996, 63.9996,  ..., 63.9996, 63.9996, 63.9996],\n",
      "        [63.9996, 63.9996, 63.9996,  ..., 63.9996, 63.9996, 63.9996],\n",
      "        [63.9996, 63.9996, 63.9996,  ..., 63.9996, 63.9996, 63.9996],\n",
      "        ...,\n",
      "        [63.9996, 63.9996, 63.9996,  ..., 63.9996, 63.9996, 63.9996],\n",
      "        [63.9996, 63.9996, 63.9996,  ..., 63.9996, 63.9996, 63.9996],\n",
      "        [63.9996, 63.9996, 63.9996,  ..., 63.9996, 63.9996, 63.9996]],\n",
      "       device='cuda:0')\n",
      "T_var after correction  tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')\n",
      "T_var stats: max 0.9999935626983643, min 0.0, mean 0.015624899417161942, std 0.12403393536806107 \n",
      "Ratio of trace to the matrix sum:  tensor(0.0469, device='cuda:0')\n",
      "Here, trace is 2.9999806880950928 and matrix sum is 63.99958801269531 \n",
      "Shape of aligned wt is  torch.Size([64, 3, 9])\n",
      "Shape of fc_layer0_weight_data is  torch.Size([64, 3, 9])\n",
      "model parameters are \n",
      " [torch.Size([64, 3, 3, 3]), torch.Size([64, 64, 3, 3]), torch.Size([64, 64, 3, 3]), torch.Size([64, 64, 3, 3]), torch.Size([64, 64, 3, 3]), torch.Size([128, 64, 3, 3]), torch.Size([128, 128, 3, 3]), torch.Size([128, 64, 1, 1]), torch.Size([128, 128, 3, 3]), torch.Size([128, 128, 3, 3]), torch.Size([256, 128, 3, 3]), torch.Size([256, 256, 3, 3]), torch.Size([256, 128, 1, 1]), torch.Size([256, 256, 3, 3]), torch.Size([256, 256, 3, 3]), torch.Size([512, 256, 3, 3]), torch.Size([512, 512, 3, 3]), torch.Size([512, 256, 1, 1]), torch.Size([512, 512, 3, 3]), torch.Size([512, 512, 3, 3]), torch.Size([10, 512])]\n",
      "len of model_state_dict is  21\n",
      "len of new_params is  1\n",
      "updated parameters for layer  conv1.weight\n",
      "\n",
      "--------- Testing in global mode ---------\n",
      "size of test_loader dataset:  10000\n",
      "\n",
      "Test set: Avg. loss: 0.0024, Accuracy: 1471/10000 (15%)\n",
      "\n",
      "accuracy after update is  14.71\n",
      "For layer idx 0, accuracy of the updated model is 14.71\n",
      "\n",
      "--------------- At layer index 1 ------------- \n",
      " \n",
      "Previous layer shape is  torch.Size([64, 3, 3, 3])\n",
      "let's see the difference in layer names layer1.0.conv1 layer1.0.conv1\n",
      "torch.Size([200, 1, 64, 32, 32]) shape of activations generally\n",
      "reorder_dim is  [1, 2, 3, 0]\n",
      "In layer layer1.0.conv1.weight: getting activation distance statistics\n",
      "Statistics of the distance from neurons of layer 1 (averaged across nodes of layer 0): \n",
      "\n",
      "Max : 45.72626495361328, Mean : 17.840486526489258, Min : 5.199441432952881, Std: 6.71624231338501\n",
      "shape of layer: model 0 torch.Size([64, 64, 9])\n",
      "shape of layer: model 1 torch.Size([64, 64, 9])\n",
      "shape of activations: model 0 torch.Size([64, 32, 32, 200])\n",
      "shape of activations: model 1 torch.Size([64, 32, 32, 200])\n",
      "shape of previous transport map torch.Size([64, 64])\n",
      "doing nothing for skips\n",
      "returns a uniform measure of cardinality:  64\n",
      "returns a uniform measure of cardinality:  64\n",
      "Refactored ground metric calc\n",
      "inside refactored\n",
      "Processing the coordinates to form ground_metric\n",
      "dont leave off the squaring of the ground metric\n",
      "# of ground metric features is  204800\n",
      "# of ground metric features in 0 is   204800\n",
      "# of ground metric features in 1 is   204800\n",
      "ground metric (m0) is  tensor([[1361.9178, 1753.1168, 1556.2338,  ..., 1197.1802, 1801.3121,\n",
      "         1850.1565],\n",
      "        [1584.5123, 1813.0815, 1511.2518,  ..., 1519.8789, 1966.9072,\n",
      "         2088.9705],\n",
      "        [ 236.5907, 2031.3800, 1555.9999,  ..., 1833.3612, 2248.9875,\n",
      "         1091.3225],\n",
      "        ...,\n",
      "        [1847.9506, 1528.9871, 1596.0126,  ..., 1735.3463, 1747.3047,\n",
      "         1866.4127],\n",
      "        [ 268.5174, 2250.4688, 1727.3850,  ..., 2030.3455, 2478.0581,\n",
      "         1158.7019],\n",
      "        [2170.0598, 1510.8926, 1450.9192,  ..., 1838.8634, 1938.2004,\n",
      "         2200.8105]], device='cuda:0')\n",
      "shape of T_var is  torch.Size([64, 64])\n",
      "T_var before correction  tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')\n",
      "marginals are  tensor([[63.9996, 63.9996, 63.9996,  ..., 63.9996, 63.9996, 63.9996],\n",
      "        [63.9996, 63.9996, 63.9996,  ..., 63.9996, 63.9996, 63.9996],\n",
      "        [63.9996, 63.9996, 63.9996,  ..., 63.9996, 63.9996, 63.9996],\n",
      "        ...,\n",
      "        [63.9996, 63.9996, 63.9996,  ..., 63.9996, 63.9996, 63.9996],\n",
      "        [63.9996, 63.9996, 63.9996,  ..., 63.9996, 63.9996, 63.9996],\n",
      "        [63.9996, 63.9996, 63.9996,  ..., 63.9996, 63.9996, 63.9996]],\n",
      "       device='cuda:0')\n",
      "T_var after correction  tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')\n",
      "T_var stats: max 0.9999935626983643, min 0.0, mean 0.015624899417161942, std 0.12403393536806107 \n",
      "Ratio of trace to the matrix sum:  tensor(0.0156, device='cuda:0')\n",
      "Here, trace is 0.9999935626983643 and matrix sum is 63.99958801269531 \n",
      "Shape of aligned wt is  torch.Size([64, 64, 9])\n",
      "Shape of fc_layer0_weight_data is  torch.Size([64, 64, 9])\n",
      "model parameters are \n",
      " [torch.Size([64, 3, 3, 3]), torch.Size([64, 64, 3, 3]), torch.Size([64, 64, 3, 3]), torch.Size([64, 64, 3, 3]), torch.Size([64, 64, 3, 3]), torch.Size([128, 64, 3, 3]), torch.Size([128, 128, 3, 3]), torch.Size([128, 64, 1, 1]), torch.Size([128, 128, 3, 3]), torch.Size([128, 128, 3, 3]), torch.Size([256, 128, 3, 3]), torch.Size([256, 256, 3, 3]), torch.Size([256, 128, 1, 1]), torch.Size([256, 256, 3, 3]), torch.Size([256, 256, 3, 3]), torch.Size([512, 256, 3, 3]), torch.Size([512, 512, 3, 3]), torch.Size([512, 256, 1, 1]), torch.Size([512, 512, 3, 3]), torch.Size([512, 512, 3, 3]), torch.Size([10, 512])]\n",
      "len of model_state_dict is  21\n",
      "len of new_params is  2\n",
      "updated parameters for layer  conv1.weight\n",
      "updated parameters for layer  layer1.0.conv1.weight\n",
      "\n",
      "--------- Testing in global mode ---------\n",
      "size of test_loader dataset:  10000\n",
      "\n",
      "Test set: Avg. loss: 0.0023, Accuracy: 1691/10000 (17%)\n",
      "\n",
      "accuracy after update is  16.91\n",
      "For layer idx 1, accuracy of the updated model is 16.91\n",
      "\n",
      "--------------- At layer index 2 ------------- \n",
      " \n",
      "Previous layer shape is  torch.Size([64, 64, 3, 3])\n",
      "let's see the difference in layer names layer1.0.conv2 layer1.0.conv2\n",
      "torch.Size([200, 1, 64, 32, 32]) shape of activations generally\n",
      "reorder_dim is  [1, 2, 3, 0]\n",
      "In layer layer1.0.conv2.weight: getting activation distance statistics\n",
      "Statistics of the distance from neurons of layer 1 (averaged across nodes of layer 0): \n",
      "\n",
      "Max : 21.4811954498291, Mean : 8.034808158874512, Min : 2.0837740898132324, Std: 3.1965601444244385\n",
      "shape of layer: model 0 torch.Size([64, 64, 9])\n",
      "shape of layer: model 1 torch.Size([64, 64, 9])\n",
      "shape of activations: model 0 torch.Size([64, 32, 32, 200])\n",
      "shape of activations: model 1 torch.Size([64, 32, 32, 200])\n",
      "shape of previous transport map torch.Size([64, 64])\n",
      "doing nothing for skips\n",
      "returns a uniform measure of cardinality:  64\n",
      "returns a uniform measure of cardinality:  64\n",
      "Refactored ground metric calc\n",
      "inside refactored\n",
      "Processing the coordinates to form ground_metric\n",
      "dont leave off the squaring of the ground metric\n",
      "# of ground metric features is  204800\n",
      "# of ground metric features in 0 is   204800\n",
      "# of ground metric features in 1 is   204800\n",
      "ground metric (m0) is  tensor([[801.2742, 607.9179, 592.8207,  ..., 608.3773, 583.9488, 863.7162],\n",
      "        [615.2021, 562.9248, 524.8652,  ..., 531.3011, 561.4574, 794.9586],\n",
      "        [832.7557, 715.8004, 469.4628,  ..., 660.8480, 681.1127, 840.4741],\n",
      "        ...,\n",
      "        [451.8075, 595.6511, 589.8693,  ..., 636.0586, 543.1049, 529.8610],\n",
      "        [831.7680, 794.5369, 727.9418,  ..., 692.1811, 760.8482, 770.9108],\n",
      "        [732.1465, 577.8206, 440.8956,  ..., 476.4978, 622.9995, 867.2696]],\n",
      "       device='cuda:0')\n",
      "shape of T_var is  torch.Size([64, 64])\n",
      "T_var before correction  tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')\n",
      "marginals are  tensor([[63.9996, 63.9996, 63.9996,  ..., 63.9996, 63.9996, 63.9996],\n",
      "        [63.9996, 63.9996, 63.9996,  ..., 63.9996, 63.9996, 63.9996],\n",
      "        [63.9996, 63.9996, 63.9996,  ..., 63.9996, 63.9996, 63.9996],\n",
      "        ...,\n",
      "        [63.9996, 63.9996, 63.9996,  ..., 63.9996, 63.9996, 63.9996],\n",
      "        [63.9996, 63.9996, 63.9996,  ..., 63.9996, 63.9996, 63.9996],\n",
      "        [63.9996, 63.9996, 63.9996,  ..., 63.9996, 63.9996, 63.9996]],\n",
      "       device='cuda:0')\n",
      "T_var after correction  tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')\n",
      "T_var stats: max 0.9999935626983643, min 0.0, mean 0.015624899417161942, std 0.12403392791748047 \n",
      "Ratio of trace to the matrix sum:  tensor(0.0312, device='cuda:0')\n",
      "Here, trace is 1.9999871253967285 and matrix sum is 63.99958801269531 \n",
      "Shape of aligned wt is  torch.Size([64, 64, 9])\n",
      "Shape of fc_layer0_weight_data is  torch.Size([64, 64, 9])\n",
      "model parameters are \n",
      " [torch.Size([64, 3, 3, 3]), torch.Size([64, 64, 3, 3]), torch.Size([64, 64, 3, 3]), torch.Size([64, 64, 3, 3]), torch.Size([64, 64, 3, 3]), torch.Size([128, 64, 3, 3]), torch.Size([128, 128, 3, 3]), torch.Size([128, 64, 1, 1]), torch.Size([128, 128, 3, 3]), torch.Size([128, 128, 3, 3]), torch.Size([256, 128, 3, 3]), torch.Size([256, 256, 3, 3]), torch.Size([256, 128, 1, 1]), torch.Size([256, 256, 3, 3]), torch.Size([256, 256, 3, 3]), torch.Size([512, 256, 3, 3]), torch.Size([512, 512, 3, 3]), torch.Size([512, 256, 1, 1]), torch.Size([512, 512, 3, 3]), torch.Size([512, 512, 3, 3]), torch.Size([10, 512])]\n",
      "len of model_state_dict is  21\n",
      "len of new_params is  3\n",
      "updated parameters for layer  conv1.weight\n",
      "updated parameters for layer  layer1.0.conv1.weight\n",
      "updated parameters for layer  layer1.0.conv2.weight\n",
      "\n",
      "--------- Testing in global mode ---------\n",
      "size of test_loader dataset:  10000\n",
      "\n",
      "Test set: Avg. loss: 0.0025, Accuracy: 1263/10000 (13%)\n",
      "\n",
      "accuracy after update is  12.63\n",
      "For layer idx 2, accuracy of the updated model is 12.63\n",
      "\n",
      "--------------- At layer index 3 ------------- \n",
      " \n",
      "Previous layer shape is  torch.Size([64, 64, 3, 3])\n",
      "let's see the difference in layer names layer1.1.conv1 layer1.1.conv1\n",
      "torch.Size([200, 1, 64, 32, 32]) shape of activations generally\n",
      "reorder_dim is  [1, 2, 3, 0]\n",
      "In layer layer1.1.conv1.weight: getting activation distance statistics\n",
      "Statistics of the distance from neurons of layer 1 (averaged across nodes of layer 0): \n",
      "\n",
      "Max : 39.12427520751953, Mean : 16.972286224365234, Min : 5.3263325691223145, Std: 5.811253547668457\n",
      "shape of layer: model 0 torch.Size([64, 64, 9])\n",
      "shape of layer: model 1 torch.Size([64, 64, 9])\n",
      "shape of activations: model 0 torch.Size([64, 32, 32, 200])\n",
      "shape of activations: model 1 torch.Size([64, 32, 32, 200])\n",
      "shape of previous transport map torch.Size([64, 64])\n",
      "doing nothing for skips\n",
      "returns a uniform measure of cardinality:  64\n",
      "returns a uniform measure of cardinality:  64\n",
      "Refactored ground metric calc\n",
      "inside refactored\n",
      "Processing the coordinates to form ground_metric\n",
      "dont leave off the squaring of the ground metric\n",
      "# of ground metric features is  204800\n",
      "# of ground metric features in 0 is   204800\n",
      "# of ground metric features in 1 is   204800\n",
      "ground metric (m0) is  tensor([[1107.6796, 1485.6388, 1028.6533,  ..., 1248.3329, 1495.1180,\n",
      "         1377.6694],\n",
      "        [1563.5142, 1947.4338, 1330.3547,  ..., 1815.3615, 1410.1276,\n",
      "         1138.9780],\n",
      "        [1981.5240, 2417.8777, 1471.9248,  ..., 1849.5250, 1430.1730,\n",
      "         1424.8280],\n",
      "        ...,\n",
      "        [1261.7264, 1585.6742, 1127.1154,  ..., 1616.7313, 1504.4587,\n",
      "         1391.1569],\n",
      "        [1197.0016, 1720.7651,  873.3365,  ..., 1294.3074, 1391.6942,\n",
      "         1486.9457],\n",
      "        [1543.3519, 2005.8491, 1295.6425,  ..., 1657.7660,  790.4594,\n",
      "         1542.1227]], device='cuda:0')\n",
      "shape of T_var is  torch.Size([64, 64])\n",
      "T_var before correction  tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')\n",
      "marginals are  tensor([[63.9996, 63.9996, 63.9996,  ..., 63.9996, 63.9996, 63.9996],\n",
      "        [63.9996, 63.9996, 63.9996,  ..., 63.9996, 63.9996, 63.9996],\n",
      "        [63.9996, 63.9996, 63.9996,  ..., 63.9996, 63.9996, 63.9996],\n",
      "        ...,\n",
      "        [63.9996, 63.9996, 63.9996,  ..., 63.9996, 63.9996, 63.9996],\n",
      "        [63.9996, 63.9996, 63.9996,  ..., 63.9996, 63.9996, 63.9996],\n",
      "        [63.9996, 63.9996, 63.9996,  ..., 63.9996, 63.9996, 63.9996]],\n",
      "       device='cuda:0')\n",
      "T_var after correction  tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')\n",
      "T_var stats: max 0.9999935626983643, min 0.0, mean 0.015624899417161942, std 0.12403392791748047 \n",
      "Ratio of trace to the matrix sum:  tensor(0., device='cuda:0')\n",
      "Here, trace is 0.0 and matrix sum is 63.99958801269531 \n",
      "Shape of aligned wt is  torch.Size([64, 64, 9])\n",
      "Shape of fc_layer0_weight_data is  torch.Size([64, 64, 9])\n",
      "model parameters are \n",
      " [torch.Size([64, 3, 3, 3]), torch.Size([64, 64, 3, 3]), torch.Size([64, 64, 3, 3]), torch.Size([64, 64, 3, 3]), torch.Size([64, 64, 3, 3]), torch.Size([128, 64, 3, 3]), torch.Size([128, 128, 3, 3]), torch.Size([128, 64, 1, 1]), torch.Size([128, 128, 3, 3]), torch.Size([128, 128, 3, 3]), torch.Size([256, 128, 3, 3]), torch.Size([256, 256, 3, 3]), torch.Size([256, 128, 1, 1]), torch.Size([256, 256, 3, 3]), torch.Size([256, 256, 3, 3]), torch.Size([512, 256, 3, 3]), torch.Size([512, 512, 3, 3]), torch.Size([512, 256, 1, 1]), torch.Size([512, 512, 3, 3]), torch.Size([512, 512, 3, 3]), torch.Size([10, 512])]\n",
      "len of model_state_dict is  21\n",
      "len of new_params is  4\n",
      "updated parameters for layer  conv1.weight\n",
      "updated parameters for layer  layer1.0.conv1.weight\n",
      "updated parameters for layer  layer1.0.conv2.weight\n",
      "updated parameters for layer  layer1.1.conv1.weight\n",
      "\n",
      "--------- Testing in global mode ---------\n",
      "size of test_loader dataset:  10000\n",
      "\n",
      "Test set: Avg. loss: 0.0024, Accuracy: 1275/10000 (13%)\n",
      "\n",
      "accuracy after update is  12.75\n",
      "For layer idx 3, accuracy of the updated model is 12.75\n",
      "\n",
      "--------------- At layer index 4 ------------- \n",
      " \n",
      "Previous layer shape is  torch.Size([64, 64, 3, 3])\n",
      "let's see the difference in layer names layer1.1.conv2 layer1.1.conv2\n",
      "torch.Size([200, 1, 64, 32, 32]) shape of activations generally\n",
      "reorder_dim is  [1, 2, 3, 0]\n",
      "In layer layer1.1.conv2.weight: getting activation distance statistics\n",
      "Statistics of the distance from neurons of layer 1 (averaged across nodes of layer 0): \n",
      "\n",
      "Max : 20.4631404876709, Mean : 7.455730438232422, Min : 1.9447412490844727, Std: 3.1500437259674072\n",
      "shape of layer: model 0 torch.Size([64, 64, 9])\n",
      "shape of layer: model 1 torch.Size([64, 64, 9])\n",
      "shape of activations: model 0 torch.Size([64, 32, 32, 200])\n",
      "shape of activations: model 1 torch.Size([64, 32, 32, 200])\n",
      "shape of previous transport map torch.Size([64, 64])\n",
      "doing nothing for skips\n",
      "returns a uniform measure of cardinality:  64\n",
      "returns a uniform measure of cardinality:  64\n",
      "Refactored ground metric calc\n",
      "inside refactored\n",
      "Processing the coordinates to form ground_metric\n",
      "dont leave off the squaring of the ground metric\n",
      "# of ground metric features is  204800\n",
      "# of ground metric features in 0 is   204800\n",
      "# of ground metric features in 1 is   204800\n",
      "ground metric (m0) is  tensor([[710.6870, 520.0338, 480.8851,  ..., 470.3382, 519.5158, 846.9173],\n",
      "        [627.7048, 536.4438, 467.0539,  ..., 478.7092, 553.4252, 802.1054],\n",
      "        [751.1664, 658.8910, 399.3027,  ..., 539.7726, 617.2564, 847.3245],\n",
      "        ...,\n",
      "        [456.3881, 548.4096, 506.0133,  ..., 523.2932, 532.9135, 593.3513],\n",
      "        [773.1729, 683.2659, 630.9697,  ..., 600.3904, 694.3964, 820.2836],\n",
      "        [613.6125, 478.9248, 412.2287,  ..., 476.2412, 485.7103, 726.8650]],\n",
      "       device='cuda:0')\n",
      "shape of T_var is  torch.Size([64, 64])\n",
      "T_var before correction  tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')\n",
      "marginals are  tensor([[63.9996, 63.9996, 63.9996,  ..., 63.9996, 63.9996, 63.9996],\n",
      "        [63.9996, 63.9996, 63.9996,  ..., 63.9996, 63.9996, 63.9996],\n",
      "        [63.9996, 63.9996, 63.9996,  ..., 63.9996, 63.9996, 63.9996],\n",
      "        ...,\n",
      "        [63.9996, 63.9996, 63.9996,  ..., 63.9996, 63.9996, 63.9996],\n",
      "        [63.9996, 63.9996, 63.9996,  ..., 63.9996, 63.9996, 63.9996],\n",
      "        [63.9996, 63.9996, 63.9996,  ..., 63.9996, 63.9996, 63.9996]],\n",
      "       device='cuda:0')\n",
      "T_var after correction  tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')\n",
      "T_var stats: max 0.9999935626983643, min 0.0, mean 0.015624899417161942, std 0.12403393536806107 \n",
      "Ratio of trace to the matrix sum:  tensor(0.0312, device='cuda:0')\n",
      "Here, trace is 1.9999871253967285 and matrix sum is 63.99958801269531 \n",
      "Shape of aligned wt is  torch.Size([64, 64, 9])\n",
      "Shape of fc_layer0_weight_data is  torch.Size([64, 64, 9])\n",
      "model parameters are \n",
      " [torch.Size([64, 3, 3, 3]), torch.Size([64, 64, 3, 3]), torch.Size([64, 64, 3, 3]), torch.Size([64, 64, 3, 3]), torch.Size([64, 64, 3, 3]), torch.Size([128, 64, 3, 3]), torch.Size([128, 128, 3, 3]), torch.Size([128, 64, 1, 1]), torch.Size([128, 128, 3, 3]), torch.Size([128, 128, 3, 3]), torch.Size([256, 128, 3, 3]), torch.Size([256, 256, 3, 3]), torch.Size([256, 128, 1, 1]), torch.Size([256, 256, 3, 3]), torch.Size([256, 256, 3, 3]), torch.Size([512, 256, 3, 3]), torch.Size([512, 512, 3, 3]), torch.Size([512, 256, 1, 1]), torch.Size([512, 512, 3, 3]), torch.Size([512, 512, 3, 3]), torch.Size([10, 512])]\n",
      "len of model_state_dict is  21\n",
      "len of new_params is  5\n",
      "updated parameters for layer  conv1.weight\n",
      "updated parameters for layer  layer1.0.conv1.weight\n",
      "updated parameters for layer  layer1.0.conv2.weight\n",
      "updated parameters for layer  layer1.1.conv1.weight\n",
      "updated parameters for layer  layer1.1.conv2.weight\n",
      "\n",
      "--------- Testing in global mode ---------\n",
      "size of test_loader dataset:  10000\n",
      "\n",
      "Test set: Avg. loss: 0.0024, Accuracy: 1286/10000 (13%)\n",
      "\n",
      "accuracy after update is  12.86\n",
      "For layer idx 4, accuracy of the updated model is 12.86\n",
      "\n",
      "--------------- At layer index 5 ------------- \n",
      " \n",
      "Previous layer shape is  torch.Size([64, 64, 3, 3])\n",
      "let's see the difference in layer names layer2.0.conv1 layer2.0.conv1\n",
      "torch.Size([200, 1, 128, 16, 16]) shape of activations generally\n",
      "reorder_dim is  [1, 2, 3, 0]\n",
      "In layer layer2.0.conv1.weight: getting activation distance statistics\n",
      "Statistics of the distance from neurons of layer 1 (averaged across nodes of layer 0): \n",
      "\n",
      "Max : 36.22886657714844, Mean : 14.695722579956055, Min : 3.789625644683838, Std: 5.74956750869751\n",
      "shape of layer: model 0 torch.Size([128, 64, 9])\n",
      "shape of layer: model 1 torch.Size([128, 64, 9])\n",
      "shape of activations: model 0 torch.Size([128, 16, 16, 200])\n",
      "shape of activations: model 1 torch.Size([128, 16, 16, 200])\n",
      "shape of previous transport map torch.Size([64, 64])\n",
      "saved skip T_var at layer 5 with shape torch.Size([128, 64, 3, 3])\n",
      "shape of previous transport map now is torch.Size([64, 64])\n",
      "returns a uniform measure of cardinality:  128\n",
      "returns a uniform measure of cardinality:  128\n",
      "Refactored ground metric calc\n",
      "inside refactored\n",
      "Processing the coordinates to form ground_metric\n",
      "dont leave off the squaring of the ground metric\n",
      "# of ground metric features is  51200\n",
      "# of ground metric features in 0 is   51200\n",
      "# of ground metric features in 1 is   51200\n",
      "ground metric (m0) is  tensor([[1192.3733, 1060.3281,  781.2112,  ...,  760.0829,  825.6096,\n",
      "          874.5796],\n",
      "        [ 989.5670,  942.6550,  689.2936,  ...,  762.0859,  699.6639,\n",
      "          762.5479],\n",
      "        [1032.6947,  957.5677,  793.7984,  ...,  974.4238,  971.4148,\n",
      "          893.9586],\n",
      "        ...,\n",
      "        [1001.8492,  878.1637,  631.3725,  ...,  643.8112,  506.0780,\n",
      "          663.3271],\n",
      "        [1284.0522,  989.6373,  701.8581,  ...,  742.4609,  934.4186,\n",
      "          932.4287],\n",
      "        [1239.4237, 1014.9827,  770.0369,  ...,  913.8577, 1092.7830,\n",
      "          955.2015]], device='cuda:0')\n",
      "shape of T_var is  torch.Size([128, 128])\n",
      "T_var before correction  tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')\n",
      "marginals are  tensor([[127.9984, 127.9984, 127.9984,  ..., 127.9984, 127.9984, 127.9984],\n",
      "        [127.9984, 127.9984, 127.9984,  ..., 127.9984, 127.9984, 127.9984],\n",
      "        [127.9984, 127.9984, 127.9984,  ..., 127.9984, 127.9984, 127.9984],\n",
      "        ...,\n",
      "        [127.9984, 127.9984, 127.9984,  ..., 127.9984, 127.9984, 127.9984],\n",
      "        [127.9984, 127.9984, 127.9984,  ..., 127.9984, 127.9984, 127.9984],\n",
      "        [127.9984, 127.9984, 127.9984,  ..., 127.9984, 127.9984, 127.9984]],\n",
      "       device='cuda:0')\n",
      "T_var after correction  tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')\n",
      "T_var stats: max 0.9999872446060181, min 0.0, mean 0.007812400348484516, std 0.08804396539926529 \n",
      "Ratio of trace to the matrix sum:  tensor(0., device='cuda:0')\n",
      "Here, trace is 0.0 and matrix sum is 127.99836730957031 \n",
      "Shape of aligned wt is  torch.Size([128, 64, 9])\n",
      "Shape of fc_layer0_weight_data is  torch.Size([128, 64, 9])\n",
      "model parameters are \n",
      " [torch.Size([64, 3, 3, 3]), torch.Size([64, 64, 3, 3]), torch.Size([64, 64, 3, 3]), torch.Size([64, 64, 3, 3]), torch.Size([64, 64, 3, 3]), torch.Size([128, 64, 3, 3]), torch.Size([128, 128, 3, 3]), torch.Size([128, 64, 1, 1]), torch.Size([128, 128, 3, 3]), torch.Size([128, 128, 3, 3]), torch.Size([256, 128, 3, 3]), torch.Size([256, 256, 3, 3]), torch.Size([256, 128, 1, 1]), torch.Size([256, 256, 3, 3]), torch.Size([256, 256, 3, 3]), torch.Size([512, 256, 3, 3]), torch.Size([512, 512, 3, 3]), torch.Size([512, 256, 1, 1]), torch.Size([512, 512, 3, 3]), torch.Size([512, 512, 3, 3]), torch.Size([10, 512])]\n",
      "len of model_state_dict is  21\n",
      "len of new_params is  6\n",
      "updated parameters for layer  conv1.weight\n",
      "updated parameters for layer  layer1.0.conv1.weight\n",
      "updated parameters for layer  layer1.0.conv2.weight\n",
      "updated parameters for layer  layer1.1.conv1.weight\n",
      "updated parameters for layer  layer1.1.conv2.weight\n",
      "updated parameters for layer  layer2.0.conv1.weight\n",
      "\n",
      "--------- Testing in global mode ---------\n",
      "size of test_loader dataset:  10000\n",
      "\n",
      "Test set: Avg. loss: 0.0026, Accuracy: 1164/10000 (12%)\n",
      "\n",
      "accuracy after update is  11.64\n",
      "For layer idx 5, accuracy of the updated model is 11.64\n",
      "\n",
      "--------------- At layer index 6 ------------- \n",
      " \n",
      "Previous layer shape is  torch.Size([128, 64, 3, 3])\n",
      "let's see the difference in layer names layer2.0.conv2 layer2.0.conv2\n",
      "torch.Size([200, 1, 128, 16, 16]) shape of activations generally\n",
      "reorder_dim is  [1, 2, 3, 0]\n",
      "In layer layer2.0.conv2.weight: getting activation distance statistics\n",
      "Statistics of the distance from neurons of layer 1 (averaged across nodes of layer 0): \n",
      "\n",
      "Max : 69.67372131347656, Mean : 28.836963653564453, Min : 7.86367130279541, Std: 10.876169204711914\n",
      "shape of layer: model 0 torch.Size([128, 128, 9])\n",
      "shape of layer: model 1 torch.Size([128, 128, 9])\n",
      "shape of activations: model 0 torch.Size([128, 16, 16, 200])\n",
      "shape of activations: model 1 torch.Size([128, 16, 16, 200])\n",
      "shape of previous transport map torch.Size([128, 128])\n",
      "doing nothing for skips\n",
      "returns a uniform measure of cardinality:  128\n",
      "returns a uniform measure of cardinality:  128\n",
      "Refactored ground metric calc\n",
      "inside refactored\n",
      "Processing the coordinates to form ground_metric\n",
      "dont leave off the squaring of the ground metric\n",
      "# of ground metric features is  51200\n",
      "# of ground metric features in 0 is   51200\n",
      "# of ground metric features in 1 is   51200\n",
      "ground metric (m0) is  tensor([[2038.8771, 1528.1042, 1854.1049,  ..., 1842.5753,  988.6506,\n",
      "         2101.8086],\n",
      "        [2186.2412, 1292.1688, 2088.1648,  ..., 2171.6926, 1817.9302,\n",
      "         1579.3339],\n",
      "        [2013.7515, 2144.6775, 1877.7771,  ..., 1786.7483, 1789.7200,\n",
      "         2120.4338],\n",
      "        ...,\n",
      "        [1955.9756, 1529.1635, 2053.0588,  ..., 2012.1855, 1846.3430,\n",
      "         1772.5256],\n",
      "        [1609.2029, 1460.7885, 1693.3337,  ..., 1770.9640, 1582.3004,\n",
      "         1656.5200],\n",
      "        [2300.3809, 1745.0087, 2120.9783,  ..., 1924.9921, 1912.7275,\n",
      "         2049.8062]], device='cuda:0')\n",
      "shape of T_var is  torch.Size([128, 128])\n",
      "T_var before correction  tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0078, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        ...,\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],\n",
      "       device='cuda:0')\n",
      "marginals are  tensor([[127.9984, 127.9984, 127.9984,  ..., 127.9984, 127.9984, 127.9984],\n",
      "        [127.9984, 127.9984, 127.9984,  ..., 127.9984, 127.9984, 127.9984],\n",
      "        [127.9984, 127.9984, 127.9984,  ..., 127.9984, 127.9984, 127.9984],\n",
      "        ...,\n",
      "        [127.9984, 127.9984, 127.9984,  ..., 127.9984, 127.9984, 127.9984],\n",
      "        [127.9984, 127.9984, 127.9984,  ..., 127.9984, 127.9984, 127.9984],\n",
      "        [127.9984, 127.9984, 127.9984,  ..., 127.9984, 127.9984, 127.9984]],\n",
      "       device='cuda:0')\n",
      "T_var after correction  tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 1.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        ...,\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],\n",
      "       device='cuda:0')\n",
      "T_var stats: max 0.9999872446060181, min 0.0, mean 0.007812400348484516, std 0.08804396539926529 \n",
      "Ratio of trace to the matrix sum:  tensor(0.0234, device='cuda:0')\n",
      "Here, trace is 2.9999618530273438 and matrix sum is 127.99836730957031 \n",
      "Shape of aligned wt is  torch.Size([128, 128, 9])\n",
      "Shape of fc_layer0_weight_data is  torch.Size([128, 128, 9])\n",
      "model parameters are \n",
      " [torch.Size([64, 3, 3, 3]), torch.Size([64, 64, 3, 3]), torch.Size([64, 64, 3, 3]), torch.Size([64, 64, 3, 3]), torch.Size([64, 64, 3, 3]), torch.Size([128, 64, 3, 3]), torch.Size([128, 128, 3, 3]), torch.Size([128, 64, 1, 1]), torch.Size([128, 128, 3, 3]), torch.Size([128, 128, 3, 3]), torch.Size([256, 128, 3, 3]), torch.Size([256, 256, 3, 3]), torch.Size([256, 128, 1, 1]), torch.Size([256, 256, 3, 3]), torch.Size([256, 256, 3, 3]), torch.Size([512, 256, 3, 3]), torch.Size([512, 512, 3, 3]), torch.Size([512, 256, 1, 1]), torch.Size([512, 512, 3, 3]), torch.Size([512, 512, 3, 3]), torch.Size([10, 512])]\n",
      "len of model_state_dict is  21\n",
      "len of new_params is  7\n",
      "updated parameters for layer  conv1.weight\n",
      "updated parameters for layer  layer1.0.conv1.weight\n",
      "updated parameters for layer  layer1.0.conv2.weight\n",
      "updated parameters for layer  layer1.1.conv1.weight\n",
      "updated parameters for layer  layer1.1.conv2.weight\n",
      "updated parameters for layer  layer2.0.conv1.weight\n",
      "updated parameters for layer  layer2.0.conv2.weight\n",
      "\n",
      "--------- Testing in global mode ---------\n",
      "size of test_loader dataset:  10000\n",
      "\n",
      "Test set: Avg. loss: 0.0026, Accuracy: 1225/10000 (12%)\n",
      "\n",
      "accuracy after update is  12.25\n",
      "For layer idx 6, accuracy of the updated model is 12.25\n",
      "\n",
      "--------------- At layer index 7 ------------- \n",
      " \n",
      "Previous layer shape is  torch.Size([128, 128, 3, 3])\n",
      "let's see the difference in layer names layer2.0.shortcut.0 layer2.0.shortcut.0\n",
      "torch.Size([200, 1, 128, 16, 16]) shape of activations generally\n",
      "reorder_dim is  [1, 2, 3, 0]\n",
      "In layer layer2.0.shortcut.0.weight: getting activation distance statistics\n",
      "Statistics of the distance from neurons of layer 1 (averaged across nodes of layer 0): \n",
      "\n",
      "Max : 9.038509368896484, Mean : 3.727484941482544, Min : 1.062530517578125, Std: 1.432983160018921\n",
      "shape of layer: model 0 torch.Size([128, 64, 1])\n",
      "shape of layer: model 1 torch.Size([128, 64, 1])\n",
      "shape of activations: model 0 torch.Size([128, 16, 16, 200])\n",
      "shape of activations: model 1 torch.Size([128, 16, 16, 200])\n",
      "shape of previous transport map torch.Size([128, 128])\n",
      "utilizing skip T_var saved from layer layer 5 with shape torch.Size([64, 64])\n",
      "shape of previous transport map now is torch.Size([64, 64])\n",
      "returns a uniform measure of cardinality:  128\n",
      "returns a uniform measure of cardinality:  128\n",
      "Refactored ground metric calc\n",
      "inside refactored\n",
      "Processing the coordinates to form ground_metric\n",
      "dont leave off the squaring of the ground metric\n",
      "# of ground metric features is  51200\n",
      "# of ground metric features in 0 is   51200\n",
      "# of ground metric features in 1 is   51200\n",
      "ground metric (m0) is  tensor([[219.1528, 174.5372, 292.6460,  ..., 277.9985, 128.8770, 359.4307],\n",
      "        [286.8748, 147.0660, 299.3468,  ..., 413.3399, 269.3244, 285.2658],\n",
      "        [241.5113, 255.0101, 322.2430,  ..., 172.1681, 118.3590, 418.7408],\n",
      "        ...,\n",
      "        [266.5919, 173.8581, 327.3127,  ..., 365.5027, 244.7614, 322.3637],\n",
      "        [137.5658, 163.0447, 223.3396,  ..., 293.1747, 201.1736, 190.3193],\n",
      "        [203.6561, 171.3126, 270.6688,  ..., 237.4770, 167.8083, 321.4987]],\n",
      "       device='cuda:0')\n",
      "shape of T_var is  torch.Size([128, 128])\n",
      "T_var before correction  tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0078, 0.0000],\n",
      "        ...,\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],\n",
      "       device='cuda:0')\n",
      "marginals are  tensor([[127.9984, 127.9984, 127.9984,  ..., 127.9984, 127.9984, 127.9984],\n",
      "        [127.9984, 127.9984, 127.9984,  ..., 127.9984, 127.9984, 127.9984],\n",
      "        [127.9984, 127.9984, 127.9984,  ..., 127.9984, 127.9984, 127.9984],\n",
      "        ...,\n",
      "        [127.9984, 127.9984, 127.9984,  ..., 127.9984, 127.9984, 127.9984],\n",
      "        [127.9984, 127.9984, 127.9984,  ..., 127.9984, 127.9984, 127.9984],\n",
      "        [127.9984, 127.9984, 127.9984,  ..., 127.9984, 127.9984, 127.9984]],\n",
      "       device='cuda:0')\n",
      "T_var after correction  tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 1.0000, 0.0000],\n",
      "        ...,\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],\n",
      "       device='cuda:0')\n",
      "T_var stats: max 0.9999872446060181, min 0.0, mean 0.007812400348484516, std 0.08804396539926529 \n",
      "Ratio of trace to the matrix sum:  tensor(0.0156, device='cuda:0')\n",
      "Here, trace is 1.9999744892120361 and matrix sum is 127.99836730957031 \n",
      "Shape of aligned wt is  torch.Size([128, 64, 1])\n",
      "Shape of fc_layer0_weight_data is  torch.Size([128, 64, 1])\n",
      "model parameters are \n",
      " [torch.Size([64, 3, 3, 3]), torch.Size([64, 64, 3, 3]), torch.Size([64, 64, 3, 3]), torch.Size([64, 64, 3, 3]), torch.Size([64, 64, 3, 3]), torch.Size([128, 64, 3, 3]), torch.Size([128, 128, 3, 3]), torch.Size([128, 64, 1, 1]), torch.Size([128, 128, 3, 3]), torch.Size([128, 128, 3, 3]), torch.Size([256, 128, 3, 3]), torch.Size([256, 256, 3, 3]), torch.Size([256, 128, 1, 1]), torch.Size([256, 256, 3, 3]), torch.Size([256, 256, 3, 3]), torch.Size([512, 256, 3, 3]), torch.Size([512, 512, 3, 3]), torch.Size([512, 256, 1, 1]), torch.Size([512, 512, 3, 3]), torch.Size([512, 512, 3, 3]), torch.Size([10, 512])]\n",
      "len of model_state_dict is  21\n",
      "len of new_params is  8\n",
      "updated parameters for layer  conv1.weight\n",
      "updated parameters for layer  layer1.0.conv1.weight\n",
      "updated parameters for layer  layer1.0.conv2.weight\n",
      "updated parameters for layer  layer1.1.conv1.weight\n",
      "updated parameters for layer  layer1.1.conv2.weight\n",
      "updated parameters for layer  layer2.0.conv1.weight\n",
      "updated parameters for layer  layer2.0.conv2.weight\n",
      "updated parameters for layer  layer2.0.shortcut.0.weight\n",
      "\n",
      "--------- Testing in global mode ---------\n",
      "size of test_loader dataset:  10000\n",
      "\n",
      "Test set: Avg. loss: 0.0025, Accuracy: 1096/10000 (11%)\n",
      "\n",
      "accuracy after update is  10.96\n",
      "For layer idx 7, accuracy of the updated model is 10.96\n",
      "\n",
      "--------------- At layer index 8 ------------- \n",
      " \n",
      "Previous layer shape is  torch.Size([128, 64, 1, 1])\n",
      "let's see the difference in layer names layer2.1.conv1 layer2.1.conv1\n",
      "torch.Size([200, 1, 128, 16, 16]) shape of activations generally\n",
      "reorder_dim is  [1, 2, 3, 0]\n",
      "In layer layer2.1.conv1.weight: getting activation distance statistics\n",
      "Statistics of the distance from neurons of layer 1 (averaged across nodes of layer 0): \n",
      "\n",
      "Max : 90.65512084960938, Mean : 38.59190368652344, Min : 11.674477577209473, Std: 13.924339294433594\n",
      "shape of layer: model 0 torch.Size([128, 128, 9])\n",
      "shape of layer: model 1 torch.Size([128, 128, 9])\n",
      "shape of activations: model 0 torch.Size([128, 16, 16, 200])\n",
      "shape of activations: model 1 torch.Size([128, 16, 16, 200])\n",
      "shape of previous transport map torch.Size([128, 128])\n",
      "averaging multiple T_var's\n",
      "returns a uniform measure of cardinality:  128\n",
      "returns a uniform measure of cardinality:  128\n",
      "Refactored ground metric calc\n",
      "inside refactored\n",
      "Processing the coordinates to form ground_metric\n",
      "dont leave off the squaring of the ground metric\n",
      "# of ground metric features is  51200\n",
      "# of ground metric features in 0 is   51200\n",
      "# of ground metric features in 1 is   51200\n",
      "ground metric (m0) is  tensor([[2770.9478, 2677.4436, 2914.1033,  ..., 2239.0000, 2284.2988,\n",
      "         2745.7456],\n",
      "        [2445.3223, 1979.1643, 3212.6855,  ..., 2067.0825, 1696.6730,\n",
      "         2394.9917],\n",
      "        [2579.4866, 1789.3162, 2797.9646,  ..., 1421.9883, 1893.0837,\n",
      "         2267.3142],\n",
      "        ...,\n",
      "        [2872.1616, 1720.8264, 3328.5049,  ..., 1993.1459, 1727.6411,\n",
      "         2354.8403],\n",
      "        [2665.3723, 2392.3972, 3192.8479,  ..., 2134.0669, 2249.1653,\n",
      "         2719.5503],\n",
      "        [2440.4844, 2567.7668, 2702.9099,  ..., 2416.9043, 2293.8047,\n",
      "         2448.1038]], device='cuda:0')\n",
      "shape of T_var is  torch.Size([128, 128])\n",
      "T_var before correction  tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')\n",
      "marginals are  tensor([[127.9984, 127.9984, 127.9984,  ..., 127.9984, 127.9984, 127.9984],\n",
      "        [127.9984, 127.9984, 127.9984,  ..., 127.9984, 127.9984, 127.9984],\n",
      "        [127.9984, 127.9984, 127.9984,  ..., 127.9984, 127.9984, 127.9984],\n",
      "        ...,\n",
      "        [127.9984, 127.9984, 127.9984,  ..., 127.9984, 127.9984, 127.9984],\n",
      "        [127.9984, 127.9984, 127.9984,  ..., 127.9984, 127.9984, 127.9984],\n",
      "        [127.9984, 127.9984, 127.9984,  ..., 127.9984, 127.9984, 127.9984]],\n",
      "       device='cuda:0')\n",
      "T_var after correction  tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')\n",
      "T_var stats: max 0.9999872446060181, min 0.0, mean 0.007812400348484516, std 0.08804396539926529 \n",
      "Ratio of trace to the matrix sum:  tensor(0.0234, device='cuda:0')\n",
      "Here, trace is 2.9999618530273438 and matrix sum is 127.99836730957031 \n",
      "Shape of aligned wt is  torch.Size([128, 128, 9])\n",
      "Shape of fc_layer0_weight_data is  torch.Size([128, 128, 9])\n",
      "model parameters are \n",
      " [torch.Size([64, 3, 3, 3]), torch.Size([64, 64, 3, 3]), torch.Size([64, 64, 3, 3]), torch.Size([64, 64, 3, 3]), torch.Size([64, 64, 3, 3]), torch.Size([128, 64, 3, 3]), torch.Size([128, 128, 3, 3]), torch.Size([128, 64, 1, 1]), torch.Size([128, 128, 3, 3]), torch.Size([128, 128, 3, 3]), torch.Size([256, 128, 3, 3]), torch.Size([256, 256, 3, 3]), torch.Size([256, 128, 1, 1]), torch.Size([256, 256, 3, 3]), torch.Size([256, 256, 3, 3]), torch.Size([512, 256, 3, 3]), torch.Size([512, 512, 3, 3]), torch.Size([512, 256, 1, 1]), torch.Size([512, 512, 3, 3]), torch.Size([512, 512, 3, 3]), torch.Size([10, 512])]\n",
      "len of model_state_dict is  21\n",
      "len of new_params is  9\n",
      "updated parameters for layer  conv1.weight\n",
      "updated parameters for layer  layer1.0.conv1.weight\n",
      "updated parameters for layer  layer1.0.conv2.weight\n",
      "updated parameters for layer  layer1.1.conv1.weight\n",
      "updated parameters for layer  layer1.1.conv2.weight\n",
      "updated parameters for layer  layer2.0.conv1.weight\n",
      "updated parameters for layer  layer2.0.conv2.weight\n",
      "updated parameters for layer  layer2.0.shortcut.0.weight\n",
      "updated parameters for layer  layer2.1.conv1.weight\n",
      "\n",
      "--------- Testing in global mode ---------\n",
      "size of test_loader dataset:  10000\n",
      "\n",
      "Test set: Avg. loss: 0.0025, Accuracy: 1123/10000 (11%)\n",
      "\n",
      "accuracy after update is  11.23\n",
      "For layer idx 8, accuracy of the updated model is 11.23\n",
      "\n",
      "--------------- At layer index 9 ------------- \n",
      " \n",
      "Previous layer shape is  torch.Size([128, 128, 3, 3])\n",
      "let's see the difference in layer names layer2.1.conv2 layer2.1.conv2\n",
      "torch.Size([200, 1, 128, 16, 16]) shape of activations generally\n",
      "reorder_dim is  [1, 2, 3, 0]\n",
      "In layer layer2.1.conv2.weight: getting activation distance statistics\n",
      "Statistics of the distance from neurons of layer 1 (averaged across nodes of layer 0): \n",
      "\n",
      "Max : 47.5161247253418, Mean : 16.236358642578125, Min : 3.310929536819458, Std: 7.720048427581787\n",
      "shape of layer: model 0 torch.Size([128, 128, 9])\n",
      "shape of layer: model 1 torch.Size([128, 128, 9])\n",
      "shape of activations: model 0 torch.Size([128, 16, 16, 200])\n",
      "shape of activations: model 1 torch.Size([128, 16, 16, 200])\n",
      "shape of previous transport map torch.Size([128, 128])\n",
      "doing nothing for skips\n",
      "returns a uniform measure of cardinality:  128\n",
      "returns a uniform measure of cardinality:  128\n",
      "Refactored ground metric calc\n",
      "inside refactored\n",
      "Processing the coordinates to form ground_metric\n",
      "dont leave off the squaring of the ground metric\n",
      "# of ground metric features is  51200\n",
      "# of ground metric features in 0 is   51200\n",
      "# of ground metric features in 1 is   51200\n",
      "ground metric (m0) is  tensor([[1139.5554,  976.5358, 1191.6093,  ..., 1016.2870,  785.7998,\n",
      "         1186.1700],\n",
      "        [1309.2720,  895.9313, 1292.3258,  ..., 1168.4280, 1194.9004,\n",
      "         1026.0941],\n",
      "        [1047.5527, 1130.5544, 1037.5256,  ...,  934.4915,  929.5496,\n",
      "         1100.9482],\n",
      "        ...,\n",
      "        [1171.8241, 1106.1378, 1250.4854,  ..., 1138.2679, 1116.2449,\n",
      "         1158.6663],\n",
      "        [ 991.7535, 1003.1339, 1072.9302,  ...,  911.4177,  941.8649,\n",
      "          940.1415],\n",
      "        [1199.9310, 1159.3146, 1231.2767,  ..., 1028.6543, 1076.5289,\n",
      "         1202.2408]], device='cuda:0')\n",
      "shape of T_var is  torch.Size([128, 128])\n",
      "T_var before correction  tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0078, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        ...,\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],\n",
      "       device='cuda:0')\n",
      "marginals are  tensor([[127.9984, 127.9984, 127.9984,  ..., 127.9984, 127.9984, 127.9984],\n",
      "        [127.9984, 127.9984, 127.9984,  ..., 127.9984, 127.9984, 127.9984],\n",
      "        [127.9984, 127.9984, 127.9984,  ..., 127.9984, 127.9984, 127.9984],\n",
      "        ...,\n",
      "        [127.9984, 127.9984, 127.9984,  ..., 127.9984, 127.9984, 127.9984],\n",
      "        [127.9984, 127.9984, 127.9984,  ..., 127.9984, 127.9984, 127.9984],\n",
      "        [127.9984, 127.9984, 127.9984,  ..., 127.9984, 127.9984, 127.9984]],\n",
      "       device='cuda:0')\n",
      "T_var after correction  tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 1.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        ...,\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],\n",
      "       device='cuda:0')\n",
      "T_var stats: max 0.9999872446060181, min 0.0, mean 0.007812400348484516, std 0.08804396539926529 \n",
      "Ratio of trace to the matrix sum:  tensor(0.0156, device='cuda:0')\n",
      "Here, trace is 1.9999744892120361 and matrix sum is 127.99836730957031 \n",
      "Shape of aligned wt is  torch.Size([128, 128, 9])\n",
      "Shape of fc_layer0_weight_data is  torch.Size([128, 128, 9])\n",
      "model parameters are \n",
      " [torch.Size([64, 3, 3, 3]), torch.Size([64, 64, 3, 3]), torch.Size([64, 64, 3, 3]), torch.Size([64, 64, 3, 3]), torch.Size([64, 64, 3, 3]), torch.Size([128, 64, 3, 3]), torch.Size([128, 128, 3, 3]), torch.Size([128, 64, 1, 1]), torch.Size([128, 128, 3, 3]), torch.Size([128, 128, 3, 3]), torch.Size([256, 128, 3, 3]), torch.Size([256, 256, 3, 3]), torch.Size([256, 128, 1, 1]), torch.Size([256, 256, 3, 3]), torch.Size([256, 256, 3, 3]), torch.Size([512, 256, 3, 3]), torch.Size([512, 512, 3, 3]), torch.Size([512, 256, 1, 1]), torch.Size([512, 512, 3, 3]), torch.Size([512, 512, 3, 3]), torch.Size([10, 512])]\n",
      "len of model_state_dict is  21\n",
      "len of new_params is  10\n",
      "updated parameters for layer  conv1.weight\n",
      "updated parameters for layer  layer1.0.conv1.weight\n",
      "updated parameters for layer  layer1.0.conv2.weight\n",
      "updated parameters for layer  layer1.1.conv1.weight\n",
      "updated parameters for layer  layer1.1.conv2.weight\n",
      "updated parameters for layer  layer2.0.conv1.weight\n",
      "updated parameters for layer  layer2.0.conv2.weight\n",
      "updated parameters for layer  layer2.0.shortcut.0.weight\n",
      "updated parameters for layer  layer2.1.conv1.weight\n",
      "updated parameters for layer  layer2.1.conv2.weight\n",
      "\n",
      "--------- Testing in global mode ---------\n",
      "size of test_loader dataset:  10000\n",
      "\n",
      "Test set: Avg. loss: 0.0025, Accuracy: 1088/10000 (11%)\n",
      "\n",
      "accuracy after update is  10.88\n",
      "For layer idx 9, accuracy of the updated model is 10.88\n",
      "\n",
      "--------------- At layer index 10 ------------- \n",
      " \n",
      "Previous layer shape is  torch.Size([128, 128, 3, 3])\n",
      "let's see the difference in layer names layer3.0.conv1 layer3.0.conv1\n",
      "torch.Size([200, 1, 256, 8, 8]) shape of activations generally\n",
      "reorder_dim is  [1, 2, 3, 0]\n",
      "In layer layer3.0.conv1.weight: getting activation distance statistics\n",
      "Statistics of the distance from neurons of layer 1 (averaged across nodes of layer 0): \n",
      "\n",
      "Max : 72.6119613647461, Mean : 28.415193557739258, Min : 7.301274299621582, Std: 11.454460144042969\n",
      "shape of layer: model 0 torch.Size([256, 128, 9])\n",
      "shape of layer: model 1 torch.Size([256, 128, 9])\n",
      "shape of activations: model 0 torch.Size([256, 8, 8, 200])\n",
      "shape of activations: model 1 torch.Size([256, 8, 8, 200])\n",
      "shape of previous transport map torch.Size([128, 128])\n",
      "saved skip T_var at layer 10 with shape torch.Size([256, 128, 3, 3])\n",
      "shape of previous transport map now is torch.Size([128, 128])\n",
      "returns a uniform measure of cardinality:  256\n",
      "returns a uniform measure of cardinality:  256\n",
      "Refactored ground metric calc\n",
      "inside refactored\n",
      "Processing the coordinates to form ground_metric\n",
      "dont leave off the squaring of the ground metric\n",
      "# of ground metric features is  12800\n",
      "# of ground metric features in 0 is   12800\n",
      "# of ground metric features in 1 is   12800\n",
      "ground metric (m0) is  tensor([[1287.4443, 1287.8524, 1168.3113,  ..., 1312.5344, 1285.8518,\n",
      "         1249.0531],\n",
      "        [1178.8639, 1298.9137, 1028.2218,  ..., 1347.7249, 1373.2987,\n",
      "         1237.7118],\n",
      "        [1271.6045, 1032.3695, 1170.3121,  ..., 1128.3137, 1347.6445,\n",
      "         1344.4656],\n",
      "        ...,\n",
      "        [ 990.0442, 1341.2604, 1319.6036,  ..., 1362.6608, 1159.8529,\n",
      "         1213.9515],\n",
      "        [1405.4014, 1216.1440, 1468.5873,  ..., 1470.3071, 1397.8615,\n",
      "         1451.5333],\n",
      "        [1451.4911, 1064.7993, 1066.6111,  ..., 1157.4167, 1499.9841,\n",
      "         1399.9978]], device='cuda:0')\n",
      "shape of T_var is  torch.Size([256, 256])\n",
      "T_var before correction  tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')\n",
      "marginals are  tensor([[255.9934, 255.9934, 255.9934,  ..., 255.9934, 255.9934, 255.9934],\n",
      "        [255.9934, 255.9934, 255.9934,  ..., 255.9934, 255.9934, 255.9934],\n",
      "        [255.9934, 255.9934, 255.9934,  ..., 255.9934, 255.9934, 255.9934],\n",
      "        ...,\n",
      "        [255.9934, 255.9934, 255.9934,  ..., 255.9934, 255.9934, 255.9934],\n",
      "        [255.9934, 255.9934, 255.9934,  ..., 255.9934, 255.9934, 255.9934],\n",
      "        [255.9934, 255.9934, 255.9934,  ..., 255.9934, 255.9934, 255.9934]],\n",
      "       device='cuda:0')\n",
      "T_var after correction  tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')\n",
      "T_var stats: max 0.9999743700027466, min 0.0, mean 0.003906149882823229, std 0.06237669289112091 \n",
      "Ratio of trace to the matrix sum:  tensor(0.0078, device='cuda:0')\n",
      "Here, trace is 1.9999487400054932 and matrix sum is 255.99343872070312 \n",
      "Shape of aligned wt is  torch.Size([256, 128, 9])\n",
      "Shape of fc_layer0_weight_data is  torch.Size([256, 128, 9])\n",
      "model parameters are \n",
      " [torch.Size([64, 3, 3, 3]), torch.Size([64, 64, 3, 3]), torch.Size([64, 64, 3, 3]), torch.Size([64, 64, 3, 3]), torch.Size([64, 64, 3, 3]), torch.Size([128, 64, 3, 3]), torch.Size([128, 128, 3, 3]), torch.Size([128, 64, 1, 1]), torch.Size([128, 128, 3, 3]), torch.Size([128, 128, 3, 3]), torch.Size([256, 128, 3, 3]), torch.Size([256, 256, 3, 3]), torch.Size([256, 128, 1, 1]), torch.Size([256, 256, 3, 3]), torch.Size([256, 256, 3, 3]), torch.Size([512, 256, 3, 3]), torch.Size([512, 512, 3, 3]), torch.Size([512, 256, 1, 1]), torch.Size([512, 512, 3, 3]), torch.Size([512, 512, 3, 3]), torch.Size([10, 512])]\n",
      "len of model_state_dict is  21\n",
      "len of new_params is  11\n",
      "updated parameters for layer  conv1.weight\n",
      "updated parameters for layer  layer1.0.conv1.weight\n",
      "updated parameters for layer  layer1.0.conv2.weight\n",
      "updated parameters for layer  layer1.1.conv1.weight\n",
      "updated parameters for layer  layer1.1.conv2.weight\n",
      "updated parameters for layer  layer2.0.conv1.weight\n",
      "updated parameters for layer  layer2.0.conv2.weight\n",
      "updated parameters for layer  layer2.0.shortcut.0.weight\n",
      "updated parameters for layer  layer2.1.conv1.weight\n",
      "updated parameters for layer  layer2.1.conv2.weight\n",
      "updated parameters for layer  layer3.0.conv1.weight\n",
      "\n",
      "--------- Testing in global mode ---------\n",
      "size of test_loader dataset:  10000\n",
      "\n",
      "Test set: Avg. loss: 0.0025, Accuracy: 1066/10000 (11%)\n",
      "\n",
      "accuracy after update is  10.66\n",
      "For layer idx 10, accuracy of the updated model is 10.66\n",
      "\n",
      "--------------- At layer index 11 ------------- \n",
      " \n",
      "Previous layer shape is  torch.Size([256, 128, 3, 3])\n",
      "let's see the difference in layer names layer3.0.conv2 layer3.0.conv2\n",
      "torch.Size([200, 1, 256, 8, 8]) shape of activations generally\n",
      "reorder_dim is  [1, 2, 3, 0]\n",
      "In layer layer3.0.conv2.weight: getting activation distance statistics\n",
      "Statistics of the distance from neurons of layer 1 (averaged across nodes of layer 0): \n",
      "\n",
      "Max : 108.44442749023438, Mean : 40.11443328857422, Min : 10.02967357635498, Std: 17.06110191345215\n",
      "shape of layer: model 0 torch.Size([256, 256, 9])\n",
      "shape of layer: model 1 torch.Size([256, 256, 9])\n",
      "shape of activations: model 0 torch.Size([256, 8, 8, 200])\n",
      "shape of activations: model 1 torch.Size([256, 8, 8, 200])\n",
      "shape of previous transport map torch.Size([256, 256])\n",
      "doing nothing for skips\n",
      "returns a uniform measure of cardinality:  256\n",
      "returns a uniform measure of cardinality:  256\n",
      "Refactored ground metric calc\n",
      "inside refactored\n",
      "Processing the coordinates to form ground_metric\n",
      "dont leave off the squaring of the ground metric\n",
      "# of ground metric features is  12800\n",
      "# of ground metric features in 0 is   12800\n",
      "# of ground metric features in 1 is   12800\n",
      "ground metric (m0) is  tensor([[1859.8282, 1559.1497, 1167.1875,  ..., 1284.0127, 1557.5787,\n",
      "         1726.9183],\n",
      "        [1933.3406, 1595.1914, 1450.6560,  ..., 1462.0144, 1518.0083,\n",
      "         1683.7645],\n",
      "        [1915.8965, 1669.1144, 2005.5115,  ..., 1995.9124, 1835.7939,\n",
      "         2404.8203],\n",
      "        ...,\n",
      "        [1922.0677, 1371.5503, 1048.1376,  ...,  843.1371, 1312.6128,\n",
      "         1404.9976],\n",
      "        [2233.7029, 1709.4801, 1452.2393,  ..., 1391.8591, 1580.8760,\n",
      "         1621.3979],\n",
      "        [2375.6836, 1837.0708, 2009.3828,  ..., 2047.4508, 2075.8267,\n",
      "         2249.4949]], device='cuda:0')\n",
      "shape of T_var is  torch.Size([256, 256])\n",
      "T_var before correction  tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')\n",
      "marginals are  tensor([[255.9934, 255.9934, 255.9934,  ..., 255.9934, 255.9934, 255.9934],\n",
      "        [255.9934, 255.9934, 255.9934,  ..., 255.9934, 255.9934, 255.9934],\n",
      "        [255.9934, 255.9934, 255.9934,  ..., 255.9934, 255.9934, 255.9934],\n",
      "        ...,\n",
      "        [255.9934, 255.9934, 255.9934,  ..., 255.9934, 255.9934, 255.9934],\n",
      "        [255.9934, 255.9934, 255.9934,  ..., 255.9934, 255.9934, 255.9934],\n",
      "        [255.9934, 255.9934, 255.9934,  ..., 255.9934, 255.9934, 255.9934]],\n",
      "       device='cuda:0')\n",
      "T_var after correction  tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')\n",
      "T_var stats: max 0.9999743700027466, min 0.0, mean 0.003906149882823229, std 0.06237669289112091 \n",
      "Ratio of trace to the matrix sum:  tensor(0.0078, device='cuda:0')\n",
      "Here, trace is 1.9999487400054932 and matrix sum is 255.99343872070312 \n",
      "Shape of aligned wt is  torch.Size([256, 256, 9])\n",
      "Shape of fc_layer0_weight_data is  torch.Size([256, 256, 9])\n",
      "model parameters are \n",
      " [torch.Size([64, 3, 3, 3]), torch.Size([64, 64, 3, 3]), torch.Size([64, 64, 3, 3]), torch.Size([64, 64, 3, 3]), torch.Size([64, 64, 3, 3]), torch.Size([128, 64, 3, 3]), torch.Size([128, 128, 3, 3]), torch.Size([128, 64, 1, 1]), torch.Size([128, 128, 3, 3]), torch.Size([128, 128, 3, 3]), torch.Size([256, 128, 3, 3]), torch.Size([256, 256, 3, 3]), torch.Size([256, 128, 1, 1]), torch.Size([256, 256, 3, 3]), torch.Size([256, 256, 3, 3]), torch.Size([512, 256, 3, 3]), torch.Size([512, 512, 3, 3]), torch.Size([512, 256, 1, 1]), torch.Size([512, 512, 3, 3]), torch.Size([512, 512, 3, 3]), torch.Size([10, 512])]\n",
      "len of model_state_dict is  21\n",
      "len of new_params is  12\n",
      "updated parameters for layer  conv1.weight\n",
      "updated parameters for layer  layer1.0.conv1.weight\n",
      "updated parameters for layer  layer1.0.conv2.weight\n",
      "updated parameters for layer  layer1.1.conv1.weight\n",
      "updated parameters for layer  layer1.1.conv2.weight\n",
      "updated parameters for layer  layer2.0.conv1.weight\n",
      "updated parameters for layer  layer2.0.conv2.weight\n",
      "updated parameters for layer  layer2.0.shortcut.0.weight\n",
      "updated parameters for layer  layer2.1.conv1.weight\n",
      "updated parameters for layer  layer2.1.conv2.weight\n",
      "updated parameters for layer  layer3.0.conv1.weight\n",
      "updated parameters for layer  layer3.0.conv2.weight\n",
      "\n",
      "--------- Testing in global mode ---------\n",
      "size of test_loader dataset:  10000\n",
      "\n",
      "Test set: Avg. loss: 0.0024, Accuracy: 1094/10000 (11%)\n",
      "\n",
      "accuracy after update is  10.94\n",
      "For layer idx 11, accuracy of the updated model is 10.94\n",
      "\n",
      "--------------- At layer index 12 ------------- \n",
      " \n",
      "Previous layer shape is  torch.Size([256, 256, 3, 3])\n",
      "let's see the difference in layer names layer3.0.shortcut.0 layer3.0.shortcut.0\n",
      "torch.Size([200, 1, 256, 8, 8]) shape of activations generally\n",
      "reorder_dim is  [1, 2, 3, 0]\n",
      "In layer layer3.0.shortcut.0.weight: getting activation distance statistics\n",
      "Statistics of the distance from neurons of layer 1 (averaged across nodes of layer 0): \n",
      "\n",
      "Max : 24.356285095214844, Mean : 10.060906410217285, Min : 2.575192928314209, Std: 3.878880023956299\n",
      "shape of layer: model 0 torch.Size([256, 128, 1])\n",
      "shape of layer: model 1 torch.Size([256, 128, 1])\n",
      "shape of activations: model 0 torch.Size([256, 8, 8, 200])\n",
      "shape of activations: model 1 torch.Size([256, 8, 8, 200])\n",
      "shape of previous transport map torch.Size([256, 256])\n",
      "utilizing skip T_var saved from layer layer 10 with shape torch.Size([128, 128])\n",
      "shape of previous transport map now is torch.Size([128, 128])\n",
      "returns a uniform measure of cardinality:  256\n",
      "returns a uniform measure of cardinality:  256\n",
      "Refactored ground metric calc\n",
      "inside refactored\n",
      "Processing the coordinates to form ground_metric\n",
      "dont leave off the squaring of the ground metric\n",
      "# of ground metric features is  12800\n",
      "# of ground metric features in 0 is   12800\n",
      "# of ground metric features in 1 is   12800\n",
      "ground metric (m0) is  tensor([[336.2382, 380.9034, 228.6416,  ..., 263.1572, 278.4709, 451.3280],\n",
      "        [511.4926, 334.8108, 355.3893,  ..., 256.4306, 338.9612, 360.4801],\n",
      "        [390.7235, 313.7056, 407.2997,  ..., 376.6057, 448.6642, 687.4634],\n",
      "        ...,\n",
      "        [428.3895, 207.5881, 199.7808,  ..., 116.6445, 275.6156, 365.2712],\n",
      "        [587.4004, 355.2289, 389.7003,  ..., 284.2259, 415.4932, 348.7348],\n",
      "        [634.6657, 321.1019, 458.5935,  ..., 350.9427, 530.0838, 536.9282]],\n",
      "       device='cuda:0')\n",
      "shape of T_var is  torch.Size([256, 256])\n",
      "T_var before correction  tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')\n",
      "marginals are  tensor([[255.9934, 255.9934, 255.9934,  ..., 255.9934, 255.9934, 255.9934],\n",
      "        [255.9934, 255.9934, 255.9934,  ..., 255.9934, 255.9934, 255.9934],\n",
      "        [255.9934, 255.9934, 255.9934,  ..., 255.9934, 255.9934, 255.9934],\n",
      "        ...,\n",
      "        [255.9934, 255.9934, 255.9934,  ..., 255.9934, 255.9934, 255.9934],\n",
      "        [255.9934, 255.9934, 255.9934,  ..., 255.9934, 255.9934, 255.9934],\n",
      "        [255.9934, 255.9934, 255.9934,  ..., 255.9934, 255.9934, 255.9934]],\n",
      "       device='cuda:0')\n",
      "T_var after correction  tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')\n",
      "T_var stats: max 0.9999743700027466, min 0.0, mean 0.003906149882823229, std 0.06237669289112091 \n",
      "Ratio of trace to the matrix sum:  tensor(0., device='cuda:0')\n",
      "Here, trace is 0.0 and matrix sum is 255.99343872070312 \n",
      "Shape of aligned wt is  torch.Size([256, 128, 1])\n",
      "Shape of fc_layer0_weight_data is  torch.Size([256, 128, 1])\n",
      "model parameters are \n",
      " [torch.Size([64, 3, 3, 3]), torch.Size([64, 64, 3, 3]), torch.Size([64, 64, 3, 3]), torch.Size([64, 64, 3, 3]), torch.Size([64, 64, 3, 3]), torch.Size([128, 64, 3, 3]), torch.Size([128, 128, 3, 3]), torch.Size([128, 64, 1, 1]), torch.Size([128, 128, 3, 3]), torch.Size([128, 128, 3, 3]), torch.Size([256, 128, 3, 3]), torch.Size([256, 256, 3, 3]), torch.Size([256, 128, 1, 1]), torch.Size([256, 256, 3, 3]), torch.Size([256, 256, 3, 3]), torch.Size([512, 256, 3, 3]), torch.Size([512, 512, 3, 3]), torch.Size([512, 256, 1, 1]), torch.Size([512, 512, 3, 3]), torch.Size([512, 512, 3, 3]), torch.Size([10, 512])]\n",
      "len of model_state_dict is  21\n",
      "len of new_params is  13\n",
      "updated parameters for layer  conv1.weight\n",
      "updated parameters for layer  layer1.0.conv1.weight\n",
      "updated parameters for layer  layer1.0.conv2.weight\n",
      "updated parameters for layer  layer1.1.conv1.weight\n",
      "updated parameters for layer  layer1.1.conv2.weight\n",
      "updated parameters for layer  layer2.0.conv1.weight\n",
      "updated parameters for layer  layer2.0.conv2.weight\n",
      "updated parameters for layer  layer2.0.shortcut.0.weight\n",
      "updated parameters for layer  layer2.1.conv1.weight\n",
      "updated parameters for layer  layer2.1.conv2.weight\n",
      "updated parameters for layer  layer3.0.conv1.weight\n",
      "updated parameters for layer  layer3.0.conv2.weight\n",
      "updated parameters for layer  layer3.0.shortcut.0.weight\n",
      "\n",
      "--------- Testing in global mode ---------\n",
      "size of test_loader dataset:  10000\n",
      "\n",
      "Test set: Avg. loss: 0.0024, Accuracy: 1170/10000 (12%)\n",
      "\n",
      "accuracy after update is  11.7\n",
      "For layer idx 12, accuracy of the updated model is 11.7\n",
      "\n",
      "--------------- At layer index 13 ------------- \n",
      " \n",
      "Previous layer shape is  torch.Size([256, 128, 1, 1])\n",
      "let's see the difference in layer names layer3.1.conv1 layer3.1.conv1\n",
      "torch.Size([200, 1, 256, 8, 8]) shape of activations generally\n",
      "reorder_dim is  [1, 2, 3, 0]\n",
      "In layer layer3.1.conv1.weight: getting activation distance statistics\n",
      "Statistics of the distance from neurons of layer 1 (averaged across nodes of layer 0): \n",
      "\n",
      "Max : 128.73101806640625, Mean : 48.026268005371094, Min : 12.486542701721191, Std: 20.23589324951172\n",
      "shape of layer: model 0 torch.Size([256, 256, 9])\n",
      "shape of layer: model 1 torch.Size([256, 256, 9])\n",
      "shape of activations: model 0 torch.Size([256, 8, 8, 200])\n",
      "shape of activations: model 1 torch.Size([256, 8, 8, 200])\n",
      "shape of previous transport map torch.Size([256, 256])\n",
      "averaging multiple T_var's\n",
      "returns a uniform measure of cardinality:  256\n",
      "returns a uniform measure of cardinality:  256\n",
      "Refactored ground metric calc\n",
      "inside refactored\n",
      "Processing the coordinates to form ground_metric\n",
      "dont leave off the squaring of the ground metric\n",
      "# of ground metric features is  12800\n",
      "# of ground metric features in 0 is   12800\n",
      "# of ground metric features in 1 is   12800\n",
      "ground metric (m0) is  tensor([[1792.4253, 1640.1943, 2216.6794,  ..., 2076.0935, 2478.2478,\n",
      "         2004.6312],\n",
      "        [1944.9020, 1813.5166, 2090.1631,  ..., 2144.4524, 2409.5674,\n",
      "         1941.3771],\n",
      "        [1723.2617, 1715.5028, 2031.5341,  ..., 2065.9661, 2364.2131,\n",
      "         2020.9152],\n",
      "        ...,\n",
      "        [2035.9301, 2170.3857, 2366.4514,  ..., 2051.6648, 2594.2280,\n",
      "         2228.4919],\n",
      "        [2206.6013, 2251.0359, 2180.0059,  ..., 1863.2313, 2402.0889,\n",
      "         2015.2363],\n",
      "        [1682.7938, 1575.5586, 2058.7185,  ..., 2192.7485, 2448.6650,\n",
      "         2082.9902]], device='cuda:0')\n",
      "shape of T_var is  torch.Size([256, 256])\n",
      "T_var before correction  tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')\n",
      "marginals are  tensor([[255.9934, 255.9934, 255.9934,  ..., 255.9934, 255.9934, 255.9934],\n",
      "        [255.9934, 255.9934, 255.9934,  ..., 255.9934, 255.9934, 255.9934],\n",
      "        [255.9934, 255.9934, 255.9934,  ..., 255.9934, 255.9934, 255.9934],\n",
      "        ...,\n",
      "        [255.9934, 255.9934, 255.9934,  ..., 255.9934, 255.9934, 255.9934],\n",
      "        [255.9934, 255.9934, 255.9934,  ..., 255.9934, 255.9934, 255.9934],\n",
      "        [255.9934, 255.9934, 255.9934,  ..., 255.9934, 255.9934, 255.9934]],\n",
      "       device='cuda:0')\n",
      "T_var after correction  tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')\n",
      "T_var stats: max 0.9999743700027466, min 0.0, mean 0.003906149882823229, std 0.06237669289112091 \n",
      "Ratio of trace to the matrix sum:  tensor(0.0039, device='cuda:0')\n",
      "Here, trace is 0.9999743700027466 and matrix sum is 255.99343872070312 \n",
      "Shape of aligned wt is  torch.Size([256, 256, 9])\n",
      "Shape of fc_layer0_weight_data is  torch.Size([256, 256, 9])\n",
      "model parameters are \n",
      " [torch.Size([64, 3, 3, 3]), torch.Size([64, 64, 3, 3]), torch.Size([64, 64, 3, 3]), torch.Size([64, 64, 3, 3]), torch.Size([64, 64, 3, 3]), torch.Size([128, 64, 3, 3]), torch.Size([128, 128, 3, 3]), torch.Size([128, 64, 1, 1]), torch.Size([128, 128, 3, 3]), torch.Size([128, 128, 3, 3]), torch.Size([256, 128, 3, 3]), torch.Size([256, 256, 3, 3]), torch.Size([256, 128, 1, 1]), torch.Size([256, 256, 3, 3]), torch.Size([256, 256, 3, 3]), torch.Size([512, 256, 3, 3]), torch.Size([512, 512, 3, 3]), torch.Size([512, 256, 1, 1]), torch.Size([512, 512, 3, 3]), torch.Size([512, 512, 3, 3]), torch.Size([10, 512])]\n",
      "len of model_state_dict is  21\n",
      "len of new_params is  14\n",
      "updated parameters for layer  conv1.weight\n",
      "updated parameters for layer  layer1.0.conv1.weight\n",
      "updated parameters for layer  layer1.0.conv2.weight\n",
      "updated parameters for layer  layer1.1.conv1.weight\n",
      "updated parameters for layer  layer1.1.conv2.weight\n",
      "updated parameters for layer  layer2.0.conv1.weight\n",
      "updated parameters for layer  layer2.0.conv2.weight\n",
      "updated parameters for layer  layer2.0.shortcut.0.weight\n",
      "updated parameters for layer  layer2.1.conv1.weight\n",
      "updated parameters for layer  layer2.1.conv2.weight\n",
      "updated parameters for layer  layer3.0.conv1.weight\n",
      "updated parameters for layer  layer3.0.conv2.weight\n",
      "updated parameters for layer  layer3.0.shortcut.0.weight\n",
      "updated parameters for layer  layer3.1.conv1.weight\n",
      "\n",
      "--------- Testing in global mode ---------\n",
      "size of test_loader dataset:  10000\n",
      "\n",
      "Test set: Avg. loss: 0.0024, Accuracy: 1197/10000 (12%)\n",
      "\n",
      "accuracy after update is  11.97\n",
      "For layer idx 13, accuracy of the updated model is 11.97\n",
      "\n",
      "--------------- At layer index 14 ------------- \n",
      " \n",
      "Previous layer shape is  torch.Size([256, 256, 3, 3])\n",
      "let's see the difference in layer names layer3.1.conv2 layer3.1.conv2\n",
      "torch.Size([200, 1, 256, 8, 8]) shape of activations generally\n",
      "reorder_dim is  [1, 2, 3, 0]\n",
      "In layer layer3.1.conv2.weight: getting activation distance statistics\n",
      "Statistics of the distance from neurons of layer 1 (averaged across nodes of layer 0): \n",
      "\n",
      "Max : 83.16030883789062, Mean : 23.44884490966797, Min : 4.489173889160156, Std: 12.928047180175781\n",
      "shape of layer: model 0 torch.Size([256, 256, 9])\n",
      "shape of layer: model 1 torch.Size([256, 256, 9])\n",
      "shape of activations: model 0 torch.Size([256, 8, 8, 200])\n",
      "shape of activations: model 1 torch.Size([256, 8, 8, 200])\n",
      "shape of previous transport map torch.Size([256, 256])\n",
      "doing nothing for skips\n",
      "returns a uniform measure of cardinality:  256\n",
      "returns a uniform measure of cardinality:  256\n",
      "Refactored ground metric calc\n",
      "inside refactored\n",
      "Processing the coordinates to form ground_metric\n",
      "dont leave off the squaring of the ground metric\n",
      "# of ground metric features is  12800\n",
      "# of ground metric features in 0 is   12800\n",
      "# of ground metric features in 1 is   12800\n",
      "ground metric (m0) is  tensor([[1152.7336, 1054.2018,  885.6846,  ..., 1026.7749, 1015.0973,\n",
      "         1220.1184],\n",
      "        [1196.1169, 1109.1519,  953.5573,  ...,  928.5739,  981.2495,\n",
      "         1179.9302],\n",
      "        [1256.8630, 1025.6886, 1001.3375,  ...,  957.8164, 1058.3186,\n",
      "         1311.8625],\n",
      "        ...,\n",
      "        [1129.7679,  902.1425,  768.6444,  ...,  582.7728,  788.3582,\n",
      "          978.7491],\n",
      "        [1320.8461, 1136.4805, 1004.9014,  ..., 1072.0662, 1035.7335,\n",
      "         1186.3518],\n",
      "        [1159.1731,  991.6613,  909.5237,  ...,  910.0724,  973.7393,\n",
      "         1176.2067]], device='cuda:0')\n",
      "shape of T_var is  torch.Size([256, 256])\n",
      "T_var before correction  tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')\n",
      "marginals are  tensor([[255.9934, 255.9934, 255.9934,  ..., 255.9934, 255.9934, 255.9934],\n",
      "        [255.9934, 255.9934, 255.9934,  ..., 255.9934, 255.9934, 255.9934],\n",
      "        [255.9934, 255.9934, 255.9934,  ..., 255.9934, 255.9934, 255.9934],\n",
      "        ...,\n",
      "        [255.9934, 255.9934, 255.9934,  ..., 255.9934, 255.9934, 255.9934],\n",
      "        [255.9934, 255.9934, 255.9934,  ..., 255.9934, 255.9934, 255.9934],\n",
      "        [255.9934, 255.9934, 255.9934,  ..., 255.9934, 255.9934, 255.9934]],\n",
      "       device='cuda:0')\n",
      "T_var after correction  tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')\n",
      "T_var stats: max 0.9999743700027466, min 0.0, mean 0.003906149882823229, std 0.06237669289112091 \n",
      "Ratio of trace to the matrix sum:  tensor(0.0039, device='cuda:0')\n",
      "Here, trace is 0.9999743700027466 and matrix sum is 255.99343872070312 \n",
      "Shape of aligned wt is  torch.Size([256, 256, 9])\n",
      "Shape of fc_layer0_weight_data is  torch.Size([256, 256, 9])\n",
      "model parameters are \n",
      " [torch.Size([64, 3, 3, 3]), torch.Size([64, 64, 3, 3]), torch.Size([64, 64, 3, 3]), torch.Size([64, 64, 3, 3]), torch.Size([64, 64, 3, 3]), torch.Size([128, 64, 3, 3]), torch.Size([128, 128, 3, 3]), torch.Size([128, 64, 1, 1]), torch.Size([128, 128, 3, 3]), torch.Size([128, 128, 3, 3]), torch.Size([256, 128, 3, 3]), torch.Size([256, 256, 3, 3]), torch.Size([256, 128, 1, 1]), torch.Size([256, 256, 3, 3]), torch.Size([256, 256, 3, 3]), torch.Size([512, 256, 3, 3]), torch.Size([512, 512, 3, 3]), torch.Size([512, 256, 1, 1]), torch.Size([512, 512, 3, 3]), torch.Size([512, 512, 3, 3]), torch.Size([10, 512])]\n",
      "len of model_state_dict is  21\n",
      "len of new_params is  15\n",
      "updated parameters for layer  conv1.weight\n",
      "updated parameters for layer  layer1.0.conv1.weight\n",
      "updated parameters for layer  layer1.0.conv2.weight\n",
      "updated parameters for layer  layer1.1.conv1.weight\n",
      "updated parameters for layer  layer1.1.conv2.weight\n",
      "updated parameters for layer  layer2.0.conv1.weight\n",
      "updated parameters for layer  layer2.0.conv2.weight\n",
      "updated parameters for layer  layer2.0.shortcut.0.weight\n",
      "updated parameters for layer  layer2.1.conv1.weight\n",
      "updated parameters for layer  layer2.1.conv2.weight\n",
      "updated parameters for layer  layer3.0.conv1.weight\n",
      "updated parameters for layer  layer3.0.conv2.weight\n",
      "updated parameters for layer  layer3.0.shortcut.0.weight\n",
      "updated parameters for layer  layer3.1.conv1.weight\n",
      "updated parameters for layer  layer3.1.conv2.weight\n",
      "\n",
      "--------- Testing in global mode ---------\n",
      "size of test_loader dataset:  10000\n",
      "\n",
      "Test set: Avg. loss: 0.0024, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "accuracy after update is  11.35\n",
      "For layer idx 14, accuracy of the updated model is 11.35\n",
      "\n",
      "--------------- At layer index 15 ------------- \n",
      " \n",
      "Previous layer shape is  torch.Size([256, 256, 3, 3])\n",
      "let's see the difference in layer names layer4.0.conv1 layer4.0.conv1\n",
      "torch.Size([200, 1, 512, 4, 4]) shape of activations generally\n",
      "reorder_dim is  [1, 2, 3, 0]\n",
      "In layer layer4.0.conv1.weight: getting activation distance statistics\n",
      "Statistics of the distance from neurons of layer 1 (averaged across nodes of layer 0): \n",
      "\n",
      "Max : 64.844482421875, Mean : 20.29819107055664, Min : 3.5866384506225586, Std: 10.222541809082031\n",
      "shape of layer: model 0 torch.Size([512, 256, 9])\n",
      "shape of layer: model 1 torch.Size([512, 256, 9])\n",
      "shape of activations: model 0 torch.Size([512, 4, 4, 200])\n",
      "shape of activations: model 1 torch.Size([512, 4, 4, 200])\n",
      "shape of previous transport map torch.Size([256, 256])\n",
      "saved skip T_var at layer 15 with shape torch.Size([512, 256, 3, 3])\n",
      "shape of previous transport map now is torch.Size([256, 256])\n",
      "returns a uniform measure of cardinality:  512\n",
      "returns a uniform measure of cardinality:  512\n",
      "Refactored ground metric calc\n",
      "inside refactored\n",
      "Processing the coordinates to form ground_metric\n",
      "dont leave off the squaring of the ground metric\n",
      "# of ground metric features is  3200\n",
      "# of ground metric features in 0 is   3200\n",
      "# of ground metric features in 1 is   3200\n",
      "ground metric (m0) is  tensor([[533.3046, 587.7498, 572.4905,  ..., 409.0110, 573.9702, 574.5157],\n",
      "        [543.2189, 505.5504, 645.4519,  ..., 555.5791, 492.6288, 550.0042],\n",
      "        [638.9666, 682.9962, 660.7392,  ..., 597.5626, 591.9052, 636.4066],\n",
      "        ...,\n",
      "        [600.6038, 537.8640, 714.3712,  ..., 633.1022, 609.5647, 609.5785],\n",
      "        [652.9348, 663.8941, 663.0782,  ..., 540.0668, 607.0424, 616.4797],\n",
      "        [578.1831, 577.5055, 618.1882,  ..., 471.6303, 565.8201, 609.0464]],\n",
      "       device='cuda:0')\n",
      "shape of T_var is  torch.Size([512, 512])\n",
      "T_var before correction  tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')\n",
      "marginals are  tensor([[511.9738, 511.9738, 511.9738,  ..., 511.9738, 511.9738, 511.9738],\n",
      "        [511.9738, 511.9738, 511.9738,  ..., 511.9738, 511.9738, 511.9738],\n",
      "        [511.9738, 511.9738, 511.9738,  ..., 511.9738, 511.9738, 511.9738],\n",
      "        ...,\n",
      "        [511.9738, 511.9738, 511.9738,  ..., 511.9738, 511.9738, 511.9738],\n",
      "        [511.9738, 511.9738, 511.9738,  ..., 511.9738, 511.9738, 511.9738],\n",
      "        [511.9738, 511.9738, 511.9738,  ..., 511.9738, 511.9738, 511.9738]],\n",
      "       device='cuda:0')\n",
      "T_var after correction  tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')\n",
      "T_var stats: max 0.9999488592147827, min 0.0, mean 0.0019530251156538725, std 0.044148821383714676 \n",
      "Ratio of trace to the matrix sum:  tensor(0., device='cuda:0')\n",
      "Here, trace is 0.0 and matrix sum is 511.97381591796875 \n",
      "Shape of aligned wt is  torch.Size([512, 256, 9])\n",
      "Shape of fc_layer0_weight_data is  torch.Size([512, 256, 9])\n",
      "model parameters are \n",
      " [torch.Size([64, 3, 3, 3]), torch.Size([64, 64, 3, 3]), torch.Size([64, 64, 3, 3]), torch.Size([64, 64, 3, 3]), torch.Size([64, 64, 3, 3]), torch.Size([128, 64, 3, 3]), torch.Size([128, 128, 3, 3]), torch.Size([128, 64, 1, 1]), torch.Size([128, 128, 3, 3]), torch.Size([128, 128, 3, 3]), torch.Size([256, 128, 3, 3]), torch.Size([256, 256, 3, 3]), torch.Size([256, 128, 1, 1]), torch.Size([256, 256, 3, 3]), torch.Size([256, 256, 3, 3]), torch.Size([512, 256, 3, 3]), torch.Size([512, 512, 3, 3]), torch.Size([512, 256, 1, 1]), torch.Size([512, 512, 3, 3]), torch.Size([512, 512, 3, 3]), torch.Size([10, 512])]\n",
      "len of model_state_dict is  21\n",
      "len of new_params is  16\n",
      "updated parameters for layer  conv1.weight\n",
      "updated parameters for layer  layer1.0.conv1.weight\n",
      "updated parameters for layer  layer1.0.conv2.weight\n",
      "updated parameters for layer  layer1.1.conv1.weight\n",
      "updated parameters for layer  layer1.1.conv2.weight\n",
      "updated parameters for layer  layer2.0.conv1.weight\n",
      "updated parameters for layer  layer2.0.conv2.weight\n",
      "updated parameters for layer  layer2.0.shortcut.0.weight\n",
      "updated parameters for layer  layer2.1.conv1.weight\n",
      "updated parameters for layer  layer2.1.conv2.weight\n",
      "updated parameters for layer  layer3.0.conv1.weight\n",
      "updated parameters for layer  layer3.0.conv2.weight\n",
      "updated parameters for layer  layer3.0.shortcut.0.weight\n",
      "updated parameters for layer  layer3.1.conv1.weight\n",
      "updated parameters for layer  layer3.1.conv2.weight\n",
      "updated parameters for layer  layer4.0.conv1.weight\n",
      "\n",
      "--------- Testing in global mode ---------\n",
      "size of test_loader dataset:  10000\n",
      "\n",
      "Test set: Avg. loss: 0.0024, Accuracy: 1034/10000 (10%)\n",
      "\n",
      "accuracy after update is  10.34\n",
      "For layer idx 15, accuracy of the updated model is 10.34\n",
      "\n",
      "--------------- At layer index 16 ------------- \n",
      " \n",
      "Previous layer shape is  torch.Size([512, 256, 3, 3])\n",
      "let's see the difference in layer names layer4.0.conv2 layer4.0.conv2\n",
      "torch.Size([200, 1, 512, 4, 4]) shape of activations generally\n",
      "reorder_dim is  [1, 2, 3, 0]\n",
      "In layer layer4.0.conv2.weight: getting activation distance statistics\n",
      "Statistics of the distance from neurons of layer 1 (averaged across nodes of layer 0): \n",
      "\n",
      "Max : 75.06297302246094, Mean : 21.695411682128906, Min : 3.682478904724121, Std: 11.794357299804688\n",
      "shape of layer: model 0 torch.Size([512, 512, 9])\n",
      "shape of layer: model 1 torch.Size([512, 512, 9])\n",
      "shape of activations: model 0 torch.Size([512, 4, 4, 200])\n",
      "shape of activations: model 1 torch.Size([512, 4, 4, 200])\n",
      "shape of previous transport map torch.Size([512, 512])\n",
      "doing nothing for skips\n",
      "returns a uniform measure of cardinality:  512\n",
      "returns a uniform measure of cardinality:  512\n",
      "Refactored ground metric calc\n",
      "inside refactored\n",
      "Processing the coordinates to form ground_metric\n",
      "dont leave off the squaring of the ground metric\n",
      "# of ground metric features is  3200\n",
      "# of ground metric features in 0 is   3200\n",
      "# of ground metric features in 1 is   3200\n",
      "ground metric (m0) is  tensor([[ 476.5078,  915.0419,  417.4079,  ...,  912.4647,  655.6782,\n",
      "          725.1682],\n",
      "        [ 546.5165,  707.3922,  615.4478,  ...,  849.7580,  673.6930,\n",
      "          673.3328],\n",
      "        [ 619.7437,  897.1295,  643.5043,  ..., 1003.8635,  954.1856,\n",
      "          764.5251],\n",
      "        ...,\n",
      "        [ 730.6028,  921.8612,  694.7200,  ...,  934.1355,  768.4517,\n",
      "          814.9694],\n",
      "        [ 712.6982,  903.8337,  706.6487,  ...,  893.9761,  776.2740,\n",
      "          809.8798],\n",
      "        [ 718.2426,  874.6301,  739.4396,  ...,  920.6509,  887.5869,\n",
      "          833.5513]], device='cuda:0')\n",
      "shape of T_var is  torch.Size([512, 512])\n",
      "T_var before correction  tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')\n",
      "marginals are  tensor([[511.9738, 511.9738, 511.9738,  ..., 511.9738, 511.9738, 511.9738],\n",
      "        [511.9738, 511.9738, 511.9738,  ..., 511.9738, 511.9738, 511.9738],\n",
      "        [511.9738, 511.9738, 511.9738,  ..., 511.9738, 511.9738, 511.9738],\n",
      "        ...,\n",
      "        [511.9738, 511.9738, 511.9738,  ..., 511.9738, 511.9738, 511.9738],\n",
      "        [511.9738, 511.9738, 511.9738,  ..., 511.9738, 511.9738, 511.9738],\n",
      "        [511.9738, 511.9738, 511.9738,  ..., 511.9738, 511.9738, 511.9738]],\n",
      "       device='cuda:0')\n",
      "T_var after correction  tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')\n",
      "T_var stats: max 0.9999488592147827, min 0.0, mean 0.0019530251156538725, std 0.044148821383714676 \n",
      "Ratio of trace to the matrix sum:  tensor(0.0020, device='cuda:0')\n",
      "Here, trace is 0.9999488592147827 and matrix sum is 511.97381591796875 \n",
      "Shape of aligned wt is  torch.Size([512, 512, 9])\n",
      "Shape of fc_layer0_weight_data is  torch.Size([512, 512, 9])\n",
      "model parameters are \n",
      " [torch.Size([64, 3, 3, 3]), torch.Size([64, 64, 3, 3]), torch.Size([64, 64, 3, 3]), torch.Size([64, 64, 3, 3]), torch.Size([64, 64, 3, 3]), torch.Size([128, 64, 3, 3]), torch.Size([128, 128, 3, 3]), torch.Size([128, 64, 1, 1]), torch.Size([128, 128, 3, 3]), torch.Size([128, 128, 3, 3]), torch.Size([256, 128, 3, 3]), torch.Size([256, 256, 3, 3]), torch.Size([256, 128, 1, 1]), torch.Size([256, 256, 3, 3]), torch.Size([256, 256, 3, 3]), torch.Size([512, 256, 3, 3]), torch.Size([512, 512, 3, 3]), torch.Size([512, 256, 1, 1]), torch.Size([512, 512, 3, 3]), torch.Size([512, 512, 3, 3]), torch.Size([10, 512])]\n",
      "len of model_state_dict is  21\n",
      "len of new_params is  17\n",
      "updated parameters for layer  conv1.weight\n",
      "updated parameters for layer  layer1.0.conv1.weight\n",
      "updated parameters for layer  layer1.0.conv2.weight\n",
      "updated parameters for layer  layer1.1.conv1.weight\n",
      "updated parameters for layer  layer1.1.conv2.weight\n",
      "updated parameters for layer  layer2.0.conv1.weight\n",
      "updated parameters for layer  layer2.0.conv2.weight\n",
      "updated parameters for layer  layer2.0.shortcut.0.weight\n",
      "updated parameters for layer  layer2.1.conv1.weight\n",
      "updated parameters for layer  layer2.1.conv2.weight\n",
      "updated parameters for layer  layer3.0.conv1.weight\n",
      "updated parameters for layer  layer3.0.conv2.weight\n",
      "updated parameters for layer  layer3.0.shortcut.0.weight\n",
      "updated parameters for layer  layer3.1.conv1.weight\n",
      "updated parameters for layer  layer3.1.conv2.weight\n",
      "updated parameters for layer  layer4.0.conv1.weight\n",
      "updated parameters for layer  layer4.0.conv2.weight\n",
      "\n",
      "--------- Testing in global mode ---------\n",
      "size of test_loader dataset:  10000\n",
      "\n",
      "Test set: Avg. loss: 0.0025, Accuracy: 624/10000 (6%)\n",
      "\n",
      "accuracy after update is  6.24\n",
      "For layer idx 16, accuracy of the updated model is 6.24\n",
      "\n",
      "--------------- At layer index 17 ------------- \n",
      " \n",
      "Previous layer shape is  torch.Size([512, 512, 3, 3])\n",
      "let's see the difference in layer names layer4.0.shortcut.0 layer4.0.shortcut.0\n",
      "torch.Size([200, 1, 512, 4, 4]) shape of activations generally\n",
      "reorder_dim is  [1, 2, 3, 0]\n",
      "In layer layer4.0.shortcut.0.weight: getting activation distance statistics\n",
      "Statistics of the distance from neurons of layer 1 (averaged across nodes of layer 0): \n",
      "\n",
      "Max : 19.006732940673828, Mean : 6.611974716186523, Min : 1.3381426334381104, Std: 3.0215609073638916\n",
      "shape of layer: model 0 torch.Size([512, 256, 1])\n",
      "shape of layer: model 1 torch.Size([512, 256, 1])\n",
      "shape of activations: model 0 torch.Size([512, 4, 4, 200])\n",
      "shape of activations: model 1 torch.Size([512, 4, 4, 200])\n",
      "shape of previous transport map torch.Size([512, 512])\n",
      "utilizing skip T_var saved from layer layer 15 with shape torch.Size([256, 256])\n",
      "shape of previous transport map now is torch.Size([256, 256])\n",
      "returns a uniform measure of cardinality:  512\n",
      "returns a uniform measure of cardinality:  512\n",
      "Refactored ground metric calc\n",
      "inside refactored\n",
      "Processing the coordinates to form ground_metric\n",
      "dont leave off the squaring of the ground metric\n",
      "# of ground metric features is  3200\n",
      "# of ground metric features in 0 is   3200\n",
      "# of ground metric features in 1 is   3200\n",
      "ground metric (m0) is  tensor([[122.2306, 299.6770, 121.1285,  ..., 320.6779, 192.2841, 158.7187],\n",
      "        [200.8535, 177.1729, 214.5141,  ..., 207.2630, 187.5013, 230.3472],\n",
      "        [174.4311, 282.0681, 187.7651,  ..., 289.4649, 205.6523, 197.6423],\n",
      "        ...,\n",
      "        [155.9816, 247.1012, 157.3964,  ..., 266.2110, 179.0422, 199.2684],\n",
      "        [175.5099, 243.5576, 170.9121,  ..., 271.1273, 184.4498, 222.8748],\n",
      "        [180.7293, 241.4457, 190.2732,  ..., 243.5837, 205.5415, 221.4334]],\n",
      "       device='cuda:0')\n",
      "shape of T_var is  torch.Size([512, 512])\n",
      "T_var before correction  tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0020, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        ...,\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],\n",
      "       device='cuda:0')\n",
      "marginals are  tensor([[511.9738, 511.9738, 511.9738,  ..., 511.9738, 511.9738, 511.9738],\n",
      "        [511.9738, 511.9738, 511.9738,  ..., 511.9738, 511.9738, 511.9738],\n",
      "        [511.9738, 511.9738, 511.9738,  ..., 511.9738, 511.9738, 511.9738],\n",
      "        ...,\n",
      "        [511.9738, 511.9738, 511.9738,  ..., 511.9738, 511.9738, 511.9738],\n",
      "        [511.9738, 511.9738, 511.9738,  ..., 511.9738, 511.9738, 511.9738],\n",
      "        [511.9738, 511.9738, 511.9738,  ..., 511.9738, 511.9738, 511.9738]],\n",
      "       device='cuda:0')\n",
      "T_var after correction  tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.9999, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        ...,\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],\n",
      "       device='cuda:0')\n",
      "T_var stats: max 0.9999488592147827, min 0.0, mean 0.0019530251156538725, std 0.044148821383714676 \n",
      "Ratio of trace to the matrix sum:  tensor(0.0020, device='cuda:0')\n",
      "Here, trace is 0.9999488592147827 and matrix sum is 511.97381591796875 \n",
      "Shape of aligned wt is  torch.Size([512, 256, 1])\n",
      "Shape of fc_layer0_weight_data is  torch.Size([512, 256, 1])\n",
      "model parameters are \n",
      " [torch.Size([64, 3, 3, 3]), torch.Size([64, 64, 3, 3]), torch.Size([64, 64, 3, 3]), torch.Size([64, 64, 3, 3]), torch.Size([64, 64, 3, 3]), torch.Size([128, 64, 3, 3]), torch.Size([128, 128, 3, 3]), torch.Size([128, 64, 1, 1]), torch.Size([128, 128, 3, 3]), torch.Size([128, 128, 3, 3]), torch.Size([256, 128, 3, 3]), torch.Size([256, 256, 3, 3]), torch.Size([256, 128, 1, 1]), torch.Size([256, 256, 3, 3]), torch.Size([256, 256, 3, 3]), torch.Size([512, 256, 3, 3]), torch.Size([512, 512, 3, 3]), torch.Size([512, 256, 1, 1]), torch.Size([512, 512, 3, 3]), torch.Size([512, 512, 3, 3]), torch.Size([10, 512])]\n",
      "len of model_state_dict is  21\n",
      "len of new_params is  18\n",
      "updated parameters for layer  conv1.weight\n",
      "updated parameters for layer  layer1.0.conv1.weight\n",
      "updated parameters for layer  layer1.0.conv2.weight\n",
      "updated parameters for layer  layer1.1.conv1.weight\n",
      "updated parameters for layer  layer1.1.conv2.weight\n",
      "updated parameters for layer  layer2.0.conv1.weight\n",
      "updated parameters for layer  layer2.0.conv2.weight\n",
      "updated parameters for layer  layer2.0.shortcut.0.weight\n",
      "updated parameters for layer  layer2.1.conv1.weight\n",
      "updated parameters for layer  layer2.1.conv2.weight\n",
      "updated parameters for layer  layer3.0.conv1.weight\n",
      "updated parameters for layer  layer3.0.conv2.weight\n",
      "updated parameters for layer  layer3.0.shortcut.0.weight\n",
      "updated parameters for layer  layer3.1.conv1.weight\n",
      "updated parameters for layer  layer3.1.conv2.weight\n",
      "updated parameters for layer  layer4.0.conv1.weight\n",
      "updated parameters for layer  layer4.0.conv2.weight\n",
      "updated parameters for layer  layer4.0.shortcut.0.weight\n",
      "\n",
      "--------- Testing in global mode ---------\n",
      "size of test_loader dataset:  10000\n",
      "\n",
      "Test set: Avg. loss: 0.0024, Accuracy: 808/10000 (8%)\n",
      "\n",
      "accuracy after update is  8.08\n",
      "For layer idx 17, accuracy of the updated model is 8.08\n",
      "\n",
      "--------------- At layer index 18 ------------- \n",
      " \n",
      "Previous layer shape is  torch.Size([512, 256, 1, 1])\n",
      "let's see the difference in layer names layer4.1.conv1 layer4.1.conv1\n",
      "torch.Size([200, 1, 512, 4, 4]) shape of activations generally\n",
      "reorder_dim is  [1, 2, 3, 0]\n",
      "In layer layer4.1.conv1.weight: getting activation distance statistics\n",
      "Statistics of the distance from neurons of layer 1 (averaged across nodes of layer 0): \n",
      "\n",
      "Max : 46.82421875, Mean : 12.00428581237793, Min : 1.8053996562957764, Std: 7.600210189819336\n",
      "shape of layer: model 0 torch.Size([512, 512, 9])\n",
      "shape of layer: model 1 torch.Size([512, 512, 9])\n",
      "shape of activations: model 0 torch.Size([512, 4, 4, 200])\n",
      "shape of activations: model 1 torch.Size([512, 4, 4, 200])\n",
      "shape of previous transport map torch.Size([512, 512])\n",
      "averaging multiple T_var's\n",
      "returns a uniform measure of cardinality:  512\n",
      "returns a uniform measure of cardinality:  512\n",
      "Refactored ground metric calc\n",
      "inside refactored\n",
      "Processing the coordinates to form ground_metric\n",
      "dont leave off the squaring of the ground metric\n",
      "# of ground metric features is  3200\n",
      "# of ground metric features in 0 is   3200\n",
      "# of ground metric features in 1 is   3200\n",
      "ground metric (m0) is  tensor([[313.9242,  73.5810, 350.9817,  ..., 217.6175, 370.9441, 545.6089],\n",
      "        [370.4481, 492.1352, 389.4560,  ..., 498.3931, 437.2660, 583.6142],\n",
      "        [330.9668, 297.3314, 363.2375,  ..., 308.8793, 415.8996, 544.2914],\n",
      "        ...,\n",
      "        [474.0930, 521.7574, 425.0722,  ..., 564.0604, 566.5095, 608.4368],\n",
      "        [323.8960, 375.1417, 325.6374,  ..., 378.9791, 413.1653, 447.5897],\n",
      "        [290.1602, 270.3532, 344.6674,  ..., 299.2924, 383.7000, 526.8427]],\n",
      "       device='cuda:0')\n",
      "shape of T_var is  torch.Size([512, 512])\n",
      "T_var before correction  tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')\n",
      "marginals are  tensor([[511.9738, 511.9738, 511.9738,  ..., 511.9738, 511.9738, 511.9738],\n",
      "        [511.9738, 511.9738, 511.9738,  ..., 511.9738, 511.9738, 511.9738],\n",
      "        [511.9738, 511.9738, 511.9738,  ..., 511.9738, 511.9738, 511.9738],\n",
      "        ...,\n",
      "        [511.9738, 511.9738, 511.9738,  ..., 511.9738, 511.9738, 511.9738],\n",
      "        [511.9738, 511.9738, 511.9738,  ..., 511.9738, 511.9738, 511.9738],\n",
      "        [511.9738, 511.9738, 511.9738,  ..., 511.9738, 511.9738, 511.9738]],\n",
      "       device='cuda:0')\n",
      "T_var after correction  tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')\n",
      "T_var stats: max 0.9999488592147827, min 0.0, mean 0.0019530251156538725, std 0.044148821383714676 \n",
      "Ratio of trace to the matrix sum:  tensor(0., device='cuda:0')\n",
      "Here, trace is 0.0 and matrix sum is 511.97381591796875 \n",
      "Shape of aligned wt is  torch.Size([512, 512, 9])\n",
      "Shape of fc_layer0_weight_data is  torch.Size([512, 512, 9])\n",
      "model parameters are \n",
      " [torch.Size([64, 3, 3, 3]), torch.Size([64, 64, 3, 3]), torch.Size([64, 64, 3, 3]), torch.Size([64, 64, 3, 3]), torch.Size([64, 64, 3, 3]), torch.Size([128, 64, 3, 3]), torch.Size([128, 128, 3, 3]), torch.Size([128, 64, 1, 1]), torch.Size([128, 128, 3, 3]), torch.Size([128, 128, 3, 3]), torch.Size([256, 128, 3, 3]), torch.Size([256, 256, 3, 3]), torch.Size([256, 128, 1, 1]), torch.Size([256, 256, 3, 3]), torch.Size([256, 256, 3, 3]), torch.Size([512, 256, 3, 3]), torch.Size([512, 512, 3, 3]), torch.Size([512, 256, 1, 1]), torch.Size([512, 512, 3, 3]), torch.Size([512, 512, 3, 3]), torch.Size([10, 512])]\n",
      "len of model_state_dict is  21\n",
      "len of new_params is  19\n",
      "updated parameters for layer  conv1.weight\n",
      "updated parameters for layer  layer1.0.conv1.weight\n",
      "updated parameters for layer  layer1.0.conv2.weight\n",
      "updated parameters for layer  layer1.1.conv1.weight\n",
      "updated parameters for layer  layer1.1.conv2.weight\n",
      "updated parameters for layer  layer2.0.conv1.weight\n",
      "updated parameters for layer  layer2.0.conv2.weight\n",
      "updated parameters for layer  layer2.0.shortcut.0.weight\n",
      "updated parameters for layer  layer2.1.conv1.weight\n",
      "updated parameters for layer  layer2.1.conv2.weight\n",
      "updated parameters for layer  layer3.0.conv1.weight\n",
      "updated parameters for layer  layer3.0.conv2.weight\n",
      "updated parameters for layer  layer3.0.shortcut.0.weight\n",
      "updated parameters for layer  layer3.1.conv1.weight\n",
      "updated parameters for layer  layer3.1.conv2.weight\n",
      "updated parameters for layer  layer4.0.conv1.weight\n",
      "updated parameters for layer  layer4.0.conv2.weight\n",
      "updated parameters for layer  layer4.0.shortcut.0.weight\n",
      "updated parameters for layer  layer4.1.conv1.weight\n",
      "\n",
      "--------- Testing in global mode ---------\n",
      "size of test_loader dataset:  10000\n",
      "\n",
      "Test set: Avg. loss: 0.0024, Accuracy: 854/10000 (9%)\n",
      "\n",
      "accuracy after update is  8.54\n",
      "For layer idx 18, accuracy of the updated model is 8.54\n",
      "\n",
      "--------------- At layer index 19 ------------- \n",
      " \n",
      "Previous layer shape is  torch.Size([512, 512, 3, 3])\n",
      "let's see the difference in layer names layer4.1.conv2 layer4.1.conv2\n",
      "torch.Size([200, 1, 512, 4, 4]) shape of activations generally\n",
      "reorder_dim is  [1, 2, 3, 0]\n",
      "In layer layer4.1.conv2.weight: getting activation distance statistics\n",
      "Statistics of the distance from neurons of layer 1 (averaged across nodes of layer 0): \n",
      "\n",
      "Max : 56.34060287475586, Mean : 10.25609016418457, Min : 1.2112878561019897, Std: 8.522884368896484\n",
      "shape of layer: model 0 torch.Size([512, 512, 9])\n",
      "shape of layer: model 1 torch.Size([512, 512, 9])\n",
      "shape of activations: model 0 torch.Size([512, 4, 4, 200])\n",
      "shape of activations: model 1 torch.Size([512, 4, 4, 200])\n",
      "shape of previous transport map torch.Size([512, 512])\n",
      "doing nothing for skips\n",
      "returns a uniform measure of cardinality:  512\n",
      "returns a uniform measure of cardinality:  512\n",
      "Refactored ground metric calc\n",
      "inside refactored\n",
      "Processing the coordinates to form ground_metric\n",
      "dont leave off the squaring of the ground metric\n",
      "# of ground metric features is  3200\n",
      "# of ground metric features in 0 is   3200\n",
      "# of ground metric features in 1 is   3200\n",
      "ground metric (m0) is  tensor([[276.5847, 406.8487, 281.6334,  ..., 511.3937, 340.8594, 300.1173],\n",
      "        [254.8671, 287.0729, 341.2603,  ..., 465.5713, 399.5401, 386.8106],\n",
      "        [314.7725, 412.8495, 337.3224,  ..., 516.0355, 459.9802, 365.2159],\n",
      "        ...,\n",
      "        [407.8747, 500.1228, 407.3343,  ..., 568.2158, 486.1939, 504.7855],\n",
      "        [295.5289, 411.0053, 361.5544,  ..., 491.6883, 426.5430, 449.5589],\n",
      "        [367.1935, 424.9929, 379.8170,  ..., 539.2059, 483.8337, 424.7657]],\n",
      "       device='cuda:0')\n",
      "shape of T_var is  torch.Size([512, 512])\n",
      "T_var before correction  tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')\n",
      "marginals are  tensor([[511.9738, 511.9738, 511.9738,  ..., 511.9738, 511.9738, 511.9738],\n",
      "        [511.9738, 511.9738, 511.9738,  ..., 511.9738, 511.9738, 511.9738],\n",
      "        [511.9738, 511.9738, 511.9738,  ..., 511.9738, 511.9738, 511.9738],\n",
      "        ...,\n",
      "        [511.9738, 511.9738, 511.9738,  ..., 511.9738, 511.9738, 511.9738],\n",
      "        [511.9738, 511.9738, 511.9738,  ..., 511.9738, 511.9738, 511.9738],\n",
      "        [511.9738, 511.9738, 511.9738,  ..., 511.9738, 511.9738, 511.9738]],\n",
      "       device='cuda:0')\n",
      "T_var after correction  tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')\n",
      "T_var stats: max 0.9999488592147827, min 0.0, mean 0.0019530251156538725, std 0.044148821383714676 \n",
      "Ratio of trace to the matrix sum:  tensor(0.0020, device='cuda:0')\n",
      "Here, trace is 0.9999488592147827 and matrix sum is 511.97381591796875 \n",
      "Shape of aligned wt is  torch.Size([512, 512, 9])\n",
      "Shape of fc_layer0_weight_data is  torch.Size([512, 512, 9])\n",
      "model parameters are \n",
      " [torch.Size([64, 3, 3, 3]), torch.Size([64, 64, 3, 3]), torch.Size([64, 64, 3, 3]), torch.Size([64, 64, 3, 3]), torch.Size([64, 64, 3, 3]), torch.Size([128, 64, 3, 3]), torch.Size([128, 128, 3, 3]), torch.Size([128, 64, 1, 1]), torch.Size([128, 128, 3, 3]), torch.Size([128, 128, 3, 3]), torch.Size([256, 128, 3, 3]), torch.Size([256, 256, 3, 3]), torch.Size([256, 128, 1, 1]), torch.Size([256, 256, 3, 3]), torch.Size([256, 256, 3, 3]), torch.Size([512, 256, 3, 3]), torch.Size([512, 512, 3, 3]), torch.Size([512, 256, 1, 1]), torch.Size([512, 512, 3, 3]), torch.Size([512, 512, 3, 3]), torch.Size([10, 512])]\n",
      "len of model_state_dict is  21\n",
      "len of new_params is  20\n",
      "updated parameters for layer  conv1.weight\n",
      "updated parameters for layer  layer1.0.conv1.weight\n",
      "updated parameters for layer  layer1.0.conv2.weight\n",
      "updated parameters for layer  layer1.1.conv1.weight\n",
      "updated parameters for layer  layer1.1.conv2.weight\n",
      "updated parameters for layer  layer2.0.conv1.weight\n",
      "updated parameters for layer  layer2.0.conv2.weight\n",
      "updated parameters for layer  layer2.0.shortcut.0.weight\n",
      "updated parameters for layer  layer2.1.conv1.weight\n",
      "updated parameters for layer  layer2.1.conv2.weight\n",
      "updated parameters for layer  layer3.0.conv1.weight\n",
      "updated parameters for layer  layer3.0.conv2.weight\n",
      "updated parameters for layer  layer3.0.shortcut.0.weight\n",
      "updated parameters for layer  layer3.1.conv1.weight\n",
      "updated parameters for layer  layer3.1.conv2.weight\n",
      "updated parameters for layer  layer4.0.conv1.weight\n",
      "updated parameters for layer  layer4.0.conv2.weight\n",
      "updated parameters for layer  layer4.0.shortcut.0.weight\n",
      "updated parameters for layer  layer4.1.conv1.weight\n",
      "updated parameters for layer  layer4.1.conv2.weight\n",
      "\n",
      "--------- Testing in global mode ---------\n",
      "size of test_loader dataset:  10000\n",
      "\n",
      "Test set: Avg. loss: 0.0024, Accuracy: 813/10000 (8%)\n",
      "\n",
      "accuracy after update is  8.13\n",
      "For layer idx 19, accuracy of the updated model is 8.13\n",
      "\n",
      "--------------- At layer index 20 ------------- \n",
      " \n",
      "Previous layer shape is  torch.Size([512, 512, 3, 3])\n",
      "let's see the difference in layer names linear linear\n",
      "torch.Size([200, 1, 10]) shape of activations generally\n",
      "In layer linear.weight: getting activation distance statistics\n",
      "Statistics of the distance from neurons of layer 1 (averaged across nodes of layer 0): \n",
      "\n",
      "Max : 570.0380249023438, Mean : 410.51934814453125, Min : 87.47673797607422, Std: 134.97982788085938\n",
      "shape of layer: model 0 torch.Size([10, 512])\n",
      "shape of layer: model 1 torch.Size([10, 512])\n",
      "shape of activations: model 0 torch.Size([10, 200])\n",
      "shape of activations: model 1 torch.Size([10, 200])\n",
      "shape of previous transport map torch.Size([512, 512])\n",
      "returns a uniform measure of cardinality:  10\n",
      "returns a uniform measure of cardinality:  10\n",
      "Refactored ground metric calc\n",
      "inside refactored\n",
      "Processing the coordinates to form ground_metric\n",
      "dont leave off the squaring of the ground metric\n",
      "# of ground metric features in 0 is   200\n",
      "# of ground metric features in 1 is   200\n",
      "ground metric (m0) is  tensor([[ 79.3512, 460.2419, 351.9902, 419.9822, 395.5430, 416.8803, 420.9088,\n",
      "         498.7486, 343.1099, 406.9589],\n",
      "        [455.4948,  81.1131, 560.5432, 573.1671, 572.1443, 585.1461, 545.9487,\n",
      "         641.5977, 456.9715, 353.4325],\n",
      "        [320.5466, 539.3425,  93.6175, 325.7182, 306.9445, 285.3652, 341.4978,\n",
      "         409.8630, 425.4712, 446.4771],\n",
      "        [431.2553, 593.3370, 340.6490, 101.3104, 379.2708, 337.6506, 424.9941,\n",
      "         494.7872, 486.8184, 518.8439],\n",
      "        [407.9597, 594.0837, 313.3925, 382.6508,  86.8609, 327.9982, 425.2204,\n",
      "         424.0853, 485.3542, 531.4028],\n",
      "        [411.4867, 572.3323, 292.2154, 324.8697, 315.5381,  96.5202, 405.6042,\n",
      "         380.8493, 470.8190, 510.7117],\n",
      "        [411.1344, 530.5370, 348.8600, 403.0289, 414.4519, 384.3753,  84.0419,\n",
      "         524.6862, 465.5421, 508.1202],\n",
      "        [497.9497, 645.5391, 422.3190, 490.4991, 416.6952, 382.6784, 543.7056,\n",
      "          81.6252, 535.2464, 556.1745],\n",
      "        [329.8593, 494.6461, 433.3280, 455.1799, 457.4376, 463.5015, 460.4180,\n",
      "         525.5513,  85.8673, 406.9421],\n",
      "        [391.4366, 384.3747, 478.6061, 484.3137, 499.1532, 508.7820, 523.8236,\n",
      "         559.3108, 394.7419,  84.4596]], device='cuda:0')\n",
      "shape of T_var is  torch.Size([10, 10])\n",
      "T_var before correction  tensor([[0.1000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0000, 0.1000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0000, 0.0000, 0.1000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.1000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.1000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.1000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.1000, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.1000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.1000,\n",
      "         0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.1000]], device='cuda:0')\n",
      "marginals are  tensor([[10.0024, 10.0024, 10.0024, 10.0024, 10.0024, 10.0024, 10.0024, 10.0024,\n",
      "         10.0024, 10.0024],\n",
      "        [10.0024, 10.0024, 10.0024, 10.0024, 10.0024, 10.0024, 10.0024, 10.0024,\n",
      "         10.0024, 10.0024],\n",
      "        [10.0024, 10.0024, 10.0024, 10.0024, 10.0024, 10.0024, 10.0024, 10.0024,\n",
      "         10.0024, 10.0024],\n",
      "        [10.0024, 10.0024, 10.0024, 10.0024, 10.0024, 10.0024, 10.0024, 10.0024,\n",
      "         10.0024, 10.0024],\n",
      "        [10.0024, 10.0024, 10.0024, 10.0024, 10.0024, 10.0024, 10.0024, 10.0024,\n",
      "         10.0024, 10.0024],\n",
      "        [10.0024, 10.0024, 10.0024, 10.0024, 10.0024, 10.0024, 10.0024, 10.0024,\n",
      "         10.0024, 10.0024],\n",
      "        [10.0024, 10.0024, 10.0024, 10.0024, 10.0024, 10.0024, 10.0024, 10.0024,\n",
      "         10.0024, 10.0024],\n",
      "        [10.0024, 10.0024, 10.0024, 10.0024, 10.0024, 10.0024, 10.0024, 10.0024,\n",
      "         10.0024, 10.0024],\n",
      "        [10.0024, 10.0024, 10.0024, 10.0024, 10.0024, 10.0024, 10.0024, 10.0024,\n",
      "         10.0024, 10.0024],\n",
      "        [10.0024, 10.0024, 10.0024, 10.0024, 10.0024, 10.0024, 10.0024, 10.0024,\n",
      "         10.0024, 10.0024]], device='cuda:0')\n",
      "T_var after correction  tensor([[1.0002, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0000, 1.0002, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0000, 0.0000, 1.0002, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 1.0002, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 1.0002, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 1.0002, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 1.0002, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 1.0002, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 1.0002,\n",
      "         0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         1.0002]], device='cuda:0')\n",
      "T_var stats: max 1.0002431869506836, min 0.0, mean 0.10002431273460388, std 0.3015846312046051 \n",
      "Ratio of trace to the matrix sum:  tensor(1., device='cuda:0')\n",
      "Here, trace is 10.002431869506836 and matrix sum is 10.002431869506836 \n",
      "Shape of aligned wt is  torch.Size([10, 512])\n",
      "Shape of fc_layer0_weight_data is  torch.Size([10, 512])\n",
      "model parameters are \n",
      " [torch.Size([64, 3, 3, 3]), torch.Size([64, 64, 3, 3]), torch.Size([64, 64, 3, 3]), torch.Size([64, 64, 3, 3]), torch.Size([64, 64, 3, 3]), torch.Size([128, 64, 3, 3]), torch.Size([128, 128, 3, 3]), torch.Size([128, 64, 1, 1]), torch.Size([128, 128, 3, 3]), torch.Size([128, 128, 3, 3]), torch.Size([256, 128, 3, 3]), torch.Size([256, 256, 3, 3]), torch.Size([256, 128, 1, 1]), torch.Size([256, 256, 3, 3]), torch.Size([256, 256, 3, 3]), torch.Size([512, 256, 3, 3]), torch.Size([512, 512, 3, 3]), torch.Size([512, 256, 1, 1]), torch.Size([512, 512, 3, 3]), torch.Size([512, 512, 3, 3]), torch.Size([10, 512])]\n",
      "len of model_state_dict is  21\n",
      "len of new_params is  21\n",
      "updated parameters for layer  conv1.weight\n",
      "updated parameters for layer  layer1.0.conv1.weight\n",
      "updated parameters for layer  layer1.0.conv2.weight\n",
      "updated parameters for layer  layer1.1.conv1.weight\n",
      "updated parameters for layer  layer1.1.conv2.weight\n",
      "updated parameters for layer  layer2.0.conv1.weight\n",
      "updated parameters for layer  layer2.0.conv2.weight\n",
      "updated parameters for layer  layer2.0.shortcut.0.weight\n",
      "updated parameters for layer  layer2.1.conv1.weight\n",
      "updated parameters for layer  layer2.1.conv2.weight\n",
      "updated parameters for layer  layer3.0.conv1.weight\n",
      "updated parameters for layer  layer3.0.conv2.weight\n",
      "updated parameters for layer  layer3.0.shortcut.0.weight\n",
      "updated parameters for layer  layer3.1.conv1.weight\n",
      "updated parameters for layer  layer3.1.conv2.weight\n",
      "updated parameters for layer  layer4.0.conv1.weight\n",
      "updated parameters for layer  layer4.0.conv2.weight\n",
      "updated parameters for layer  layer4.0.shortcut.0.weight\n",
      "updated parameters for layer  layer4.1.conv1.weight\n",
      "updated parameters for layer  layer4.1.conv2.weight\n",
      "updated parameters for layer  linear.weight\n",
      "\n",
      "--------- Testing in global mode ---------\n",
      "size of test_loader dataset:  10000\n",
      "\n",
      "Test set: Avg. loss: 0.0016, Accuracy: 5507/10000 (55%)\n",
      "\n",
      "accuracy after update is  55.07\n",
      "For layer idx 20, accuracy of the updated model is 55.07\n",
      "using independent method\n",
      "model parameters are \n",
      " [torch.Size([64, 3, 3, 3]), torch.Size([64, 64, 3, 3]), torch.Size([64, 64, 3, 3]), torch.Size([64, 64, 3, 3]), torch.Size([64, 64, 3, 3]), torch.Size([128, 64, 3, 3]), torch.Size([128, 128, 3, 3]), torch.Size([128, 64, 1, 1]), torch.Size([128, 128, 3, 3]), torch.Size([128, 128, 3, 3]), torch.Size([256, 128, 3, 3]), torch.Size([256, 256, 3, 3]), torch.Size([256, 128, 1, 1]), torch.Size([256, 256, 3, 3]), torch.Size([256, 256, 3, 3]), torch.Size([512, 256, 3, 3]), torch.Size([512, 512, 3, 3]), torch.Size([512, 256, 1, 1]), torch.Size([512, 512, 3, 3]), torch.Size([512, 512, 3, 3]), torch.Size([10, 512])]\n",
      "\n",
      "--------- Testing in global mode ---------\n",
      "size of test_loader dataset:  10000\n",
      "\n",
      "Test set: Avg. loss: 0.0023, Accuracy: 870/10000 (9%)\n",
      "\n",
      "len of model parameters and avg aligned layers is  21 21\n",
      "len of model_state_dict is  21\n",
      "len of param_list is  21\n",
      "\n",
      "--------- Testing in global mode ---------\n",
      "size of test_loader dataset:  10000\n",
      "\n",
      "Test set: Avg. loss: 0.0014, Accuracy: 6860/10000 (69%)\n",
      "\n",
      "===============================================\n",
      "===============================================\n",
      "After Ensemble,acc: 68.6 loss 0.0014497128248214721\n",
      "===============================================\n",
      "===============================================\n",
      "Timer ends\n",
      "Time taken for geometric ensembling is 46.40585733577609 seconds\n",
      "------- Prediction based ensembling -------\n",
      "\n",
      "Test set: Avg. loss: 0.0003, Accuracy: 9363/10000 (94%)\n",
      "\n",
      "------- Naive ensembling of weights -------\n",
      "[torch.Size([64, 3, 3, 3]), torch.Size([64, 3, 3, 3])]\n",
      "torch.Size([64, 3, 3, 3])\n",
      "[torch.Size([64, 64, 3, 3]), torch.Size([64, 64, 3, 3])]\n",
      "torch.Size([64, 64, 3, 3])\n",
      "[torch.Size([64, 64, 3, 3]), torch.Size([64, 64, 3, 3])]\n",
      "torch.Size([64, 64, 3, 3])\n",
      "[torch.Size([64, 64, 3, 3]), torch.Size([64, 64, 3, 3])]\n",
      "torch.Size([64, 64, 3, 3])\n",
      "[torch.Size([64, 64, 3, 3]), torch.Size([64, 64, 3, 3])]\n",
      "torch.Size([64, 64, 3, 3])\n",
      "[torch.Size([128, 64, 3, 3]), torch.Size([128, 64, 3, 3])]\n",
      "torch.Size([128, 64, 3, 3])\n",
      "[torch.Size([128, 128, 3, 3]), torch.Size([128, 128, 3, 3])]\n",
      "torch.Size([128, 128, 3, 3])\n",
      "[torch.Size([128, 64, 1, 1]), torch.Size([128, 64, 1, 1])]\n",
      "torch.Size([128, 64, 1, 1])\n",
      "[torch.Size([128, 128, 3, 3]), torch.Size([128, 128, 3, 3])]\n",
      "torch.Size([128, 128, 3, 3])\n",
      "[torch.Size([128, 128, 3, 3]), torch.Size([128, 128, 3, 3])]\n",
      "torch.Size([128, 128, 3, 3])\n",
      "[torch.Size([256, 128, 3, 3]), torch.Size([256, 128, 3, 3])]\n",
      "torch.Size([256, 128, 3, 3])\n",
      "[torch.Size([256, 256, 3, 3]), torch.Size([256, 256, 3, 3])]\n",
      "torch.Size([256, 256, 3, 3])\n",
      "[torch.Size([256, 128, 1, 1]), torch.Size([256, 128, 1, 1])]\n",
      "torch.Size([256, 128, 1, 1])\n",
      "[torch.Size([256, 256, 3, 3]), torch.Size([256, 256, 3, 3])]\n",
      "torch.Size([256, 256, 3, 3])\n",
      "[torch.Size([256, 256, 3, 3]), torch.Size([256, 256, 3, 3])]\n",
      "torch.Size([256, 256, 3, 3])\n",
      "[torch.Size([512, 256, 3, 3]), torch.Size([512, 256, 3, 3])]\n",
      "torch.Size([512, 256, 3, 3])\n",
      "[torch.Size([512, 512, 3, 3]), torch.Size([512, 512, 3, 3])]\n",
      "torch.Size([512, 512, 3, 3])\n",
      "[torch.Size([512, 256, 1, 1]), torch.Size([512, 256, 1, 1])]\n",
      "torch.Size([512, 256, 1, 1])\n",
      "[torch.Size([512, 512, 3, 3]), torch.Size([512, 512, 3, 3])]\n",
      "torch.Size([512, 512, 3, 3])\n",
      "[torch.Size([512, 512, 3, 3]), torch.Size([512, 512, 3, 3])]\n",
      "torch.Size([512, 512, 3, 3])\n",
      "[torch.Size([10, 512]), torch.Size([10, 512])]\n",
      "torch.Size([10, 512])\n",
      "model parameters are \n",
      " [torch.Size([64, 3, 3, 3]), torch.Size([64, 64, 3, 3]), torch.Size([64, 64, 3, 3]), torch.Size([64, 64, 3, 3]), torch.Size([64, 64, 3, 3]), torch.Size([128, 64, 3, 3]), torch.Size([128, 128, 3, 3]), torch.Size([128, 64, 1, 1]), torch.Size([128, 128, 3, 3]), torch.Size([128, 128, 3, 3]), torch.Size([256, 128, 3, 3]), torch.Size([256, 256, 3, 3]), torch.Size([256, 128, 1, 1]), torch.Size([256, 256, 3, 3]), torch.Size([256, 256, 3, 3]), torch.Size([512, 256, 3, 3]), torch.Size([512, 512, 3, 3]), torch.Size([512, 256, 1, 1]), torch.Size([512, 512, 3, 3]), torch.Size([512, 512, 3, 3]), torch.Size([10, 512])]\n",
      "\n",
      "--------- Testing in global mode ---------\n",
      "size of test_loader dataset:  10000\n",
      "\n",
      "Test set: Avg. loss: 0.0023, Accuracy: 1000/10000 (10%)\n",
      "\n",
      "\n",
      "--------- Testing in global mode ---------\n",
      "size of test_loader dataset:  10000\n",
      "\n",
      "Test set: Avg. loss: 0.0023, Accuracy: 2021/10000 (20%)\n",
      "\n",
      "-------- Retraining the models ---------\n",
      "Retraining model :  model_0\n",
      "num_epochs---------------+++))))))+++ 300\n",
      "num_epochs---------------+++))))))+++ 300\n",
      "lr is  0.1\n",
      "number of epochs would be  30\n",
      "num_epochs-------------- 30\n",
      "Epoch 000\n",
      "/root/miniconda3/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:129: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\"Detected call of `lr_scheduler.step()` before `optimizer.step()`. \"\n",
      "/root/miniconda3/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:154: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "accuracy: {'epoch': 0, 'value': 0.8135199999999997} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 0, 'value': 0.575594990723207} ({'split': 'train'})\n",
      "accuracy: {'epoch': 0, 'value': 0.7734000325202942} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 0, 'value': 0.7460569262504578} ({'split': 'test'})\n",
      "Epoch 001\n",
      "accuracy: {'epoch': 1, 'value': 0.8480999999999995} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 1, 'value': 0.46817468343734764} ({'split': 'train'})\n",
      "accuracy: {'epoch': 1, 'value': 0.810200035572052} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 1, 'value': 0.578046101331711} ({'split': 'test'})\n",
      "Epoch 002\n",
      "accuracy: {'epoch': 2, 'value': 0.8654600000000006} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 2, 'value': 0.40688778419017785} ({'split': 'train'})\n",
      "accuracy: {'epoch': 2, 'value': 0.8037000358104707} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 2, 'value': 0.6359024047851562} ({'split': 'test'})\n",
      "Epoch 003\n",
      "accuracy: {'epoch': 3, 'value': 0.8757599999999983} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 3, 'value': 0.37494965206146236} ({'split': 'train'})\n",
      "accuracy: {'epoch': 3, 'value': 0.8006000399589538} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 3, 'value': 0.6582456171512604} ({'split': 'test'})\n",
      "Epoch 004\n",
      "accuracy: {'epoch': 4, 'value': 0.8729600000000003} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 4, 'value': 0.3881009641456606} ({'split': 'train'})\n",
      "accuracy: {'epoch': 4, 'value': 0.788100039958954} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 4, 'value': 0.6887905955314637} ({'split': 'test'})\n",
      "Epoch 005\n",
      "accuracy: {'epoch': 5, 'value': 0.8737400000000004} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 5, 'value': 0.3868285530567171} ({'split': 'train'})\n",
      "accuracy: {'epoch': 5, 'value': 0.7760000348091125} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 5, 'value': 0.7359437465667724} ({'split': 'test'})\n",
      "Epoch 006\n",
      "accuracy: {'epoch': 6, 'value': 0.8651200000000003} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 6, 'value': 0.415418141450882} ({'split': 'train'})\n",
      "accuracy: {'epoch': 6, 'value': 0.7534000337123871} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 6, 'value': 0.8638333976268767} ({'split': 'test'})\n",
      "Epoch 007\n",
      "accuracy: {'epoch': 7, 'value': 0.8581799999999997} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 7, 'value': 0.43791941660881} ({'split': 'train'})\n",
      "accuracy: {'epoch': 7, 'value': 0.7578000307083129} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 7, 'value': 0.8941134870052337} ({'split': 'test'})\n",
      "Epoch 008\n",
      "accuracy: {'epoch': 8, 'value': 0.8457800000000001} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 8, 'value': 0.47615909712791415} ({'split': 'train'})\n",
      "accuracy: {'epoch': 8, 'value': 0.7592000305652619} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 8, 'value': 0.7630761086940766} ({'split': 'test'})\n",
      "Epoch 009\n",
      "accuracy: {'epoch': 9, 'value': 0.8364600000000001} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 9, 'value': 0.5114893735504151} ({'split': 'train'})\n",
      "accuracy: {'epoch': 9, 'value': 0.6958000242710114} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 9, 'value': 0.9924814343452453} ({'split': 'test'})\n",
      "Epoch 010\n",
      "accuracy: {'epoch': 10, 'value': 0.8110399999999994} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 10, 'value': 0.5879631518173213} ({'split': 'train'})\n",
      "accuracy: {'epoch': 10, 'value': 0.754200029373169} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 10, 'value': 0.8295304775238037} ({'split': 'test'})\n",
      "Epoch 011\n",
      "accuracy: {'epoch': 11, 'value': 0.8021199999999999} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 11, 'value': 0.6211653963804239} ({'split': 'train'})\n",
      "accuracy: {'epoch': 11, 'value': 0.7350000262260437} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 11, 'value': 0.953263258934021} ({'split': 'test'})\n",
      "Epoch 012\n",
      "accuracy: {'epoch': 12, 'value': 0.7753} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 12, 'value': 0.7012429848861683} ({'split': 'train'})\n",
      "accuracy: {'epoch': 12, 'value': 0.7007000267505645} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 12, 'value': 0.960261195898056} ({'split': 'test'})\n",
      "Epoch 013\n",
      "accuracy: {'epoch': 13, 'value': 0.7565200000000002} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 13, 'value': 0.7623451134204867} ({'split': 'train'})\n",
      "accuracy: {'epoch': 13, 'value': 0.6862000286579132} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 13, 'value': 1.02908735871315} ({'split': 'test'})\n",
      "Epoch 014\n",
      "accuracy: {'epoch': 14, 'value': 0.7413400000000001} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 14, 'value': 0.8119505078125004} ({'split': 'train'})\n",
      "accuracy: {'epoch': 14, 'value': 0.6714000284671783} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 14, 'value': 1.1307095646858216} ({'split': 'test'})\n",
      "Epoch 015\n",
      "accuracy: {'epoch': 15, 'value': 0.7199800000000002} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 15, 'value': 0.877944295654298} ({'split': 'train'})\n",
      "accuracy: {'epoch': 15, 'value': 0.684000027179718} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 15, 'value': 1.1077483773231507} ({'split': 'test'})\n",
      "Epoch 016\n",
      "accuracy: {'epoch': 16, 'value': 0.6857200000000002} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 16, 'value': 0.9871518617630005} ({'split': 'train'})\n",
      "accuracy: {'epoch': 16, 'value': 0.6110000312328339} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 16, 'value': 1.2190929770469665} ({'split': 'test'})\n",
      "Epoch 017\n",
      "accuracy: {'epoch': 17, 'value': 0.5641800000000003} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 17, 'value': 1.3502880066299432} ({'split': 'train'})\n",
      "accuracy: {'epoch': 17, 'value': 0.5267000287771225} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 17, 'value': 1.4346413135528564} ({'split': 'test'})\n",
      "Epoch 018\n",
      "accuracy: {'epoch': 18, 'value': 0.5489} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 18, 'value': 1.3624223974609366} ({'split': 'train'})\n",
      "accuracy: {'epoch': 18, 'value': 0.5512000322341919} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 18, 'value': 1.4254674196243287} ({'split': 'test'})\n",
      "Epoch 019\n",
      "accuracy: {'epoch': 19, 'value': 0.5058199999999997} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 19, 'value': 1.487933646545411} ({'split': 'train'})\n",
      "accuracy: {'epoch': 19, 'value': 0.47830002903938296} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 19, 'value': 1.575791347026825} ({'split': 'test'})\n",
      "Epoch 020\n",
      "accuracy: {'epoch': 20, 'value': 0.49626000000000037} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 20, 'value': 1.4976259297370917} ({'split': 'train'})\n",
      "accuracy: {'epoch': 20, 'value': 0.4314000248908996} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 20, 'value': 1.7924413323402404} ({'split': 'test'})\n",
      "Epoch 021\n",
      "accuracy: {'epoch': 21, 'value': 0.49544000000000016} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 21, 'value': 1.499908244380949} ({'split': 'train'})\n",
      "accuracy: {'epoch': 21, 'value': 0.549500024318695} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 21, 'value': 1.3816498160362245} ({'split': 'test'})\n",
      "Epoch 022\n",
      "accuracy: {'epoch': 22, 'value': 0.5162800000000005} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 22, 'value': 1.4470457321166994} ({'split': 'train'})\n",
      "accuracy: {'epoch': 22, 'value': 0.5173000216484069} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 22, 'value': 1.493699562549591} ({'split': 'test'})\n",
      "Epoch 023\n",
      "accuracy: {'epoch': 23, 'value': 0.5444999999999999} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 23, 'value': 1.3598003190994266} ({'split': 'train'})\n",
      "accuracy: {'epoch': 23, 'value': 0.5373000264167785} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 23, 'value': 1.3678892970085141} ({'split': 'test'})\n",
      "Epoch 024\n",
      "accuracy: {'epoch': 24, 'value': 0.5589999999999995} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 24, 'value': 1.3233901527786276} ({'split': 'train'})\n",
      "accuracy: {'epoch': 24, 'value': 0.543800014257431} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 24, 'value': 1.438993239402771} ({'split': 'test'})\n",
      "Epoch 025\n",
      "accuracy: {'epoch': 25, 'value': 0.5645200000000005} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 25, 'value': 1.3020253450775143} ({'split': 'train'})\n",
      "accuracy: {'epoch': 25, 'value': 0.5780000388622284} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 25, 'value': 1.2987822055816651} ({'split': 'test'})\n",
      "Epoch 026\n",
      "accuracy: {'epoch': 26, 'value': 0.5837599999999997} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 26, 'value': 1.250035167541503} ({'split': 'train'})\n",
      "accuracy: {'epoch': 26, 'value': 0.5363000273704529} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 26, 'value': 1.4343976855278016} ({'split': 'test'})\n",
      "Epoch 027\n",
      "accuracy: {'epoch': 27, 'value': 0.5925399999999998} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 27, 'value': 1.232986131858826} ({'split': 'train'})\n",
      "accuracy: {'epoch': 27, 'value': 0.5570000231266021} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 27, 'value': 1.3335936307907106} ({'split': 'test'})\n",
      "Epoch 028\n",
      "accuracy: {'epoch': 28, 'value': 0.61232} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 28, 'value': 1.174659649715426} ({'split': 'train'})\n",
      "accuracy: {'epoch': 28, 'value': 0.5834000229835511} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 28, 'value': 1.2921091556549071} ({'split': 'test'})\n",
      "Epoch 029\n",
      "accuracy: {'epoch': 29, 'value': 0.6174000000000005} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 29, 'value': 1.1575186905288688} ({'split': 'train'})\n",
      "accuracy: {'epoch': 29, 'value': 0.5723000288009643} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 29, 'value': 1.2972044110298158} ({'split': 'test'})\n",
      "mean_test_accuracy <cifar_utils.accumulators.Mean object at 0x7f8a5d3e55e0>\n",
      "QWERTY: Enter Cifar 2, acc 0.810200035572052\n",
      "Retraining model :  model_1\n",
      "num_epochs---------------+++))))))+++ 30\n",
      "num_epochs---------------+++))))))+++ 30\n",
      "lr is  0.1\n",
      "number of epochs would be  30\n",
      "num_epochs-------------- 30\n",
      "Epoch 000\n",
      "accuracy: {'epoch': 0, 'value': 0.8071800000000003} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 0, 'value': 0.5934541005312655} ({'split': 'train'})\n",
      "accuracy: {'epoch': 0, 'value': 0.7930000305175781} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 0, 'value': 0.6409581720829012} ({'split': 'test'})\n",
      "Epoch 001\n",
      "accuracy: {'epoch': 1, 'value': 0.8452799999999998} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 1, 'value': 0.47080313059806866} ({'split': 'train'})\n",
      "accuracy: {'epoch': 1, 'value': 0.8030000448226928} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 1, 'value': 0.6140623986721039} ({'split': 'test'})\n",
      "Epoch 002\n",
      "accuracy: {'epoch': 2, 'value': 0.86874} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 2, 'value': 0.40012081851959197} ({'split': 'train'})\n",
      "accuracy: {'epoch': 2, 'value': 0.798900055885315} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 2, 'value': 0.6326294124126434} ({'split': 'test'})\n",
      "Epoch 003\n",
      "accuracy: {'epoch': 3, 'value': 0.8759000000000003} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 3, 'value': 0.37731402702331557} ({'split': 'train'})\n",
      "accuracy: {'epoch': 3, 'value': 0.7968000233173371} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 3, 'value': 0.6207426488399505} ({'split': 'test'})\n",
      "Epoch 004\n",
      "accuracy: {'epoch': 4, 'value': 0.8807199999999998} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 4, 'value': 0.36824558845520045} ({'split': 'train'})\n",
      "accuracy: {'epoch': 4, 'value': 0.798900032043457} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 4, 'value': 0.6681198120117188} ({'split': 'test'})\n",
      "Epoch 005\n",
      "accuracy: {'epoch': 5, 'value': 0.8689799999999995} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 5, 'value': 0.4038058076190951} ({'split': 'train'})\n",
      "accuracy: {'epoch': 5, 'value': 0.7817000448703765} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 5, 'value': 0.6895958781242371} ({'split': 'test'})\n",
      "Epoch 006\n",
      "accuracy: {'epoch': 6, 'value': 0.8689199999999996} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 6, 'value': 0.4084564109039302} ({'split': 'train'})\n",
      "accuracy: {'epoch': 6, 'value': 0.7712000250816345} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 6, 'value': 0.8167868077754975} ({'split': 'test'})\n",
      "Epoch 007\n",
      "accuracy: {'epoch': 7, 'value': 0.8513600000000004} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 7, 'value': 0.46577773869514494} ({'split': 'train'})\n",
      "accuracy: {'epoch': 7, 'value': 0.7872000336647034} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 7, 'value': 0.7473360061645508} ({'split': 'test'})\n",
      "Epoch 008\n",
      "accuracy: {'epoch': 8, 'value': 0.8449200000000002} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 8, 'value': 0.48450633012771605} ({'split': 'train'})\n",
      "accuracy: {'epoch': 8, 'value': 0.7606000363826751} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 8, 'value': 0.7834489703178406} ({'split': 'test'})\n",
      "Epoch 009\n",
      "accuracy: {'epoch': 9, 'value': 0.8332399999999998} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 9, 'value': 0.5193913487815862} ({'split': 'train'})\n",
      "accuracy: {'epoch': 9, 'value': 0.7231000304222107} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 9, 'value': 0.9153842091560364} ({'split': 'test'})\n",
      "Epoch 010\n",
      "accuracy: {'epoch': 10, 'value': 0.8266399999999996} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 10, 'value': 0.5423227576160432} ({'split': 'train'})\n",
      "accuracy: {'epoch': 10, 'value': 0.746200031042099} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 10, 'value': 0.8652363657951355} ({'split': 'test'})\n",
      "Epoch 011\n",
      "accuracy: {'epoch': 11, 'value': 0.7990200000000001} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 11, 'value': 0.6296073825645448} ({'split': 'train'})\n",
      "accuracy: {'epoch': 11, 'value': 0.6981000304222107} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 11, 'value': 1.009074294567108} ({'split': 'test'})\n",
      "Epoch 012\n",
      "accuracy: {'epoch': 12, 'value': 0.7792599999999994} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 12, 'value': 0.6928966199111934} ({'split': 'train'})\n",
      "accuracy: {'epoch': 12, 'value': 0.7079000353813172} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 12, 'value': 0.9456889688968658} ({'split': 'test'})\n",
      "Epoch 013\n",
      "accuracy: {'epoch': 13, 'value': 0.7579599999999997} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 13, 'value': 0.7620773366165158} ({'split': 'train'})\n",
      "accuracy: {'epoch': 13, 'value': 0.6920000314712524} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 13, 'value': 0.960436898469925} ({'split': 'test'})\n",
      "Epoch 014\n",
      "accuracy: {'epoch': 14, 'value': 0.7343400000000003} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 14, 'value': 0.8347451781845103} ({'split': 'train'})\n",
      "accuracy: {'epoch': 14, 'value': 0.6767000198364257} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 14, 'value': 1.0250142395496369} ({'split': 'test'})\n",
      "Epoch 015\n",
      "accuracy: {'epoch': 15, 'value': 0.7047800000000006} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 15, 'value': 0.9246886465072627} ({'split': 'train'})\n",
      "accuracy: {'epoch': 15, 'value': 0.6564000248908997} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 15, 'value': 1.0852120757102965} ({'split': 'test'})\n",
      "Epoch 016\n",
      "accuracy: {'epoch': 16, 'value': 0.6851200000000004} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 16, 'value': 0.9839615579223632} ({'split': 'train'})\n",
      "accuracy: {'epoch': 16, 'value': 0.6433000266551971} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 16, 'value': 1.2049953579902648} ({'split': 'test'})\n",
      "Epoch 017\n",
      "accuracy: {'epoch': 17, 'value': 0.6416799999999999} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 17, 'value': 1.1102478008842454} ({'split': 'train'})\n",
      "accuracy: {'epoch': 17, 'value': 0.556900018453598} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 17, 'value': 1.3386213421821596} ({'split': 'test'})\n",
      "Epoch 018\n",
      "accuracy: {'epoch': 18, 'value': 0.6028599999999991} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 18, 'value': 1.2253601052093501} ({'split': 'train'})\n",
      "accuracy: {'epoch': 18, 'value': 0.5630000293254852} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 18, 'value': 1.3686593890190124} ({'split': 'test'})\n",
      "Epoch 019\n",
      "accuracy: {'epoch': 19, 'value': 0.5446999999999996} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 19, 'value': 1.3847499325180062} ({'split': 'train'})\n",
      "accuracy: {'epoch': 19, 'value': 0.4826000213623047} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 19, 'value': 1.5978935718536378} ({'split': 'test'})\n",
      "Epoch 020\n",
      "accuracy: {'epoch': 20, 'value': 0.5092600000000005} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 20, 'value': 1.466814922275541} ({'split': 'train'})\n",
      "accuracy: {'epoch': 20, 'value': 0.4727000236511231} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 20, 'value': 1.6826262474060059} ({'split': 'test'})\n",
      "Epoch 021\n",
      "accuracy: {'epoch': 21, 'value': 0.4652600000000001} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 21, 'value': 1.5944834714508076} ({'split': 'train'})\n",
      "accuracy: {'epoch': 21, 'value': 0.43690001964569086} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 21, 'value': 1.7463786363601683} ({'split': 'test'})\n",
      "Epoch 022\n",
      "accuracy: {'epoch': 22, 'value': 0.23144000000000048} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 22, 'value': nan} ({'split': 'train'})\n",
      "accuracy: {'epoch': 22, 'value': 0.10000000447034836} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 22, 'value': nan} ({'split': 'test'})\n",
      "Epoch 023\n",
      "accuracy: {'epoch': 23, 'value': 0.10000000000000013} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 23, 'value': nan} ({'split': 'train'})\n",
      "accuracy: {'epoch': 23, 'value': 0.10000000521540642} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 23, 'value': nan} ({'split': 'test'})\n",
      "Epoch 024\n",
      "accuracy: {'epoch': 24, 'value': 0.10000000000000005} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 24, 'value': nan} ({'split': 'train'})\n",
      "accuracy: {'epoch': 24, 'value': 0.10000000521540642} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 24, 'value': nan} ({'split': 'test'})\n",
      "Epoch 025\n",
      "accuracy: {'epoch': 25, 'value': 0.10000000000000005} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 25, 'value': nan} ({'split': 'train'})\n",
      "accuracy: {'epoch': 25, 'value': 0.10000000521540642} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 25, 'value': nan} ({'split': 'test'})\n",
      "Epoch 026\n",
      "accuracy: {'epoch': 26, 'value': 0.10000000000000013} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 26, 'value': nan} ({'split': 'train'})\n",
      "accuracy: {'epoch': 26, 'value': 0.1000000037252903} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 26, 'value': nan} ({'split': 'test'})\n",
      "Epoch 027\n",
      "accuracy: {'epoch': 27, 'value': 0.09999999999999991} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 27, 'value': nan} ({'split': 'train'})\n",
      "accuracy: {'epoch': 27, 'value': 0.10000000447034836} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 27, 'value': nan} ({'split': 'test'})\n",
      "Epoch 028\n",
      "accuracy: {'epoch': 28, 'value': 0.10000000000000005} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 28, 'value': nan} ({'split': 'train'})\n",
      "accuracy: {'epoch': 28, 'value': 0.10000000447034836} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 28, 'value': nan} ({'split': 'test'})\n",
      "Epoch 029\n",
      "accuracy: {'epoch': 29, 'value': 0.10000000000000007} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 29, 'value': nan} ({'split': 'train'})\n",
      "accuracy: {'epoch': 29, 'value': 0.10000000596046449} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 29, 'value': nan} ({'split': 'test'})\n",
      "mean_test_accuracy <cifar_utils.accumulators.Mean object at 0x7f8a5d3cca00>\n",
      "QWERTY: Enter Cifar 2, acc 0.8030000448226928\n",
      "Retraining model :  geometric\n",
      "num_epochs---------------+++))))))+++ 30\n",
      "num_epochs---------------+++))))))+++ 30\n",
      "lr is  0.1\n",
      "number of epochs would be  30\n",
      "num_epochs-------------- 30\n",
      "Epoch 000\n",
      "accuracy: {'epoch': 0, 'value': 0.7566400000000006} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 0, 'value': 0.7435530398178104} ({'split': 'train'})\n",
      "accuracy: {'epoch': 0, 'value': 0.7703000307083129} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 0, 'value': 0.7237870395183564} ({'split': 'test'})\n",
      "Epoch 001\n",
      "accuracy: {'epoch': 1, 'value': 0.8258000000000001} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 1, 'value': 0.5245594719028471} ({'split': 'train'})\n",
      "accuracy: {'epoch': 1, 'value': 0.7883000493049621} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 1, 'value': 0.7053246974945069} ({'split': 'test'})\n",
      "Epoch 002\n",
      "accuracy: {'epoch': 2, 'value': 0.8584800000000006} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 2, 'value': 0.4259520017242431} ({'split': 'train'})\n",
      "accuracy: {'epoch': 2, 'value': 0.792600029706955} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 2, 'value': 0.6623495995998382} ({'split': 'test'})\n",
      "Epoch 003\n",
      "accuracy: {'epoch': 3, 'value': 0.8716200000000002} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 3, 'value': 0.384125645637512} ({'split': 'train'})\n",
      "accuracy: {'epoch': 3, 'value': 0.8075000405311584} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 3, 'value': 0.6271618008613586} ({'split': 'test'})\n",
      "Epoch 004\n",
      "accuracy: {'epoch': 4, 'value': 0.8844200000000004} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 4, 'value': 0.3471110781812668} ({'split': 'train'})\n",
      "accuracy: {'epoch': 4, 'value': 0.8015000343322753} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 4, 'value': 0.6811460196971894} ({'split': 'test'})\n",
      "Epoch 005\n",
      "accuracy: {'epoch': 5, 'value': 0.8823599999999994} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 5, 'value': 0.356698959960938} ({'split': 'train'})\n",
      "accuracy: {'epoch': 5, 'value': 0.8010000348091126} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 5, 'value': 0.6348281979560852} ({'split': 'test'})\n",
      "Epoch 006\n",
      "accuracy: {'epoch': 6, 'value': 0.8789800000000008} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 6, 'value': 0.36594230140685985} ({'split': 'train'})\n",
      "accuracy: {'epoch': 6, 'value': 0.7874000310897827} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 6, 'value': 0.6977882862091065} ({'split': 'test'})\n",
      "Epoch 007\n",
      "accuracy: {'epoch': 7, 'value': 0.8771200000000009} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 7, 'value': 0.3763167985916135} ({'split': 'train'})\n",
      "accuracy: {'epoch': 7, 'value': 0.7709000468254089} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 7, 'value': 0.7971551179885865} ({'split': 'test'})\n",
      "Epoch 008\n",
      "accuracy: {'epoch': 8, 'value': 0.8670400000000003} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 8, 'value': 0.4038465286445617} ({'split': 'train'})\n",
      "accuracy: {'epoch': 8, 'value': 0.7514000415802002} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 8, 'value': 0.8612055122852327} ({'split': 'test'})\n",
      "Epoch 009\n",
      "accuracy: {'epoch': 9, 'value': 0.854339999999999} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 9, 'value': 0.45157356819152844} ({'split': 'train'})\n",
      "accuracy: {'epoch': 9, 'value': 0.7579000294208527} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 9, 'value': 0.8548810720443726} ({'split': 'test'})\n",
      "Epoch 010\n",
      "accuracy: {'epoch': 10, 'value': 0.8412600000000006} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 10, 'value': 0.4918111669158938} ({'split': 'train'})\n",
      "accuracy: {'epoch': 10, 'value': 0.7748000383377075} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 10, 'value': 0.779127871990204} ({'split': 'test'})\n",
      "Epoch 011\n",
      "accuracy: {'epoch': 11, 'value': 0.8317800000000007} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 11, 'value': 0.5251419363784787} ({'split': 'train'})\n",
      "accuracy: {'epoch': 11, 'value': 0.7403000295162201} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 11, 'value': 0.9232225060462952} ({'split': 'test'})\n",
      "Epoch 012\n",
      "accuracy: {'epoch': 12, 'value': 0.8159200000000004} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 12, 'value': 0.5684086506843566} ({'split': 'train'})\n",
      "accuracy: {'epoch': 12, 'value': 0.7393000423908234} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 12, 'value': 0.9024530827999115} ({'split': 'test'})\n",
      "Epoch 013\n",
      "accuracy: {'epoch': 13, 'value': 0.7880999999999995} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 13, 'value': 0.658033368835449} ({'split': 'train'})\n",
      "accuracy: {'epoch': 13, 'value': 0.6826000332832337} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 13, 'value': 1.065796858072281} ({'split': 'test'})\n",
      "Epoch 014\n",
      "accuracy: {'epoch': 14, 'value': 0.7649000000000002} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 14, 'value': 0.7364365868759147} ({'split': 'train'})\n",
      "accuracy: {'epoch': 14, 'value': 0.6357000291347503} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 14, 'value': 1.354617440700531} ({'split': 'test'})\n",
      "Epoch 015\n",
      "accuracy: {'epoch': 15, 'value': 0.7363400000000004} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 15, 'value': 0.8299468695068354} ({'split': 'train'})\n",
      "accuracy: {'epoch': 15, 'value': 0.6756000339984893} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 15, 'value': 1.0057396233081817} ({'split': 'test'})\n",
      "Epoch 016\n",
      "accuracy: {'epoch': 16, 'value': 0.7084799999999993} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 16, 'value': 0.9168062688827522} ({'split': 'train'})\n",
      "accuracy: {'epoch': 16, 'value': 0.6433000206947326} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 16, 'value': 1.1687525510787964} ({'split': 'test'})\n",
      "Epoch 017\n",
      "accuracy: {'epoch': 17, 'value': 0.6875999999999997} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 17, 'value': 0.9749475424575808} ({'split': 'train'})\n",
      "accuracy: {'epoch': 17, 'value': 0.6167000353336334} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 17, 'value': 1.2147154688835144} ({'split': 'test'})\n",
      "Epoch 018\n",
      "accuracy: {'epoch': 18, 'value': 0.6476799999999994} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 18, 'value': 1.0996192830276483} ({'split': 'train'})\n",
      "accuracy: {'epoch': 18, 'value': 0.5943000316619873} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 18, 'value': 1.2871100425720214} ({'split': 'test'})\n",
      "Epoch 019\n",
      "accuracy: {'epoch': 19, 'value': 0.6264399999999998} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 19, 'value': 1.158512366962432} ({'split': 'train'})\n",
      "accuracy: {'epoch': 19, 'value': 0.601600033044815} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 19, 'value': 1.3177136421203615} ({'split': 'test'})\n",
      "Epoch 020\n",
      "accuracy: {'epoch': 20, 'value': 0.5986400000000001} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 20, 'value': 1.2380939616394049} ({'split': 'train'})\n",
      "accuracy: {'epoch': 20, 'value': 0.5280000269412994} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 20, 'value': 1.4272313356399535} ({'split': 'test'})\n",
      "Epoch 021\n",
      "accuracy: {'epoch': 21, 'value': 0.5591599999999999} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 21, 'value': 1.3422255584907543} ({'split': 'train'})\n",
      "accuracy: {'epoch': 21, 'value': 0.5411000192165376} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 21, 'value': 1.4774892330169676} ({'split': 'test'})\n",
      "Epoch 022\n",
      "accuracy: {'epoch': 22, 'value': 0.5201799999999999} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 22, 'value': 1.448701253623962} ({'split': 'train'})\n",
      "accuracy: {'epoch': 22, 'value': 0.5371000230312348} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 22, 'value': 1.5051968932151794} ({'split': 'test'})\n",
      "Epoch 023\n",
      "accuracy: {'epoch': 23, 'value': 0.5127200000000002} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 23, 'value': 1.4671406794738775} ({'split': 'train'})\n",
      "accuracy: {'epoch': 23, 'value': 0.43660001754760747} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 23, 'value': 1.6443220615386962} ({'split': 'test'})\n",
      "Epoch 024\n",
      "accuracy: {'epoch': 24, 'value': 0.23727999999999994} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 24, 'value': nan} ({'split': 'train'})\n",
      "accuracy: {'epoch': 24, 'value': 0.10000000521540642} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 24, 'value': nan} ({'split': 'test'})\n",
      "Epoch 025\n",
      "accuracy: {'epoch': 25, 'value': 0.10000000000000005} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 25, 'value': nan} ({'split': 'train'})\n",
      "accuracy: {'epoch': 25, 'value': 0.10000000521540642} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 25, 'value': nan} ({'split': 'test'})\n",
      "Epoch 026\n",
      "accuracy: {'epoch': 26, 'value': 0.10000000000000013} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 26, 'value': nan} ({'split': 'train'})\n",
      "accuracy: {'epoch': 26, 'value': 0.1000000037252903} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 26, 'value': nan} ({'split': 'test'})\n",
      "Epoch 027\n",
      "accuracy: {'epoch': 27, 'value': 0.09999999999999991} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 27, 'value': nan} ({'split': 'train'})\n",
      "accuracy: {'epoch': 27, 'value': 0.10000000447034836} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 27, 'value': nan} ({'split': 'test'})\n",
      "Epoch 028\n",
      "accuracy: {'epoch': 28, 'value': 0.10000000000000005} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 28, 'value': nan} ({'split': 'train'})\n",
      "accuracy: {'epoch': 28, 'value': 0.10000000447034836} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 28, 'value': nan} ({'split': 'test'})\n",
      "Epoch 029\n",
      "accuracy: {'epoch': 29, 'value': 0.10000000000000007} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 29, 'value': nan} ({'split': 'train'})\n",
      "accuracy: {'epoch': 29, 'value': 0.10000000596046449} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 29, 'value': nan} ({'split': 'test'})\n",
      "mean_test_accuracy <cifar_utils.accumulators.Mean object at 0x7f8a5d3e56a0>\n",
      "QWERTY: Enter Cifar 2, acc 0.8075000405311584\n",
      "Retraining model :  naive_averaging\n",
      "num_epochs---------------+++))))))+++ 30\n",
      "num_epochs---------------+++))))))+++ 30\n",
      "lr is  0.1\n",
      "number of epochs would be  30\n",
      "num_epochs-------------- 30\n",
      "Epoch 000\n",
      "accuracy: {'epoch': 0, 'value': 0.4758799999999999} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 0, 'value': 1.4591751414489755} ({'split': 'train'})\n",
      "accuracy: {'epoch': 0, 'value': 0.706000030040741} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 0, 'value': 0.8780550718307496} ({'split': 'test'})\n",
      "Epoch 001\n",
      "accuracy: {'epoch': 1, 'value': 0.7476200000000008} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 1, 'value': 0.759469866466523} ({'split': 'train'})\n",
      "accuracy: {'epoch': 1, 'value': 0.7792000293731688} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 1, 'value': 0.6518420219421386} ({'split': 'test'})\n",
      "Epoch 002\n",
      "accuracy: {'epoch': 2, 'value': 0.8118} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 2, 'value': 0.56609174935341} ({'split': 'train'})\n",
      "accuracy: {'epoch': 2, 'value': 0.7783000409603119} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 2, 'value': 0.6799935042858123} ({'split': 'test'})\n",
      "Epoch 003\n",
      "accuracy: {'epoch': 3, 'value': 0.8414799999999998} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 3, 'value': 0.4732842648029329} ({'split': 'train'})\n",
      "accuracy: {'epoch': 3, 'value': 0.7858000457286834} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 3, 'value': 0.6504068613052368} ({'split': 'test'})\n",
      "Epoch 004\n",
      "accuracy: {'epoch': 4, 'value': 0.8590400000000001} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 4, 'value': 0.4171971133804328} ({'split': 'train'})\n",
      "accuracy: {'epoch': 4, 'value': 0.7679000318050384} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 4, 'value': 0.7709817349910736} ({'split': 'test'})\n",
      "Epoch 005\n",
      "accuracy: {'epoch': 5, 'value': 0.8702600000000003} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 5, 'value': 0.38517502621650695} ({'split': 'train'})\n",
      "accuracy: {'epoch': 5, 'value': 0.7727000415325165} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 5, 'value': 0.6907514810562133} ({'split': 'test'})\n",
      "Epoch 006\n",
      "accuracy: {'epoch': 6, 'value': 0.8767000000000004} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 6, 'value': 0.37340770084381086} ({'split': 'train'})\n",
      "accuracy: {'epoch': 6, 'value': 0.781400042772293} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 6, 'value': 0.7686898350715636} ({'split': 'test'})\n",
      "Epoch 007\n",
      "accuracy: {'epoch': 7, 'value': 0.8679399999999997} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 7, 'value': 0.3930807989788056} ({'split': 'train'})\n",
      "accuracy: {'epoch': 7, 'value': 0.7854000329971313} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 7, 'value': 0.684454756975174} ({'split': 'test'})\n",
      "Epoch 008\n",
      "accuracy: {'epoch': 8, 'value': 0.8619999999999995} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 8, 'value': 0.42008668082237244} ({'split': 'train'})\n",
      "accuracy: {'epoch': 8, 'value': 0.7664000332355498} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 8, 'value': 0.8280030250549316} ({'split': 'test'})\n",
      "Epoch 009\n",
      "accuracy: {'epoch': 9, 'value': 0.8637400000000002} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 9, 'value': 0.4152648346996305} ({'split': 'train'})\n",
      "accuracy: {'epoch': 9, 'value': 0.7575000345706939} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 9, 'value': 0.8892470359802246} ({'split': 'test'})\n",
      "Epoch 010\n",
      "^C\n",
      "Traceback (most recent call last):\n",
      "  File \"main.py\", line 480, in <module>\n",
      "    for idx, acc in enumerate(accuracies):\n",
      "  File \"/root/autodl-tmp/OTfusion_pruning/otfusion/routines.py\", line 394, in retrain_models\n",
      "    retrained_network, acc = cifar_train.get_retrained_model(args, retrain_loader, test_loader, old_networks[i], config, output_root_dir, tensorboard_obj=tensorboard_obj, nick=nick, start_acc=initial_acc[i])\n",
      "  File \"/root/autodl-tmp/OTfusion_pruning/otfusion/./cifar/train.py\", line 333, in get_retrained_model\n",
      "    best_acc,model = main(config, output_dir, args.gpu_id, pretrained_model=old_network, pretrained_dataset=(train_loader, test_loader), tensorboard_obj=tensorboard_obj,return_model=True)\n",
      "  File \"/root/autodl-tmp/OTfusion_pruning/otfusion/./cifar/train.py\", line 67, in main\n",
      "    for batch_x, batch_y in training_loader:\n",
      "  File \"/root/miniconda3/lib/python3.8/site-packages/torch/utils/data/dataloader.py\", line 521, in __next__\n",
      "    data = self._next_data()\n",
      "  File \"/root/miniconda3/lib/python3.8/site-packages/torch/utils/data/dataloader.py\", line 561, in _next_data\n",
      "    data = self._dataset_fetcher.fetch(index)  # may raise StopIteration\n",
      "  File \"/root/miniconda3/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py\", line 49, in fetch\n",
      "    data = [self.dataset[idx] for idx in possibly_batched_index]\n",
      "  File \"/root/miniconda3/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py\", line 49, in <listcomp>\n",
      "    data = [self.dataset[idx] for idx in possibly_batched_index]\n",
      "  File \"/root/miniconda3/lib/python3.8/site-packages/torchvision/datasets/cifar.py\", line 118, in __getitem__\n",
      "    img = Image.fromarray(img)\n",
      "  File \"/root/miniconda3/lib/python3.8/site-packages/PIL/Image.py\", line 2846, in fromarray\n",
      "    if ndim > ndmax:\n",
      "KeyboardInterrupt\n"
     ]
    }
   ],
   "source": [
    "!python main.py --gpu-id 0 --model-name resnet18_nobias_nobn --n-epochs 10 --save-result-file sample.csv \\\n",
    "--sweep-name exp_sample --exact --correction --ground-metric euclidean --weight-stats \\\n",
    "--activation-histograms --activation-mode raw --geom-ensemble-type acts --sweep-id 21 \\\n",
    "--act-num-samples 200 --ground-metric-normalize none --activation-seed {activation_seed} --prelu-acts --recheck-acc \\\n",
    "--load-models ./resnet_models/ --ckpt-type best --past-correction --not-squared  --dataset Cifar10 \\\n",
    "--handle-skips\\\n",
    "--learning-rate 0.004\\\n",
    "--momentum 0.5\\\n",
    "--batch-size-train 64 --experiment-dir {experiment_dir}\\\n",
    "--prunint-rate {prunint_rate}\\\n",
    "--run-mode {run_mode}\\\n",
    "--load-model-dir {load_model_dir}\\\n",
    "--to-download\\\n",
    "--retrain 30\\\n",
    "--eval-aligned\\\n",
    "--prune_times 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29a14370-8231-4282-b050-cda48f71b3f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#剪枝实验1:prune without finetune\n",
    "activation_seed=21\n",
    "experiment_dir=\"experiment2\"\n",
    "prunint_rate=0.2\n",
    "run_mode='prune'\n",
    "load_model_dir='./cifar_models'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a2488ee6-9aab-4165-9663-cf713df42dee",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------- Setting up parameters -------\n",
      "dumping parameters at  /root/autodl-tmp/OTfusion_pruning/otfusion/exp_sample/configurations\n",
      "The parameters are: \n",
      " Namespace(act_bug=False, act_num_samples=100, activation_histograms=False, activation_mode=None, activation_seed=21, alpha=0.7, baseroot='/root/autodl-tmp/OTfusion_pruning/otfusion', batch_size_test=1000, batch_size_train=64, center_acts=False, choice='0 2 4 6 8', cifar_style_data=False, ckpt_type='best', clip_gm=False, clip_max=5, clip_min=0, config_dir='/root/autodl-tmp/OTfusion_pruning/otfusion/exp_sample/configurations', config_file=None, correction=True, csv_dir='/root/autodl-tmp/OTfusion_pruning/otfusion/exp_sample/csv', dataset='Cifar10', debug=False, deprecated=None, deterministic=False, diff_init=False, disable_bias=True, dist_epochs=60, dist_normalize=False, dump_final_models=False, dump_model=False, enable_dropout=False, ensemble_step=0.5, eval_aligned=False, exact=True, exp_name='exp_2023-09-16_16-31-35_546443', experiment_dir='experiment2', geom_ensemble_type='wts', gpu_id=0, gromov=False, gromov_loss='square_loss', ground_metric='euclidean', ground_metric_eff=True, ground_metric_normalize='none', handle_skips=False, importance=None, learning_rate=0.08, load_model_dir='./cifar_models', load_models='', log_interval=100, model_name='vgg11_nobias', momentum=0.5, n_epochs=90, no_random_trainloaders=False, normalize_acts=False, normalize_wts=False, not_squared=True, num_hidden_nodes=400, num_hidden_nodes1=400, num_hidden_nodes2=200, num_hidden_nodes3=100, num_hidden_nodes4=50, num_models=2, options_type='generic', partial_reshape=False, partition_dataloader=-1, partition_type='labels', past_correction=True, personal_class_idx=9, personal_split_frac=0.1, pool_acts=False, pool_relu=False, prediction_wts=False, prelu_acts=True, print_distances=False, proper_marginals=False, prunint_rate=0.2, recheck_acc=False, recheck_cifar=True, reg=0.01, reg_m=0.001, reinit_trainloaders=False, result_dir='/root/autodl-tmp/OTfusion_pruning/otfusion/exp_sample/results', retrain=30, retrain_avg_only=False, retrain_geometric_only=False, retrain_lr_decay=-1, retrain_lr_decay_epochs=None, retrain_lr_decay_factor=None, retrain_seed=-1, rootdir='/root/autodl-tmp/OTfusion_pruning/otfusion/exp_sample', run_mode='prune', same_model=-1, save_result_file='sample.csv', second_model_name=None, sinkhorn_type='normal', skip_last_layer=False, skip_last_layer_type='average', skip_personal_idx=False, skip_retrain=-1, softmax_temperature=1, standardize_acts=False, sweep_id=90, sweep_name='exp_sample', temperature=20, tensorboard=False, tensorboard_root='./tensorboard', timestamp='2023-09-16_16-31-35_546443', tmap_stats=False, to_download=True, transform_acts=False, unbalanced=False, update_acts=False, weight_stats=True, width_ratio=1)\n",
      "refactored get_config\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "in _make_layers [Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), AvgPool2d(kernel_size=1, stride=1, padding=0)]\n",
      "Relu Inplace is  False\n",
      "model parameters are \n",
      " [torch.Size([64, 3, 3, 3]), torch.Size([128, 64, 3, 3]), torch.Size([256, 128, 3, 3]), torch.Size([256, 256, 3, 3]), torch.Size([512, 256, 3, 3]), torch.Size([512, 512, 3, 3]), torch.Size([512, 512, 3, 3]), torch.Size([512, 512, 3, 3]), torch.Size([10, 512])]\n",
      "in _make_layers [Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), AvgPool2d(kernel_size=1, stride=1, padding=0)]\n",
      "Relu Inplace is  False\n",
      "model parameters are \n",
      " [torch.Size([64, 3, 3, 3]), torch.Size([128, 64, 3, 3]), torch.Size([256, 128, 3, 3]), torch.Size([256, 256, 3, 3]), torch.Size([512, 256, 3, 3]), torch.Size([512, 512, 3, 3]), torch.Size([512, 512, 3, 3]), torch.Size([512, 512, 3, 3]), torch.Size([10, 512])]\n",
      "Apply Unstructured L1 Pruning Globally (all conv layers)\n",
      "in _make_layers [Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), AvgPool2d(kernel_size=1, stride=1, padding=0)]\n",
      "Relu Inplace is  False\n",
      "model parameters are \n",
      " [torch.Size([64, 3, 3, 3]), torch.Size([128, 64, 3, 3]), torch.Size([256, 128, 3, 3]), torch.Size([256, 256, 3, 3]), torch.Size([512, 256, 3, 3]), torch.Size([512, 512, 3, 3]), torch.Size([512, 512, 3, 3]), torch.Size([512, 512, 3, 3]), torch.Size([10, 512])]\n",
      "in _make_layers [Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), AvgPool2d(kernel_size=1, stride=1, padding=0)]\n",
      "Relu Inplace is  False\n",
      "model parameters are \n",
      " [torch.Size([64, 3, 3, 3]), torch.Size([128, 64, 3, 3]), torch.Size([256, 128, 3, 3]), torch.Size([256, 256, 3, 3]), torch.Size([512, 256, 3, 3]), torch.Size([512, 512, 3, 3]), torch.Size([512, 512, 3, 3]), torch.Size([512, 512, 3, 3]), torch.Size([10, 512])]\n",
      "Apply Unstructured L1 Pruning Globally (all conv layers)\n",
      "Pruning with custom mask (all conv layers)\n",
      "\n",
      "--------- Testing in global mode ---------\n",
      "size of test_loader dataset:  10000\n",
      "\n",
      "Test set: Avg. loss: 0.0003, Accuracy: 9019/10000 (90%)\n",
      "\n",
      "Pruning with custom mask (all conv layers)\n",
      "\n",
      "--------- Testing in global mode ---------\n",
      "size of test_loader dataset:  10000\n",
      "\n",
      "Test set: Avg. loss: 0.0003, Accuracy: 9041/10000 (90%)\n",
      "\n",
      "========================================================\n",
      "accuracies [90.19, 90.41]\n",
      "=========================================================\n",
      "layer features.0.weight_orig has #params  1728\n",
      "layer features.3.weight_orig has #params  73728\n",
      "layer features.6.weight_orig has #params  294912\n",
      "layer features.8.weight_orig has #params  589824\n",
      "layer features.11.weight_orig has #params  1179648\n",
      "layer features.13.weight_orig has #params  2359296\n",
      "layer features.16.weight_orig has #params  2359296\n",
      "layer features.18.weight_orig has #params  2359296\n",
      "layer classifier.weight_orig has #params  5120\n",
      "Activation Timer start\n",
      "Activation Timer ends\n",
      "------- Geometric Ensembling -------\n",
      "Timer start\n",
      "Previous layer shape is  None\n",
      "Processing the coordinates to form ground_metric\n",
      "dont leave off the squaring of the ground metric\n",
      "returns a uniform measure of cardinality:  64\n",
      "returns a uniform measure of cardinality:  64\n",
      "the transport map is  tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')\n",
      "Ratio of trace to the matrix sum:  tensor(0.0469, device='cuda:0')\n",
      "Here, trace is 3.0 and matrix sum is 64.0 \n",
      "this is past correction for weight mode\n",
      "Shape of aligned wt is  torch.Size([64, 3, 9])\n",
      "Shape of fc_layer0_weight_data is  torch.Size([64, 3, 9])\n",
      "Previous layer shape is  torch.Size([64, 3, 3, 3])\n",
      "shape of layer: model 0 torch.Size([128, 64, 9])\n",
      "shape of layer: model 1 torch.Size([128, 64, 9])\n",
      "shape of previous transport map torch.Size([64, 64])\n",
      "Processing the coordinates to form ground_metric\n",
      "dont leave off the squaring of the ground metric\n",
      "returns a uniform measure of cardinality:  128\n",
      "returns a uniform measure of cardinality:  128\n",
      "the transport map is  tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        ...,\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0078],\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],\n",
      "       device='cuda:0')\n",
      "Ratio of trace to the matrix sum:  tensor(0.0078, device='cuda:0')\n",
      "Here, trace is 1.0 and matrix sum is 128.0 \n",
      "this is past correction for weight mode\n",
      "Shape of aligned wt is  torch.Size([128, 64, 9])\n",
      "Shape of fc_layer0_weight_data is  torch.Size([128, 64, 9])\n",
      "Previous layer shape is  torch.Size([128, 64, 3, 3])\n",
      "shape of layer: model 0 torch.Size([256, 128, 9])\n",
      "shape of layer: model 1 torch.Size([256, 128, 9])\n",
      "shape of previous transport map torch.Size([128, 128])\n",
      "Processing the coordinates to form ground_metric\n",
      "dont leave off the squaring of the ground metric\n",
      "returns a uniform measure of cardinality:  256\n",
      "returns a uniform measure of cardinality:  256\n",
      "the transport map is  tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')\n",
      "Ratio of trace to the matrix sum:  tensor(0., device='cuda:0')\n",
      "Here, trace is 0.0 and matrix sum is 256.0 \n",
      "this is past correction for weight mode\n",
      "Shape of aligned wt is  torch.Size([256, 128, 9])\n",
      "Shape of fc_layer0_weight_data is  torch.Size([256, 128, 9])\n",
      "Previous layer shape is  torch.Size([256, 128, 3, 3])\n",
      "shape of layer: model 0 torch.Size([256, 256, 9])\n",
      "shape of layer: model 1 torch.Size([256, 256, 9])\n",
      "shape of previous transport map torch.Size([256, 256])\n",
      "Processing the coordinates to form ground_metric\n",
      "dont leave off the squaring of the ground metric\n",
      "returns a uniform measure of cardinality:  256\n",
      "returns a uniform measure of cardinality:  256\n",
      "the transport map is  tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        ...,\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.0039, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],\n",
      "       device='cuda:0')\n",
      "Ratio of trace to the matrix sum:  tensor(0.0078, device='cuda:0')\n",
      "Here, trace is 2.0 and matrix sum is 256.0 \n",
      "this is past correction for weight mode\n",
      "Shape of aligned wt is  torch.Size([256, 256, 9])\n",
      "Shape of fc_layer0_weight_data is  torch.Size([256, 256, 9])\n",
      "Previous layer shape is  torch.Size([256, 256, 3, 3])\n",
      "shape of layer: model 0 torch.Size([512, 256, 9])\n",
      "shape of layer: model 1 torch.Size([512, 256, 9])\n",
      "shape of previous transport map torch.Size([256, 256])\n",
      "Processing the coordinates to form ground_metric\n",
      "dont leave off the squaring of the ground metric\n",
      "returns a uniform measure of cardinality:  512\n",
      "returns a uniform measure of cardinality:  512\n",
      "the transport map is  tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')\n",
      "Ratio of trace to the matrix sum:  tensor(0.0039, device='cuda:0')\n",
      "Here, trace is 2.0 and matrix sum is 512.0 \n",
      "this is past correction for weight mode\n",
      "Shape of aligned wt is  torch.Size([512, 256, 9])\n",
      "Shape of fc_layer0_weight_data is  torch.Size([512, 256, 9])\n",
      "Previous layer shape is  torch.Size([512, 256, 3, 3])\n",
      "shape of layer: model 0 torch.Size([512, 512, 9])\n",
      "shape of layer: model 1 torch.Size([512, 512, 9])\n",
      "shape of previous transport map torch.Size([512, 512])\n",
      "Processing the coordinates to form ground_metric\n",
      "dont leave off the squaring of the ground metric\n",
      "returns a uniform measure of cardinality:  512\n",
      "returns a uniform measure of cardinality:  512\n",
      "the transport map is  tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')\n",
      "Ratio of trace to the matrix sum:  tensor(0.0020, device='cuda:0')\n",
      "Here, trace is 1.0 and matrix sum is 512.0 \n",
      "this is past correction for weight mode\n",
      "Shape of aligned wt is  torch.Size([512, 512, 9])\n",
      "Shape of fc_layer0_weight_data is  torch.Size([512, 512, 9])\n",
      "Previous layer shape is  torch.Size([512, 512, 3, 3])\n",
      "shape of layer: model 0 torch.Size([512, 512, 9])\n",
      "shape of layer: model 1 torch.Size([512, 512, 9])\n",
      "shape of previous transport map torch.Size([512, 512])\n",
      "Processing the coordinates to form ground_metric\n",
      "dont leave off the squaring of the ground metric\n",
      "returns a uniform measure of cardinality:  512\n",
      "returns a uniform measure of cardinality:  512\n",
      "the transport map is  tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')\n",
      "Ratio of trace to the matrix sum:  tensor(0.0020, device='cuda:0')\n",
      "Here, trace is 1.0 and matrix sum is 512.0 \n",
      "this is past correction for weight mode\n",
      "Shape of aligned wt is  torch.Size([512, 512, 9])\n",
      "Shape of fc_layer0_weight_data is  torch.Size([512, 512, 9])\n",
      "Previous layer shape is  torch.Size([512, 512, 3, 3])\n",
      "shape of layer: model 0 torch.Size([512, 512, 9])\n",
      "shape of layer: model 1 torch.Size([512, 512, 9])\n",
      "shape of previous transport map torch.Size([512, 512])\n",
      "Processing the coordinates to form ground_metric\n",
      "dont leave off the squaring of the ground metric\n",
      "returns a uniform measure of cardinality:  512\n",
      "returns a uniform measure of cardinality:  512\n",
      "the transport map is  tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        [0.0020, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        ...,\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],\n",
      "       device='cuda:0')\n",
      "Ratio of trace to the matrix sum:  tensor(0., device='cuda:0')\n",
      "Here, trace is 0.0 and matrix sum is 512.0 \n",
      "this is past correction for weight mode\n",
      "Shape of aligned wt is  torch.Size([512, 512, 9])\n",
      "Shape of fc_layer0_weight_data is  torch.Size([512, 512, 9])\n",
      "Previous layer shape is  torch.Size([512, 512, 3, 3])\n",
      "shape of layer: model 0 torch.Size([10, 512])\n",
      "shape of layer: model 1 torch.Size([10, 512])\n",
      "shape of previous transport map torch.Size([512, 512])\n",
      "Processing the coordinates to form ground_metric\n",
      "dont leave off the squaring of the ground metric\n",
      "ground metric is  tensor([[1.4241, 2.4872, 2.7804, 2.8719, 2.7374, 2.8799, 2.7276, 2.6342, 2.5641,\n",
      "         2.5922],\n",
      "        [2.4889, 1.3253, 2.6732, 2.5914, 2.7584, 2.5501, 2.4174, 2.5215, 2.3048,\n",
      "         2.4101],\n",
      "        [2.7600, 2.6711, 1.3832, 2.9465, 2.9065, 2.8701, 2.7276, 2.8385, 2.7485,\n",
      "         2.7866],\n",
      "        [2.8352, 2.6791, 2.8919, 1.5430, 2.8605, 2.9159, 2.7574, 2.7980, 2.8164,\n",
      "         2.6354],\n",
      "        [2.7551, 2.5960, 2.8635, 2.8450, 1.5042, 2.7447, 2.7853, 2.6811, 2.7031,\n",
      "         2.7294],\n",
      "        [2.9190, 2.5396, 2.9469, 2.8416, 2.7272, 1.5502, 2.7781, 2.7303, 2.7977,\n",
      "         2.6697],\n",
      "        [2.6778, 2.4457, 2.7190, 2.9111, 2.6488, 2.8526, 1.3931, 2.6890, 2.4981,\n",
      "         2.5560],\n",
      "        [2.7394, 2.5496, 2.7799, 2.7903, 2.7151, 2.6590, 2.7190, 1.3924, 2.5583,\n",
      "         2.5121],\n",
      "        [2.5972, 2.3660, 2.7693, 2.6872, 2.7026, 2.8280, 2.5325, 2.6366, 1.2999,\n",
      "         2.4041],\n",
      "        [2.5918, 2.2730, 2.7816, 2.7576, 2.6697, 2.7273, 2.5482, 2.5290, 2.5207,\n",
      "         1.2878]], device='cuda:0', grad_fn=<PowBackward0>)\n",
      "returns a uniform measure of cardinality:  10\n",
      "returns a uniform measure of cardinality:  10\n",
      "the transport map is  tensor([[0.1000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0000, 0.1000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0000, 0.0000, 0.1000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.1000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.1000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.1000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.1000, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.1000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.1000,\n",
      "         0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.1000]], device='cuda:0')\n",
      "Ratio of trace to the matrix sum:  tensor(1., device='cuda:0')\n",
      "Here, trace is 9.99755859375 and matrix sum is 9.99755859375 \n",
      "this is past correction for weight mode\n",
      "Shape of aligned wt is  torch.Size([10, 512])\n",
      "Shape of fc_layer0_weight_data is  torch.Size([10, 512])\n",
      "using independent method\n",
      "in _make_layers [Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), AvgPool2d(kernel_size=1, stride=1, padding=0)]\n",
      "Relu Inplace is  False\n",
      "model parameters are \n",
      " [torch.Size([64, 3, 3, 3]), torch.Size([128, 64, 3, 3]), torch.Size([256, 128, 3, 3]), torch.Size([256, 256, 3, 3]), torch.Size([512, 256, 3, 3]), torch.Size([512, 512, 3, 3]), torch.Size([512, 512, 3, 3]), torch.Size([512, 512, 3, 3]), torch.Size([10, 512])]\n",
      "\n",
      "--------- Testing in global mode ---------\n",
      "size of test_loader dataset:  10000\n",
      "\n",
      "Test set: Avg. loss: 0.0023, Accuracy: 1019/10000 (10%)\n",
      "\n",
      "len of model parameters and avg aligned layers is  9 9\n",
      "len of model_state_dict is  9\n",
      "len of param_list is  9\n",
      "\n",
      "--------- Testing in global mode ---------\n",
      "size of test_loader dataset:  10000\n",
      "\n",
      "Test set: Avg. loss: 0.0015, Accuracy: 8545/10000 (85%)\n",
      "\n",
      "===============================================\n",
      "===============================================\n",
      "After Ensemble: 85.45\n",
      "===============================================\n",
      "===============================================\n",
      "Timer ends\n",
      "Time taken for geometric ensembling is 3.2870662473142147 seconds\n",
      "------- Prediction based ensembling -------\n",
      "\n",
      "Test set: Avg. loss: 0.0003, Accuracy: 9148/10000 (91%)\n",
      "\n",
      "------- Naive ensembling of weights -------\n",
      "[torch.Size([64, 3, 3, 3]), torch.Size([64, 3, 3, 3])]\n",
      "torch.Size([64, 3, 3, 3])\n",
      "[torch.Size([128, 64, 3, 3]), torch.Size([128, 64, 3, 3])]\n",
      "torch.Size([128, 64, 3, 3])\n",
      "[torch.Size([256, 128, 3, 3]), torch.Size([256, 128, 3, 3])]\n",
      "torch.Size([256, 128, 3, 3])\n",
      "[torch.Size([256, 256, 3, 3]), torch.Size([256, 256, 3, 3])]\n",
      "torch.Size([256, 256, 3, 3])\n",
      "[torch.Size([512, 256, 3, 3]), torch.Size([512, 256, 3, 3])]\n",
      "torch.Size([512, 256, 3, 3])\n",
      "[torch.Size([512, 512, 3, 3]), torch.Size([512, 512, 3, 3])]\n",
      "torch.Size([512, 512, 3, 3])\n",
      "[torch.Size([512, 512, 3, 3]), torch.Size([512, 512, 3, 3])]\n",
      "torch.Size([512, 512, 3, 3])\n",
      "[torch.Size([512, 512, 3, 3]), torch.Size([512, 512, 3, 3])]\n",
      "torch.Size([512, 512, 3, 3])\n",
      "[torch.Size([10, 512]), torch.Size([10, 512])]\n",
      "torch.Size([10, 512])\n",
      "in _make_layers [Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), AvgPool2d(kernel_size=1, stride=1, padding=0)]\n",
      "Relu Inplace is  False\n",
      "model parameters are \n",
      " [torch.Size([64, 3, 3, 3]), torch.Size([128, 64, 3, 3]), torch.Size([256, 128, 3, 3]), torch.Size([256, 256, 3, 3]), torch.Size([512, 256, 3, 3]), torch.Size([512, 512, 3, 3]), torch.Size([512, 512, 3, 3]), torch.Size([512, 512, 3, 3]), torch.Size([10, 512])]\n",
      "\n",
      "--------- Testing in global mode ---------\n",
      "size of test_loader dataset:  10000\n",
      "\n",
      "Test set: Avg. loss: 0.0023, Accuracy: 1143/10000 (11%)\n",
      "\n",
      "\n",
      "--------- Testing in global mode ---------\n",
      "size of test_loader dataset:  10000\n",
      "\n",
      "Test set: Avg. loss: 0.0023, Accuracy: 1689/10000 (17%)\n",
      "\n",
      "-------- Retraining the models ---------\n",
      "Retraining model :  model_0\n",
      "num_epochs---------------+++))))))+++ 300\n",
      "num_epochs---------------+++))))))+++ 300\n",
      "lr is  0.05\n",
      "number of epochs would be  30\n",
      "num_epochs-------------- 30\n",
      "Epoch 000\n",
      "/root/miniconda3/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:129: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\"Detected call of `lr_scheduler.step()` before `optimizer.step()`. \"\n",
      "/root/miniconda3/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:154: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "accuracy: {'epoch': 0, 'value': 0.49919999999999987} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 0, 'value': 1.41390775835991} ({'split': 'train'})\n",
      "accuracy: {'epoch': 0, 'value': 0.6688000321388244} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 0, 'value': 0.9856472849845885} ({'split': 'test'})\n",
      "Epoch 001\n",
      "accuracy: {'epoch': 1, 'value': 0.7408000000000005} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 1, 'value': 0.7864720391082766} ({'split': 'train'})\n",
      "accuracy: {'epoch': 1, 'value': 0.7529000401496887} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 1, 'value': 0.758338338136673} ({'split': 'test'})\n",
      "Epoch 002\n",
      "accuracy: {'epoch': 2, 'value': 0.7893599999999995} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 2, 'value': 0.6390110291671761} ({'split': 'train'})\n",
      "accuracy: {'epoch': 2, 'value': 0.7684000313282012} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 2, 'value': 0.7656171202659607} ({'split': 'test'})\n",
      "Epoch 003\n",
      "accuracy: {'epoch': 3, 'value': 0.8208800000000008} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 3, 'value': 0.5455826361083989} ({'split': 'train'})\n",
      "accuracy: {'epoch': 3, 'value': 0.7685000360012054} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 3, 'value': 0.7341591715812683} ({'split': 'test'})\n",
      "Epoch 004\n",
      "accuracy: {'epoch': 4, 'value': 0.8330600000000012} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 4, 'value': 0.5049560075283045} ({'split': 'train'})\n",
      "accuracy: {'epoch': 4, 'value': 0.7599000334739685} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 4, 'value': 0.8119345843791962} ({'split': 'test'})\n",
      "Epoch 005\n",
      "accuracy: {'epoch': 5, 'value': 0.8414999999999994} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 5, 'value': 0.4784306293964389} ({'split': 'train'})\n",
      "accuracy: {'epoch': 5, 'value': 0.7962000370025635} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 5, 'value': 0.6198970317840575} ({'split': 'test'})\n",
      "Epoch 006\n",
      "accuracy: {'epoch': 6, 'value': 0.8530599999999995} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 6, 'value': 0.44407017860412623} ({'split': 'train'})\n",
      "accuracy: {'epoch': 6, 'value': 0.7887000322341917} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 6, 'value': 0.6731993198394776} ({'split': 'test'})\n",
      "Epoch 007\n",
      "^C\n",
      "Traceback (most recent call last):\n",
      "  File \"main.py\", line 349, in <module>\n",
      "    _, best_retrain_acc = routines.retrain_models(args, [*original_models, geometric_model, naive_model], retrain_loader, test_loader, config, tensorboard_obj=tensorboard_obj, initial_acc=initial_acc, nicks=nicks)\n",
      "  File \"/root/autodl-tmp/OTfusion_pruning/otfusion/routines.py\", line 392, in retrain_models\n",
      "    retrained_network, acc = cifar_train.get_retrained_model(args, retrain_loader, test_loader, old_networks[i], config, output_root_dir, tensorboard_obj=tensorboard_obj, nick=nick, start_acc=initial_acc[i])\n",
      "  File \"/root/autodl-tmp/OTfusion_pruning/otfusion/./cifar/train.py\", line 333, in get_retrained_model\n",
      "    best_acc,model = main(config, output_dir, args.gpu_id, pretrained_model=old_network, pretrained_dataset=(train_loader, test_loader), tensorboard_obj=tensorboard_obj,return_model=True)\n",
      "  File \"/root/autodl-tmp/OTfusion_pruning/otfusion/./cifar/train.py\", line 67, in main\n",
      "    for batch_x, batch_y in training_loader:\n",
      "  File \"/root/miniconda3/lib/python3.8/site-packages/torch/utils/data/dataloader.py\", line 521, in __next__\n",
      "    data = self._next_data()\n",
      "  File \"/root/miniconda3/lib/python3.8/site-packages/torch/utils/data/dataloader.py\", line 561, in _next_data\n",
      "    data = self._dataset_fetcher.fetch(index)  # may raise StopIteration\n",
      "  File \"/root/miniconda3/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py\", line 49, in fetch\n",
      "    data = [self.dataset[idx] for idx in possibly_batched_index]\n",
      "  File \"/root/miniconda3/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py\", line 49, in <listcomp>\n",
      "    data = [self.dataset[idx] for idx in possibly_batched_index]\n",
      "  File \"/root/miniconda3/lib/python3.8/site-packages/torchvision/datasets/cifar.py\", line 121, in __getitem__\n",
      "    img = self.transform(img)\n",
      "  File \"/root/miniconda3/lib/python3.8/site-packages/torchvision/transforms/transforms.py\", line 61, in __call__\n",
      "    img = t(img)\n",
      "  File \"/root/miniconda3/lib/python3.8/site-packages/torchvision/transforms/transforms.py\", line 98, in __call__\n",
      "    return F.to_tensor(pic)\n",
      "  File \"/root/miniconda3/lib/python3.8/site-packages/torchvision/transforms/functional.py\", line 146, in to_tensor\n",
      "    img = img.view(pic.size[1], pic.size[0], len(pic.getbands()))\n",
      "KeyboardInterrupt\n"
     ]
    }
   ],
   "source": [
    "!python main.py --gpu-id 0 --model-name vgg11_nobias --n-epochs 90 --save-result-file sample.csv \\\n",
    "--sweep-name exp_sample --correction --ground-metric euclidean --weight-stats \\\n",
    "--geom-ensemble-type wts --ground-metric-normalize none --sweep-id 90 \\\n",
    "--ckpt-type best --dataset Cifar10 --ground-metric-eff --recheck-cifar --activation-seed {activation_seed} \\\n",
    "--prelu-acts --past-correction --not-squared --exact\\\n",
    "--learning-rate 0.08\\\n",
    "--momentum 0.5\\\n",
    "--batch-size-train 64 --experiment-dir {experiment_dir}\\\n",
    "--prunint-rate {prunint_rate}\\\n",
    "--run-mode {run_mode}\\\n",
    "--to-download\\\n",
    "--load-model-dir {load_model_dir}\\\n",
    "--retrain 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "19ce2567-adad-4f09-95a3-4449ed54d54d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#剪枝实验2:fine prune without finetune\n",
    "activation_seed=21\n",
    "experiment_dir=\"experiment2\"\n",
    "prunint_rate=0.2\n",
    "run_mode='finetune'\n",
    "load_model_dir='./cifar_models'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d983a6fc-e847-4657-aef4-0dc04f6d6138",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python main.py --gpu-id 0 --model-name vgg11_nobias --n-epochs 10 --save-result-file sample.csv \\\n",
    "--sweep-name exp_sample --correction --ground-metric euclidean --weight-stats \\\n",
    "--geom-ensemble-type wts --ground-metric-normalize none --sweep-id 90 \\\n",
    "--ckpt-type best --dataset Cifar10 --ground-metric-eff --recheck-cifar --activation-seed {activation_seed} \\\n",
    "--prelu-acts --past-correction --not-squared --exact\\\n",
    "--learning-rate 0.0001\\\n",
    "--momentum 0.5\\\n",
    "--batch-size-train 256 --experiment-dir {experiment_dir}\\\n",
    "--prunint-rate {prunint_rate}\\\n",
    "--run-mode {run_mode}\\\n",
    "--to-download\\\n",
    "--load-model-dir {load_model_dir}\\\n",
    "--retrain 30\n",
    "# After Ensemble: 86.93"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c577ae36-7f34-416e-b893-957ade768633",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------- Setting up parameters -------\n",
      "dumping parameters at  /root/autodl-tmp/OTfusion_pruning/otfusion/exp_sample/configurations\n",
      "The parameters are: \n",
      " Namespace(act_bug=False, act_num_samples=100, activation_histograms=False, activation_mode=None, activation_seed=21, alpha=0.7, baseroot='/root/autodl-tmp/OTfusion_pruning/otfusion', batch_size_test=1000, batch_size_train=256, center_acts=False, choice='0 2 4 6 8', cifar_style_data=False, ckpt_type='best', clip_gm=False, clip_max=5, clip_min=0, config_dir='/root/autodl-tmp/OTfusion_pruning/otfusion/exp_sample/configurations', config_file=None, correction=True, csv_dir='/root/autodl-tmp/OTfusion_pruning/otfusion/exp_sample/csv', dataset='Cifar10', debug=False, deprecated=None, deterministic=False, diff_init=False, disable_bias=True, dist_epochs=60, dist_normalize=False, dump_final_models=False, dump_model=False, enable_dropout=False, ensemble_step=0.5, eval_aligned=False, exact=True, exp_name='exp_2023-09-17_11-47-12_968015', experiment_dir='experiment2', geom_ensemble_type='wts', gpu_id=0, gromov=False, gromov_loss='square_loss', ground_metric='euclidean', ground_metric_eff=True, ground_metric_normalize='none', handle_skips=False, importance=None, learning_rate=0.0001, load_model_dir='./cifar_models', load_models='', log_interval=100, model_name='vgg11_nobias', momentum=0.5, n_epochs=10, no_random_trainloaders=False, normalize_acts=False, normalize_wts=False, not_squared=True, num_hidden_nodes=400, num_hidden_nodes1=400, num_hidden_nodes2=200, num_hidden_nodes3=100, num_hidden_nodes4=50, num_models=2, options_type='generic', partial_reshape=False, partition_dataloader=-1, partition_type='labels', past_correction=True, personal_class_idx=9, personal_split_frac=0.1, pool_acts=False, pool_relu=False, prediction_wts=False, prelu_acts=True, print_distances=False, proper_marginals=False, prunint_rate=0.2, recheck_acc=False, recheck_cifar=True, reg=0.01, reg_m=0.001, reinit_trainloaders=False, result_dir='/root/autodl-tmp/OTfusion_pruning/otfusion/exp_sample/results', retrain=30, retrain_avg_only=False, retrain_geometric_only=False, retrain_lr_decay=-1, retrain_lr_decay_epochs=None, retrain_lr_decay_factor=None, retrain_seed=-1, rootdir='/root/autodl-tmp/OTfusion_pruning/otfusion/exp_sample', run_mode='finetune', same_model=-1, save_result_file='sample.csv', second_model_name=None, sinkhorn_type='normal', skip_last_layer=False, skip_last_layer_type='average', skip_personal_idx=False, skip_retrain=-1, softmax_temperature=1, standardize_acts=False, sweep_id=90, sweep_name='exp_sample', temperature=20, tensorboard=False, tensorboard_root='./tensorboard', timestamp='2023-09-17_11-47-12_968015', tmap_stats=False, to_download=True, transform_acts=False, unbalanced=False, update_acts=False, weight_stats=True, width_ratio=1)\n",
      "refactored get_config\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "in _make_layers [Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), AvgPool2d(kernel_size=1, stride=1, padding=0)]\n",
      "Relu Inplace is  False\n",
      "model parameters are \n",
      " [torch.Size([64, 3, 3, 3]), torch.Size([128, 64, 3, 3]), torch.Size([256, 128, 3, 3]), torch.Size([256, 256, 3, 3]), torch.Size([512, 256, 3, 3]), torch.Size([512, 512, 3, 3]), torch.Size([512, 512, 3, 3]), torch.Size([512, 512, 3, 3]), torch.Size([10, 512])]\n",
      "in _make_layers [Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), AvgPool2d(kernel_size=1, stride=1, padding=0)]\n",
      "Relu Inplace is  False\n",
      "model parameters are \n",
      " [torch.Size([64, 3, 3, 3]), torch.Size([128, 64, 3, 3]), torch.Size([256, 128, 3, 3]), torch.Size([256, 256, 3, 3]), torch.Size([512, 256, 3, 3]), torch.Size([512, 512, 3, 3]), torch.Size([512, 512, 3, 3]), torch.Size([512, 512, 3, 3]), torch.Size([10, 512])]\n",
      "in _make_layers [Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), AvgPool2d(kernel_size=1, stride=1, padding=0)]\n",
      "Relu Inplace is  False\n",
      "model parameters are \n",
      " [torch.Size([64, 3, 3, 3]), torch.Size([128, 64, 3, 3]), torch.Size([256, 128, 3, 3]), torch.Size([256, 256, 3, 3]), torch.Size([512, 256, 3, 3]), torch.Size([512, 512, 3, 3]), torch.Size([512, 512, 3, 3]), torch.Size([512, 512, 3, 3]), torch.Size([10, 512])]\n",
      "in _make_layers [Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), AvgPool2d(kernel_size=1, stride=1, padding=0)]\n",
      "Relu Inplace is  False\n",
      "model parameters are \n",
      " [torch.Size([64, 3, 3, 3]), torch.Size([128, 64, 3, 3]), torch.Size([256, 128, 3, 3]), torch.Size([256, 256, 3, 3]), torch.Size([512, 256, 3, 3]), torch.Size([512, 512, 3, 3]), torch.Size([512, 512, 3, 3]), torch.Size([512, 512, 3, 3]), torch.Size([10, 512])]\n",
      "Train Epoch: 1 [0/50000 (0%)]\tLoss: 0.104310\n",
      "Train Epoch: 1 [25600/50000 (51%)]\tLoss: 0.096724\n",
      "\n",
      "--------- Testing in global mode ---------\n",
      "size of test_loader dataset:  10000\n",
      "\n",
      "Test set: Avg. loss: 0.0003, Accuracy: 9034/10000 (90%)\n",
      "\n",
      "Train Epoch: 2 [0/50000 (0%)]\tLoss: 0.074068\n",
      "Train Epoch: 2 [25600/50000 (51%)]\tLoss: 0.090521\n",
      "\n",
      "--------- Testing in global mode ---------\n",
      "size of test_loader dataset:  10000\n",
      "\n",
      "Test set: Avg. loss: 0.0003, Accuracy: 9046/10000 (90%)\n",
      "\n",
      "Train Epoch: 3 [0/50000 (0%)]\tLoss: 0.064977\n",
      "Train Epoch: 3 [25600/50000 (51%)]\tLoss: 0.059664\n",
      "\n",
      "--------- Testing in global mode ---------\n",
      "size of test_loader dataset:  10000\n",
      "\n",
      "Test set: Avg. loss: 0.0003, Accuracy: 9050/10000 (90%)\n",
      "\n",
      "Train Epoch: 4 [0/50000 (0%)]\tLoss: 0.071340\n",
      "Train Epoch: 4 [25600/50000 (51%)]\tLoss: 0.056232\n",
      "\n",
      "--------- Testing in global mode ---------\n",
      "size of test_loader dataset:  10000\n",
      "\n",
      "Test set: Avg. loss: 0.0003, Accuracy: 9046/10000 (90%)\n",
      "\n",
      "Train Epoch: 5 [0/50000 (0%)]\tLoss: 0.069877\n",
      "Train Epoch: 5 [25600/50000 (51%)]\tLoss: 0.069029\n",
      "\n",
      "--------- Testing in global mode ---------\n",
      "size of test_loader dataset:  10000\n",
      "\n",
      "Test set: Avg. loss: 0.0003, Accuracy: 9039/10000 (90%)\n",
      "\n",
      "Train Epoch: 6 [0/50000 (0%)]\tLoss: 0.064794\n",
      "Train Epoch: 6 [25600/50000 (51%)]\tLoss: 0.064495\n",
      "\n",
      "--------- Testing in global mode ---------\n",
      "size of test_loader dataset:  10000\n",
      "\n",
      "Test set: Avg. loss: 0.0003, Accuracy: 9040/10000 (90%)\n",
      "\n",
      "Train Epoch: 7 [0/50000 (0%)]\tLoss: 0.070269\n",
      "Train Epoch: 7 [25600/50000 (51%)]\tLoss: 0.074729\n",
      "\n",
      "--------- Testing in global mode ---------\n",
      "size of test_loader dataset:  10000\n",
      "\n",
      "Test set: Avg. loss: 0.0003, Accuracy: 9045/10000 (90%)\n",
      "\n",
      "Train Epoch: 8 [0/50000 (0%)]\tLoss: 0.056572\n",
      "Train Epoch: 8 [25600/50000 (51%)]\tLoss: 0.074367\n",
      "\n",
      "--------- Testing in global mode ---------\n",
      "size of test_loader dataset:  10000\n",
      "\n",
      "Test set: Avg. loss: 0.0003, Accuracy: 9047/10000 (90%)\n",
      "\n",
      "Train Epoch: 9 [0/50000 (0%)]\tLoss: 0.059067\n",
      "Train Epoch: 9 [25600/50000 (51%)]\tLoss: 0.066785\n",
      "\n",
      "--------- Testing in global mode ---------\n",
      "size of test_loader dataset:  10000\n",
      "\n",
      "Test set: Avg. loss: 0.0003, Accuracy: 9047/10000 (90%)\n",
      "\n",
      "Train Epoch: 10 [0/50000 (0%)]\tLoss: 0.063044\n",
      "Train Epoch: 10 [25600/50000 (51%)]\tLoss: 0.063622\n",
      "\n",
      "--------- Testing in global mode ---------\n",
      "size of test_loader dataset:  10000\n",
      "\n",
      "Test set: Avg. loss: 0.0003, Accuracy: 9045/10000 (90%)\n",
      "\n",
      "Train Epoch: 1 [0/50000 (0%)]\tLoss: 0.097949\n",
      "Train Epoch: 1 [25600/50000 (51%)]\tLoss: 0.080470\n",
      "\n",
      "--------- Testing in global mode ---------\n",
      "size of test_loader dataset:  10000\n",
      "\n",
      "Test set: Avg. loss: 0.0003, Accuracy: 9058/10000 (91%)\n",
      "\n",
      "Train Epoch: 2 [0/50000 (0%)]\tLoss: 0.072372\n",
      "Train Epoch: 2 [25600/50000 (51%)]\tLoss: 0.098847\n",
      "\n",
      "--------- Testing in global mode ---------\n",
      "size of test_loader dataset:  10000\n",
      "\n",
      "Test set: Avg. loss: 0.0003, Accuracy: 9069/10000 (91%)\n",
      "\n",
      "Train Epoch: 3 [0/50000 (0%)]\tLoss: 0.075549\n",
      "Train Epoch: 3 [25600/50000 (51%)]\tLoss: 0.076532\n",
      "\n",
      "--------- Testing in global mode ---------\n",
      "size of test_loader dataset:  10000\n",
      "\n",
      "Test set: Avg. loss: 0.0003, Accuracy: 9065/10000 (91%)\n",
      "\n",
      "Train Epoch: 4 [0/50000 (0%)]\tLoss: 0.090253\n",
      "Train Epoch: 4 [25600/50000 (51%)]\tLoss: 0.069432\n",
      "\n",
      "--------- Testing in global mode ---------\n",
      "size of test_loader dataset:  10000\n",
      "\n",
      "Test set: Avg. loss: 0.0003, Accuracy: 9069/10000 (91%)\n",
      "\n",
      "Train Epoch: 5 [0/50000 (0%)]\tLoss: 0.066264\n",
      "Train Epoch: 5 [25600/50000 (51%)]\tLoss: 0.093912\n",
      "\n",
      "--------- Testing in global mode ---------\n",
      "size of test_loader dataset:  10000\n",
      "\n",
      "Test set: Avg. loss: 0.0003, Accuracy: 9066/10000 (91%)\n",
      "\n",
      "Train Epoch: 6 [0/50000 (0%)]\tLoss: 0.066122\n",
      "Train Epoch: 6 [25600/50000 (51%)]\tLoss: 0.063633\n",
      "\n",
      "--------- Testing in global mode ---------\n",
      "size of test_loader dataset:  10000\n",
      "\n",
      "Test set: Avg. loss: 0.0003, Accuracy: 9065/10000 (91%)\n",
      "\n",
      "Train Epoch: 7 [0/50000 (0%)]\tLoss: 0.068083\n",
      "Train Epoch: 7 [25600/50000 (51%)]\tLoss: 0.056225\n",
      "\n",
      "--------- Testing in global mode ---------\n",
      "size of test_loader dataset:  10000\n",
      "\n",
      "Test set: Avg. loss: 0.0003, Accuracy: 9067/10000 (91%)\n",
      "\n",
      "Train Epoch: 8 [0/50000 (0%)]\tLoss: 0.053332\n",
      "Train Epoch: 8 [25600/50000 (51%)]\tLoss: 0.066723\n",
      "\n",
      "--------- Testing in global mode ---------\n",
      "size of test_loader dataset:  10000\n",
      "\n",
      "Test set: Avg. loss: 0.0003, Accuracy: 9069/10000 (91%)\n",
      "\n",
      "Train Epoch: 9 [0/50000 (0%)]\tLoss: 0.061641\n",
      "Train Epoch: 9 [25600/50000 (51%)]\tLoss: 0.063002\n",
      "\n",
      "--------- Testing in global mode ---------\n",
      "size of test_loader dataset:  10000\n",
      "\n",
      "Test set: Avg. loss: 0.0003, Accuracy: 9068/10000 (91%)\n",
      "\n",
      "Train Epoch: 10 [0/50000 (0%)]\tLoss: 0.058168\n",
      "Train Epoch: 10 [25600/50000 (51%)]\tLoss: 0.056644\n",
      "\n",
      "--------- Testing in global mode ---------\n",
      "size of test_loader dataset:  10000\n",
      "\n",
      "Test set: Avg. loss: 0.0003, Accuracy: 9060/10000 (91%)\n",
      "\n",
      "check zero rate for model 0\n",
      "odict_keys(['features.0.weight', 'features.3.weight', 'features.6.weight', 'features.8.weight', 'features.11.weight', 'features.13.weight', 'features.16.weight', 'features.18.weight', 'classifier.weight'])\n",
      "the ratio of zero for features.0.weight\n",
      "tensor(0.2002, device='cuda:0')\n",
      "the ratio of zero for features.3.weight\n",
      "tensor(0.2000, device='cuda:0')\n",
      "the ratio of zero for features.6.weight\n",
      "tensor(0.2000, device='cuda:0')\n",
      "the ratio of zero for features.8.weight\n",
      "tensor(0.2000, device='cuda:0')\n",
      "the ratio of zero for features.11.weight\n",
      "tensor(0.2000, device='cuda:0')\n",
      "the ratio of zero for features.13.weight\n",
      "tensor(0.2000, device='cuda:0')\n",
      "the ratio of zero for features.16.weight\n",
      "tensor(0.2000, device='cuda:0')\n",
      "the ratio of zero for features.18.weight\n",
      "tensor(0.2000, device='cuda:0')\n",
      "the ratio of zero for classifier.weight\n",
      "tensor(0.2000, device='cuda:0')\n",
      "check zero rate for model 1\n",
      "odict_keys(['features.0.weight', 'features.3.weight', 'features.6.weight', 'features.8.weight', 'features.11.weight', 'features.13.weight', 'features.16.weight', 'features.18.weight', 'classifier.weight'])\n",
      "the ratio of zero for features.0.weight\n",
      "tensor(0.2002, device='cuda:0')\n",
      "the ratio of zero for features.3.weight\n",
      "tensor(0.2000, device='cuda:0')\n",
      "the ratio of zero for features.6.weight\n",
      "tensor(0.2000, device='cuda:0')\n",
      "the ratio of zero for features.8.weight\n",
      "tensor(0.2000, device='cuda:0')\n",
      "the ratio of zero for features.11.weight\n",
      "tensor(0.2000, device='cuda:0')\n",
      "the ratio of zero for features.13.weight\n",
      "tensor(0.2000, device='cuda:0')\n",
      "the ratio of zero for features.16.weight\n",
      "tensor(0.2000, device='cuda:0')\n",
      "the ratio of zero for features.18.weight\n",
      "tensor(0.2000, device='cuda:0')\n",
      "the ratio of zero for classifier.weight\n",
      "tensor(0.2000, device='cuda:0')\n",
      "layer features.0.weight has #params  1728\n",
      "layer features.3.weight has #params  73728\n",
      "layer features.6.weight has #params  294912\n",
      "layer features.8.weight has #params  589824\n",
      "layer features.11.weight has #params  1179648\n",
      "layer features.13.weight has #params  2359296\n",
      "layer features.16.weight has #params  2359296\n",
      "layer features.18.weight has #params  2359296\n",
      "layer classifier.weight has #params  5120\n",
      "Activation Timer start\n",
      "Activation Timer ends\n",
      "------- Geometric Ensembling -------\n",
      "Timer start\n",
      "odict_keys(['features.0.weight', 'features.3.weight', 'features.6.weight', 'features.8.weight', 'features.11.weight', 'features.13.weight', 'features.16.weight', 'features.18.weight', 'classifier.weight'])\n",
      "the ratio of zero for features.0.weight\n",
      "tensor(0.2002, device='cuda:0')\n",
      "the ratio of zero for features.3.weight\n",
      "tensor(0.2000, device='cuda:0')\n",
      "the ratio of zero for features.6.weight\n",
      "tensor(0.2000, device='cuda:0')\n",
      "the ratio of zero for features.8.weight\n",
      "tensor(0.2000, device='cuda:0')\n",
      "the ratio of zero for features.11.weight\n",
      "tensor(0.2000, device='cuda:0')\n",
      "the ratio of zero for features.13.weight\n",
      "tensor(0.2000, device='cuda:0')\n",
      "the ratio of zero for features.16.weight\n",
      "tensor(0.2000, device='cuda:0')\n",
      "the ratio of zero for features.18.weight\n",
      "tensor(0.2000, device='cuda:0')\n",
      "the ratio of zero for classifier.weight\n",
      "tensor(0.2000, device='cuda:0')\n",
      "Previous layer shape is  None\n",
      "Processing the coordinates to form ground_metric\n",
      "dont leave off the squaring of the ground metric\n",
      "returns a uniform measure of cardinality:  64\n",
      "returns a uniform measure of cardinality:  64\n",
      "the transport map is  tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')\n",
      "Ratio of trace to the matrix sum:  tensor(0.0469, device='cuda:0')\n",
      "Here, trace is 3.0 and matrix sum is 64.0 \n",
      "this is past correction for weight mode\n",
      "Shape of aligned wt is  torch.Size([64, 3, 9])\n",
      "Shape of fc_layer0_weight_data is  torch.Size([64, 3, 9])\n",
      "Previous layer shape is  torch.Size([64, 3, 3, 3])\n",
      "shape of layer: model 0 torch.Size([128, 64, 9])\n",
      "shape of layer: model 1 torch.Size([128, 64, 9])\n",
      "shape of previous transport map torch.Size([64, 64])\n",
      "Processing the coordinates to form ground_metric\n",
      "dont leave off the squaring of the ground metric\n",
      "returns a uniform measure of cardinality:  128\n",
      "returns a uniform measure of cardinality:  128\n",
      "the transport map is  tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        ...,\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0078],\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],\n",
      "       device='cuda:0')\n",
      "Ratio of trace to the matrix sum:  tensor(0.0078, device='cuda:0')\n",
      "Here, trace is 1.0 and matrix sum is 128.0 \n",
      "this is past correction for weight mode\n",
      "Shape of aligned wt is  torch.Size([128, 64, 9])\n",
      "Shape of fc_layer0_weight_data is  torch.Size([128, 64, 9])\n",
      "Previous layer shape is  torch.Size([128, 64, 3, 3])\n",
      "shape of layer: model 0 torch.Size([256, 128, 9])\n",
      "shape of layer: model 1 torch.Size([256, 128, 9])\n",
      "shape of previous transport map torch.Size([128, 128])\n",
      "Processing the coordinates to form ground_metric\n",
      "dont leave off the squaring of the ground metric\n",
      "returns a uniform measure of cardinality:  256\n",
      "returns a uniform measure of cardinality:  256\n",
      "the transport map is  tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')\n",
      "Ratio of trace to the matrix sum:  tensor(0., device='cuda:0')\n",
      "Here, trace is 0.0 and matrix sum is 256.0 \n",
      "this is past correction for weight mode\n",
      "Shape of aligned wt is  torch.Size([256, 128, 9])\n",
      "Shape of fc_layer0_weight_data is  torch.Size([256, 128, 9])\n",
      "Previous layer shape is  torch.Size([256, 128, 3, 3])\n",
      "shape of layer: model 0 torch.Size([256, 256, 9])\n",
      "shape of layer: model 1 torch.Size([256, 256, 9])\n",
      "shape of previous transport map torch.Size([256, 256])\n",
      "Processing the coordinates to form ground_metric\n",
      "dont leave off the squaring of the ground metric\n",
      "returns a uniform measure of cardinality:  256\n",
      "returns a uniform measure of cardinality:  256\n",
      "the transport map is  tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        ...,\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.0039, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],\n",
      "       device='cuda:0')\n",
      "Ratio of trace to the matrix sum:  tensor(0.0117, device='cuda:0')\n",
      "Here, trace is 3.0 and matrix sum is 256.0 \n",
      "this is past correction for weight mode\n",
      "Shape of aligned wt is  torch.Size([256, 256, 9])\n",
      "Shape of fc_layer0_weight_data is  torch.Size([256, 256, 9])\n",
      "Previous layer shape is  torch.Size([256, 256, 3, 3])\n",
      "shape of layer: model 0 torch.Size([512, 256, 9])\n",
      "shape of layer: model 1 torch.Size([512, 256, 9])\n",
      "shape of previous transport map torch.Size([256, 256])\n",
      "Processing the coordinates to form ground_metric\n",
      "dont leave off the squaring of the ground metric\n",
      "returns a uniform measure of cardinality:  512\n",
      "returns a uniform measure of cardinality:  512\n",
      "the transport map is  tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')\n",
      "Ratio of trace to the matrix sum:  tensor(0.0020, device='cuda:0')\n",
      "Here, trace is 1.0 and matrix sum is 512.0 \n",
      "this is past correction for weight mode\n",
      "Shape of aligned wt is  torch.Size([512, 256, 9])\n",
      "Shape of fc_layer0_weight_data is  torch.Size([512, 256, 9])\n",
      "Previous layer shape is  torch.Size([512, 256, 3, 3])\n",
      "shape of layer: model 0 torch.Size([512, 512, 9])\n",
      "shape of layer: model 1 torch.Size([512, 512, 9])\n",
      "shape of previous transport map torch.Size([512, 512])\n",
      "Processing the coordinates to form ground_metric\n",
      "dont leave off the squaring of the ground metric\n",
      "returns a uniform measure of cardinality:  512\n",
      "returns a uniform measure of cardinality:  512\n",
      "the transport map is  tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')\n",
      "Ratio of trace to the matrix sum:  tensor(0.0020, device='cuda:0')\n",
      "Here, trace is 1.0 and matrix sum is 512.0 \n",
      "this is past correction for weight mode\n",
      "Shape of aligned wt is  torch.Size([512, 512, 9])\n",
      "Shape of fc_layer0_weight_data is  torch.Size([512, 512, 9])\n",
      "Previous layer shape is  torch.Size([512, 512, 3, 3])\n",
      "shape of layer: model 0 torch.Size([512, 512, 9])\n",
      "shape of layer: model 1 torch.Size([512, 512, 9])\n",
      "shape of previous transport map torch.Size([512, 512])\n",
      "Processing the coordinates to form ground_metric\n",
      "dont leave off the squaring of the ground metric\n",
      "returns a uniform measure of cardinality:  512\n",
      "returns a uniform measure of cardinality:  512\n",
      "the transport map is  tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')\n",
      "Ratio of trace to the matrix sum:  tensor(0.0020, device='cuda:0')\n",
      "Here, trace is 1.0 and matrix sum is 512.0 \n",
      "this is past correction for weight mode\n",
      "Shape of aligned wt is  torch.Size([512, 512, 9])\n",
      "Shape of fc_layer0_weight_data is  torch.Size([512, 512, 9])\n",
      "Previous layer shape is  torch.Size([512, 512, 3, 3])\n",
      "shape of layer: model 0 torch.Size([512, 512, 9])\n",
      "shape of layer: model 1 torch.Size([512, 512, 9])\n",
      "shape of previous transport map torch.Size([512, 512])\n",
      "Processing the coordinates to form ground_metric\n",
      "dont leave off the squaring of the ground metric\n",
      "returns a uniform measure of cardinality:  512\n",
      "returns a uniform measure of cardinality:  512\n",
      "the transport map is  tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        [0.0020, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        ...,\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],\n",
      "       device='cuda:0')\n",
      "Ratio of trace to the matrix sum:  tensor(0., device='cuda:0')\n",
      "Here, trace is 0.0 and matrix sum is 512.0 \n",
      "this is past correction for weight mode\n",
      "Shape of aligned wt is  torch.Size([512, 512, 9])\n",
      "Shape of fc_layer0_weight_data is  torch.Size([512, 512, 9])\n",
      "Previous layer shape is  torch.Size([512, 512, 3, 3])\n",
      "shape of layer: model 0 torch.Size([10, 512])\n",
      "shape of layer: model 1 torch.Size([10, 512])\n",
      "shape of previous transport map torch.Size([512, 512])\n",
      "Processing the coordinates to form ground_metric\n",
      "dont leave off the squaring of the ground metric\n",
      "ground metric is  tensor([[1.4114, 2.4801, 2.7999, 2.8826, 2.7448, 2.8668, 2.7255, 2.6601, 2.5695,\n",
      "         2.5919],\n",
      "        [2.5105, 1.3110, 2.6937, 2.5688, 2.7590, 2.5401, 2.4221, 2.5317, 2.3108,\n",
      "         2.4173],\n",
      "        [2.7697, 2.6603, 1.3567, 2.9763, 2.9210, 2.8576, 2.7261, 2.8650, 2.7639,\n",
      "         2.7646],\n",
      "        [2.8155, 2.6631, 2.8864, 1.5393, 2.8489, 2.9660, 2.7519, 2.8051, 2.8248,\n",
      "         2.6687],\n",
      "        [2.7962, 2.5952, 2.8294, 2.8948, 1.5111, 2.7352, 2.7941, 2.6735, 2.6779,\n",
      "         2.7314],\n",
      "        [2.9293, 2.5633, 2.9782, 2.7970, 2.7281, 1.5545, 2.7586, 2.7154, 2.8096,\n",
      "         2.7038],\n",
      "        [2.6918, 2.4569, 2.7158, 2.9082, 2.6382, 2.8538, 1.4451, 2.7062, 2.4831,\n",
      "         2.5497],\n",
      "        [2.7189, 2.5685, 2.7741, 2.7862, 2.7339, 2.6891, 2.7069, 1.3632, 2.5778,\n",
      "         2.5102],\n",
      "        [2.5763, 2.3923, 2.7647, 2.7291, 2.7070, 2.8389, 2.5323, 2.6240, 1.3008,\n",
      "         2.3776],\n",
      "        [2.5919, 2.2594, 2.8131, 2.7337, 2.6723, 2.7130, 2.5628, 2.5266, 2.5263,\n",
      "         1.2929]], device='cuda:0', grad_fn=<PowBackward0>)\n",
      "returns a uniform measure of cardinality:  10\n",
      "returns a uniform measure of cardinality:  10\n",
      "the transport map is  tensor([[0.1000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0000, 0.1000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0000, 0.0000, 0.1000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.1000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.1000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.1000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.1000, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.1000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.1000,\n",
      "         0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.1000]], device='cuda:0')\n",
      "Ratio of trace to the matrix sum:  tensor(1., device='cuda:0')\n",
      "Here, trace is 9.99755859375 and matrix sum is 9.99755859375 \n",
      "this is past correction for weight mode\n",
      "Shape of aligned wt is  torch.Size([10, 512])\n",
      "Shape of fc_layer0_weight_data is  torch.Size([10, 512])\n",
      "using independent method\n",
      "in _make_layers [Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), AvgPool2d(kernel_size=1, stride=1, padding=0)]\n",
      "Relu Inplace is  False\n",
      "model parameters are \n",
      " [torch.Size([64, 3, 3, 3]), torch.Size([128, 64, 3, 3]), torch.Size([256, 128, 3, 3]), torch.Size([256, 256, 3, 3]), torch.Size([512, 256, 3, 3]), torch.Size([512, 512, 3, 3]), torch.Size([512, 512, 3, 3]), torch.Size([512, 512, 3, 3]), torch.Size([10, 512])]\n",
      "\n",
      "--------- Testing in global mode ---------\n",
      "size of test_loader dataset:  10000\n",
      "\n",
      "Test set: Avg. loss: 0.0023, Accuracy: 776/10000 (8%)\n",
      "\n",
      "len of model parameters and avg aligned layers is  9 9\n",
      "len of model_state_dict is  9\n",
      "len of param_list is  9\n",
      "\n",
      "--------- Testing in global mode ---------\n",
      "size of test_loader dataset:  10000\n",
      "\n",
      "Test set: Avg. loss: 0.0014, Accuracy: 8624/10000 (86%)\n",
      "\n",
      "===============================================\n",
      "===============================================\n",
      "After Ensemble: 86.24\n",
      "===============================================\n",
      "===============================================\n",
      "Timer ends\n",
      "Time taken for geometric ensembling is 3.329503733664751 seconds\n",
      "------- Prediction based ensembling -------\n",
      "\n",
      "Test set: Avg. loss: 0.0003, Accuracy: 9125/10000 (91%)\n",
      "\n",
      "------- Naive ensembling of weights -------\n",
      "[torch.Size([64, 3, 3, 3]), torch.Size([64, 3, 3, 3])]\n",
      "torch.Size([64, 3, 3, 3])\n",
      "[torch.Size([128, 64, 3, 3]), torch.Size([128, 64, 3, 3])]\n",
      "torch.Size([128, 64, 3, 3])\n",
      "[torch.Size([256, 128, 3, 3]), torch.Size([256, 128, 3, 3])]\n",
      "torch.Size([256, 128, 3, 3])\n",
      "[torch.Size([256, 256, 3, 3]), torch.Size([256, 256, 3, 3])]\n",
      "torch.Size([256, 256, 3, 3])\n",
      "[torch.Size([512, 256, 3, 3]), torch.Size([512, 256, 3, 3])]\n",
      "torch.Size([512, 256, 3, 3])\n",
      "[torch.Size([512, 512, 3, 3]), torch.Size([512, 512, 3, 3])]\n",
      "torch.Size([512, 512, 3, 3])\n",
      "[torch.Size([512, 512, 3, 3]), torch.Size([512, 512, 3, 3])]\n",
      "torch.Size([512, 512, 3, 3])\n",
      "[torch.Size([512, 512, 3, 3]), torch.Size([512, 512, 3, 3])]\n",
      "torch.Size([512, 512, 3, 3])\n",
      "[torch.Size([10, 512]), torch.Size([10, 512])]\n",
      "torch.Size([10, 512])\n",
      "in _make_layers [Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), AvgPool2d(kernel_size=1, stride=1, padding=0)]\n",
      "Relu Inplace is  False\n",
      "model parameters are \n",
      " [torch.Size([64, 3, 3, 3]), torch.Size([128, 64, 3, 3]), torch.Size([256, 128, 3, 3]), torch.Size([256, 256, 3, 3]), torch.Size([512, 256, 3, 3]), torch.Size([512, 512, 3, 3]), torch.Size([512, 512, 3, 3]), torch.Size([512, 512, 3, 3]), torch.Size([10, 512])]\n",
      "\n",
      "--------- Testing in global mode ---------\n",
      "size of test_loader dataset:  10000\n",
      "\n",
      "Test set: Avg. loss: 0.0023, Accuracy: 1066/10000 (11%)\n",
      "\n",
      "\n",
      "--------- Testing in global mode ---------\n",
      "size of test_loader dataset:  10000\n",
      "\n",
      "Test set: Avg. loss: 0.0023, Accuracy: 1567/10000 (16%)\n",
      "\n",
      "-------- Retraining the models ---------\n",
      "Retraining model :  model_0\n",
      "num_epochs---------------+++))))))+++ 300\n",
      "num_epochs---------------+++))))))+++ 300\n",
      "lr is  0.05\n",
      "number of epochs would be  30\n",
      "num_epochs-------------- 30\n",
      "Epoch 000\n",
      "/root/miniconda3/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:129: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\"Detected call of `lr_scheduler.step()` before `optimizer.step()`. \"\n",
      "/root/miniconda3/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:154: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "accuracy: {'epoch': 0, 'value': 0.9071399999809264} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 0, 'value': 0.27822885355949406} ({'split': 'train'})\n",
      "accuracy: {'epoch': 0, 'value': 0.8457000315189362} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 0, 'value': 0.48620404303073883} ({'split': 'test'})\n",
      "Epoch 001\n",
      "accuracy: {'epoch': 1, 'value': 0.9284200000381473} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 1, 'value': 0.21343119597434995} ({'split': 'train'})\n",
      "accuracy: {'epoch': 1, 'value': 0.8533000409603119} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 1, 'value': 0.45945611894130706} ({'split': 'test'})\n",
      "Epoch 002\n",
      "accuracy: {'epoch': 2, 'value': 0.9500199999809266} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 2, 'value': 0.14761418771266932} ({'split': 'train'})\n",
      "accuracy: {'epoch': 2, 'value': 0.849500036239624} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 2, 'value': 0.5225148260593415} ({'split': 'test'})\n",
      "Epoch 003\n",
      "accuracy: {'epoch': 3, 'value': 0.9636799999999998} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 3, 'value': 0.10544244619250301} ({'split': 'train'})\n",
      "accuracy: {'epoch': 3, 'value': 0.8461000382900238} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 3, 'value': 0.5642054289579392} ({'split': 'test'})\n",
      "Epoch 004\n",
      "accuracy: {'epoch': 4, 'value': 0.9701200000572204} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 4, 'value': 0.0916630233812332} ({'split': 'train'})\n",
      "accuracy: {'epoch': 4, 'value': 0.8535000324249267} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 4, 'value': 0.5359272003173828} ({'split': 'test'})\n",
      "Epoch 005\n",
      "accuracy: {'epoch': 5, 'value': 0.9726200000381471} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 5, 'value': 0.08404756773471828} ({'split': 'train'})\n",
      "accuracy: {'epoch': 5, 'value': 0.8572000443935395} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 5, 'value': 0.5691687196493148} ({'split': 'test'})\n",
      "Epoch 006\n",
      "accuracy: {'epoch': 6, 'value': 0.9785000000190734} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 6, 'value': 0.06788021917700761} ({'split': 'train'})\n",
      "accuracy: {'epoch': 6, 'value': 0.8574000418186187} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 6, 'value': 0.62178675532341} ({'split': 'test'})\n",
      "Epoch 007\n",
      "accuracy: {'epoch': 7, 'value': 0.98002} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 7, 'value': 0.059257652641534805} ({'split': 'train'})\n",
      "accuracy: {'epoch': 7, 'value': 0.853300040960312} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 7, 'value': 0.5803293347358702} ({'split': 'test'})\n",
      "Epoch 008\n",
      "accuracy: {'epoch': 8, 'value': 0.9869800000190739} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 8, 'value': 0.04209615451097491} ({'split': 'train'})\n",
      "accuracy: {'epoch': 8, 'value': 0.8638000428676604} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 8, 'value': 0.5519194900989532} ({'split': 'test'})\n",
      "Epoch 009\n",
      "accuracy: {'epoch': 9, 'value': 0.9847000000381473} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 9, 'value': 0.04851233090400695} ({'split': 'train'})\n",
      "accuracy: {'epoch': 9, 'value': 0.8617000460624695} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 9, 'value': 0.5663160562515259} ({'split': 'test'})\n",
      "Epoch 010\n",
      "accuracy: {'epoch': 10, 'value': 0.985780000019074} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 10, 'value': 0.044744903629422204} ({'split': 'train'})\n",
      "accuracy: {'epoch': 10, 'value': 0.8498000383377076} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 10, 'value': 0.5971804857254028} ({'split': 'test'})\n",
      "Epoch 011\n",
      "accuracy: {'epoch': 11, 'value': 0.9884000000190737} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 11, 'value': 0.03658294462621211} ({'split': 'train'})\n",
      "accuracy: {'epoch': 11, 'value': 0.8618000447750092} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 11, 'value': 0.5850017368793489} ({'split': 'test'})\n",
      "Epoch 012\n",
      "accuracy: {'epoch': 12, 'value': 0.9874999999999995} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 12, 'value': 0.039126188335418706} ({'split': 'train'})\n",
      "accuracy: {'epoch': 12, 'value': 0.8524000465869903} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 12, 'value': 0.6339874505996705} ({'split': 'test'})\n",
      "Epoch 013\n",
      "accuracy: {'epoch': 13, 'value': 0.9879000000190735} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 13, 'value': 0.03845071150898932} ({'split': 'train'})\n",
      "accuracy: {'epoch': 13, 'value': 0.8581000447273255} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 13, 'value': 0.619011515378952} ({'split': 'test'})\n",
      "Epoch 014\n",
      "accuracy: {'epoch': 14, 'value': 0.9862399999999999} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 14, 'value': 0.045439788550138485} ({'split': 'train'})\n",
      "accuracy: {'epoch': 14, 'value': 0.8587000429630279} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 14, 'value': 0.6137809991836548} ({'split': 'test'})\n",
      "Epoch 015\n",
      "accuracy: {'epoch': 15, 'value': 0.9857200000000002} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 15, 'value': 0.04521413399994372} ({'split': 'train'})\n",
      "accuracy: {'epoch': 15, 'value': 0.8626000344753266} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 15, 'value': 0.6120266616344452} ({'split': 'test'})\n",
      "Epoch 016\n",
      "accuracy: {'epoch': 16, 'value': 0.9901399999809262} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 16, 'value': 0.034077319514751424} ({'split': 'train'})\n",
      "accuracy: {'epoch': 16, 'value': 0.8587000370025635} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 16, 'value': 0.649476432800293} ({'split': 'test'})\n",
      "Epoch 017\n",
      "accuracy: {'epoch': 17, 'value': 0.9890599999999998} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 17, 'value': 0.035279727765321726} ({'split': 'train'})\n",
      "accuracy: {'epoch': 17, 'value': 0.8591000437736512} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 17, 'value': 0.5840388417243958} ({'split': 'test'})\n",
      "Epoch 018\n",
      "accuracy: {'epoch': 18, 'value': 0.9907400000190736} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 18, 'value': 0.03123576682090759} ({'split': 'train'})\n",
      "accuracy: {'epoch': 18, 'value': 0.8430000364780426} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 18, 'value': 0.5947779953479767} ({'split': 'test'})\n",
      "Epoch 019\n",
      "accuracy: {'epoch': 19, 'value': 0.9874400000000001} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 19, 'value': 0.042325001920461625} ({'split': 'train'})\n",
      "accuracy: {'epoch': 19, 'value': 0.8626000404357909} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 19, 'value': 0.5554082930088043} ({'split': 'test'})\n",
      "Epoch 020\n",
      "accuracy: {'epoch': 20, 'value': 0.9903000000000002} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 20, 'value': 0.030627320730388166} ({'split': 'train'})\n",
      "accuracy: {'epoch': 20, 'value': 0.8582000374794007} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 20, 'value': 0.5824505507946014} ({'split': 'test'})\n",
      "Epoch 021\n",
      "accuracy: {'epoch': 21, 'value': 0.9866800000190735} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 21, 'value': 0.0419280099105835} ({'split': 'train'})\n",
      "accuracy: {'epoch': 21, 'value': 0.8568000376224518} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 21, 'value': 0.6126148998737335} ({'split': 'test'})\n",
      "Epoch 022\n",
      "accuracy: {'epoch': 22, 'value': 0.9893599999999996} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 22, 'value': 0.0353905863532424} ({'split': 'train'})\n",
      "accuracy: {'epoch': 22, 'value': 0.8606000363826752} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 22, 'value': 0.5599132299423217} ({'split': 'test'})\n",
      "Epoch 023\n",
      "accuracy: {'epoch': 23, 'value': 0.9898000000381472} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 23, 'value': 0.031108805888891217} ({'split': 'train'})\n",
      "accuracy: {'epoch': 23, 'value': 0.8395000338554381} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 23, 'value': 0.6084883630275727} ({'split': 'test'})\n",
      "Epoch 024\n",
      "accuracy: {'epoch': 24, 'value': 0.9862000000000001} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 24, 'value': 0.04478724157065155} ({'split': 'train'})\n",
      "accuracy: {'epoch': 24, 'value': 0.8460000395774842} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 24, 'value': 0.6674679934978485} ({'split': 'test'})\n",
      "Epoch 025\n",
      "accuracy: {'epoch': 25, 'value': 0.9852800000381468} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 25, 'value': 0.04670753823280332} ({'split': 'train'})\n",
      "accuracy: {'epoch': 25, 'value': 0.8512000381946564} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 25, 'value': 0.6297615230083465} ({'split': 'test'})\n",
      "Epoch 026\n",
      "accuracy: {'epoch': 26, 'value': 0.9845600000381463} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 26, 'value': 0.048151583452224755} ({'split': 'train'})\n",
      "accuracy: {'epoch': 26, 'value': 0.8524000346660614} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 26, 'value': 0.5718307316303254} ({'split': 'test'})\n",
      "Epoch 027\n",
      "accuracy: {'epoch': 27, 'value': 0.9839400000190731} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 27, 'value': 0.050780314708948164} ({'split': 'train'})\n",
      "accuracy: {'epoch': 27, 'value': 0.8483000457286834} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 27, 'value': 0.6263522744178772} ({'split': 'test'})\n",
      "Epoch 028\n",
      "accuracy: {'epoch': 28, 'value': 0.98632} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 28, 'value': 0.04258109762609003} ({'split': 'train'})\n",
      "accuracy: {'epoch': 28, 'value': 0.8431000351905823} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 28, 'value': 0.6469143152236938} ({'split': 'test'})\n",
      "Epoch 029\n",
      "accuracy: {'epoch': 29, 'value': 0.9889600000190737} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 29, 'value': 0.03444906195521354} ({'split': 'train'})\n",
      "accuracy: {'epoch': 29, 'value': 0.8575000405311585} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 29, 'value': 0.6099827826023102} ({'split': 'test'})\n",
      "mean_test_accuracy <cifar_utils.accumulators.Mean object at 0x7f3519476af0>\n",
      "QWERTY: Enter Cifar 2, acc 0.8638000428676604\n",
      "Retraining model :  model_1\n",
      "num_epochs---------------+++))))))+++ 30\n",
      "num_epochs---------------+++))))))+++ 30\n",
      "lr is  0.05\n",
      "number of epochs would be  30\n",
      "num_epochs-------------- 30\n",
      "Epoch 000\n",
      "accuracy: {'epoch': 0, 'value': 0.8954400000572205} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 0, 'value': 0.31292945937156674} ({'split': 'train'})\n",
      "accuracy: {'epoch': 0, 'value': 0.8360000431537628} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 0, 'value': 0.5226405054330825} ({'split': 'test'})\n",
      "Epoch 001\n",
      "accuracy: {'epoch': 1, 'value': 0.9284200000572206} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 1, 'value': 0.21239790417671198} ({'split': 'train'})\n",
      "accuracy: {'epoch': 1, 'value': 0.8466000437736511} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 1, 'value': 0.4941443860530853} ({'split': 'test'})\n",
      "Epoch 002\n",
      "accuracy: {'epoch': 2, 'value': 0.9510600000381473} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 2, 'value': 0.1480586694669724} ({'split': 'train'})\n",
      "accuracy: {'epoch': 2, 'value': 0.8394000351428986} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 2, 'value': 0.5695794314146042} ({'split': 'test'})\n",
      "Epoch 003\n",
      "accuracy: {'epoch': 3, 'value': 0.962920000038147} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 3, 'value': 0.11326372450828553} ({'split': 'train'})\n",
      "accuracy: {'epoch': 3, 'value': 0.861100035905838} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 3, 'value': 0.49253780543804165} ({'split': 'test'})\n",
      "Epoch 004\n",
      "accuracy: {'epoch': 4, 'value': 0.9685600000572202} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 4, 'value': 0.0909472603559494} ({'split': 'train'})\n",
      "accuracy: {'epoch': 4, 'value': 0.8503000378608704} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 4, 'value': 0.5680630236864089} ({'split': 'test'})\n",
      "Epoch 005\n",
      "accuracy: {'epoch': 5, 'value': 0.9751400000572208} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 5, 'value': 0.07480831011295327} ({'split': 'train'})\n",
      "accuracy: {'epoch': 5, 'value': 0.8543000400066375} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 5, 'value': 0.6043914198875427} ({'split': 'test'})\n",
      "Epoch 006\n",
      "accuracy: {'epoch': 6, 'value': 0.9770599999999998} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 6, 'value': 0.0711451101469994} ({'split': 'train'})\n",
      "accuracy: {'epoch': 6, 'value': 0.8490000426769256} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 6, 'value': 0.6479315102100371} ({'split': 'test'})\n",
      "Epoch 007\n",
      "accuracy: {'epoch': 7, 'value': 0.9839200000190736} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 7, 'value': 0.05026460187971589} ({'split': 'train'})\n",
      "accuracy: {'epoch': 7, 'value': 0.8601000368595123} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 7, 'value': 0.5613380968570709} ({'split': 'test'})\n",
      "Epoch 008\n",
      "accuracy: {'epoch': 8, 'value': 0.9851000000190738} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 8, 'value': 0.04549187757730482} ({'split': 'train'})\n",
      "accuracy: {'epoch': 8, 'value': 0.874200040102005} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 8, 'value': 0.5505640596151353} ({'split': 'test'})\n",
      "Epoch 009\n"
     ]
    }
   ],
   "source": [
    "!python main.py --gpu-id 0 --model-name vgg11_nobias --n-epochs 10 --save-result-file sample.csv \\\n",
    "--sweep-name exp_sample --correction --ground-metric euclidean --weight-stats \\\n",
    "--geom-ensemble-type wts --ground-metric-normalize none --sweep-id 90 \\\n",
    "--ckpt-type best --dataset Cifar10 --ground-metric-eff --recheck-cifar --activation-seed {activation_seed} \\\n",
    "--prelu-acts --past-correction --not-squared --exact\\\n",
    "--learning-rate 0.0001\\\n",
    "--momentum 0.5\\\n",
    "--batch-size-train 256 --experiment-dir {experiment_dir}\\\n",
    "--prunint-rate {prunint_rate}\\\n",
    "--run-mode {run_mode}\\\n",
    "--to-download\\\n",
    "--load-model-dir {load_model_dir}\\\n",
    "--retrain 30\n",
    "# After Ensemble: 86.24"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85412569-b402-468e-9014-991148ecb970",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------- Setting up parameters -------\n",
      "dumping parameters at  /root/autodl-tmp/OTfusion_pruning/otfusion/exp_sample/configurations\n",
      "The parameters are: \n",
      " Namespace(act_bug=False, act_num_samples=100, activation_histograms=False, activation_mode=None, activation_seed=21, alpha=0.7, baseroot='/root/autodl-tmp/OTfusion_pruning/otfusion', batch_size_test=1000, batch_size_train=256, center_acts=False, choice='0 2 4 6 8', cifar_style_data=False, ckpt_type='best', clip_gm=False, clip_max=5, clip_min=0, config_dir='/root/autodl-tmp/OTfusion_pruning/otfusion/exp_sample/configurations', config_file=None, correction=True, csv_dir='/root/autodl-tmp/OTfusion_pruning/otfusion/exp_sample/csv', dataset='Cifar10', debug=False, deprecated=None, deterministic=False, diff_init=False, disable_bias=True, dist_epochs=60, dist_normalize=False, dump_final_models=False, dump_model=False, enable_dropout=False, ensemble_step=0.5, eval_aligned=False, exact=True, exp_name='exp_2023-09-16_17-42-51_756852', experiment_dir='experiment2', geom_ensemble_type='wts', gpu_id=0, gromov=False, gromov_loss='square_loss', ground_metric='euclidean', ground_metric_eff=True, ground_metric_normalize='none', handle_skips=False, importance=None, learning_rate=3e-05, load_model_dir='./cifar_models', load_models='', log_interval=100, model_name='vgg11_nobias', momentum=0.5, n_epochs=30, no_random_trainloaders=False, normalize_acts=False, normalize_wts=False, not_squared=True, num_hidden_nodes=400, num_hidden_nodes1=400, num_hidden_nodes2=200, num_hidden_nodes3=100, num_hidden_nodes4=50, num_models=2, options_type='generic', partial_reshape=False, partition_dataloader=-1, partition_type='labels', past_correction=True, personal_class_idx=9, personal_split_frac=0.1, pool_acts=False, pool_relu=False, prediction_wts=False, prelu_acts=True, print_distances=False, proper_marginals=False, prunint_rate=0.2, recheck_acc=False, recheck_cifar=True, reg=0.01, reg_m=0.001, reinit_trainloaders=False, result_dir='/root/autodl-tmp/OTfusion_pruning/otfusion/exp_sample/results', retrain=30, retrain_avg_only=False, retrain_geometric_only=False, retrain_lr_decay=-1, retrain_lr_decay_epochs=None, retrain_lr_decay_factor=None, retrain_seed=-1, rootdir='/root/autodl-tmp/OTfusion_pruning/otfusion/exp_sample', run_mode='finetune', same_model=-1, save_result_file='sample.csv', second_model_name=None, sinkhorn_type='normal', skip_last_layer=False, skip_last_layer_type='average', skip_personal_idx=False, skip_retrain=-1, softmax_temperature=1, standardize_acts=False, sweep_id=90, sweep_name='exp_sample', temperature=20, tensorboard=False, tensorboard_root='./tensorboard', timestamp='2023-09-16_17-42-51_756852', tmap_stats=False, to_download=True, transform_acts=False, unbalanced=False, update_acts=False, weight_stats=True, width_ratio=1)\n",
      "refactored get_config\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "in _make_layers [Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), AvgPool2d(kernel_size=1, stride=1, padding=0)]\n",
      "Relu Inplace is  False\n",
      "model parameters are \n",
      " [torch.Size([64, 3, 3, 3]), torch.Size([128, 64, 3, 3]), torch.Size([256, 128, 3, 3]), torch.Size([256, 256, 3, 3]), torch.Size([512, 256, 3, 3]), torch.Size([512, 512, 3, 3]), torch.Size([512, 512, 3, 3]), torch.Size([512, 512, 3, 3]), torch.Size([10, 512])]\n",
      "in _make_layers [Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), AvgPool2d(kernel_size=1, stride=1, padding=0)]\n",
      "Relu Inplace is  False\n",
      "model parameters are \n",
      " [torch.Size([64, 3, 3, 3]), torch.Size([128, 64, 3, 3]), torch.Size([256, 128, 3, 3]), torch.Size([256, 256, 3, 3]), torch.Size([512, 256, 3, 3]), torch.Size([512, 512, 3, 3]), torch.Size([512, 512, 3, 3]), torch.Size([512, 512, 3, 3]), torch.Size([10, 512])]\n",
      "in _make_layers [Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), AvgPool2d(kernel_size=1, stride=1, padding=0)]\n",
      "Relu Inplace is  False\n",
      "model parameters are \n",
      " [torch.Size([64, 3, 3, 3]), torch.Size([128, 64, 3, 3]), torch.Size([256, 128, 3, 3]), torch.Size([256, 256, 3, 3]), torch.Size([512, 256, 3, 3]), torch.Size([512, 512, 3, 3]), torch.Size([512, 512, 3, 3]), torch.Size([512, 512, 3, 3]), torch.Size([10, 512])]\n",
      "in _make_layers [Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), AvgPool2d(kernel_size=1, stride=1, padding=0)]\n",
      "Relu Inplace is  False\n",
      "model parameters are \n",
      " [torch.Size([64, 3, 3, 3]), torch.Size([128, 64, 3, 3]), torch.Size([256, 128, 3, 3]), torch.Size([256, 256, 3, 3]), torch.Size([512, 256, 3, 3]), torch.Size([512, 512, 3, 3]), torch.Size([512, 512, 3, 3]), torch.Size([512, 512, 3, 3]), torch.Size([10, 512])]\n",
      "Train Epoch: 1 [0/50000 (0%)]\tLoss: 0.104261\n",
      "Train Epoch: 1 [25600/50000 (51%)]\tLoss: 0.099476\n",
      "\n",
      "--------- Testing in global mode ---------\n",
      "size of test_loader dataset:  10000\n",
      "\n",
      "Test set: Avg. loss: 0.0003, Accuracy: 9025/10000 (90%)\n",
      "\n",
      "Train Epoch: 2 [0/50000 (0%)]\tLoss: 0.077032\n",
      "Train Epoch: 2 [25600/50000 (51%)]\tLoss: 0.096654\n",
      "\n",
      "--------- Testing in global mode ---------\n",
      "size of test_loader dataset:  10000\n",
      "\n",
      "Test set: Avg. loss: 0.0003, Accuracy: 9030/10000 (90%)\n",
      "\n",
      "Train Epoch: 3 [0/50000 (0%)]\tLoss: 0.071083\n",
      "Train Epoch: 3 [25600/50000 (51%)]\tLoss: 0.067032\n",
      "\n",
      "--------- Testing in global mode ---------\n",
      "size of test_loader dataset:  10000\n",
      "\n",
      "Test set: Avg. loss: 0.0003, Accuracy: 9037/10000 (90%)\n",
      "\n",
      "Train Epoch: 4 [0/50000 (0%)]\tLoss: 0.079727\n",
      "Train Epoch: 4 [25600/50000 (51%)]\tLoss: 0.065154\n",
      "\n",
      "--------- Testing in global mode ---------\n",
      "size of test_loader dataset:  10000\n",
      "\n",
      "Test set: Avg. loss: 0.0003, Accuracy: 9038/10000 (90%)\n",
      "\n",
      "Train Epoch: 5 [0/50000 (0%)]\tLoss: 0.077752\n",
      "Train Epoch: 5 [25600/50000 (51%)]\tLoss: 0.081237\n",
      "\n",
      "--------- Testing in global mode ---------\n",
      "size of test_loader dataset:  10000\n",
      "\n",
      "Test set: Avg. loss: 0.0003, Accuracy: 9040/10000 (90%)\n",
      "\n",
      "Train Epoch: 6 [0/50000 (0%)]\tLoss: 0.074283\n",
      "Train Epoch: 6 [25600/50000 (51%)]\tLoss: 0.075979\n",
      "\n",
      "--------- Testing in global mode ---------\n",
      "size of test_loader dataset:  10000\n",
      "\n",
      "Test set: Avg. loss: 0.0003, Accuracy: 9041/10000 (90%)\n",
      "\n",
      "Train Epoch: 7 [0/50000 (0%)]\tLoss: 0.085717\n",
      "Train Epoch: 7 [25600/50000 (51%)]\tLoss: 0.086501\n",
      "\n",
      "--------- Testing in global mode ---------\n",
      "size of test_loader dataset:  10000\n",
      "\n",
      "Test set: Avg. loss: 0.0003, Accuracy: 9045/10000 (90%)\n",
      "\n",
      "Train Epoch: 8 [0/50000 (0%)]\tLoss: 0.069545\n",
      "Train Epoch: 8 [25600/50000 (51%)]\tLoss: 0.087994\n",
      "\n",
      "--------- Testing in global mode ---------\n",
      "size of test_loader dataset:  10000\n",
      "\n",
      "Test set: Avg. loss: 0.0003, Accuracy: 9049/10000 (90%)\n",
      "\n",
      "Train Epoch: 9 [0/50000 (0%)]\tLoss: 0.075486\n",
      "Train Epoch: 9 [25600/50000 (51%)]\tLoss: 0.080120\n",
      "\n",
      "--------- Testing in global mode ---------\n",
      "size of test_loader dataset:  10000\n",
      "\n",
      "Test set: Avg. loss: 0.0003, Accuracy: 9048/10000 (90%)\n",
      "\n",
      "Train Epoch: 10 [0/50000 (0%)]\tLoss: 0.078696\n",
      "Train Epoch: 10 [25600/50000 (51%)]\tLoss: 0.079160\n",
      "\n",
      "--------- Testing in global mode ---------\n",
      "size of test_loader dataset:  10000\n",
      "\n",
      "Test set: Avg. loss: 0.0003, Accuracy: 9049/10000 (90%)\n",
      "\n",
      "Train Epoch: 11 [0/50000 (0%)]\tLoss: 0.068520\n",
      "Train Epoch: 11 [25600/50000 (51%)]\tLoss: 0.058188\n",
      "\n",
      "--------- Testing in global mode ---------\n",
      "size of test_loader dataset:  10000\n",
      "\n",
      "Test set: Avg. loss: 0.0003, Accuracy: 9051/10000 (91%)\n",
      "\n",
      "Train Epoch: 12 [0/50000 (0%)]\tLoss: 0.053162\n",
      "Train Epoch: 12 [25600/50000 (51%)]\tLoss: 0.081524\n",
      "\n",
      "--------- Testing in global mode ---------\n",
      "size of test_loader dataset:  10000\n",
      "\n",
      "Test set: Avg. loss: 0.0003, Accuracy: 9049/10000 (90%)\n",
      "\n",
      "Train Epoch: 13 [0/50000 (0%)]\tLoss: 0.064279\n",
      "Train Epoch: 13 [25600/50000 (51%)]\tLoss: 0.064323\n",
      "\n",
      "--------- Testing in global mode ---------\n",
      "size of test_loader dataset:  10000\n",
      "\n",
      "Test set: Avg. loss: 0.0003, Accuracy: 9048/10000 (90%)\n",
      "\n",
      "Train Epoch: 14 [0/50000 (0%)]\tLoss: 0.074403\n",
      "Train Epoch: 14 [25600/50000 (51%)]\tLoss: 0.065026\n",
      "\n",
      "--------- Testing in global mode ---------\n",
      "size of test_loader dataset:  10000\n",
      "\n",
      "Test set: Avg. loss: 0.0003, Accuracy: 9046/10000 (90%)\n",
      "\n",
      "Train Epoch: 15 [0/50000 (0%)]\tLoss: 0.068268\n",
      "Train Epoch: 15 [25600/50000 (51%)]\tLoss: 0.070094\n",
      "\n",
      "--------- Testing in global mode ---------\n",
      "size of test_loader dataset:  10000\n",
      "\n",
      "Test set: Avg. loss: 0.0003, Accuracy: 9043/10000 (90%)\n",
      "\n",
      "Train Epoch: 16 [0/50000 (0%)]\tLoss: 0.065257\n",
      "Train Epoch: 16 [25600/50000 (51%)]\tLoss: 0.060155\n",
      "\n",
      "--------- Testing in global mode ---------\n",
      "size of test_loader dataset:  10000\n",
      "\n",
      "Test set: Avg. loss: 0.0003, Accuracy: 9041/10000 (90%)\n",
      "\n",
      "Train Epoch: 17 [0/50000 (0%)]\tLoss: 0.065473\n",
      "Train Epoch: 17 [25600/50000 (51%)]\tLoss: 0.052789\n",
      "\n",
      "--------- Testing in global mode ---------\n",
      "size of test_loader dataset:  10000\n",
      "\n",
      "Test set: Avg. loss: 0.0003, Accuracy: 9040/10000 (90%)\n",
      "\n",
      "Train Epoch: 18 [0/50000 (0%)]\tLoss: 0.054574\n",
      "Train Epoch: 18 [25600/50000 (51%)]\tLoss: 0.059772\n",
      "\n",
      "--------- Testing in global mode ---------\n",
      "size of test_loader dataset:  10000\n",
      "\n",
      "Test set: Avg. loss: 0.0003, Accuracy: 9040/10000 (90%)\n",
      "\n",
      "Train Epoch: 19 [0/50000 (0%)]\tLoss: 0.057253\n",
      "Train Epoch: 19 [25600/50000 (51%)]\tLoss: 0.060042\n",
      "\n",
      "--------- Testing in global mode ---------\n",
      "size of test_loader dataset:  10000\n",
      "\n",
      "Test set: Avg. loss: 0.0003, Accuracy: 9040/10000 (90%)\n",
      "\n",
      "Train Epoch: 20 [0/50000 (0%)]\tLoss: 0.069399\n",
      "Train Epoch: 20 [25600/50000 (51%)]\tLoss: 0.057831\n",
      "\n",
      "--------- Testing in global mode ---------\n",
      "size of test_loader dataset:  10000\n",
      "\n",
      "Test set: Avg. loss: 0.0003, Accuracy: 9039/10000 (90%)\n",
      "\n",
      "Train Epoch: 21 [0/50000 (0%)]\tLoss: 0.047542\n",
      "Train Epoch: 21 [25600/50000 (51%)]\tLoss: 0.067094\n",
      "\n",
      "--------- Testing in global mode ---------\n",
      "size of test_loader dataset:  10000\n",
      "\n",
      "Test set: Avg. loss: 0.0003, Accuracy: 9040/10000 (90%)\n",
      "\n",
      "Train Epoch: 22 [0/50000 (0%)]\tLoss: 0.078798\n",
      "Train Epoch: 22 [25600/50000 (51%)]\tLoss: 0.058467\n",
      "\n",
      "--------- Testing in global mode ---------\n",
      "size of test_loader dataset:  10000\n",
      "\n",
      "Test set: Avg. loss: 0.0003, Accuracy: 9042/10000 (90%)\n",
      "\n",
      "Train Epoch: 23 [0/50000 (0%)]\tLoss: 0.058753\n",
      "Train Epoch: 23 [25600/50000 (51%)]\tLoss: 0.062569\n",
      "\n",
      "--------- Testing in global mode ---------\n",
      "size of test_loader dataset:  10000\n",
      "\n",
      "Test set: Avg. loss: 0.0003, Accuracy: 9044/10000 (90%)\n",
      "\n",
      "Train Epoch: 24 [0/50000 (0%)]\tLoss: 0.048322\n",
      "Train Epoch: 24 [25600/50000 (51%)]\tLoss: 0.060467\n",
      "\n",
      "--------- Testing in global mode ---------\n",
      "size of test_loader dataset:  10000\n",
      "\n",
      "Test set: Avg. loss: 0.0003, Accuracy: 9044/10000 (90%)\n",
      "\n",
      "Train Epoch: 25 [0/50000 (0%)]\tLoss: 0.050152\n",
      "Train Epoch: 25 [25600/50000 (51%)]\tLoss: 0.054410\n",
      "\n",
      "--------- Testing in global mode ---------\n",
      "size of test_loader dataset:  10000\n",
      "\n",
      "Test set: Avg. loss: 0.0003, Accuracy: 9045/10000 (90%)\n",
      "\n",
      "Train Epoch: 26 [0/50000 (0%)]\tLoss: 0.063586\n",
      "Train Epoch: 26 [25600/50000 (51%)]\tLoss: 0.049462\n",
      "\n",
      "--------- Testing in global mode ---------\n",
      "size of test_loader dataset:  10000\n",
      "\n",
      "Test set: Avg. loss: 0.0003, Accuracy: 9045/10000 (90%)\n",
      "\n",
      "Train Epoch: 27 [0/50000 (0%)]\tLoss: 0.043810\n",
      "Train Epoch: 27 [25600/50000 (51%)]\tLoss: 0.066313\n",
      "\n",
      "--------- Testing in global mode ---------\n",
      "size of test_loader dataset:  10000\n",
      "\n",
      "Test set: Avg. loss: 0.0003, Accuracy: 9047/10000 (90%)\n",
      "\n",
      "Train Epoch: 28 [0/50000 (0%)]\tLoss: 0.058286\n",
      "Train Epoch: 28 [25600/50000 (51%)]\tLoss: 0.053142\n",
      "\n",
      "--------- Testing in global mode ---------\n",
      "size of test_loader dataset:  10000\n",
      "\n",
      "Test set: Avg. loss: 0.0003, Accuracy: 9046/10000 (90%)\n",
      "\n",
      "Train Epoch: 29 [0/50000 (0%)]\tLoss: 0.059199\n",
      "Train Epoch: 29 [25600/50000 (51%)]\tLoss: 0.053426\n",
      "\n",
      "--------- Testing in global mode ---------\n",
      "size of test_loader dataset:  10000\n",
      "\n",
      "Test set: Avg. loss: 0.0003, Accuracy: 9048/10000 (90%)\n",
      "\n",
      "Train Epoch: 30 [0/50000 (0%)]\tLoss: 0.067050\n",
      "Train Epoch: 30 [25600/50000 (51%)]\tLoss: 0.053070\n",
      "\n",
      "--------- Testing in global mode ---------\n",
      "size of test_loader dataset:  10000\n",
      "\n",
      "Test set: Avg. loss: 0.0003, Accuracy: 9047/10000 (90%)\n",
      "\n",
      "Train Epoch: 1 [0/50000 (0%)]\tLoss: 0.088863\n",
      "Train Epoch: 1 [25600/50000 (51%)]\tLoss: 0.080079\n",
      "\n",
      "--------- Testing in global mode ---------\n",
      "size of test_loader dataset:  10000\n",
      "\n",
      "Test set: Avg. loss: 0.0003, Accuracy: 9051/10000 (91%)\n",
      "\n",
      "Train Epoch: 2 [0/50000 (0%)]\tLoss: 0.093360\n",
      "Train Epoch: 2 [25600/50000 (51%)]\tLoss: 0.085492\n",
      "\n",
      "--------- Testing in global mode ---------\n",
      "size of test_loader dataset:  10000\n",
      "\n",
      "Test set: Avg. loss: 0.0003, Accuracy: 9054/10000 (91%)\n",
      "\n",
      "Train Epoch: 3 [0/50000 (0%)]\tLoss: 0.089713\n",
      "Train Epoch: 3 [25600/50000 (51%)]\tLoss: 0.085880\n",
      "\n",
      "--------- Testing in global mode ---------\n",
      "size of test_loader dataset:  10000\n",
      "\n",
      "Test set: Avg. loss: 0.0003, Accuracy: 9056/10000 (91%)\n",
      "\n",
      "Train Epoch: 4 [0/50000 (0%)]\tLoss: 0.097871\n",
      "Train Epoch: 4 [25600/50000 (51%)]\tLoss: 0.075384\n",
      "\n",
      "--------- Testing in global mode ---------\n",
      "size of test_loader dataset:  10000\n",
      "\n",
      "Test set: Avg. loss: 0.0003, Accuracy: 9061/10000 (91%)\n",
      "\n",
      "Train Epoch: 5 [0/50000 (0%)]\tLoss: 0.075559\n",
      "Train Epoch: 5 [25600/50000 (51%)]\tLoss: 0.095856\n",
      "\n",
      "--------- Testing in global mode ---------\n",
      "size of test_loader dataset:  10000\n",
      "\n",
      "Test set: Avg. loss: 0.0003, Accuracy: 9060/10000 (91%)\n",
      "\n",
      "Train Epoch: 6 [0/50000 (0%)]\tLoss: 0.076149\n",
      "Train Epoch: 6 [25600/50000 (51%)]\tLoss: 0.068957\n",
      "\n",
      "--------- Testing in global mode ---------\n",
      "size of test_loader dataset:  10000\n",
      "\n",
      "Test set: Avg. loss: 0.0003, Accuracy: 9062/10000 (91%)\n",
      "\n",
      "Train Epoch: 7 [0/50000 (0%)]\tLoss: 0.059246\n",
      "Train Epoch: 7 [25600/50000 (51%)]\tLoss: 0.071553\n",
      "\n",
      "--------- Testing in global mode ---------\n",
      "size of test_loader dataset:  10000\n",
      "\n",
      "Test set: Avg. loss: 0.0003, Accuracy: 9068/10000 (91%)\n",
      "\n",
      "Train Epoch: 8 [0/50000 (0%)]\tLoss: 0.075608\n",
      "Train Epoch: 8 [25600/50000 (51%)]\tLoss: 0.078435\n",
      "\n",
      "--------- Testing in global mode ---------\n",
      "size of test_loader dataset:  10000\n",
      "\n",
      "Test set: Avg. loss: 0.0003, Accuracy: 9073/10000 (91%)\n",
      "\n",
      "Train Epoch: 9 [0/50000 (0%)]\tLoss: 0.092692\n",
      "Train Epoch: 9 [25600/50000 (51%)]\tLoss: 0.075124\n",
      "\n",
      "--------- Testing in global mode ---------\n",
      "size of test_loader dataset:  10000\n",
      "\n",
      "Test set: Avg. loss: 0.0003, Accuracy: 9073/10000 (91%)\n",
      "\n",
      "Train Epoch: 10 [0/50000 (0%)]\tLoss: 0.078839\n",
      "Train Epoch: 10 [25600/50000 (51%)]\tLoss: 0.081890\n",
      "\n",
      "--------- Testing in global mode ---------\n",
      "size of test_loader dataset:  10000\n",
      "\n",
      "Test set: Avg. loss: 0.0003, Accuracy: 9070/10000 (91%)\n",
      "\n",
      "Train Epoch: 11 [0/50000 (0%)]\tLoss: 0.068546\n",
      "Train Epoch: 11 [25600/50000 (51%)]\tLoss: 0.069791\n",
      "\n",
      "--------- Testing in global mode ---------\n",
      "size of test_loader dataset:  10000\n",
      "\n",
      "Test set: Avg. loss: 0.0003, Accuracy: 9066/10000 (91%)\n",
      "\n",
      "Train Epoch: 12 [0/50000 (0%)]\tLoss: 0.077572\n",
      "Train Epoch: 12 [25600/50000 (51%)]\tLoss: 0.062933\n",
      "\n",
      "--------- Testing in global mode ---------\n",
      "size of test_loader dataset:  10000\n",
      "\n",
      "Test set: Avg. loss: 0.0003, Accuracy: 9070/10000 (91%)\n",
      "\n",
      "Train Epoch: 13 [0/50000 (0%)]\tLoss: 0.089173\n",
      "Train Epoch: 13 [25600/50000 (51%)]\tLoss: 0.068156\n",
      "\n",
      "--------- Testing in global mode ---------\n",
      "size of test_loader dataset:  10000\n",
      "\n",
      "Test set: Avg. loss: 0.0003, Accuracy: 9068/10000 (91%)\n",
      "\n",
      "Train Epoch: 14 [0/50000 (0%)]\tLoss: 0.070633\n",
      "Train Epoch: 14 [25600/50000 (51%)]\tLoss: 0.062428\n",
      "\n",
      "--------- Testing in global mode ---------\n",
      "size of test_loader dataset:  10000\n",
      "\n",
      "Test set: Avg. loss: 0.0003, Accuracy: 9066/10000 (91%)\n",
      "\n",
      "Train Epoch: 15 [0/50000 (0%)]\tLoss: 0.069825\n",
      "Train Epoch: 15 [25600/50000 (51%)]\tLoss: 0.071413\n",
      "\n",
      "--------- Testing in global mode ---------\n",
      "size of test_loader dataset:  10000\n",
      "\n",
      "Test set: Avg. loss: 0.0003, Accuracy: 9065/10000 (91%)\n",
      "\n",
      "Train Epoch: 16 [0/50000 (0%)]\tLoss: 0.076022\n",
      "Train Epoch: 16 [25600/50000 (51%)]\tLoss: 0.050538\n",
      "\n",
      "--------- Testing in global mode ---------\n",
      "size of test_loader dataset:  10000\n",
      "\n",
      "Test set: Avg. loss: 0.0003, Accuracy: 9067/10000 (91%)\n",
      "\n",
      "Train Epoch: 17 [0/50000 (0%)]\tLoss: 0.061727\n",
      "Train Epoch: 17 [25600/50000 (51%)]\tLoss: 0.065578\n",
      "\n",
      "--------- Testing in global mode ---------\n",
      "size of test_loader dataset:  10000\n",
      "\n",
      "Test set: Avg. loss: 0.0003, Accuracy: 9066/10000 (91%)\n",
      "\n",
      "Train Epoch: 18 [0/50000 (0%)]\tLoss: 0.076982\n",
      "Train Epoch: 18 [25600/50000 (51%)]\tLoss: 0.064926\n",
      "\n",
      "--------- Testing in global mode ---------\n",
      "size of test_loader dataset:  10000\n",
      "\n",
      "Test set: Avg. loss: 0.0003, Accuracy: 9069/10000 (91%)\n",
      "\n",
      "Train Epoch: 19 [0/50000 (0%)]\tLoss: 0.059013\n",
      "Train Epoch: 19 [25600/50000 (51%)]\tLoss: 0.059925\n",
      "\n",
      "--------- Testing in global mode ---------\n",
      "size of test_loader dataset:  10000\n",
      "\n",
      "Test set: Avg. loss: 0.0003, Accuracy: 9069/10000 (91%)\n",
      "\n",
      "Train Epoch: 20 [0/50000 (0%)]\tLoss: 0.074368\n",
      "Train Epoch: 20 [25600/50000 (51%)]\tLoss: 0.069893\n",
      "\n",
      "--------- Testing in global mode ---------\n",
      "size of test_loader dataset:  10000\n",
      "\n",
      "Test set: Avg. loss: 0.0003, Accuracy: 9068/10000 (91%)\n",
      "\n",
      "Train Epoch: 21 [0/50000 (0%)]\tLoss: 0.056733\n",
      "Train Epoch: 21 [25600/50000 (51%)]\tLoss: 0.061063\n",
      "\n",
      "--------- Testing in global mode ---------\n",
      "size of test_loader dataset:  10000\n",
      "\n",
      "Test set: Avg. loss: 0.0003, Accuracy: 9065/10000 (91%)\n",
      "\n",
      "Train Epoch: 22 [0/50000 (0%)]\tLoss: 0.049868\n",
      "Train Epoch: 22 [25600/50000 (51%)]\tLoss: 0.063242\n",
      "\n",
      "--------- Testing in global mode ---------\n",
      "size of test_loader dataset:  10000\n",
      "\n",
      "Test set: Avg. loss: 0.0003, Accuracy: 9067/10000 (91%)\n",
      "\n",
      "Train Epoch: 23 [0/50000 (0%)]\tLoss: 0.069445\n",
      "Train Epoch: 23 [25600/50000 (51%)]\tLoss: 0.057784\n",
      "\n",
      "--------- Testing in global mode ---------\n",
      "size of test_loader dataset:  10000\n",
      "\n",
      "Test set: Avg. loss: 0.0003, Accuracy: 9065/10000 (91%)\n",
      "\n",
      "Train Epoch: 24 [0/50000 (0%)]\tLoss: 0.049788\n",
      "Train Epoch: 24 [25600/50000 (51%)]\tLoss: 0.061648\n",
      "\n",
      "--------- Testing in global mode ---------\n",
      "size of test_loader dataset:  10000\n",
      "\n",
      "Test set: Avg. loss: 0.0003, Accuracy: 9064/10000 (91%)\n",
      "\n",
      "Train Epoch: 25 [0/50000 (0%)]\tLoss: 0.067175\n",
      "Train Epoch: 25 [25600/50000 (51%)]\tLoss: 0.053345\n",
      "\n",
      "--------- Testing in global mode ---------\n",
      "size of test_loader dataset:  10000\n",
      "\n",
      "Test set: Avg. loss: 0.0003, Accuracy: 9066/10000 (91%)\n",
      "\n",
      "Train Epoch: 26 [0/50000 (0%)]\tLoss: 0.053120\n",
      "Train Epoch: 26 [25600/50000 (51%)]\tLoss: 0.060254\n",
      "\n",
      "--------- Testing in global mode ---------\n",
      "size of test_loader dataset:  10000\n",
      "\n",
      "Test set: Avg. loss: 0.0003, Accuracy: 9067/10000 (91%)\n",
      "\n",
      "Train Epoch: 27 [0/50000 (0%)]\tLoss: 0.065097\n",
      "Train Epoch: 27 [25600/50000 (51%)]\tLoss: 0.068547\n",
      "\n",
      "--------- Testing in global mode ---------\n",
      "size of test_loader dataset:  10000\n",
      "\n",
      "Test set: Avg. loss: 0.0003, Accuracy: 9067/10000 (91%)\n",
      "\n",
      "Train Epoch: 28 [0/50000 (0%)]\tLoss: 0.059631\n",
      "Train Epoch: 28 [25600/50000 (51%)]\tLoss: 0.057361\n",
      "\n",
      "--------- Testing in global mode ---------\n",
      "size of test_loader dataset:  10000\n",
      "\n",
      "Test set: Avg. loss: 0.0003, Accuracy: 9068/10000 (91%)\n",
      "\n",
      "Train Epoch: 29 [0/50000 (0%)]\tLoss: 0.050734\n",
      "Train Epoch: 29 [25600/50000 (51%)]\tLoss: 0.067965\n",
      "\n",
      "--------- Testing in global mode ---------\n",
      "size of test_loader dataset:  10000\n",
      "\n",
      "Test set: Avg. loss: 0.0003, Accuracy: 9069/10000 (91%)\n",
      "\n",
      "Train Epoch: 30 [0/50000 (0%)]\tLoss: 0.051376\n",
      "Train Epoch: 30 [25600/50000 (51%)]\tLoss: 0.061955\n",
      "\n",
      "--------- Testing in global mode ---------\n",
      "size of test_loader dataset:  10000\n",
      "\n",
      "Test set: Avg. loss: 0.0003, Accuracy: 9067/10000 (91%)\n",
      "\n",
      "========================================================\n",
      "accuracies [90.47, 90.67]\n",
      "=========================================================\n",
      "layer features.0.weight_orig has #params  1728\n",
      "layer features.3.weight_orig has #params  73728\n",
      "layer features.6.weight_orig has #params  294912\n",
      "layer features.8.weight_orig has #params  589824\n",
      "layer features.11.weight_orig has #params  1179648\n",
      "layer features.13.weight_orig has #params  2359296\n",
      "layer features.16.weight_orig has #params  2359296\n",
      "layer features.18.weight_orig has #params  2359296\n",
      "layer classifier.weight has #params  5120\n",
      "Activation Timer start\n",
      "Activation Timer ends\n",
      "------- Geometric Ensembling -------\n",
      "Timer start\n",
      "Previous layer shape is  None\n",
      "Processing the coordinates to form ground_metric\n",
      "dont leave off the squaring of the ground metric\n",
      "returns a uniform measure of cardinality:  64\n",
      "returns a uniform measure of cardinality:  64\n",
      "the transport map is  tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')\n",
      "Ratio of trace to the matrix sum:  tensor(0.0469, device='cuda:0')\n",
      "Here, trace is 3.0 and matrix sum is 64.0 \n",
      "this is past correction for weight mode\n",
      "Shape of aligned wt is  torch.Size([64, 3, 9])\n",
      "Shape of fc_layer0_weight_data is  torch.Size([64, 3, 9])\n",
      "Previous layer shape is  torch.Size([64, 3, 3, 3])\n",
      "shape of layer: model 0 torch.Size([128, 64, 9])\n",
      "shape of layer: model 1 torch.Size([128, 64, 9])\n",
      "shape of previous transport map torch.Size([64, 64])\n",
      "Processing the coordinates to form ground_metric\n",
      "dont leave off the squaring of the ground metric\n",
      "returns a uniform measure of cardinality:  128\n",
      "returns a uniform measure of cardinality:  128\n",
      "the transport map is  tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        ...,\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0078],\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],\n",
      "       device='cuda:0')\n",
      "Ratio of trace to the matrix sum:  tensor(0.0078, device='cuda:0')\n",
      "Here, trace is 1.0 and matrix sum is 128.0 \n",
      "this is past correction for weight mode\n",
      "Shape of aligned wt is  torch.Size([128, 64, 9])\n",
      "Shape of fc_layer0_weight_data is  torch.Size([128, 64, 9])\n",
      "Previous layer shape is  torch.Size([128, 64, 3, 3])\n",
      "shape of layer: model 0 torch.Size([256, 128, 9])\n",
      "shape of layer: model 1 torch.Size([256, 128, 9])\n",
      "shape of previous transport map torch.Size([128, 128])\n",
      "Processing the coordinates to form ground_metric\n",
      "dont leave off the squaring of the ground metric\n",
      "returns a uniform measure of cardinality:  256\n",
      "returns a uniform measure of cardinality:  256\n",
      "the transport map is  tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')\n",
      "Ratio of trace to the matrix sum:  tensor(0., device='cuda:0')\n",
      "Here, trace is 0.0 and matrix sum is 256.0 \n",
      "this is past correction for weight mode\n",
      "Shape of aligned wt is  torch.Size([256, 128, 9])\n",
      "Shape of fc_layer0_weight_data is  torch.Size([256, 128, 9])\n",
      "Previous layer shape is  torch.Size([256, 128, 3, 3])\n",
      "shape of layer: model 0 torch.Size([256, 256, 9])\n",
      "shape of layer: model 1 torch.Size([256, 256, 9])\n",
      "shape of previous transport map torch.Size([256, 256])\n",
      "Processing the coordinates to form ground_metric\n",
      "dont leave off the squaring of the ground metric\n",
      "returns a uniform measure of cardinality:  256\n",
      "returns a uniform measure of cardinality:  256\n",
      "the transport map is  tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        ...,\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.0039, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],\n",
      "       device='cuda:0')\n",
      "Ratio of trace to the matrix sum:  tensor(0.0078, device='cuda:0')\n",
      "Here, trace is 2.0 and matrix sum is 256.0 \n",
      "this is past correction for weight mode\n",
      "Shape of aligned wt is  torch.Size([256, 256, 9])\n",
      "Shape of fc_layer0_weight_data is  torch.Size([256, 256, 9])\n",
      "Previous layer shape is  torch.Size([256, 256, 3, 3])\n",
      "shape of layer: model 0 torch.Size([512, 256, 9])\n",
      "shape of layer: model 1 torch.Size([512, 256, 9])\n",
      "shape of previous transport map torch.Size([256, 256])\n",
      "Processing the coordinates to form ground_metric\n",
      "dont leave off the squaring of the ground metric\n",
      "returns a uniform measure of cardinality:  512\n",
      "returns a uniform measure of cardinality:  512\n",
      "the transport map is  tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')\n",
      "Ratio of trace to the matrix sum:  tensor(0.0039, device='cuda:0')\n",
      "Here, trace is 2.0 and matrix sum is 512.0 \n",
      "this is past correction for weight mode\n",
      "Shape of aligned wt is  torch.Size([512, 256, 9])\n",
      "Shape of fc_layer0_weight_data is  torch.Size([512, 256, 9])\n",
      "Previous layer shape is  torch.Size([512, 256, 3, 3])\n",
      "shape of layer: model 0 torch.Size([512, 512, 9])\n",
      "shape of layer: model 1 torch.Size([512, 512, 9])\n",
      "shape of previous transport map torch.Size([512, 512])\n",
      "Processing the coordinates to form ground_metric\n",
      "dont leave off the squaring of the ground metric\n",
      "returns a uniform measure of cardinality:  512\n",
      "returns a uniform measure of cardinality:  512\n",
      "the transport map is  tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')\n",
      "Ratio of trace to the matrix sum:  tensor(0.0020, device='cuda:0')\n",
      "Here, trace is 1.0 and matrix sum is 512.0 \n",
      "this is past correction for weight mode\n",
      "Shape of aligned wt is  torch.Size([512, 512, 9])\n",
      "Shape of fc_layer0_weight_data is  torch.Size([512, 512, 9])\n",
      "Previous layer shape is  torch.Size([512, 512, 3, 3])\n",
      "shape of layer: model 0 torch.Size([512, 512, 9])\n",
      "shape of layer: model 1 torch.Size([512, 512, 9])\n",
      "shape of previous transport map torch.Size([512, 512])\n",
      "Processing the coordinates to form ground_metric\n",
      "dont leave off the squaring of the ground metric\n",
      "returns a uniform measure of cardinality:  512\n",
      "returns a uniform measure of cardinality:  512\n",
      "the transport map is  tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')\n",
      "Ratio of trace to the matrix sum:  tensor(0.0020, device='cuda:0')\n",
      "Here, trace is 1.0 and matrix sum is 512.0 \n",
      "this is past correction for weight mode\n",
      "Shape of aligned wt is  torch.Size([512, 512, 9])\n",
      "Shape of fc_layer0_weight_data is  torch.Size([512, 512, 9])\n",
      "Previous layer shape is  torch.Size([512, 512, 3, 3])\n",
      "shape of layer: model 0 torch.Size([512, 512, 9])\n",
      "shape of layer: model 1 torch.Size([512, 512, 9])\n",
      "shape of previous transport map torch.Size([512, 512])\n",
      "Processing the coordinates to form ground_metric\n",
      "dont leave off the squaring of the ground metric\n",
      "returns a uniform measure of cardinality:  512\n",
      "returns a uniform measure of cardinality:  512\n",
      "the transport map is  tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        [0.0020, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        ...,\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],\n",
      "       device='cuda:0')\n",
      "Ratio of trace to the matrix sum:  tensor(0., device='cuda:0')\n",
      "Here, trace is 0.0 and matrix sum is 512.0 \n",
      "this is past correction for weight mode\n",
      "Shape of aligned wt is  torch.Size([512, 512, 9])\n",
      "Shape of fc_layer0_weight_data is  torch.Size([512, 512, 9])\n",
      "Previous layer shape is  torch.Size([512, 512, 3, 3])\n",
      "shape of layer: model 0 torch.Size([10, 512])\n",
      "shape of layer: model 1 torch.Size([10, 512])\n",
      "shape of previous transport map torch.Size([512, 512])\n",
      "Processing the coordinates to form ground_metric\n",
      "dont leave off the squaring of the ground metric\n",
      "ground metric is  tensor([[1.4271, 2.4922, 2.7862, 2.8781, 2.7431, 2.8863, 2.7321, 2.6394, 2.5695,\n",
      "         2.5972],\n",
      "        [2.4934, 1.3270, 2.6780, 2.5959, 2.7628, 2.5542, 2.4201, 2.5251, 2.3091,\n",
      "         2.4135],\n",
      "        [2.7651, 2.6757, 1.3856, 2.9520, 2.9119, 2.8756, 2.7317, 2.8432, 2.7539,\n",
      "         2.7912],\n",
      "        [2.8407, 2.6840, 2.8979, 1.5460, 2.8659, 2.9219, 2.7620, 2.8031, 2.8222,\n",
      "         2.6401],\n",
      "        [2.7604, 2.6006, 2.8687, 2.8505, 1.5070, 2.7499, 2.7895, 2.6856, 2.7087,\n",
      "         2.7341],\n",
      "        [2.9245, 2.5438, 2.9523, 2.8471, 2.7322, 1.5530, 2.7822, 2.7347, 2.8033,\n",
      "         2.6742],\n",
      "        [2.6821, 2.4485, 2.7233, 2.9156, 2.6525, 2.8568, 1.3945, 2.6925, 2.5020,\n",
      "         2.5591],\n",
      "        [2.7440, 2.5532, 2.7847, 2.7952, 2.7197, 2.6634, 2.7222, 1.3944, 2.5630,\n",
      "         2.5156],\n",
      "        [2.6027, 2.3700, 2.7749, 2.6922, 2.7075, 2.8337, 2.5358, 2.6410, 1.3028,\n",
      "         2.4081],\n",
      "        [2.5963, 2.2761, 2.7862, 2.7621, 2.6737, 2.7317, 2.5508, 2.5323, 2.5252,\n",
      "         1.2894]], device='cuda:0', grad_fn=<PowBackward0>)\n",
      "returns a uniform measure of cardinality:  10\n",
      "returns a uniform measure of cardinality:  10\n",
      "the transport map is  tensor([[0.1000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0000, 0.1000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0000, 0.0000, 0.1000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.1000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.1000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.1000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.1000, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.1000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.1000,\n",
      "         0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.1000]], device='cuda:0')\n",
      "Ratio of trace to the matrix sum:  tensor(1., device='cuda:0')\n",
      "Here, trace is 9.99755859375 and matrix sum is 9.99755859375 \n",
      "this is past correction for weight mode\n",
      "Shape of aligned wt is  torch.Size([10, 512])\n",
      "Shape of fc_layer0_weight_data is  torch.Size([10, 512])\n",
      "using independent method\n",
      "in _make_layers [Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), AvgPool2d(kernel_size=1, stride=1, padding=0)]\n",
      "Relu Inplace is  False\n",
      "model parameters are \n",
      " [torch.Size([64, 3, 3, 3]), torch.Size([128, 64, 3, 3]), torch.Size([256, 128, 3, 3]), torch.Size([256, 256, 3, 3]), torch.Size([512, 256, 3, 3]), torch.Size([512, 512, 3, 3]), torch.Size([512, 512, 3, 3]), torch.Size([512, 512, 3, 3]), torch.Size([10, 512])]\n",
      "\n",
      "--------- Testing in global mode ---------\n",
      "size of test_loader dataset:  10000\n",
      "\n",
      "Test set: Avg. loss: 0.0023, Accuracy: 1000/10000 (10%)\n",
      "\n",
      "len of model parameters and avg aligned layers is  9 9\n",
      "len of model_state_dict is  9\n",
      "len of param_list is  9\n",
      "\n",
      "--------- Testing in global mode ---------\n",
      "size of test_loader dataset:  10000\n",
      "\n",
      "Test set: Avg. loss: 0.0014, Accuracy: 8692/10000 (87%)\n",
      "\n",
      "===============================================\n",
      "===============================================\n",
      "After Ensemble: 86.92\n",
      "===============================================\n",
      "===============================================\n",
      "Timer ends\n",
      "Time taken for geometric ensembling is 3.268563900142908 seconds\n",
      "------- Prediction based ensembling -------\n",
      "\n",
      "Test set: Avg. loss: 0.0003, Accuracy: 9126/10000 (91%)\n",
      "\n",
      "------- Naive ensembling of weights -------\n",
      "[torch.Size([64, 3, 3, 3]), torch.Size([64, 3, 3, 3])]\n",
      "torch.Size([64, 3, 3, 3])\n",
      "[torch.Size([128, 64, 3, 3]), torch.Size([128, 64, 3, 3])]\n",
      "torch.Size([128, 64, 3, 3])\n",
      "[torch.Size([256, 128, 3, 3]), torch.Size([256, 128, 3, 3])]\n",
      "torch.Size([256, 128, 3, 3])\n",
      "[torch.Size([256, 256, 3, 3]), torch.Size([256, 256, 3, 3])]\n",
      "torch.Size([256, 256, 3, 3])\n",
      "[torch.Size([512, 256, 3, 3]), torch.Size([512, 256, 3, 3])]\n",
      "torch.Size([512, 256, 3, 3])\n",
      "[torch.Size([512, 512, 3, 3]), torch.Size([512, 512, 3, 3])]\n",
      "torch.Size([512, 512, 3, 3])\n",
      "[torch.Size([512, 512, 3, 3]), torch.Size([512, 512, 3, 3])]\n",
      "torch.Size([512, 512, 3, 3])\n",
      "[torch.Size([512, 512, 3, 3]), torch.Size([512, 512, 3, 3])]\n",
      "torch.Size([512, 512, 3, 3])\n",
      "[torch.Size([10, 512]), torch.Size([10, 512])]\n",
      "torch.Size([10, 512])\n",
      "in _make_layers [Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), AvgPool2d(kernel_size=1, stride=1, padding=0)]\n",
      "Relu Inplace is  False\n",
      "model parameters are \n",
      " [torch.Size([64, 3, 3, 3]), torch.Size([128, 64, 3, 3]), torch.Size([256, 128, 3, 3]), torch.Size([256, 256, 3, 3]), torch.Size([512, 256, 3, 3]), torch.Size([512, 512, 3, 3]), torch.Size([512, 512, 3, 3]), torch.Size([512, 512, 3, 3]), torch.Size([10, 512])]\n",
      "\n",
      "--------- Testing in global mode ---------\n",
      "size of test_loader dataset:  10000\n",
      "\n",
      "Test set: Avg. loss: 0.0023, Accuracy: 904/10000 (9%)\n",
      "\n",
      "\n",
      "--------- Testing in global mode ---------\n",
      "size of test_loader dataset:  10000\n",
      "\n",
      "Test set: Avg. loss: 0.0023, Accuracy: 1610/10000 (16%)\n",
      "\n",
      "-------- Retraining the models ---------\n",
      "Retraining model :  model_0\n",
      "num_epochs---------------+++))))))+++ 300\n",
      "num_epochs---------------+++))))))+++ 300\n",
      "lr is  0.05\n",
      "number of epochs would be  30\n",
      "num_epochs-------------- 30\n",
      "Epoch 000\n",
      "/root/miniconda3/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:129: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\"Detected call of `lr_scheduler.step()` before `optimizer.step()`. \"\n",
      "/root/miniconda3/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:154: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "accuracy: {'epoch': 0, 'value': 0.9152000000572206} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 0, 'value': 0.2573326487636566} ({'split': 'train'})\n",
      "accuracy: {'epoch': 0, 'value': 0.845900046825409} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 0, 'value': 0.48958107829093933} ({'split': 'test'})\n",
      "Epoch 001\n",
      "accuracy: {'epoch': 1, 'value': 0.9382599999809267} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 1, 'value': 0.18455429563522344} ({'split': 'train'})\n",
      "accuracy: {'epoch': 1, 'value': 0.8386000454425812} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 1, 'value': 0.5406209081411362} ({'split': 'test'})\n",
      "Epoch 002\n",
      "accuracy: {'epoch': 2, 'value': 0.95368} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 2, 'value': 0.13821284977912898} ({'split': 'train'})\n",
      "accuracy: {'epoch': 2, 'value': 0.8605000317096709} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 2, 'value': 0.4791329145431519} ({'split': 'test'})\n",
      "Epoch 003\n",
      "accuracy: {'epoch': 3, 'value': 0.9675400000381476} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 3, 'value': 0.09821827132344246} ({'split': 'train'})\n",
      "accuracy: {'epoch': 3, 'value': 0.847100031375885} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 3, 'value': 0.6057005524635315} ({'split': 'test'})\n",
      "Epoch 004\n",
      "accuracy: {'epoch': 4, 'value': 0.9749200000572206} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 4, 'value': 0.07695993330001831} ({'split': 'train'})\n",
      "accuracy: {'epoch': 4, 'value': 0.8703000485897064} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 4, 'value': 0.4759733289480209} ({'split': 'test'})\n",
      "Epoch 005\n",
      "accuracy: {'epoch': 5, 'value': 0.9786000000572208} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 5, 'value': 0.06710543148517611} ({'split': 'train'})\n",
      "accuracy: {'epoch': 5, 'value': 0.8683000445365906} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 5, 'value': 0.5575224369764328} ({'split': 'test'})\n",
      "Epoch 006\n",
      "accuracy: {'epoch': 6, 'value': 0.9791400000000002} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 6, 'value': 0.06468789183616641} ({'split': 'train'})\n",
      "accuracy: {'epoch': 6, 'value': 0.854300045967102} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 6, 'value': 0.6146420121192933} ({'split': 'test'})\n",
      "Epoch 007\n",
      "accuracy: {'epoch': 7, 'value': 0.9817600000381469} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 7, 'value': 0.057966810228824606} ({'split': 'train'})\n",
      "accuracy: {'epoch': 7, 'value': 0.8583000361919404} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 7, 'value': 0.5900658071041106} ({'split': 'test'})\n",
      "Epoch 008\n",
      "accuracy: {'epoch': 8, 'value': 0.9825800000190735} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 8, 'value': 0.053353335274457944} ({'split': 'train'})\n",
      "accuracy: {'epoch': 8, 'value': 0.8596000432968139} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 8, 'value': 0.5483353108167649} ({'split': 'test'})\n",
      "Epoch 009\n",
      "accuracy: {'epoch': 9, 'value': 0.9898600000190735} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 9, 'value': 0.033669225981235514} ({'split': 'train'})\n",
      "accuracy: {'epoch': 9, 'value': 0.8620000422000885} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 9, 'value': 0.53200224339962} ({'split': 'test'})\n",
      "Epoch 010\n",
      "accuracy: {'epoch': 10, 'value': 0.9891599999999999} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 10, 'value': 0.03427870583117008} ({'split': 'train'})\n",
      "accuracy: {'epoch': 10, 'value': 0.8778000414371491} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 10, 'value': 0.5089761853218078} ({'split': 'test'})\n",
      "Epoch 011\n",
      "accuracy: {'epoch': 11, 'value': 0.9908800000000001} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 11, 'value': 0.03086342088997363} ({'split': 'train'})\n",
      "accuracy: {'epoch': 11, 'value': 0.8595000267028808} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 11, 'value': 0.5946291238069534} ({'split': 'test'})\n",
      "Epoch 012\n",
      "accuracy: {'epoch': 12, 'value': 0.989460000038147} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 12, 'value': 0.036124360507726674} ({'split': 'train'})\n",
      "accuracy: {'epoch': 12, 'value': 0.8610000371932984} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 12, 'value': 0.5177317142486573} ({'split': 'test'})\n",
      "Epoch 013\n",
      "accuracy: {'epoch': 13, 'value': 0.9903000000000003} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 13, 'value': 0.03169752392351628} ({'split': 'train'})\n",
      "accuracy: {'epoch': 13, 'value': 0.8625000417232513} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 13, 'value': 0.546364089846611} ({'split': 'test'})\n",
      "Epoch 014\n",
      "accuracy: {'epoch': 14, 'value': 0.9877599999999999} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 14, 'value': 0.04067933338701725} ({'split': 'train'})\n",
      "accuracy: {'epoch': 14, 'value': 0.8622000396251679} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 14, 'value': 0.5471359699964524} ({'split': 'test'})\n",
      "Epoch 015\n",
      "accuracy: {'epoch': 15, 'value': 0.9901000000190735} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 15, 'value': 0.031597539637088785} ({'split': 'train'})\n",
      "accuracy: {'epoch': 15, 'value': 0.8654000461101532} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 15, 'value': 0.534986475110054} ({'split': 'test'})\n",
      "Epoch 016\n",
      "accuracy: {'epoch': 16, 'value': 0.9909400000381472} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 16, 'value': 0.030707070574760433} ({'split': 'train'})\n",
      "accuracy: {'epoch': 16, 'value': 0.8600000381469727} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 16, 'value': 0.5582717955112457} ({'split': 'test'})\n",
      "Epoch 017\n",
      "accuracy: {'epoch': 17, 'value': 0.9877199999809266} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 17, 'value': 0.04048573212623596} ({'split': 'train'})\n",
      "accuracy: {'epoch': 17, 'value': 0.8595000386238099} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 17, 'value': 0.6174550503492355} ({'split': 'test'})\n",
      "Epoch 018\n",
      "accuracy: {'epoch': 18, 'value': 0.9858799999999998} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 18, 'value': 0.04454881580471993} ({'split': 'train'})\n",
      "accuracy: {'epoch': 18, 'value': 0.8615000486373903} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 18, 'value': 0.5270050674676896} ({'split': 'test'})\n",
      "Epoch 019\n",
      "accuracy: {'epoch': 19, 'value': 0.98844000005722} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 19, 'value': 0.03719635285377503} ({'split': 'train'})\n",
      "accuracy: {'epoch': 19, 'value': 0.8633000373840332} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 19, 'value': 0.5528099119663238} ({'split': 'test'})\n",
      "Epoch 020\n",
      "accuracy: {'epoch': 20, 'value': 0.98368} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 20, 'value': 0.05335847288668157} ({'split': 'train'})\n",
      "accuracy: {'epoch': 20, 'value': 0.8493000388145446} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 20, 'value': 0.5953926086425781} ({'split': 'test'})\n",
      "Epoch 021\n",
      "accuracy: {'epoch': 21, 'value': 0.9876200000190731} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 21, 'value': 0.03861548553526401} ({'split': 'train'})\n",
      "accuracy: {'epoch': 21, 'value': 0.8639000415802002} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 21, 'value': 0.5931665837764739} ({'split': 'test'})\n",
      "Epoch 022\n",
      "accuracy: {'epoch': 22, 'value': 0.9881799999999998} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 22, 'value': 0.03795458248198036} ({'split': 'train'})\n",
      "accuracy: {'epoch': 22, 'value': 0.8585000455379487} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 22, 'value': 0.6157381594181062} ({'split': 'test'})\n",
      "Epoch 023\n",
      "accuracy: {'epoch': 23, 'value': 0.9874800000190737} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 23, 'value': 0.040454833548069} ({'split': 'train'})\n",
      "accuracy: {'epoch': 23, 'value': 0.8657000422477722} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 23, 'value': 0.5103138118982316} ({'split': 'test'})\n",
      "Epoch 024\n",
      "accuracy: {'epoch': 24, 'value': 0.9894800000000002} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 24, 'value': 0.034635194418281307} ({'split': 'train'})\n",
      "accuracy: {'epoch': 24, 'value': 0.8574000358581543} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 24, 'value': 0.6252937495708467} ({'split': 'test'})\n",
      "Epoch 025\n",
      "accuracy: {'epoch': 25, 'value': 0.9925599999999999} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 25, 'value': 0.025624177465736857} ({'split': 'train'})\n",
      "accuracy: {'epoch': 25, 'value': 0.8613000392913819} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 25, 'value': 0.6069830507040024} ({'split': 'test'})\n",
      "Epoch 026\n",
      "accuracy: {'epoch': 26, 'value': 0.9889800000190734} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 26, 'value': 0.03657554663658142} ({'split': 'train'})\n",
      "accuracy: {'epoch': 26, 'value': 0.8533000409603119} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 26, 'value': 0.5854728460311889} ({'split': 'test'})\n",
      "Epoch 027\n",
      "accuracy: {'epoch': 27, 'value': 0.9917400000190733} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 27, 'value': 0.02638020851135253} ({'split': 'train'})\n",
      "accuracy: {'epoch': 27, 'value': 0.8586000442504882} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 27, 'value': 0.5831950426101685} ({'split': 'test'})\n",
      "Epoch 028\n",
      "accuracy: {'epoch': 28, 'value': 0.9920600000000003} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 28, 'value': 0.02743642715990544} ({'split': 'train'})\n",
      "accuracy: {'epoch': 28, 'value': 0.8461000382900239} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 28, 'value': 0.615286123752594} ({'split': 'test'})\n",
      "Epoch 029\n",
      "accuracy: {'epoch': 29, 'value': 0.9903400000572203} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 29, 'value': 0.030538869609832764} ({'split': 'train'})\n",
      "accuracy: {'epoch': 29, 'value': 0.8526000440120697} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 29, 'value': 0.6188903331756591} ({'split': 'test'})\n",
      "mean_test_accuracy <cifar_utils.accumulators.Mean object at 0x7f500e286520>\n",
      "QWERTY: Enter Cifar 2, acc 0.8778000414371491\n",
      "Retraining model :  model_1\n",
      "num_epochs---------------+++))))))+++ 30\n",
      "num_epochs---------------+++))))))+++ 30\n",
      "lr is  0.05\n",
      "number of epochs would be  30\n",
      "num_epochs-------------- 30\n",
      "Epoch 000\n",
      "accuracy: {'epoch': 0, 'value': 0.9147000000572203} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 0, 'value': 0.260452842645645} ({'split': 'train'})\n",
      "accuracy: {'epoch': 0, 'value': 0.8491000413894654} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 0, 'value': 0.4883124232292176} ({'split': 'test'})\n",
      "Epoch 001\n",
      "accuracy: {'epoch': 1, 'value': 0.9420000000190729} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 1, 'value': 0.17121312939643862} ({'split': 'train'})\n",
      "accuracy: {'epoch': 1, 'value': 0.8481000363826751} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 1, 'value': 0.5072705805301666} ({'split': 'test'})\n",
      "Epoch 002\n",
      "accuracy: {'epoch': 2, 'value': 0.9565000000000002} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 2, 'value': 0.13053088589906703} ({'split': 'train'})\n",
      "accuracy: {'epoch': 2, 'value': 0.8478000402450562} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 2, 'value': 0.565441381931305} ({'split': 'test'})\n",
      "Epoch 003\n",
      "accuracy: {'epoch': 3, 'value': 0.9659000000381468} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 3, 'value': 0.10287666073322292} ({'split': 'train'})\n",
      "accuracy: {'epoch': 3, 'value': 0.8527000308036804} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 3, 'value': 0.5231015652418137} ({'split': 'test'})\n",
      "Epoch 004\n",
      "accuracy: {'epoch': 4, 'value': 0.9748999999809266} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 4, 'value': 0.07679137050151828} ({'split': 'train'})\n",
      "accuracy: {'epoch': 4, 'value': 0.8512000501155854} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 4, 'value': 0.5938333451747895} ({'split': 'test'})\n",
      "Epoch 005\n",
      "accuracy: {'epoch': 5, 'value': 0.97592} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 5, 'value': 0.07297497313499446} ({'split': 'train'})\n",
      "accuracy: {'epoch': 5, 'value': 0.861100047826767} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 5, 'value': 0.5337436527013778} ({'split': 'test'})\n",
      "Epoch 006\n",
      "accuracy: {'epoch': 6, 'value': 0.9805600000381468} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 6, 'value': 0.060080642378330225} ({'split': 'train'})\n",
      "accuracy: {'epoch': 6, 'value': 0.8658000409603118} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 6, 'value': 0.5252657413482666} ({'split': 'test'})\n",
      "Epoch 007\n",
      "accuracy: {'epoch': 7, 'value': 0.9866199999999996} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 7, 'value': 0.041937977937459965} ({'split': 'train'})\n",
      "accuracy: {'epoch': 7, 'value': 0.8633000373840333} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 7, 'value': 0.5633464932441713} ({'split': 'test'})\n",
      "Epoch 008\n",
      "accuracy: {'epoch': 8, 'value': 0.9872399999999997} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 8, 'value': 0.040232415927052505} ({'split': 'train'})\n",
      "accuracy: {'epoch': 8, 'value': 0.8692000508308411} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 8, 'value': 0.5343274444341659} ({'split': 'test'})\n",
      "Epoch 009\n",
      "accuracy: {'epoch': 9, 'value': 0.9875000000190733} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 9, 'value': 0.03961496969223023} ({'split': 'train'})\n",
      "accuracy: {'epoch': 9, 'value': 0.8769000351428984} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 9, 'value': 0.48420770764350896} ({'split': 'test'})\n",
      "Epoch 010\n",
      "accuracy: {'epoch': 10, 'value': 0.9887000000000002} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 10, 'value': 0.037295006837546836} ({'split': 'train'})\n",
      "accuracy: {'epoch': 10, 'value': 0.8684000432491302} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 10, 'value': 0.5331309109926224} ({'split': 'test'})\n",
      "Epoch 011\n",
      "accuracy: {'epoch': 11, 'value': 0.9895999999999997} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 11, 'value': 0.03426178315222263} ({'split': 'train'})\n",
      "accuracy: {'epoch': 11, 'value': 0.8629000365734101} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 11, 'value': 0.5369384407997132} ({'split': 'test'})\n",
      "Epoch 012\n",
      "accuracy: {'epoch': 12, 'value': 0.9916400000190733} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 12, 'value': 0.0293814580720663} ({'split': 'train'})\n",
      "accuracy: {'epoch': 12, 'value': 0.8669000387191772} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 12, 'value': 0.52722969353199} ({'split': 'test'})\n",
      "Epoch 013\n",
      "accuracy: {'epoch': 13, 'value': 0.9889599999809262} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 13, 'value': 0.035142258057594286} ({'split': 'train'})\n",
      "accuracy: {'epoch': 13, 'value': 0.855500042438507} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 13, 'value': 0.6242181479930877} ({'split': 'test'})\n",
      "Epoch 014\n",
      "accuracy: {'epoch': 14, 'value': 0.9881200000190735} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 14, 'value': 0.038431129592657064} ({'split': 'train'})\n",
      "accuracy: {'epoch': 14, 'value': 0.8573000431060791} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 14, 'value': 0.6184102654457093} ({'split': 'test'})\n",
      "Epoch 015\n",
      "accuracy: {'epoch': 15, 'value': 0.9903800000190738} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 15, 'value': 0.03163159935951232} ({'split': 'train'})\n",
      "accuracy: {'epoch': 15, 'value': 0.8628000438213348} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 15, 'value': 0.5641337037086487} ({'split': 'test'})\n",
      "Epoch 016\n",
      "accuracy: {'epoch': 16, 'value': 0.9865800000190734} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 16, 'value': 0.04232359866619112} ({'split': 'train'})\n",
      "accuracy: {'epoch': 16, 'value': 0.8512000441551208} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 16, 'value': 0.6579134702682495} ({'split': 'test'})\n",
      "Epoch 017\n",
      "accuracy: {'epoch': 17, 'value': 0.9901000000190733} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 17, 'value': 0.03285635039687157} ({'split': 'train'})\n",
      "accuracy: {'epoch': 17, 'value': 0.871500039100647} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 17, 'value': 0.5071184754371644} ({'split': 'test'})\n",
      "Epoch 018\n",
      "accuracy: {'epoch': 18, 'value': 0.9909800000190733} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 18, 'value': 0.03148969022452831} ({'split': 'train'})\n",
      "accuracy: {'epoch': 18, 'value': 0.8588000416755676} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 18, 'value': 0.5696226537227631} ({'split': 'test'})\n",
      "Epoch 019\n",
      "accuracy: {'epoch': 19, 'value': 0.9903400000190736} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 19, 'value': 0.03300848416686057} ({'split': 'train'})\n",
      "accuracy: {'epoch': 19, 'value': 0.8708000361919404} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 19, 'value': 0.5191742807626724} ({'split': 'test'})\n",
      "Epoch 020\n",
      "accuracy: {'epoch': 20, 'value': 0.9911000000190737} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 20, 'value': 0.029930600541830058} ({'split': 'train'})\n",
      "accuracy: {'epoch': 20, 'value': 0.8609000384807588} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 20, 'value': 0.574293303489685} ({'split': 'test'})\n",
      "Epoch 021\n",
      "accuracy: {'epoch': 21, 'value': 0.9919400000190735} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 21, 'value': 0.02680735088706016} ({'split': 'train'})\n",
      "accuracy: {'epoch': 21, 'value': 0.8684000372886658} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 21, 'value': 0.5294922918081284} ({'split': 'test'})\n",
      "Epoch 022\n",
      "accuracy: {'epoch': 22, 'value': 0.9908800000000003} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 22, 'value': 0.0298619554179907} ({'split': 'train'})\n",
      "accuracy: {'epoch': 22, 'value': 0.8572000443935395} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 22, 'value': 0.6321509420871735} ({'split': 'test'})\n",
      "Epoch 023\n",
      "accuracy: {'epoch': 23, 'value': 0.9879800000381469} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 23, 'value': 0.039903861670494076} ({'split': 'train'})\n",
      "accuracy: {'epoch': 23, 'value': 0.8627000391483307} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 23, 'value': 0.5108248859643937} ({'split': 'test'})\n",
      "Epoch 024\n",
      "accuracy: {'epoch': 24, 'value': 0.9842800000381471} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 24, 'value': 0.049522915165424355} ({'split': 'train'})\n",
      "accuracy: {'epoch': 24, 'value': 0.852200037240982} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 24, 'value': 0.6426631152629851} ({'split': 'test'})\n",
      "Epoch 025\n",
      "accuracy: {'epoch': 25, 'value': 0.9872400000381474} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 25, 'value': 0.041020989079475365} ({'split': 'train'})\n",
      "accuracy: {'epoch': 25, 'value': 0.8601000368595123} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 25, 'value': 0.597728431224823} ({'split': 'test'})\n",
      "Epoch 026\n",
      "accuracy: {'epoch': 26, 'value': 0.9852400000000003} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 26, 'value': 0.04650070916652678} ({'split': 'train'})\n",
      "accuracy: {'epoch': 26, 'value': 0.8629000425338744} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 26, 'value': 0.5812793880701065} ({'split': 'test'})\n",
      "Epoch 027\n",
      "accuracy: {'epoch': 27, 'value': 0.9882600000000002} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 27, 'value': 0.03787535741865635} ({'split': 'train'})\n",
      "accuracy: {'epoch': 27, 'value': 0.8568000435829163} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 27, 'value': 0.6554000079631807} ({'split': 'test'})\n",
      "Epoch 028\n",
      "accuracy: {'epoch': 28, 'value': 0.9923200000381465} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 28, 'value': 0.026261712193489065} ({'split': 'train'})\n",
      "accuracy: {'epoch': 28, 'value': 0.8490000486373901} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 28, 'value': 0.635520899295807} ({'split': 'test'})\n",
      "Epoch 029\n",
      "accuracy: {'epoch': 29, 'value': 0.9891400000381466} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 29, 'value': 0.03485698883771895} ({'split': 'train'})\n",
      "accuracy: {'epoch': 29, 'value': 0.8564000427722931} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 29, 'value': 0.6065239787101746} ({'split': 'test'})\n",
      "mean_test_accuracy <cifar_utils.accumulators.Mean object at 0x7f5012279e80>\n",
      "QWERTY: Enter Cifar 2, acc 0.8769000351428984\n",
      "Retraining model :  geometric\n",
      "num_epochs---------------+++))))))+++ 30\n",
      "num_epochs---------------+++))))))+++ 30\n",
      "lr is  0.05\n",
      "number of epochs would be  30\n",
      "num_epochs-------------- 30\n",
      "Epoch 000\n",
      "accuracy: {'epoch': 0, 'value': 0.14946000000953685} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 0, 'value': 2.402203867721559} ({'split': 'train'})\n",
      "accuracy: {'epoch': 0, 'value': 0.14160000532865524} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 0, 'value': 2.2895836353302004} ({'split': 'test'})\n",
      "Epoch 001\n",
      "accuracy: {'epoch': 1, 'value': 0.13188000000000008} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 1, 'value': 2.21680511428833} ({'split': 'train'})\n",
      "accuracy: {'epoch': 1, 'value': 0.11410000473260878} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 1, 'value': 2.2076149225234984} ({'split': 'test'})\n",
      "Epoch 002\n",
      "accuracy: {'epoch': 2, 'value': 0.22274000002861022} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 2, 'value': 2.0024989862442015} ({'split': 'train'})\n",
      "accuracy: {'epoch': 2, 'value': 0.28070001602172856} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 2, 'value': 1.8732013702392578} ({'split': 'test'})\n",
      "Epoch 003\n",
      "accuracy: {'epoch': 3, 'value': 0.31368000002861024} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 3, 'value': 1.8004127941131594} ({'split': 'train'})\n",
      "accuracy: {'epoch': 3, 'value': 0.37000001668930055} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 3, 'value': 1.6761927127838137} ({'split': 'test'})\n",
      "Epoch 004\n",
      "accuracy: {'epoch': 4, 'value': 0.39120000001907346} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 4, 'value': 1.6128876578140257} ({'split': 'train'})\n",
      "accuracy: {'epoch': 4, 'value': 0.41570002138614653} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 4, 'value': 1.5744562625885008} ({'split': 'test'})\n",
      "Epoch 005\n",
      "accuracy: {'epoch': 5, 'value': 0.4624800000286102} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 5, 'value': 1.4608652771759032} ({'split': 'train'})\n",
      "accuracy: {'epoch': 5, 'value': 0.4782000213861466} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 5, 'value': 1.4313624739646913} ({'split': 'test'})\n",
      "Epoch 006\n",
      "accuracy: {'epoch': 6, 'value': 0.5115600000000002} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 6, 'value': 1.3477421820449826} ({'split': 'train'})\n",
      "accuracy: {'epoch': 6, 'value': 0.49260002076625825} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 6, 'value': 1.5001825928688048} ({'split': 'test'})\n",
      "Epoch 007\n",
      "accuracy: {'epoch': 7, 'value': 0.5543200000000003} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 7, 'value': 1.2517400452423086} ({'split': 'train'})\n",
      "accuracy: {'epoch': 7, 'value': 0.5410000205039978} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 7, 'value': 1.3023215055465696} ({'split': 'test'})\n",
      "Epoch 008\n",
      "accuracy: {'epoch': 8, 'value': 0.6037800000572204} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 8, 'value': 1.118882740154267} ({'split': 'train'})\n",
      "accuracy: {'epoch': 8, 'value': 0.5642000257968902} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 8, 'value': 1.2607917904853818} ({'split': 'test'})\n",
      "Epoch 009\n",
      "accuracy: {'epoch': 9, 'value': 0.635040000038147} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 9, 'value': 1.03784497642517} ({'split': 'train'})\n",
      "accuracy: {'epoch': 9, 'value': 0.5779000282287597} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 9, 'value': 1.220839250087738} ({'split': 'test'})\n",
      "Epoch 010\n",
      "accuracy: {'epoch': 10, 'value': 0.6669000000000002} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 10, 'value': 0.9540842436218261} ({'split': 'train'})\n",
      "accuracy: {'epoch': 10, 'value': 0.6186000347137451} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 10, 'value': 1.105293095111847} ({'split': 'test'})\n",
      "Epoch 011\n",
      "accuracy: {'epoch': 11, 'value': 0.6985800000190733} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 11, 'value': 0.8587545590591431} ({'split': 'train'})\n",
      "accuracy: {'epoch': 11, 'value': 0.6504000246524811} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 11, 'value': 1.0509942173957825} ({'split': 'test'})\n",
      "Epoch 012\n",
      "accuracy: {'epoch': 12, 'value': 0.7274800000190733} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 12, 'value': 0.7801955489921566} ({'split': 'train'})\n",
      "accuracy: {'epoch': 12, 'value': 0.6716000318527221} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 12, 'value': 0.9748183429241182} ({'split': 'test'})\n",
      "Epoch 013\n",
      "accuracy: {'epoch': 13, 'value': 0.7520799999809265} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 13, 'value': 0.7130848260879515} ({'split': 'train'})\n",
      "accuracy: {'epoch': 13, 'value': 0.6735000371932983} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 13, 'value': 0.9858799874782562} ({'split': 'test'})\n",
      "Epoch 014\n",
      "accuracy: {'epoch': 14, 'value': 0.7737200000572209} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 14, 'value': 0.6489729078578944} ({'split': 'train'})\n",
      "accuracy: {'epoch': 14, 'value': 0.6759000301361083} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 14, 'value': 1.0039806723594664} ({'split': 'test'})\n",
      "Epoch 015\n",
      "accuracy: {'epoch': 15, 'value': 0.7932000000190736} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 15, 'value': 0.59233730670929} ({'split': 'train'})\n",
      "accuracy: {'epoch': 15, 'value': 0.6828000366687774} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 15, 'value': 0.9824602425098419} ({'split': 'test'})\n",
      "Epoch 016\n",
      "accuracy: {'epoch': 16, 'value': 0.8162599999999997} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 16, 'value': 0.5288584117126466} ({'split': 'train'})\n",
      "accuracy: {'epoch': 16, 'value': 0.6465000331401823} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 16, 'value': 1.162063717842102} ({'split': 'test'})\n",
      "Epoch 017\n",
      "accuracy: {'epoch': 17, 'value': 0.8310400000381463} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 17, 'value': 0.48297335572242717} ({'split': 'train'})\n",
      "accuracy: {'epoch': 17, 'value': 0.6310000360012056} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 17, 'value': 1.3088776230812074} ({'split': 'test'})\n",
      "Epoch 018\n",
      "accuracy: {'epoch': 18, 'value': 0.8429000000190733} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 18, 'value': 0.45160695908546455} ({'split': 'train'})\n",
      "accuracy: {'epoch': 18, 'value': 0.671500027179718} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 18, 'value': 1.1093480467796328} ({'split': 'test'})\n",
      "Epoch 019\n",
      "accuracy: {'epoch': 19, 'value': 0.8642000000000001} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 19, 'value': 0.39410776943206793} ({'split': 'train'})\n",
      "accuracy: {'epoch': 19, 'value': 0.6760000288486481} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 19, 'value': 1.1803273200988769} ({'split': 'test'})\n",
      "Epoch 020\n",
      "accuracy: {'epoch': 20, 'value': 0.8729599999809261} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 20, 'value': 0.36627115371704083} ({'split': 'train'})\n",
      "accuracy: {'epoch': 20, 'value': 0.6764000296592713} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 20, 'value': 1.2001104831695557} ({'split': 'test'})\n",
      "Epoch 021\n",
      "accuracy: {'epoch': 21, 'value': 0.8848600000572207} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 21, 'value': 0.3330754294490815} ({'split': 'train'})\n",
      "accuracy: {'epoch': 21, 'value': 0.6586000323295593} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 21, 'value': 1.3612267494201662} ({'split': 'test'})\n",
      "Epoch 022\n",
      "accuracy: {'epoch': 22, 'value': 0.8893000000190739} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 22, 'value': 0.32212121148109446} ({'split': 'train'})\n",
      "accuracy: {'epoch': 22, 'value': 0.682500046491623} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 22, 'value': 1.248389256000519} ({'split': 'test'})\n",
      "Epoch 023\n",
      "accuracy: {'epoch': 23, 'value': 0.9050799999809265} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 23, 'value': 0.2800083024215699} ({'split': 'train'})\n",
      "accuracy: {'epoch': 23, 'value': 0.6865000367164612} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 23, 'value': 1.2551151275634767} ({'split': 'test'})\n",
      "Epoch 024\n",
      "accuracy: {'epoch': 24, 'value': 0.9128000000572204} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 24, 'value': 0.25373248421669015} ({'split': 'train'})\n",
      "accuracy: {'epoch': 24, 'value': 0.6848000228404999} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 24, 'value': 1.3577993750572204} ({'split': 'test'})\n",
      "Epoch 025\n",
      "accuracy: {'epoch': 25, 'value': 0.9211} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 25, 'value': 0.2323446141004563} ({'split': 'train'})\n",
      "accuracy: {'epoch': 25, 'value': 0.6540000200271607} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 25, 'value': 1.5130586624145508} ({'split': 'test'})\n",
      "Epoch 026\n",
      "accuracy: {'epoch': 26, 'value': 0.9155600000381472} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 26, 'value': 0.2507215150260925} ({'split': 'train'})\n",
      "accuracy: {'epoch': 26, 'value': 0.6750000298023223} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 26, 'value': 1.2971309900283812} ({'split': 'test'})\n",
      "Epoch 027\n",
      "accuracy: {'epoch': 27, 'value': 0.9289600000381474} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 27, 'value': 0.21319688903331757} ({'split': 'train'})\n",
      "accuracy: {'epoch': 27, 'value': 0.6848000347614288} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 27, 'value': 1.322657871246338} ({'split': 'test'})\n",
      "Epoch 028\n",
      "accuracy: {'epoch': 28, 'value': 0.9326599999809264} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 28, 'value': 0.1988973051643371} ({'split': 'train'})\n",
      "accuracy: {'epoch': 28, 'value': 0.6618000268936157} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 28, 'value': 1.5572775602340698} ({'split': 'test'})\n",
      "Epoch 029\n",
      "accuracy: {'epoch': 29, 'value': 0.9331600000572204} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 29, 'value': 0.20300840456962582} ({'split': 'train'})\n",
      "accuracy: {'epoch': 29, 'value': 0.6828000366687774} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 29, 'value': 1.3678458333015442} ({'split': 'test'})\n",
      "mean_test_accuracy <cifar_utils.accumulators.Mean object at 0x7f5012259880>\n",
      "QWERTY: Enter Cifar 2, acc 0.6865000367164612\n",
      "Retraining model :  naive_averaging\n",
      "num_epochs---------------+++))))))+++ 30\n",
      "num_epochs---------------+++))))))+++ 30\n",
      "lr is  0.05\n",
      "number of epochs would be  30\n",
      "num_epochs-------------- 30\n",
      "Epoch 000\n",
      "accuracy: {'epoch': 0, 'value': 0.38526000005722044} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 0, 'value': 1.6991427669525145} ({'split': 'train'})\n",
      "accuracy: {'epoch': 0, 'value': 0.5624000251293182} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 0, 'value': 1.2830089688301085} ({'split': 'test'})\n",
      "Epoch 001\n",
      "accuracy: {'epoch': 1, 'value': 0.7115200000381467} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 1, 'value': 0.8496749090003963} ({'split': 'train'})\n",
      "accuracy: {'epoch': 1, 'value': 0.7668000280857085} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 1, 'value': 0.7167498826980591} ({'split': 'test'})\n",
      "Epoch 002\n",
      "accuracy: {'epoch': 2, 'value': 0.8151600000000001} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 2, 'value': 0.5451683435249326} ({'split': 'train'})\n",
      "accuracy: {'epoch': 2, 'value': 0.8122000396251678} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 2, 'value': 0.5620096176862717} ({'split': 'test'})\n",
      "Epoch 003\n",
      "accuracy: {'epoch': 3, 'value': 0.8590000000190733} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 3, 'value': 0.41685803015708905} ({'split': 'train'})\n",
      "accuracy: {'epoch': 3, 'value': 0.8218000292778014} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 3, 'value': 0.5612656474113464} ({'split': 'test'})\n",
      "Epoch 004\n",
      "accuracy: {'epoch': 4, 'value': 0.8943000000572208} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 4, 'value': 0.3088664338493347} ({'split': 'train'})\n",
      "accuracy: {'epoch': 4, 'value': 0.81640003323555} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 4, 'value': 0.5741850316524506} ({'split': 'test'})\n",
      "Epoch 005\n",
      "accuracy: {'epoch': 5, 'value': 0.9227200000000001} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 5, 'value': 0.22540445897102362} ({'split': 'train'})\n",
      "accuracy: {'epoch': 5, 'value': 0.8235000491142273} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 5, 'value': 0.5996475994586944} ({'split': 'test'})\n",
      "Epoch 006\n",
      "accuracy: {'epoch': 6, 'value': 0.9419800000381466} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 6, 'value': 0.1716551009917259} ({'split': 'train'})\n",
      "accuracy: {'epoch': 6, 'value': 0.8455000281333923} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 6, 'value': 0.5556852847337722} ({'split': 'test'})\n",
      "Epoch 007\n",
      "accuracy: {'epoch': 7, 'value': 0.95204} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 7, 'value': 0.14259729103088384} ({'split': 'train'})\n",
      "accuracy: {'epoch': 7, 'value': 0.8311000406742096} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 7, 'value': 0.5892740845680237} ({'split': 'test'})\n",
      "Epoch 008\n",
      "accuracy: {'epoch': 8, 'value': 0.956940000038147} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 8, 'value': 0.12547837949991228} ({'split': 'train'})\n",
      "accuracy: {'epoch': 8, 'value': 0.8370000422000885} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 8, 'value': 0.6135058403015136} ({'split': 'test'})\n",
      "Epoch 009\n",
      "accuracy: {'epoch': 9, 'value': 0.9669599999809266} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 9, 'value': 0.09963233341217045} ({'split': 'train'})\n",
      "accuracy: {'epoch': 9, 'value': 0.8351000368595124} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 9, 'value': 0.6097926259040833} ({'split': 'test'})\n",
      "Epoch 010\n",
      "accuracy: {'epoch': 10, 'value': 0.9674200000572207} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 10, 'value': 0.09710951803207397} ({'split': 'train'})\n",
      "accuracy: {'epoch': 10, 'value': 0.8384000480175018} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 10, 'value': 0.6548097372055053} ({'split': 'test'})\n",
      "Epoch 011\n",
      "accuracy: {'epoch': 11, 'value': 0.9720800000381468} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 11, 'value': 0.08116553901672366} ({'split': 'train'})\n",
      "accuracy: {'epoch': 11, 'value': 0.8303000390529632} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 11, 'value': 0.7637226581573486} ({'split': 'test'})\n",
      "Epoch 012\n",
      "accuracy: {'epoch': 12, 'value': 0.9711400000190734} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 12, 'value': 0.08323176696538931} ({'split': 'train'})\n",
      "accuracy: {'epoch': 12, 'value': 0.8482000470161438} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 12, 'value': 0.6027654051780701} ({'split': 'test'})\n",
      "Epoch 013\n"
     ]
    }
   ],
   "source": [
    "!python main.py --gpu-id 0 --model-name vgg11_nobias --n-epochs 30 --save-result-file sample.csv \\\n",
    "--sweep-name exp_sample --correction --ground-metric euclidean --weight-stats \\\n",
    "--geom-ensemble-type wts --ground-metric-normalize none --sweep-id 90 \\\n",
    "--ckpt-type best --dataset Cifar10 --ground-metric-eff --recheck-cifar --activation-seed {activation_seed} \\\n",
    "--prelu-acts --past-correction --not-squared --exact\\\n",
    "--learning-rate 0.00003\\\n",
    "--momentum 0.5\\\n",
    "--batch-size-train 256 --experiment-dir {experiment_dir}\\\n",
    "--prunint-rate {prunint_rate}\\\n",
    "--run-mode {run_mode}\\\n",
    "--to-download\\\n",
    "--load-model-dir {load_model_dir}\\\n",
    "--retrain 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6dd3f2b3-a28e-48de-9734-3c48c3d6514d",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------- Setting up parameters -------\n",
      "dumping parameters at  /root/autodl-tmp/OTfusion_pruning/otfusion/exp_sample/configurations\n",
      "The parameters are: \n",
      " Namespace(act_bug=False, act_num_samples=100, activation_histograms=False, activation_mode=None, activation_seed=21, alpha=0.7, baseroot='/root/autodl-tmp/OTfusion_pruning/otfusion', batch_size_test=1000, batch_size_train=256, center_acts=False, choice='0 2 4 6 8', cifar_style_data=False, ckpt_type='best', clip_gm=False, clip_max=5, clip_min=0, config_dir='/root/autodl-tmp/OTfusion_pruning/otfusion/exp_sample/configurations', config_file=None, correction=True, csv_dir='/root/autodl-tmp/OTfusion_pruning/otfusion/exp_sample/csv', dataset='Cifar10', debug=False, deprecated=None, deterministic=False, diff_init=False, disable_bias=True, dist_epochs=60, dist_normalize=False, dump_final_models=False, dump_model=False, enable_dropout=False, ensemble_step=0.5, eval_aligned=False, exact=True, exp_name='exp_2023-09-16_17-12-35_115737', experiment_dir='experiment2', geom_ensemble_type='wts', gpu_id=0, gromov=False, gromov_loss='square_loss', ground_metric='euclidean', ground_metric_eff=True, ground_metric_normalize='none', handle_skips=False, importance=None, learning_rate=0.0001, load_model_dir='./cifar_models', load_models='', log_interval=100, model_name='vgg11_nobias', momentum=0.5, n_epochs=10, no_random_trainloaders=False, normalize_acts=False, normalize_wts=False, not_squared=True, num_hidden_nodes=400, num_hidden_nodes1=400, num_hidden_nodes2=200, num_hidden_nodes3=100, num_hidden_nodes4=50, num_models=2, options_type='generic', partial_reshape=False, partition_dataloader=-1, partition_type='labels', past_correction=True, personal_class_idx=9, personal_split_frac=0.1, pool_acts=False, pool_relu=False, prediction_wts=False, prelu_acts=True, print_distances=False, proper_marginals=False, prunint_rate=0.2, recheck_acc=False, recheck_cifar=True, reg=0.01, reg_m=0.001, reinit_trainloaders=False, result_dir='/root/autodl-tmp/OTfusion_pruning/otfusion/exp_sample/results', retrain=30, retrain_avg_only=False, retrain_geometric_only=False, retrain_lr_decay=-1, retrain_lr_decay_epochs=None, retrain_lr_decay_factor=None, retrain_seed=-1, rootdir='/root/autodl-tmp/OTfusion_pruning/otfusion/exp_sample', run_mode='finetune', same_model=-1, save_result_file='sample.csv', second_model_name=None, sinkhorn_type='normal', skip_last_layer=False, skip_last_layer_type='average', skip_personal_idx=False, skip_retrain=-1, softmax_temperature=1, standardize_acts=False, sweep_id=90, sweep_name='exp_sample', temperature=20, tensorboard=False, tensorboard_root='./tensorboard', timestamp='2023-09-16_17-12-35_115737', tmap_stats=False, to_download=True, transform_acts=False, unbalanced=False, update_acts=False, weight_stats=True, width_ratio=1)\n",
      "refactored get_config\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "in _make_layers [Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), AvgPool2d(kernel_size=1, stride=1, padding=0)]\n",
      "Relu Inplace is  False\n",
      "model parameters are \n",
      " [torch.Size([64, 3, 3, 3]), torch.Size([128, 64, 3, 3]), torch.Size([256, 128, 3, 3]), torch.Size([256, 256, 3, 3]), torch.Size([512, 256, 3, 3]), torch.Size([512, 512, 3, 3]), torch.Size([512, 512, 3, 3]), torch.Size([512, 512, 3, 3]), torch.Size([10, 512])]\n",
      "in _make_layers [Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), AvgPool2d(kernel_size=1, stride=1, padding=0)]\n",
      "Relu Inplace is  False\n",
      "model parameters are \n",
      " [torch.Size([64, 3, 3, 3]), torch.Size([128, 64, 3, 3]), torch.Size([256, 128, 3, 3]), torch.Size([256, 256, 3, 3]), torch.Size([512, 256, 3, 3]), torch.Size([512, 512, 3, 3]), torch.Size([512, 512, 3, 3]), torch.Size([512, 512, 3, 3]), torch.Size([10, 512])]\n",
      "in _make_layers [Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), AvgPool2d(kernel_size=1, stride=1, padding=0)]\n",
      "Relu Inplace is  False\n",
      "model parameters are \n",
      " [torch.Size([64, 3, 3, 3]), torch.Size([128, 64, 3, 3]), torch.Size([256, 128, 3, 3]), torch.Size([256, 256, 3, 3]), torch.Size([512, 256, 3, 3]), torch.Size([512, 512, 3, 3]), torch.Size([512, 512, 3, 3]), torch.Size([512, 512, 3, 3]), torch.Size([10, 512])]\n",
      "in _make_layers [Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), AvgPool2d(kernel_size=1, stride=1, padding=0)]\n",
      "Relu Inplace is  False\n",
      "model parameters are \n",
      " [torch.Size([64, 3, 3, 3]), torch.Size([128, 64, 3, 3]), torch.Size([256, 128, 3, 3]), torch.Size([256, 256, 3, 3]), torch.Size([512, 256, 3, 3]), torch.Size([512, 512, 3, 3]), torch.Size([512, 512, 3, 3]), torch.Size([512, 512, 3, 3]), torch.Size([10, 512])]\n",
      "Train Epoch: 1 [0/50000 (0%)]\tLoss: 0.104261\n",
      "Train Epoch: 1 [25600/50000 (51%)]\tLoss: 0.096386\n",
      "\n",
      "--------- Testing in global mode ---------\n",
      "size of test_loader dataset:  10000\n",
      "\n",
      "Test set: Avg. loss: 0.0003, Accuracy: 9038/10000 (90%)\n",
      "\n",
      "Train Epoch: 2 [0/50000 (0%)]\tLoss: 0.073829\n",
      "Train Epoch: 2 [25600/50000 (51%)]\tLoss: 0.090221\n",
      "\n",
      "--------- Testing in global mode ---------\n",
      "size of test_loader dataset:  10000\n",
      "\n",
      "Test set: Avg. loss: 0.0003, Accuracy: 9043/10000 (90%)\n",
      "\n",
      "Train Epoch: 3 [0/50000 (0%)]\tLoss: 0.064929\n",
      "Train Epoch: 3 [25600/50000 (51%)]\tLoss: 0.059550\n",
      "\n",
      "--------- Testing in global mode ---------\n",
      "size of test_loader dataset:  10000\n",
      "\n",
      "Test set: Avg. loss: 0.0003, Accuracy: 9050/10000 (90%)\n",
      "\n",
      "Train Epoch: 4 [0/50000 (0%)]\tLoss: 0.071164\n",
      "Train Epoch: 4 [25600/50000 (51%)]\tLoss: 0.056062\n",
      "\n",
      "--------- Testing in global mode ---------\n",
      "size of test_loader dataset:  10000\n",
      "\n",
      "Test set: Avg. loss: 0.0003, Accuracy: 9048/10000 (90%)\n",
      "\n",
      "Train Epoch: 5 [0/50000 (0%)]\tLoss: 0.069778\n",
      "Train Epoch: 5 [25600/50000 (51%)]\tLoss: 0.068953\n",
      "\n",
      "--------- Testing in global mode ---------\n",
      "size of test_loader dataset:  10000\n",
      "\n",
      "Test set: Avg. loss: 0.0003, Accuracy: 9040/10000 (90%)\n",
      "\n",
      "Train Epoch: 6 [0/50000 (0%)]\tLoss: 0.064667\n",
      "Train Epoch: 6 [25600/50000 (51%)]\tLoss: 0.064324\n",
      "\n",
      "--------- Testing in global mode ---------\n",
      "size of test_loader dataset:  10000\n",
      "\n",
      "Test set: Avg. loss: 0.0003, Accuracy: 9040/10000 (90%)\n",
      "\n",
      "Train Epoch: 7 [0/50000 (0%)]\tLoss: 0.070104\n",
      "Train Epoch: 7 [25600/50000 (51%)]\tLoss: 0.074657\n",
      "\n",
      "--------- Testing in global mode ---------\n",
      "size of test_loader dataset:  10000\n",
      "\n",
      "Test set: Avg. loss: 0.0003, Accuracy: 9044/10000 (90%)\n",
      "\n",
      "Train Epoch: 8 [0/50000 (0%)]\tLoss: 0.056472\n",
      "Train Epoch: 8 [25600/50000 (51%)]\tLoss: 0.074212\n",
      "\n",
      "--------- Testing in global mode ---------\n",
      "size of test_loader dataset:  10000\n",
      "\n",
      "Test set: Avg. loss: 0.0003, Accuracy: 9047/10000 (90%)\n",
      "\n",
      "Train Epoch: 9 [0/50000 (0%)]\tLoss: 0.058916\n",
      "Train Epoch: 9 [25600/50000 (51%)]\tLoss: 0.066633\n",
      "\n",
      "--------- Testing in global mode ---------\n",
      "size of test_loader dataset:  10000\n",
      "\n",
      "Test set: Avg. loss: 0.0003, Accuracy: 9046/10000 (90%)\n",
      "\n",
      "Train Epoch: 10 [0/50000 (0%)]\tLoss: 0.062924\n",
      "Train Epoch: 10 [25600/50000 (51%)]\tLoss: 0.063471\n",
      "\n",
      "--------- Testing in global mode ---------\n",
      "size of test_loader dataset:  10000\n",
      "\n",
      "Test set: Avg. loss: 0.0003, Accuracy: 9044/10000 (90%)\n",
      "\n",
      "Train Epoch: 1 [0/50000 (0%)]\tLoss: 0.097751\n",
      "Train Epoch: 1 [25600/50000 (51%)]\tLoss: 0.080212\n",
      "\n",
      "--------- Testing in global mode ---------\n",
      "size of test_loader dataset:  10000\n",
      "\n",
      "Test set: Avg. loss: 0.0003, Accuracy: 9056/10000 (91%)\n",
      "\n",
      "Train Epoch: 2 [0/50000 (0%)]\tLoss: 0.072035\n",
      "Train Epoch: 2 [25600/50000 (51%)]\tLoss: 0.098448\n",
      "\n",
      "--------- Testing in global mode ---------\n",
      "size of test_loader dataset:  10000\n",
      "\n",
      "Test set: Avg. loss: 0.0003, Accuracy: 9065/10000 (91%)\n",
      "\n",
      "Train Epoch: 3 [0/50000 (0%)]\tLoss: 0.075295\n",
      "Train Epoch: 3 [25600/50000 (51%)]\tLoss: 0.076242\n",
      "\n",
      "--------- Testing in global mode ---------\n",
      "size of test_loader dataset:  10000\n",
      "\n",
      "Test set: Avg. loss: 0.0003, Accuracy: 9066/10000 (91%)\n",
      "\n",
      "Train Epoch: 4 [0/50000 (0%)]\tLoss: 0.089911\n",
      "Train Epoch: 4 [25600/50000 (51%)]\tLoss: 0.069176\n",
      "\n",
      "--------- Testing in global mode ---------\n",
      "size of test_loader dataset:  10000\n",
      "\n",
      "Test set: Avg. loss: 0.0003, Accuracy: 9068/10000 (91%)\n",
      "\n",
      "Train Epoch: 5 [0/50000 (0%)]\tLoss: 0.066074\n",
      "Train Epoch: 5 [25600/50000 (51%)]\tLoss: 0.093553\n",
      "\n",
      "--------- Testing in global mode ---------\n",
      "size of test_loader dataset:  10000\n",
      "\n",
      "Test set: Avg. loss: 0.0003, Accuracy: 9067/10000 (91%)\n",
      "\n",
      "Train Epoch: 6 [0/50000 (0%)]\tLoss: 0.065930\n",
      "Train Epoch: 6 [25600/50000 (51%)]\tLoss: 0.063445\n",
      "\n",
      "--------- Testing in global mode ---------\n",
      "size of test_loader dataset:  10000\n",
      "\n",
      "Test set: Avg. loss: 0.0003, Accuracy: 9068/10000 (91%)\n",
      "\n",
      "Train Epoch: 7 [0/50000 (0%)]\tLoss: 0.067796\n",
      "Train Epoch: 7 [25600/50000 (51%)]\tLoss: 0.056102\n",
      "\n",
      "--------- Testing in global mode ---------\n",
      "size of test_loader dataset:  10000\n",
      "\n",
      "Test set: Avg. loss: 0.0003, Accuracy: 9064/10000 (91%)\n",
      "\n",
      "Train Epoch: 8 [0/50000 (0%)]\tLoss: 0.053087\n",
      "Train Epoch: 8 [25600/50000 (51%)]\tLoss: 0.066485\n",
      "\n",
      "--------- Testing in global mode ---------\n",
      "size of test_loader dataset:  10000\n",
      "\n",
      "Test set: Avg. loss: 0.0003, Accuracy: 9067/10000 (91%)\n",
      "\n",
      "Train Epoch: 9 [0/50000 (0%)]\tLoss: 0.061531\n",
      "Train Epoch: 9 [25600/50000 (51%)]\tLoss: 0.062868\n",
      "\n",
      "--------- Testing in global mode ---------\n",
      "size of test_loader dataset:  10000\n",
      "\n",
      "Test set: Avg. loss: 0.0003, Accuracy: 9067/10000 (91%)\n",
      "\n",
      "Train Epoch: 10 [0/50000 (0%)]\tLoss: 0.057892\n",
      "Train Epoch: 10 [25600/50000 (51%)]\tLoss: 0.056442\n",
      "\n",
      "--------- Testing in global mode ---------\n",
      "size of test_loader dataset:  10000\n",
      "\n",
      "Test set: Avg. loss: 0.0003, Accuracy: 9058/10000 (91%)\n",
      "\n",
      "========================================================\n",
      "accuracies [90.44, 90.58]\n",
      "=========================================================\n",
      "layer features.0.weight_orig has #params  1728\n",
      "layer features.3.weight_orig has #params  73728\n",
      "layer features.6.weight_orig has #params  294912\n",
      "layer features.8.weight_orig has #params  589824\n",
      "layer features.11.weight_orig has #params  1179648\n",
      "layer features.13.weight_orig has #params  2359296\n",
      "layer features.16.weight_orig has #params  2359296\n",
      "layer features.18.weight_orig has #params  2359296\n",
      "layer classifier.weight has #params  5120\n",
      "Activation Timer start\n",
      "Activation Timer ends\n",
      "------- Geometric Ensembling -------\n",
      "Timer start\n",
      "Previous layer shape is  None\n",
      "Processing the coordinates to form ground_metric\n",
      "dont leave off the squaring of the ground metric\n",
      "returns a uniform measure of cardinality:  64\n",
      "returns a uniform measure of cardinality:  64\n",
      "the transport map is  tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')\n",
      "Ratio of trace to the matrix sum:  tensor(0.0469, device='cuda:0')\n",
      "Here, trace is 3.0 and matrix sum is 64.0 \n",
      "this is past correction for weight mode\n",
      "Shape of aligned wt is  torch.Size([64, 3, 9])\n",
      "Shape of fc_layer0_weight_data is  torch.Size([64, 3, 9])\n",
      "Previous layer shape is  torch.Size([64, 3, 3, 3])\n",
      "shape of layer: model 0 torch.Size([128, 64, 9])\n",
      "shape of layer: model 1 torch.Size([128, 64, 9])\n",
      "shape of previous transport map torch.Size([64, 64])\n",
      "Processing the coordinates to form ground_metric\n",
      "dont leave off the squaring of the ground metric\n",
      "returns a uniform measure of cardinality:  128\n",
      "returns a uniform measure of cardinality:  128\n",
      "the transport map is  tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        ...,\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0078],\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],\n",
      "       device='cuda:0')\n",
      "Ratio of trace to the matrix sum:  tensor(0.0078, device='cuda:0')\n",
      "Here, trace is 1.0 and matrix sum is 128.0 \n",
      "this is past correction for weight mode\n",
      "Shape of aligned wt is  torch.Size([128, 64, 9])\n",
      "Shape of fc_layer0_weight_data is  torch.Size([128, 64, 9])\n",
      "Previous layer shape is  torch.Size([128, 64, 3, 3])\n",
      "shape of layer: model 0 torch.Size([256, 128, 9])\n",
      "shape of layer: model 1 torch.Size([256, 128, 9])\n",
      "shape of previous transport map torch.Size([128, 128])\n",
      "Processing the coordinates to form ground_metric\n",
      "dont leave off the squaring of the ground metric\n",
      "returns a uniform measure of cardinality:  256\n",
      "returns a uniform measure of cardinality:  256\n",
      "the transport map is  tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')\n",
      "Ratio of trace to the matrix sum:  tensor(0., device='cuda:0')\n",
      "Here, trace is 0.0 and matrix sum is 256.0 \n",
      "this is past correction for weight mode\n",
      "Shape of aligned wt is  torch.Size([256, 128, 9])\n",
      "Shape of fc_layer0_weight_data is  torch.Size([256, 128, 9])\n",
      "Previous layer shape is  torch.Size([256, 128, 3, 3])\n",
      "shape of layer: model 0 torch.Size([256, 256, 9])\n",
      "shape of layer: model 1 torch.Size([256, 256, 9])\n",
      "shape of previous transport map torch.Size([256, 256])\n",
      "Processing the coordinates to form ground_metric\n",
      "dont leave off the squaring of the ground metric\n",
      "returns a uniform measure of cardinality:  256\n",
      "returns a uniform measure of cardinality:  256\n",
      "the transport map is  tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        ...,\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.0039, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],\n",
      "       device='cuda:0')\n",
      "Ratio of trace to the matrix sum:  tensor(0.0078, device='cuda:0')\n",
      "Here, trace is 2.0 and matrix sum is 256.0 \n",
      "this is past correction for weight mode\n",
      "Shape of aligned wt is  torch.Size([256, 256, 9])\n",
      "Shape of fc_layer0_weight_data is  torch.Size([256, 256, 9])\n",
      "Previous layer shape is  torch.Size([256, 256, 3, 3])\n",
      "shape of layer: model 0 torch.Size([512, 256, 9])\n",
      "shape of layer: model 1 torch.Size([512, 256, 9])\n",
      "shape of previous transport map torch.Size([256, 256])\n",
      "Processing the coordinates to form ground_metric\n",
      "dont leave off the squaring of the ground metric\n",
      "returns a uniform measure of cardinality:  512\n",
      "returns a uniform measure of cardinality:  512\n",
      "the transport map is  tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')\n",
      "Ratio of trace to the matrix sum:  tensor(0.0039, device='cuda:0')\n",
      "Here, trace is 2.0 and matrix sum is 512.0 \n",
      "this is past correction for weight mode\n",
      "Shape of aligned wt is  torch.Size([512, 256, 9])\n",
      "Shape of fc_layer0_weight_data is  torch.Size([512, 256, 9])\n",
      "Previous layer shape is  torch.Size([512, 256, 3, 3])\n",
      "shape of layer: model 0 torch.Size([512, 512, 9])\n",
      "shape of layer: model 1 torch.Size([512, 512, 9])\n",
      "shape of previous transport map torch.Size([512, 512])\n",
      "Processing the coordinates to form ground_metric\n",
      "dont leave off the squaring of the ground metric\n",
      "returns a uniform measure of cardinality:  512\n",
      "returns a uniform measure of cardinality:  512\n",
      "the transport map is  tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')\n",
      "Ratio of trace to the matrix sum:  tensor(0.0020, device='cuda:0')\n",
      "Here, trace is 1.0 and matrix sum is 512.0 \n",
      "this is past correction for weight mode\n",
      "Shape of aligned wt is  torch.Size([512, 512, 9])\n",
      "Shape of fc_layer0_weight_data is  torch.Size([512, 512, 9])\n",
      "Previous layer shape is  torch.Size([512, 512, 3, 3])\n",
      "shape of layer: model 0 torch.Size([512, 512, 9])\n",
      "shape of layer: model 1 torch.Size([512, 512, 9])\n",
      "shape of previous transport map torch.Size([512, 512])\n",
      "Processing the coordinates to form ground_metric\n",
      "dont leave off the squaring of the ground metric\n",
      "returns a uniform measure of cardinality:  512\n",
      "returns a uniform measure of cardinality:  512\n",
      "the transport map is  tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')\n",
      "Ratio of trace to the matrix sum:  tensor(0.0020, device='cuda:0')\n",
      "Here, trace is 1.0 and matrix sum is 512.0 \n",
      "this is past correction for weight mode\n",
      "Shape of aligned wt is  torch.Size([512, 512, 9])\n",
      "Shape of fc_layer0_weight_data is  torch.Size([512, 512, 9])\n",
      "Previous layer shape is  torch.Size([512, 512, 3, 3])\n",
      "shape of layer: model 0 torch.Size([512, 512, 9])\n",
      "shape of layer: model 1 torch.Size([512, 512, 9])\n",
      "shape of previous transport map torch.Size([512, 512])\n",
      "Processing the coordinates to form ground_metric\n",
      "dont leave off the squaring of the ground metric\n",
      "returns a uniform measure of cardinality:  512\n",
      "returns a uniform measure of cardinality:  512\n",
      "the transport map is  tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        [0.0020, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        ...,\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],\n",
      "       device='cuda:0')\n",
      "Ratio of trace to the matrix sum:  tensor(0., device='cuda:0')\n",
      "Here, trace is 0.0 and matrix sum is 512.0 \n",
      "this is past correction for weight mode\n",
      "Shape of aligned wt is  torch.Size([512, 512, 9])\n",
      "Shape of fc_layer0_weight_data is  torch.Size([512, 512, 9])\n",
      "Previous layer shape is  torch.Size([512, 512, 3, 3])\n",
      "shape of layer: model 0 torch.Size([10, 512])\n",
      "shape of layer: model 1 torch.Size([10, 512])\n",
      "shape of previous transport map torch.Size([512, 512])\n",
      "Processing the coordinates to form ground_metric\n",
      "dont leave off the squaring of the ground metric\n",
      "ground metric is  tensor([[1.4271, 2.4927, 2.7867, 2.8786, 2.7436, 2.8868, 2.7326, 2.6399, 2.5700,\n",
      "         2.5976],\n",
      "        [2.4939, 1.3272, 2.6784, 2.5964, 2.7632, 2.5546, 2.4204, 2.5255, 2.3095,\n",
      "         2.4138],\n",
      "        [2.7656, 2.6762, 1.3861, 2.9526, 2.9123, 2.8761, 2.7322, 2.8437, 2.7544,\n",
      "         2.7917],\n",
      "        [2.8411, 2.6845, 2.8984, 1.5462, 2.8664, 2.9223, 2.7624, 2.8035, 2.8226,\n",
      "         2.6405],\n",
      "        [2.7609, 2.6010, 2.8692, 2.8510, 1.5072, 2.7504, 2.7899, 2.6860, 2.7091,\n",
      "         2.7345],\n",
      "        [2.9250, 2.5443, 2.9528, 2.8477, 2.7327, 1.5533, 2.7827, 2.7352, 2.8038,\n",
      "         2.6747],\n",
      "        [2.6826, 2.4488, 2.7238, 2.9161, 2.6529, 2.8572, 1.3947, 2.6929, 2.5024,\n",
      "         2.5595],\n",
      "        [2.7443, 2.5535, 2.7852, 2.7956, 2.7201, 2.6639, 2.7226, 1.3946, 2.5633,\n",
      "         2.5158],\n",
      "        [2.6031, 2.3702, 2.7753, 2.6925, 2.7078, 2.8341, 2.5361, 2.6413, 1.3028,\n",
      "         2.4083],\n",
      "        [2.5967, 2.2764, 2.7867, 2.7625, 2.6741, 2.7321, 2.5512, 2.5326, 2.5255,\n",
      "         1.2897]], device='cuda:0', grad_fn=<PowBackward0>)\n",
      "returns a uniform measure of cardinality:  10\n",
      "returns a uniform measure of cardinality:  10\n",
      "the transport map is  tensor([[0.1000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0000, 0.1000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0000, 0.0000, 0.1000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.1000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.1000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.1000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.1000, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.1000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.1000,\n",
      "         0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.1000]], device='cuda:0')\n",
      "Ratio of trace to the matrix sum:  tensor(1., device='cuda:0')\n",
      "Here, trace is 9.99755859375 and matrix sum is 9.99755859375 \n",
      "this is past correction for weight mode\n",
      "Shape of aligned wt is  torch.Size([10, 512])\n",
      "Shape of fc_layer0_weight_data is  torch.Size([10, 512])\n",
      "using independent method\n",
      "in _make_layers [Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), AvgPool2d(kernel_size=1, stride=1, padding=0)]\n",
      "Relu Inplace is  False\n",
      "model parameters are \n",
      " [torch.Size([64, 3, 3, 3]), torch.Size([128, 64, 3, 3]), torch.Size([256, 128, 3, 3]), torch.Size([256, 256, 3, 3]), torch.Size([512, 256, 3, 3]), torch.Size([512, 512, 3, 3]), torch.Size([512, 512, 3, 3]), torch.Size([512, 512, 3, 3]), torch.Size([10, 512])]\n",
      "\n",
      "--------- Testing in global mode ---------\n",
      "size of test_loader dataset:  10000\n",
      "\n",
      "Test set: Avg. loss: 0.0023, Accuracy: 776/10000 (8%)\n",
      "\n",
      "len of model parameters and avg aligned layers is  9 9\n",
      "len of model_state_dict is  9\n",
      "len of param_list is  9\n",
      "\n",
      "--------- Testing in global mode ---------\n",
      "size of test_loader dataset:  10000\n",
      "\n",
      "Test set: Avg. loss: 0.0014, Accuracy: 8693/10000 (87%)\n",
      "\n",
      "===============================================\n",
      "===============================================\n",
      "After Ensemble: 86.93\n",
      "===============================================\n",
      "===============================================\n",
      "Timer ends\n",
      "Time taken for geometric ensembling is 4.122280936688185 seconds\n",
      "------- Prediction based ensembling -------\n",
      "\n",
      "Test set: Avg. loss: 0.0003, Accuracy: 9125/10000 (91%)\n",
      "\n",
      "------- Naive ensembling of weights -------\n",
      "[torch.Size([64, 3, 3, 3]), torch.Size([64, 3, 3, 3])]\n",
      "torch.Size([64, 3, 3, 3])\n",
      "[torch.Size([128, 64, 3, 3]), torch.Size([128, 64, 3, 3])]\n",
      "torch.Size([128, 64, 3, 3])\n",
      "[torch.Size([256, 128, 3, 3]), torch.Size([256, 128, 3, 3])]\n",
      "torch.Size([256, 128, 3, 3])\n",
      "[torch.Size([256, 256, 3, 3]), torch.Size([256, 256, 3, 3])]\n",
      "torch.Size([256, 256, 3, 3])\n",
      "[torch.Size([512, 256, 3, 3]), torch.Size([512, 256, 3, 3])]\n",
      "torch.Size([512, 256, 3, 3])\n",
      "[torch.Size([512, 512, 3, 3]), torch.Size([512, 512, 3, 3])]\n",
      "torch.Size([512, 512, 3, 3])\n",
      "[torch.Size([512, 512, 3, 3]), torch.Size([512, 512, 3, 3])]\n",
      "torch.Size([512, 512, 3, 3])\n",
      "[torch.Size([512, 512, 3, 3]), torch.Size([512, 512, 3, 3])]\n",
      "torch.Size([512, 512, 3, 3])\n",
      "[torch.Size([10, 512]), torch.Size([10, 512])]\n",
      "torch.Size([10, 512])\n",
      "in _make_layers [Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), AvgPool2d(kernel_size=1, stride=1, padding=0)]\n",
      "Relu Inplace is  False\n",
      "model parameters are \n",
      " [torch.Size([64, 3, 3, 3]), torch.Size([128, 64, 3, 3]), torch.Size([256, 128, 3, 3]), torch.Size([256, 256, 3, 3]), torch.Size([512, 256, 3, 3]), torch.Size([512, 512, 3, 3]), torch.Size([512, 512, 3, 3]), torch.Size([512, 512, 3, 3]), torch.Size([10, 512])]\n",
      "\n",
      "--------- Testing in global mode ---------\n",
      "size of test_loader dataset:  10000\n",
      "\n",
      "Test set: Avg. loss: 0.0023, Accuracy: 1066/10000 (11%)\n",
      "\n",
      "\n",
      "--------- Testing in global mode ---------\n",
      "size of test_loader dataset:  10000\n",
      "\n",
      "Test set: Avg. loss: 0.0023, Accuracy: 1596/10000 (16%)\n",
      "\n",
      "-------- Retraining the models ---------\n",
      "Retraining model :  model_0\n",
      "num_epochs---------------+++))))))+++ 300\n",
      "num_epochs---------------+++))))))+++ 300\n",
      "lr is  0.05\n",
      "number of epochs would be  30\n",
      "num_epochs-------------- 30\n",
      "Epoch 000\n",
      "/root/miniconda3/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:129: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\"Detected call of `lr_scheduler.step()` before `optimizer.step()`. \"\n",
      "/root/miniconda3/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:154: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "accuracy: {'epoch': 0, 'value': 0.9122000000190741} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 0, 'value': 0.2636602558898925} ({'split': 'train'})\n",
      "accuracy: {'epoch': 0, 'value': 0.8344000399112701} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 0, 'value': 0.5101864248514175} ({'split': 'test'})\n",
      "Epoch 001\n",
      "accuracy: {'epoch': 1, 'value': 0.9403200000190738} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 1, 'value': 0.17580926229715352} ({'split': 'train'})\n",
      "accuracy: {'epoch': 1, 'value': 0.8468000352382661} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 1, 'value': 0.5193602144718171} ({'split': 'test'})\n",
      "Epoch 002\n",
      "accuracy: {'epoch': 2, 'value': 0.9563600000381468} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 2, 'value': 0.13225128568172462} ({'split': 'train'})\n",
      "accuracy: {'epoch': 2, 'value': 0.8551000356674194} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 2, 'value': 0.5112047284841538} ({'split': 'test'})\n",
      "Epoch 003\n",
      "accuracy: {'epoch': 3, 'value': 0.9671600000572209} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 3, 'value': 0.10211080060482026} ({'split': 'train'})\n",
      "accuracy: {'epoch': 3, 'value': 0.8414000451564789} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 3, 'value': 0.5935918271541595} ({'split': 'test'})\n",
      "Epoch 004\n",
      "accuracy: {'epoch': 4, 'value': 0.9735000000572205} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 4, 'value': 0.07740002972126014} ({'split': 'train'})\n",
      "accuracy: {'epoch': 4, 'value': 0.8538000345230102} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 4, 'value': 0.576252418756485} ({'split': 'test'})\n",
      "Epoch 005\n",
      "accuracy: {'epoch': 5, 'value': 0.9776800000572204} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 5, 'value': 0.06936297762870791} ({'split': 'train'})\n",
      "accuracy: {'epoch': 5, 'value': 0.8592000424861909} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 5, 'value': 0.5628400206565856} ({'split': 'test'})\n",
      "Epoch 006\n",
      "accuracy: {'epoch': 6, 'value': 0.9816400000190737} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 6, 'value': 0.05853538508176803} ({'split': 'train'})\n",
      "accuracy: {'epoch': 6, 'value': 0.866800045967102} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 6, 'value': 0.5465849697589873} ({'split': 'test'})\n",
      "Epoch 007\n",
      "accuracy: {'epoch': 7, 'value': 0.984740000019074} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 7, 'value': 0.04710702549099921} ({'split': 'train'})\n",
      "accuracy: {'epoch': 7, 'value': 0.8397000372409821} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 7, 'value': 0.6637592494487763} ({'split': 'test'})\n",
      "Epoch 008\n",
      "accuracy: {'epoch': 8, 'value': 0.9859000000572207} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 8, 'value': 0.043933117284774746} ({'split': 'train'})\n",
      "accuracy: {'epoch': 8, 'value': 0.8613000571727752} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 8, 'value': 0.5364737719297409} ({'split': 'test'})\n",
      "Epoch 009\n",
      "accuracy: {'epoch': 9, 'value': 0.985540000038147} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 9, 'value': 0.04429035544872284} ({'split': 'train'})\n",
      "accuracy: {'epoch': 9, 'value': 0.84560005068779} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 9, 'value': 0.6203998327255248} ({'split': 'test'})\n",
      "Epoch 010\n",
      "accuracy: {'epoch': 10, 'value': 0.980520000019073} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 10, 'value': 0.062353112208843216} ({'split': 'train'})\n",
      "accuracy: {'epoch': 10, 'value': 0.8550000369548798} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 10, 'value': 0.5902862787246704} ({'split': 'test'})\n",
      "Epoch 011\n",
      "accuracy: {'epoch': 11, 'value': 0.9872200000381468} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 11, 'value': 0.042148954392671585} ({'split': 'train'})\n",
      "accuracy: {'epoch': 11, 'value': 0.8621000409126282} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 11, 'value': 0.6437871098518372} ({'split': 'test'})\n",
      "Epoch 012\n",
      "accuracy: {'epoch': 12, 'value': 0.9894400000190738} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 12, 'value': 0.03598721664547921} ({'split': 'train'})\n",
      "accuracy: {'epoch': 12, 'value': 0.8698000431060791} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 12, 'value': 0.5256512731313705} ({'split': 'test'})\n",
      "Epoch 013\n",
      "accuracy: {'epoch': 13, 'value': 0.990660000038147} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 13, 'value': 0.030652406332492835} ({'split': 'train'})\n",
      "accuracy: {'epoch': 13, 'value': 0.8585000395774841} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 13, 'value': 0.5822527706623077} ({'split': 'test'})\n",
      "Epoch 014\n",
      "accuracy: {'epoch': 14, 'value': 0.9904200000000004} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 14, 'value': 0.031495134933590886} ({'split': 'train'})\n",
      "accuracy: {'epoch': 14, 'value': 0.8659000396728516} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 14, 'value': 0.5479746639728547} ({'split': 'test'})\n",
      "Epoch 015\n",
      "accuracy: {'epoch': 15, 'value': 0.9911200000381467} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 15, 'value': 0.02920728697061539} ({'split': 'train'})\n",
      "accuracy: {'epoch': 15, 'value': 0.8625000357627868} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 15, 'value': 0.5630833506584167} ({'split': 'test'})\n",
      "Epoch 016\n",
      "accuracy: {'epoch': 16, 'value': 0.9894400000190737} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 16, 'value': 0.0346684301567078} ({'split': 'train'})\n",
      "accuracy: {'epoch': 16, 'value': 0.8657000362873077} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 16, 'value': 0.5487367331981658} ({'split': 'test'})\n",
      "Epoch 017\n",
      "accuracy: {'epoch': 17, 'value': 0.9887399999999998} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 17, 'value': 0.03685286352962254} ({'split': 'train'})\n",
      "accuracy: {'epoch': 17, 'value': 0.8545000314712525} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 17, 'value': 0.6119182616472244} ({'split': 'test'})\n",
      "Epoch 018\n",
      "accuracy: {'epoch': 18, 'value': 0.9890800000190734} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 18, 'value': 0.035313584553003305} ({'split': 'train'})\n",
      "accuracy: {'epoch': 18, 'value': 0.8621000349521637} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 18, 'value': 0.555291822552681} ({'split': 'test'})\n",
      "Epoch 019\n",
      "accuracy: {'epoch': 19, 'value': 0.9899600000190744} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 19, 'value': 0.03316129726171496} ({'split': 'train'})\n",
      "accuracy: {'epoch': 19, 'value': 0.8682000398635864} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 19, 'value': 0.5559415608644485} ({'split': 'test'})\n",
      "Epoch 020\n",
      "accuracy: {'epoch': 20, 'value': 0.9898600000190733} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 20, 'value': 0.03351216415524483} ({'split': 'train'})\n",
      "accuracy: {'epoch': 20, 'value': 0.8601000487804413} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 20, 'value': 0.5782888352870942} ({'split': 'test'})\n",
      "Epoch 021\n",
      "accuracy: {'epoch': 21, 'value': 0.9883600000381472} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 21, 'value': 0.03587706937551499} ({'split': 'train'})\n",
      "accuracy: {'epoch': 21, 'value': 0.8587000370025635} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 21, 'value': 0.6062435984611511} ({'split': 'test'})\n",
      "Epoch 022\n",
      "accuracy: {'epoch': 22, 'value': 0.9858000000381465} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 22, 'value': 0.04532035360813142} ({'split': 'train'})\n",
      "accuracy: {'epoch': 22, 'value': 0.8450000405311585} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 22, 'value': 0.6278650104999542} ({'split': 'test'})\n",
      "Epoch 023\n",
      "accuracy: {'epoch': 23, 'value': 0.9867000000190733} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 23, 'value': 0.04363761785984038} ({'split': 'train'})\n",
      "accuracy: {'epoch': 23, 'value': 0.847100043296814} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 23, 'value': 0.6292963147163391} ({'split': 'test'})\n",
      "Epoch 024\n",
      "accuracy: {'epoch': 24, 'value': 0.9867600000190734} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 24, 'value': 0.042320175387263334} ({'split': 'train'})\n",
      "accuracy: {'epoch': 24, 'value': 0.8553000450134276} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 24, 'value': 0.630251544713974} ({'split': 'test'})\n",
      "Epoch 025\n",
      "^C\n",
      "Traceback (most recent call last):\n",
      "  File \"main.py\", line 351, in <module>\n",
      "    _, best_retrain_acc = routines.retrain_models(args, [*original_models, geometric_model, naive_model], retrain_loader, test_loader, config, tensorboard_obj=tensorboard_obj, initial_acc=initial_acc, nicks=nicks)\n",
      "  File \"/root/autodl-tmp/OTfusion_pruning/otfusion/routines.py\", line 394, in retrain_models\n",
      "    retrained_network, acc = cifar_train.get_retrained_model(args, retrain_loader, test_loader, old_networks[i], config, output_root_dir, tensorboard_obj=tensorboard_obj, nick=nick, start_acc=initial_acc[i])\n",
      "  File \"/root/autodl-tmp/OTfusion_pruning/otfusion/./cifar/train.py\", line 333, in get_retrained_model\n",
      "    best_acc,model = main(config, output_dir, args.gpu_id, pretrained_model=old_network, pretrained_dataset=(train_loader, test_loader), tensorboard_obj=tensorboard_obj,return_model=True)\n",
      "  File \"/root/autodl-tmp/OTfusion_pruning/otfusion/./cifar/train.py\", line 67, in main\n",
      "    for batch_x, batch_y in training_loader:\n",
      "  File \"/root/miniconda3/lib/python3.8/site-packages/torch/utils/data/dataloader.py\", line 521, in __next__\n",
      "    data = self._next_data()\n",
      "  File \"/root/miniconda3/lib/python3.8/site-packages/torch/utils/data/dataloader.py\", line 561, in _next_data\n",
      "    data = self._dataset_fetcher.fetch(index)  # may raise StopIteration\n",
      "  File \"/root/miniconda3/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py\", line 49, in fetch\n",
      "    data = [self.dataset[idx] for idx in possibly_batched_index]\n",
      "  File \"/root/miniconda3/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py\", line 49, in <listcomp>\n",
      "    data = [self.dataset[idx] for idx in possibly_batched_index]\n",
      "  File \"/root/miniconda3/lib/python3.8/site-packages/torchvision/datasets/cifar.py\", line 121, in __getitem__\n",
      "    img = self.transform(img)\n",
      "  File \"/root/miniconda3/lib/python3.8/site-packages/torchvision/transforms/transforms.py\", line 61, in __call__\n",
      "    img = t(img)\n",
      "  File \"/root/miniconda3/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1102, in _call_impl\n",
      "    return forward_call(*input, **kwargs)\n",
      "  File \"/root/miniconda3/lib/python3.8/site-packages/torchvision/transforms/transforms.py\", line 226, in forward\n",
      "    return F.normalize(tensor, self.mean, self.std, self.inplace)\n",
      "  File \"/root/miniconda3/lib/python3.8/site-packages/torchvision/transforms/functional.py\", line 351, in normalize\n",
      "    tensor.sub_(mean).div_(std)\n",
      "KeyboardInterrupt\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m~/miniconda3/lib/python3.8/site-packages/IPython/utils/_process_posix.py\u001b[0m in \u001b[0;36msystem\u001b[0;34m(self, cmd)\u001b[0m\n\u001b[1;32m    161\u001b[0m                 \u001b[0;31m# know whether we've finished (if we matched EOF) or not\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 162\u001b[0;31m                 \u001b[0mres_idx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mchild\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexpect_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpatterns\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_timeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    163\u001b[0m                 \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchild\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbefore\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mout_size\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'replace'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m''\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.8/site-packages/pexpect/spawnbase.py\u001b[0m in \u001b[0;36mexpect_list\u001b[0;34m(self, pattern_list, timeout, searchwindowsize, async_, **kw)\u001b[0m\n\u001b[1;32m    371\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 372\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mexp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexpect_loop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    373\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.8/site-packages/pexpect/expect.py\u001b[0m in \u001b[0;36mexpect_loop\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    168\u001b[0m                 \u001b[0;31m# Still have time left, so read more data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 169\u001b[0;31m                 \u001b[0mincoming\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mspawn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_nonblocking\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mspawn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmaxread\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    170\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mspawn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdelayafterread\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.8/site-packages/pexpect/pty_spawn.py\u001b[0m in \u001b[0;36mread_nonblocking\u001b[0;34m(self, size, timeout)\u001b[0m\n\u001b[1;32m    499\u001b[0m         \u001b[0;31m# (possibly timeout=None), we call select() with a timeout.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 500\u001b[0;31m         \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mselect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    501\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mspawn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_nonblocking\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.8/site-packages/pexpect/pty_spawn.py\u001b[0m in \u001b[0;36mselect\u001b[0;34m(timeout)\u001b[0m\n\u001b[1;32m    449\u001b[0m             \u001b[0;32mdef\u001b[0m \u001b[0mselect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 450\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mselect_ignore_interrupts\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchild_fd\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    451\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.8/site-packages/pexpect/utils.py\u001b[0m in \u001b[0;36mselect_ignore_interrupts\u001b[0;34m(iwtd, owtd, ewtd, timeout)\u001b[0m\n\u001b[1;32m    142\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 143\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mselect\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miwtd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mowtd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mewtd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    144\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mInterruptedError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: ",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_9919/1363029856.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msystem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'python main.py --gpu-id 0 --model-name vgg11_nobias --n-epochs 10 --save-result-file sample.csv  --sweep-name exp_sample --correction --ground-metric euclidean --weight-stats  --geom-ensemble-type wts --ground-metric-normalize none --sweep-id 90  --ckpt-type best --dataset Cifar10 --ground-metric-eff --recheck-cifar --activation-seed {activation_seed}  --prelu-acts --past-correction --not-squared --exact --learning-rate 0.0001 --momentum 0.5 --batch-size-train 256 --experiment-dir {experiment_dir} --prunint-rate {prunint_rate} --run-mode {run_mode} --to-download --load-model-dir {load_model_dir} --retrain 30'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/miniconda3/lib/python3.8/site-packages/ipykernel/zmqshell.py\u001b[0m in \u001b[0;36msystem_piped\u001b[0;34m(self, cmd)\u001b[0m\n\u001b[1;32m    635\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muser_ns\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'_exit_code'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msystem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcmd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    636\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 637\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muser_ns\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'_exit_code'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msystem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvar_expand\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcmd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdepth\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    638\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    639\u001b[0m     \u001b[0;31m# Ensure new system_piped implementation is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.8/site-packages/IPython/utils/_process_posix.py\u001b[0m in \u001b[0;36msystem\u001b[0;34m(self, cmd)\u001b[0m\n\u001b[1;32m    184\u001b[0m             \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    185\u001b[0m                 \u001b[0;31m# Ensure the subprocess really is terminated\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 186\u001b[0;31m                 \u001b[0mchild\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mterminate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mforce\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    187\u001b[0m         \u001b[0;31m# add isalive check, to ensure exitstatus is set:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    188\u001b[0m         \u001b[0mchild\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misalive\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.8/site-packages/pexpect/pty_spawn.py\u001b[0m in \u001b[0;36mterminate\u001b[0;34m(self, force)\u001b[0m\n\u001b[1;32m    653\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mforce\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    654\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkill\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msignal\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSIGKILL\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 655\u001b[0;31m                 \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdelayafterterminate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    656\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misalive\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    657\u001b[0m                     \u001b[0;32mreturn\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "!python main.py --gpu-id 0 --model-name vgg11_nobias --n-epochs 10 --save-result-file sample.csv \\\n",
    "--sweep-name exp_sample --correction --ground-metric euclidean --weight-stats \\\n",
    "--geom-ensemble-type wts --ground-metric-normalize none --sweep-id 90 \\\n",
    "--ckpt-type best --dataset Cifar10 --ground-metric-eff --recheck-cifar --activation-seed {activation_seed} \\\n",
    "--prelu-acts --past-correction --not-squared --exact\\\n",
    "--learning-rate 0.0001\\\n",
    "--momentum 0.5\\\n",
    "--batch-size-train 256 --experiment-dir {experiment_dir}\\\n",
    "--prunint-rate {prunint_rate}\\\n",
    "--run-mode {run_mode}\\\n",
    "--to-download\\\n",
    "--load-model-dir {load_model_dir}\\\n",
    "--retrain 30\n",
    "# After Ensemble: 86.93"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e1701397-8c63-4095-be56-b3464267c277",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------- Setting up parameters -------\n",
      "dumping parameters at  /root/autodl-tmp/OTfusion_pruning/otfusion/exp_sample/configurations\n",
      "The parameters are: \n",
      " Namespace(act_bug=False, act_num_samples=100, activation_histograms=False, activation_mode=None, activation_seed=21, alpha=0.7, baseroot='/root/autodl-tmp/OTfusion_pruning/otfusion', batch_size_test=1000, batch_size_train=64, center_acts=False, choice='0 2 4 6 8', cifar_style_data=False, ckpt_type='best', clip_gm=False, clip_max=5, clip_min=0, config_dir='/root/autodl-tmp/OTfusion_pruning/otfusion/exp_sample/configurations', config_file=None, correction=True, csv_dir='/root/autodl-tmp/OTfusion_pruning/otfusion/exp_sample/csv', dataset='Cifar10', debug=False, deprecated=None, deterministic=False, diff_init=False, disable_bias=True, dist_epochs=60, dist_normalize=False, dump_final_models=False, dump_model=False, enable_dropout=False, ensemble_step=0.5, eval_aligned=False, exact=True, exp_name='exp_2023-09-16_17-01-31_730310', experiment_dir='experiment2', geom_ensemble_type='wts', gpu_id=0, gromov=False, gromov_loss='square_loss', ground_metric='euclidean', ground_metric_eff=True, ground_metric_normalize='none', handle_skips=False, importance=None, learning_rate=0.0001, load_model_dir='./cifar_models', load_models='', log_interval=100, model_name='vgg11_nobias', momentum=0.5, n_epochs=10, no_random_trainloaders=False, normalize_acts=False, normalize_wts=False, not_squared=True, num_hidden_nodes=400, num_hidden_nodes1=400, num_hidden_nodes2=200, num_hidden_nodes3=100, num_hidden_nodes4=50, num_models=2, options_type='generic', partial_reshape=False, partition_dataloader=-1, partition_type='labels', past_correction=True, personal_class_idx=9, personal_split_frac=0.1, pool_acts=False, pool_relu=False, prediction_wts=False, prelu_acts=True, print_distances=False, proper_marginals=False, prunint_rate=0.2, recheck_acc=False, recheck_cifar=True, reg=0.01, reg_m=0.001, reinit_trainloaders=False, result_dir='/root/autodl-tmp/OTfusion_pruning/otfusion/exp_sample/results', retrain=30, retrain_avg_only=False, retrain_geometric_only=False, retrain_lr_decay=-1, retrain_lr_decay_epochs=None, retrain_lr_decay_factor=None, retrain_seed=-1, rootdir='/root/autodl-tmp/OTfusion_pruning/otfusion/exp_sample', run_mode='finetune', same_model=-1, save_result_file='sample.csv', second_model_name=None, sinkhorn_type='normal', skip_last_layer=False, skip_last_layer_type='average', skip_personal_idx=False, skip_retrain=-1, softmax_temperature=1, standardize_acts=False, sweep_id=90, sweep_name='exp_sample', temperature=20, tensorboard=False, tensorboard_root='./tensorboard', timestamp='2023-09-16_17-01-31_730310', tmap_stats=False, to_download=True, transform_acts=False, unbalanced=False, update_acts=False, weight_stats=True, width_ratio=1)\n",
      "refactored get_config\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "in _make_layers [Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), AvgPool2d(kernel_size=1, stride=1, padding=0)]\n",
      "Relu Inplace is  False\n",
      "model parameters are \n",
      " [torch.Size([64, 3, 3, 3]), torch.Size([128, 64, 3, 3]), torch.Size([256, 128, 3, 3]), torch.Size([256, 256, 3, 3]), torch.Size([512, 256, 3, 3]), torch.Size([512, 512, 3, 3]), torch.Size([512, 512, 3, 3]), torch.Size([512, 512, 3, 3]), torch.Size([10, 512])]\n",
      "in _make_layers [Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), AvgPool2d(kernel_size=1, stride=1, padding=0)]\n",
      "Relu Inplace is  False\n",
      "model parameters are \n",
      " [torch.Size([64, 3, 3, 3]), torch.Size([128, 64, 3, 3]), torch.Size([256, 128, 3, 3]), torch.Size([256, 256, 3, 3]), torch.Size([512, 256, 3, 3]), torch.Size([512, 512, 3, 3]), torch.Size([512, 512, 3, 3]), torch.Size([512, 512, 3, 3]), torch.Size([10, 512])]\n",
      "in _make_layers [Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), AvgPool2d(kernel_size=1, stride=1, padding=0)]\n",
      "Relu Inplace is  False\n",
      "model parameters are \n",
      " [torch.Size([64, 3, 3, 3]), torch.Size([128, 64, 3, 3]), torch.Size([256, 128, 3, 3]), torch.Size([256, 256, 3, 3]), torch.Size([512, 256, 3, 3]), torch.Size([512, 512, 3, 3]), torch.Size([512, 512, 3, 3]), torch.Size([512, 512, 3, 3]), torch.Size([10, 512])]\n",
      "in _make_layers [Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), AvgPool2d(kernel_size=1, stride=1, padding=0)]\n",
      "Relu Inplace is  False\n",
      "model parameters are \n",
      " [torch.Size([64, 3, 3, 3]), torch.Size([128, 64, 3, 3]), torch.Size([256, 128, 3, 3]), torch.Size([256, 256, 3, 3]), torch.Size([512, 256, 3, 3]), torch.Size([512, 512, 3, 3]), torch.Size([512, 512, 3, 3]), torch.Size([512, 512, 3, 3]), torch.Size([10, 512])]\n",
      "Train Epoch: 1 [0/50000 (0%)]\tLoss: 0.116993\n",
      "Train Epoch: 1 [6400/50000 (13%)]\tLoss: 0.069534\n",
      "Train Epoch: 1 [12800/50000 (26%)]\tLoss: 0.115613\n",
      "Train Epoch: 1 [19200/50000 (38%)]\tLoss: 0.062993\n",
      "Train Epoch: 1 [25600/50000 (51%)]\tLoss: 0.083842\n",
      "Train Epoch: 1 [32000/50000 (64%)]\tLoss: 0.060105\n",
      "Train Epoch: 1 [38400/50000 (77%)]\tLoss: 0.070053\n",
      "Train Epoch: 1 [44800/50000 (90%)]\tLoss: 0.066773\n",
      "\n",
      "--------- Testing in global mode ---------\n",
      "size of test_loader dataset:  10000\n",
      "\n",
      "Test set: Avg. loss: 0.0003, Accuracy: 9049/10000 (90%)\n",
      "\n",
      "Train Epoch: 2 [0/50000 (0%)]\tLoss: 0.059949\n",
      "Train Epoch: 2 [6400/50000 (13%)]\tLoss: 0.060339\n",
      "Train Epoch: 2 [12800/50000 (26%)]\tLoss: 0.044316\n",
      "Train Epoch: 2 [19200/50000 (38%)]\tLoss: 0.052728\n",
      "Train Epoch: 2 [25600/50000 (51%)]\tLoss: 0.062013\n",
      "Train Epoch: 2 [32000/50000 (64%)]\tLoss: 0.036824\n",
      "Train Epoch: 2 [38400/50000 (77%)]\tLoss: 0.046963\n",
      "Train Epoch: 2 [44800/50000 (90%)]\tLoss: 0.037231\n",
      "\n",
      "--------- Testing in global mode ---------\n",
      "size of test_loader dataset:  10000\n",
      "\n",
      "Test set: Avg. loss: 0.0003, Accuracy: 9044/10000 (90%)\n",
      "\n",
      "Train Epoch: 3 [0/50000 (0%)]\tLoss: 0.050803\n",
      "Train Epoch: 3 [6400/50000 (13%)]\tLoss: 0.039283\n",
      "Train Epoch: 3 [12800/50000 (26%)]\tLoss: 0.063139\n",
      "Train Epoch: 3 [19200/50000 (38%)]\tLoss: 0.077094\n",
      "Train Epoch: 3 [25600/50000 (51%)]\tLoss: 0.052441\n",
      "Train Epoch: 3 [32000/50000 (64%)]\tLoss: 0.038409\n",
      "Train Epoch: 3 [38400/50000 (77%)]\tLoss: 0.046758\n",
      "Train Epoch: 3 [44800/50000 (90%)]\tLoss: 0.033687\n",
      "\n",
      "--------- Testing in global mode ---------\n",
      "size of test_loader dataset:  10000\n",
      "\n",
      "Test set: Avg. loss: 0.0003, Accuracy: 9044/10000 (90%)\n",
      "\n",
      "Train Epoch: 4 [0/50000 (0%)]\tLoss: 0.052528\n",
      "Train Epoch: 4 [6400/50000 (13%)]\tLoss: 0.055730\n",
      "Train Epoch: 4 [12800/50000 (26%)]\tLoss: 0.040544\n",
      "Train Epoch: 4 [19200/50000 (38%)]\tLoss: 0.040515\n",
      "Train Epoch: 4 [25600/50000 (51%)]\tLoss: 0.040505\n",
      "Train Epoch: 4 [32000/50000 (64%)]\tLoss: 0.044889\n",
      "Train Epoch: 4 [38400/50000 (77%)]\tLoss: 0.032337\n",
      "Train Epoch: 4 [44800/50000 (90%)]\tLoss: 0.031939\n",
      "\n",
      "--------- Testing in global mode ---------\n",
      "size of test_loader dataset:  10000\n",
      "\n",
      "Test set: Avg. loss: 0.0003, Accuracy: 9049/10000 (90%)\n",
      "\n",
      "Train Epoch: 5 [0/50000 (0%)]\tLoss: 0.054305\n",
      "Train Epoch: 5 [6400/50000 (13%)]\tLoss: 0.024743\n",
      "Train Epoch: 5 [12800/50000 (26%)]\tLoss: 0.029276\n",
      "Train Epoch: 5 [19200/50000 (38%)]\tLoss: 0.028308\n",
      "Train Epoch: 5 [25600/50000 (51%)]\tLoss: 0.063834\n",
      "Train Epoch: 5 [32000/50000 (64%)]\tLoss: 0.038146\n",
      "Train Epoch: 5 [38400/50000 (77%)]\tLoss: 0.038030\n",
      "Train Epoch: 5 [44800/50000 (90%)]\tLoss: 0.045305\n",
      "\n",
      "--------- Testing in global mode ---------\n",
      "size of test_loader dataset:  10000\n",
      "\n",
      "Test set: Avg. loss: 0.0003, Accuracy: 9047/10000 (90%)\n",
      "\n",
      "Train Epoch: 6 [0/50000 (0%)]\tLoss: 0.043455\n",
      "Train Epoch: 6 [6400/50000 (13%)]\tLoss: 0.025087\n",
      "Train Epoch: 6 [12800/50000 (26%)]\tLoss: 0.029782\n",
      "Train Epoch: 6 [19200/50000 (38%)]\tLoss: 0.026195\n",
      "Train Epoch: 6 [25600/50000 (51%)]\tLoss: 0.041571\n",
      "Train Epoch: 6 [32000/50000 (64%)]\tLoss: 0.041046\n",
      "Train Epoch: 6 [38400/50000 (77%)]\tLoss: 0.033723\n",
      "Train Epoch: 6 [44800/50000 (90%)]\tLoss: 0.025436\n",
      "\n",
      "--------- Testing in global mode ---------\n",
      "size of test_loader dataset:  10000\n",
      "\n",
      "Test set: Avg. loss: 0.0003, Accuracy: 9043/10000 (90%)\n",
      "\n",
      "Train Epoch: 7 [0/50000 (0%)]\tLoss: 0.038252\n",
      "Train Epoch: 7 [6400/50000 (13%)]\tLoss: 0.047840\n",
      "Train Epoch: 7 [12800/50000 (26%)]\tLoss: 0.070959\n",
      "Train Epoch: 7 [19200/50000 (38%)]\tLoss: 0.033207\n",
      "Train Epoch: 7 [25600/50000 (51%)]\tLoss: 0.050586\n",
      "Train Epoch: 7 [32000/50000 (64%)]\tLoss: 0.029546\n",
      "Train Epoch: 7 [38400/50000 (77%)]\tLoss: 0.061426\n",
      "Train Epoch: 7 [44800/50000 (90%)]\tLoss: 0.057670\n",
      "\n",
      "--------- Testing in global mode ---------\n",
      "size of test_loader dataset:  10000\n",
      "\n",
      "Test set: Avg. loss: 0.0003, Accuracy: 9041/10000 (90%)\n",
      "\n",
      "Train Epoch: 8 [0/50000 (0%)]\tLoss: 0.032923\n",
      "Train Epoch: 8 [6400/50000 (13%)]\tLoss: 0.028961\n",
      "Train Epoch: 8 [12800/50000 (26%)]\tLoss: 0.033977\n",
      "Train Epoch: 8 [19200/50000 (38%)]\tLoss: 0.038018\n",
      "Train Epoch: 8 [25600/50000 (51%)]\tLoss: 0.065654\n",
      "Train Epoch: 8 [32000/50000 (64%)]\tLoss: 0.035545\n",
      "Train Epoch: 8 [38400/50000 (77%)]\tLoss: 0.021207\n",
      "Train Epoch: 8 [44800/50000 (90%)]\tLoss: 0.038753\n",
      "\n",
      "--------- Testing in global mode ---------\n",
      "size of test_loader dataset:  10000\n",
      "\n",
      "Test set: Avg. loss: 0.0003, Accuracy: 9035/10000 (90%)\n",
      "\n",
      "Train Epoch: 9 [0/50000 (0%)]\tLoss: 0.026012\n",
      "Train Epoch: 9 [6400/50000 (13%)]\tLoss: 0.019942\n",
      "Train Epoch: 9 [12800/50000 (26%)]\tLoss: 0.039452\n",
      "Train Epoch: 9 [19200/50000 (38%)]\tLoss: 0.027666\n",
      "Train Epoch: 9 [25600/50000 (51%)]\tLoss: 0.038081\n",
      "Train Epoch: 9 [32000/50000 (64%)]\tLoss: 0.053346\n",
      "Train Epoch: 9 [38400/50000 (77%)]\tLoss: 0.049406\n",
      "Train Epoch: 9 [44800/50000 (90%)]\tLoss: 0.057711\n",
      "\n",
      "--------- Testing in global mode ---------\n",
      "size of test_loader dataset:  10000\n",
      "\n",
      "Test set: Avg. loss: 0.0003, Accuracy: 9031/10000 (90%)\n",
      "\n",
      "Train Epoch: 10 [0/50000 (0%)]\tLoss: 0.052621\n",
      "Train Epoch: 10 [6400/50000 (13%)]\tLoss: 0.036410\n",
      "Train Epoch: 10 [12800/50000 (26%)]\tLoss: 0.038551\n",
      "Train Epoch: 10 [19200/50000 (38%)]\tLoss: 0.024619\n",
      "Train Epoch: 10 [25600/50000 (51%)]\tLoss: 0.056248\n",
      "Train Epoch: 10 [32000/50000 (64%)]\tLoss: 0.025337\n",
      "Train Epoch: 10 [38400/50000 (77%)]\tLoss: 0.039602\n",
      "Train Epoch: 10 [44800/50000 (90%)]\tLoss: 0.037355\n",
      "\n",
      "--------- Testing in global mode ---------\n",
      "size of test_loader dataset:  10000\n",
      "\n",
      "Test set: Avg. loss: 0.0003, Accuracy: 9032/10000 (90%)\n",
      "\n",
      "Train Epoch: 1 [0/50000 (0%)]\tLoss: 0.142689\n",
      "Train Epoch: 1 [6400/50000 (13%)]\tLoss: 0.052603\n",
      "Train Epoch: 1 [12800/50000 (26%)]\tLoss: 0.093420\n",
      "Train Epoch: 1 [19200/50000 (38%)]\tLoss: 0.100826\n",
      "Train Epoch: 1 [25600/50000 (51%)]\tLoss: 0.070845\n",
      "Train Epoch: 1 [32000/50000 (64%)]\tLoss: 0.096255\n",
      "Train Epoch: 1 [38400/50000 (77%)]\tLoss: 0.041809\n",
      "Train Epoch: 1 [44800/50000 (90%)]\tLoss: 0.077177\n",
      "\n",
      "--------- Testing in global mode ---------\n",
      "size of test_loader dataset:  10000\n",
      "\n",
      "Test set: Avg. loss: 0.0003, Accuracy: 9069/10000 (91%)\n",
      "\n",
      "Train Epoch: 2 [0/50000 (0%)]\tLoss: 0.084418\n",
      "Train Epoch: 2 [6400/50000 (13%)]\tLoss: 0.082418\n",
      "Train Epoch: 2 [12800/50000 (26%)]\tLoss: 0.057694\n",
      "Train Epoch: 2 [19200/50000 (38%)]\tLoss: 0.075087\n",
      "Train Epoch: 2 [25600/50000 (51%)]\tLoss: 0.113169\n",
      "Train Epoch: 2 [32000/50000 (64%)]\tLoss: 0.061594\n",
      "Train Epoch: 2 [38400/50000 (77%)]\tLoss: 0.083725\n",
      "Train Epoch: 2 [44800/50000 (90%)]\tLoss: 0.055700\n",
      "\n",
      "--------- Testing in global mode ---------\n",
      "size of test_loader dataset:  10000\n",
      "\n",
      "Test set: Avg. loss: 0.0003, Accuracy: 9069/10000 (91%)\n",
      "\n",
      "Train Epoch: 3 [0/50000 (0%)]\tLoss: 0.065325\n",
      "Train Epoch: 3 [6400/50000 (13%)]\tLoss: 0.036698\n",
      "Train Epoch: 3 [12800/50000 (26%)]\tLoss: 0.071318\n",
      "Train Epoch: 3 [19200/50000 (38%)]\tLoss: 0.054485\n",
      "Train Epoch: 3 [25600/50000 (51%)]\tLoss: 0.058405\n",
      "Train Epoch: 3 [32000/50000 (64%)]\tLoss: 0.041059\n",
      "Train Epoch: 3 [38400/50000 (77%)]\tLoss: 0.044897\n",
      "Train Epoch: 3 [44800/50000 (90%)]\tLoss: 0.085841\n",
      "\n",
      "--------- Testing in global mode ---------\n",
      "size of test_loader dataset:  10000\n",
      "\n",
      "Test set: Avg. loss: 0.0003, Accuracy: 9060/10000 (91%)\n",
      "\n",
      "Train Epoch: 4 [0/50000 (0%)]\tLoss: 0.088576\n",
      "Train Epoch: 4 [6400/50000 (13%)]\tLoss: 0.050531\n",
      "Train Epoch: 4 [12800/50000 (26%)]\tLoss: 0.083464\n",
      "Train Epoch: 4 [19200/50000 (38%)]\tLoss: 0.039458\n",
      "Train Epoch: 4 [25600/50000 (51%)]\tLoss: 0.040206\n",
      "Train Epoch: 4 [32000/50000 (64%)]\tLoss: 0.070399\n",
      "Train Epoch: 4 [38400/50000 (77%)]\tLoss: 0.072445\n",
      "Train Epoch: 4 [44800/50000 (90%)]\tLoss: 0.071258\n",
      "\n",
      "--------- Testing in global mode ---------\n",
      "size of test_loader dataset:  10000\n",
      "\n",
      "Test set: Avg. loss: 0.0003, Accuracy: 9050/10000 (90%)\n",
      "\n",
      "Train Epoch: 5 [0/50000 (0%)]\tLoss: 0.052611\n",
      "Train Epoch: 5 [6400/50000 (13%)]\tLoss: 0.061676\n",
      "Train Epoch: 5 [12800/50000 (26%)]\tLoss: 0.049378\n",
      "Train Epoch: 5 [19200/50000 (38%)]\tLoss: 0.048139\n",
      "Train Epoch: 5 [25600/50000 (51%)]\tLoss: 0.042785\n",
      "Train Epoch: 5 [32000/50000 (64%)]\tLoss: 0.037133\n",
      "Train Epoch: 5 [38400/50000 (77%)]\tLoss: 0.076163\n",
      "Train Epoch: 5 [44800/50000 (90%)]\tLoss: 0.027200\n",
      "\n",
      "--------- Testing in global mode ---------\n",
      "size of test_loader dataset:  10000\n",
      "\n",
      "Test set: Avg. loss: 0.0003, Accuracy: 9043/10000 (90%)\n",
      "\n",
      "Train Epoch: 6 [0/50000 (0%)]\tLoss: 0.032286\n",
      "Train Epoch: 6 [6400/50000 (13%)]\tLoss: 0.055140\n",
      "Train Epoch: 6 [12800/50000 (26%)]\tLoss: 0.044526\n",
      "Train Epoch: 6 [19200/50000 (38%)]\tLoss: 0.038840\n",
      "Train Epoch: 6 [25600/50000 (51%)]\tLoss: 0.022438\n",
      "Train Epoch: 6 [32000/50000 (64%)]\tLoss: 0.055642\n",
      "Train Epoch: 6 [38400/50000 (77%)]\tLoss: 0.045887\n",
      "Train Epoch: 6 [44800/50000 (90%)]\tLoss: 0.034372\n",
      "\n",
      "--------- Testing in global mode ---------\n",
      "size of test_loader dataset:  10000\n",
      "\n",
      "Test set: Avg. loss: 0.0003, Accuracy: 9037/10000 (90%)\n",
      "\n",
      "Train Epoch: 7 [0/50000 (0%)]\tLoss: 0.086913\n",
      "Train Epoch: 7 [6400/50000 (13%)]\tLoss: 0.054592\n",
      "Train Epoch: 7 [12800/50000 (26%)]\tLoss: 0.040316\n",
      "Train Epoch: 7 [19200/50000 (38%)]\tLoss: 0.038977\n",
      "Train Epoch: 7 [25600/50000 (51%)]\tLoss: 0.042053\n",
      "Train Epoch: 7 [32000/50000 (64%)]\tLoss: 0.047258\n",
      "Train Epoch: 7 [38400/50000 (77%)]\tLoss: 0.033528\n",
      "Train Epoch: 7 [44800/50000 (90%)]\tLoss: 0.064025\n",
      "\n",
      "--------- Testing in global mode ---------\n",
      "size of test_loader dataset:  10000\n",
      "\n",
      "Test set: Avg. loss: 0.0003, Accuracy: 9036/10000 (90%)\n",
      "\n",
      "Train Epoch: 8 [0/50000 (0%)]\tLoss: 0.038836\n",
      "Train Epoch: 8 [6400/50000 (13%)]\tLoss: 0.025743\n",
      "Train Epoch: 8 [12800/50000 (26%)]\tLoss: 0.024755\n",
      "Train Epoch: 8 [19200/50000 (38%)]\tLoss: 0.039706\n",
      "Train Epoch: 8 [25600/50000 (51%)]\tLoss: 0.056052\n",
      "Train Epoch: 8 [32000/50000 (64%)]\tLoss: 0.033363\n",
      "Train Epoch: 8 [38400/50000 (77%)]\tLoss: 0.028756\n",
      "Train Epoch: 8 [44800/50000 (90%)]\tLoss: 0.029979\n",
      "\n",
      "--------- Testing in global mode ---------\n",
      "size of test_loader dataset:  10000\n",
      "\n",
      "Test set: Avg. loss: 0.0003, Accuracy: 9030/10000 (90%)\n",
      "\n",
      "Train Epoch: 9 [0/50000 (0%)]\tLoss: 0.037950\n",
      "Train Epoch: 9 [6400/50000 (13%)]\tLoss: 0.018366\n",
      "Train Epoch: 9 [12800/50000 (26%)]\tLoss: 0.028059\n",
      "Train Epoch: 9 [19200/50000 (38%)]\tLoss: 0.046587\n",
      "Train Epoch: 9 [25600/50000 (51%)]\tLoss: 0.012991\n",
      "Train Epoch: 9 [32000/50000 (64%)]\tLoss: 0.055657\n",
      "Train Epoch: 9 [38400/50000 (77%)]\tLoss: 0.052003\n",
      "Train Epoch: 9 [44800/50000 (90%)]\tLoss: 0.021347\n",
      "\n",
      "--------- Testing in global mode ---------\n",
      "size of test_loader dataset:  10000\n",
      "\n",
      "Test set: Avg. loss: 0.0003, Accuracy: 9029/10000 (90%)\n",
      "\n",
      "Train Epoch: 10 [0/50000 (0%)]\tLoss: 0.032026\n",
      "Train Epoch: 10 [6400/50000 (13%)]\tLoss: 0.039842\n",
      "Train Epoch: 10 [12800/50000 (26%)]\tLoss: 0.050016\n",
      "Train Epoch: 10 [19200/50000 (38%)]\tLoss: 0.027208\n",
      "Train Epoch: 10 [25600/50000 (51%)]\tLoss: 0.032963\n",
      "Train Epoch: 10 [32000/50000 (64%)]\tLoss: 0.055067\n",
      "Train Epoch: 10 [38400/50000 (77%)]\tLoss: 0.031083\n",
      "Train Epoch: 10 [44800/50000 (90%)]\tLoss: 0.043165\n",
      "\n",
      "--------- Testing in global mode ---------\n",
      "size of test_loader dataset:  10000\n",
      "\n",
      "Test set: Avg. loss: 0.0003, Accuracy: 9027/10000 (90%)\n",
      "\n",
      "========================================================\n",
      "accuracies [90.32, 90.27]\n",
      "=========================================================\n",
      "layer features.0.weight_orig has #params  1728\n",
      "layer features.3.weight_orig has #params  73728\n",
      "layer features.6.weight_orig has #params  294912\n",
      "layer features.8.weight_orig has #params  589824\n",
      "layer features.11.weight_orig has #params  1179648\n",
      "layer features.13.weight_orig has #params  2359296\n",
      "layer features.16.weight_orig has #params  2359296\n",
      "layer features.18.weight_orig has #params  2359296\n",
      "layer classifier.weight has #params  5120\n",
      "Activation Timer start\n",
      "Activation Timer ends\n",
      "------- Geometric Ensembling -------\n",
      "Timer start\n",
      "Previous layer shape is  None\n",
      "Processing the coordinates to form ground_metric\n",
      "dont leave off the squaring of the ground metric\n",
      "returns a uniform measure of cardinality:  64\n",
      "returns a uniform measure of cardinality:  64\n",
      "the transport map is  tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')\n",
      "Ratio of trace to the matrix sum:  tensor(0.0469, device='cuda:0')\n",
      "Here, trace is 3.0 and matrix sum is 64.0 \n",
      "this is past correction for weight mode\n",
      "Shape of aligned wt is  torch.Size([64, 3, 9])\n",
      "Shape of fc_layer0_weight_data is  torch.Size([64, 3, 9])\n",
      "Previous layer shape is  torch.Size([64, 3, 3, 3])\n",
      "shape of layer: model 0 torch.Size([128, 64, 9])\n",
      "shape of layer: model 1 torch.Size([128, 64, 9])\n",
      "shape of previous transport map torch.Size([64, 64])\n",
      "Processing the coordinates to form ground_metric\n",
      "dont leave off the squaring of the ground metric\n",
      "returns a uniform measure of cardinality:  128\n",
      "returns a uniform measure of cardinality:  128\n",
      "the transport map is  tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        ...,\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0078],\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],\n",
      "       device='cuda:0')\n",
      "Ratio of trace to the matrix sum:  tensor(0.0078, device='cuda:0')\n",
      "Here, trace is 1.0 and matrix sum is 128.0 \n",
      "this is past correction for weight mode\n",
      "Shape of aligned wt is  torch.Size([128, 64, 9])\n",
      "Shape of fc_layer0_weight_data is  torch.Size([128, 64, 9])\n",
      "Previous layer shape is  torch.Size([128, 64, 3, 3])\n",
      "shape of layer: model 0 torch.Size([256, 128, 9])\n",
      "shape of layer: model 1 torch.Size([256, 128, 9])\n",
      "shape of previous transport map torch.Size([128, 128])\n",
      "Processing the coordinates to form ground_metric\n",
      "dont leave off the squaring of the ground metric\n",
      "returns a uniform measure of cardinality:  256\n",
      "returns a uniform measure of cardinality:  256\n",
      "the transport map is  tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')\n",
      "Ratio of trace to the matrix sum:  tensor(0., device='cuda:0')\n",
      "Here, trace is 0.0 and matrix sum is 256.0 \n",
      "this is past correction for weight mode\n",
      "Shape of aligned wt is  torch.Size([256, 128, 9])\n",
      "Shape of fc_layer0_weight_data is  torch.Size([256, 128, 9])\n",
      "Previous layer shape is  torch.Size([256, 128, 3, 3])\n",
      "shape of layer: model 0 torch.Size([256, 256, 9])\n",
      "shape of layer: model 1 torch.Size([256, 256, 9])\n",
      "shape of previous transport map torch.Size([256, 256])\n",
      "Processing the coordinates to form ground_metric\n",
      "dont leave off the squaring of the ground metric\n",
      "returns a uniform measure of cardinality:  256\n",
      "returns a uniform measure of cardinality:  256\n",
      "the transport map is  tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        ...,\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.0039, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],\n",
      "       device='cuda:0')\n",
      "Ratio of trace to the matrix sum:  tensor(0.0117, device='cuda:0')\n",
      "Here, trace is 3.0 and matrix sum is 256.0 \n",
      "this is past correction for weight mode\n",
      "Shape of aligned wt is  torch.Size([256, 256, 9])\n",
      "Shape of fc_layer0_weight_data is  torch.Size([256, 256, 9])\n",
      "Previous layer shape is  torch.Size([256, 256, 3, 3])\n",
      "shape of layer: model 0 torch.Size([512, 256, 9])\n",
      "shape of layer: model 1 torch.Size([512, 256, 9])\n",
      "shape of previous transport map torch.Size([256, 256])\n",
      "Processing the coordinates to form ground_metric\n",
      "dont leave off the squaring of the ground metric\n",
      "returns a uniform measure of cardinality:  512\n",
      "returns a uniform measure of cardinality:  512\n",
      "the transport map is  tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')\n",
      "Ratio of trace to the matrix sum:  tensor(0.0039, device='cuda:0')\n",
      "Here, trace is 2.0 and matrix sum is 512.0 \n",
      "this is past correction for weight mode\n",
      "Shape of aligned wt is  torch.Size([512, 256, 9])\n",
      "Shape of fc_layer0_weight_data is  torch.Size([512, 256, 9])\n",
      "Previous layer shape is  torch.Size([512, 256, 3, 3])\n",
      "shape of layer: model 0 torch.Size([512, 512, 9])\n",
      "shape of layer: model 1 torch.Size([512, 512, 9])\n",
      "shape of previous transport map torch.Size([512, 512])\n",
      "Processing the coordinates to form ground_metric\n",
      "dont leave off the squaring of the ground metric\n",
      "returns a uniform measure of cardinality:  512\n",
      "returns a uniform measure of cardinality:  512\n",
      "the transport map is  tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')\n",
      "Ratio of trace to the matrix sum:  tensor(0.0020, device='cuda:0')\n",
      "Here, trace is 1.0 and matrix sum is 512.0 \n",
      "this is past correction for weight mode\n",
      "Shape of aligned wt is  torch.Size([512, 512, 9])\n",
      "Shape of fc_layer0_weight_data is  torch.Size([512, 512, 9])\n",
      "Previous layer shape is  torch.Size([512, 512, 3, 3])\n",
      "shape of layer: model 0 torch.Size([512, 512, 9])\n",
      "shape of layer: model 1 torch.Size([512, 512, 9])\n",
      "shape of previous transport map torch.Size([512, 512])\n",
      "Processing the coordinates to form ground_metric\n",
      "dont leave off the squaring of the ground metric\n",
      "returns a uniform measure of cardinality:  512\n",
      "returns a uniform measure of cardinality:  512\n",
      "the transport map is  tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')\n",
      "Ratio of trace to the matrix sum:  tensor(0.0020, device='cuda:0')\n",
      "Here, trace is 1.0 and matrix sum is 512.0 \n",
      "this is past correction for weight mode\n",
      "Shape of aligned wt is  torch.Size([512, 512, 9])\n",
      "Shape of fc_layer0_weight_data is  torch.Size([512, 512, 9])\n",
      "Previous layer shape is  torch.Size([512, 512, 3, 3])\n",
      "shape of layer: model 0 torch.Size([512, 512, 9])\n",
      "shape of layer: model 1 torch.Size([512, 512, 9])\n",
      "shape of previous transport map torch.Size([512, 512])\n",
      "Processing the coordinates to form ground_metric\n",
      "dont leave off the squaring of the ground metric\n",
      "returns a uniform measure of cardinality:  512\n",
      "returns a uniform measure of cardinality:  512\n",
      "the transport map is  tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        [0.0020, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        ...,\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],\n",
      "       device='cuda:0')\n",
      "Ratio of trace to the matrix sum:  tensor(0., device='cuda:0')\n",
      "Here, trace is 0.0 and matrix sum is 512.0 \n",
      "this is past correction for weight mode\n",
      "Shape of aligned wt is  torch.Size([512, 512, 9])\n",
      "Shape of fc_layer0_weight_data is  torch.Size([512, 512, 9])\n",
      "Previous layer shape is  torch.Size([512, 512, 3, 3])\n",
      "shape of layer: model 0 torch.Size([10, 512])\n",
      "shape of layer: model 1 torch.Size([10, 512])\n",
      "shape of previous transport map torch.Size([512, 512])\n",
      "Processing the coordinates to form ground_metric\n",
      "dont leave off the squaring of the ground metric\n",
      "ground metric is  tensor([[1.4325, 2.4911, 2.8079, 2.8856, 2.7468, 2.8870, 2.7353, 2.6645, 2.5804,\n",
      "         2.6195],\n",
      "        [2.5208, 1.3107, 2.7097, 2.5832, 2.7581, 2.5509, 2.4414, 2.5393, 2.3284,\n",
      "         2.4193],\n",
      "        [2.7692, 2.6807, 1.3577, 2.9864, 2.9482, 2.8599, 2.7595, 2.8709, 2.7691,\n",
      "         2.7716],\n",
      "        [2.8418, 2.6804, 2.9077, 1.5495, 2.8531, 2.9723, 2.7705, 2.8076, 2.8298,\n",
      "         2.6796],\n",
      "        [2.8005, 2.5947, 2.8385, 2.9068, 1.5305, 2.7492, 2.8050, 2.6826, 2.6967,\n",
      "         2.7531],\n",
      "        [2.9442, 2.5952, 2.9950, 2.8124, 2.7448, 1.5445, 2.7571, 2.7091, 2.8377,\n",
      "         2.7059],\n",
      "        [2.6948, 2.4699, 2.7141, 2.9192, 2.6649, 2.8837, 1.4110, 2.7135, 2.4951,\n",
      "         2.5593],\n",
      "        [2.7428, 2.5757, 2.7854, 2.8189, 2.7297, 2.7164, 2.7106, 1.3886, 2.5619,\n",
      "         2.5139],\n",
      "        [2.5881, 2.3878, 2.7745, 2.7406, 2.7236, 2.8404, 2.5578, 2.6629, 1.3044,\n",
      "         2.3747],\n",
      "        [2.5971, 2.2644, 2.8401, 2.7362, 2.6806, 2.7179, 2.5752, 2.5408, 2.5460,\n",
      "         1.3207]], device='cuda:0', grad_fn=<PowBackward0>)\n",
      "returns a uniform measure of cardinality:  10\n",
      "returns a uniform measure of cardinality:  10\n",
      "the transport map is  tensor([[0.1000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0000, 0.1000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0000, 0.0000, 0.1000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.1000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.1000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.1000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.1000, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.1000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.1000,\n",
      "         0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.1000]], device='cuda:0')\n",
      "Ratio of trace to the matrix sum:  tensor(1., device='cuda:0')\n",
      "Here, trace is 9.99755859375 and matrix sum is 9.99755859375 \n",
      "this is past correction for weight mode\n",
      "Shape of aligned wt is  torch.Size([10, 512])\n",
      "Shape of fc_layer0_weight_data is  torch.Size([10, 512])\n",
      "using independent method\n",
      "in _make_layers [Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), AvgPool2d(kernel_size=1, stride=1, padding=0)]\n",
      "Relu Inplace is  False\n",
      "model parameters are \n",
      " [torch.Size([64, 3, 3, 3]), torch.Size([128, 64, 3, 3]), torch.Size([256, 128, 3, 3]), torch.Size([256, 256, 3, 3]), torch.Size([512, 256, 3, 3]), torch.Size([512, 512, 3, 3]), torch.Size([512, 512, 3, 3]), torch.Size([512, 512, 3, 3]), torch.Size([10, 512])]\n",
      "\n",
      "--------- Testing in global mode ---------\n",
      "size of test_loader dataset:  10000\n",
      "\n",
      "Test set: Avg. loss: 0.0023, Accuracy: 776/10000 (8%)\n",
      "\n",
      "len of model parameters and avg aligned layers is  9 9\n",
      "len of model_state_dict is  9\n",
      "len of param_list is  9\n",
      "\n",
      "--------- Testing in global mode ---------\n",
      "size of test_loader dataset:  10000\n",
      "\n",
      "Test set: Avg. loss: 0.0012, Accuracy: 8607/10000 (86%)\n",
      "\n",
      "===============================================\n",
      "===============================================\n",
      "After Ensemble: 86.07\n",
      "===============================================\n",
      "===============================================\n",
      "Timer ends\n",
      "Time taken for geometric ensembling is 3.543765887618065 seconds\n",
      "------- Prediction based ensembling -------\n",
      "\n",
      "Test set: Avg. loss: 0.0003, Accuracy: 9120/10000 (91%)\n",
      "\n",
      "------- Naive ensembling of weights -------\n",
      "[torch.Size([64, 3, 3, 3]), torch.Size([64, 3, 3, 3])]\n",
      "torch.Size([64, 3, 3, 3])\n",
      "[torch.Size([128, 64, 3, 3]), torch.Size([128, 64, 3, 3])]\n",
      "torch.Size([128, 64, 3, 3])\n",
      "[torch.Size([256, 128, 3, 3]), torch.Size([256, 128, 3, 3])]\n",
      "torch.Size([256, 128, 3, 3])\n",
      "[torch.Size([256, 256, 3, 3]), torch.Size([256, 256, 3, 3])]\n",
      "torch.Size([256, 256, 3, 3])\n",
      "[torch.Size([512, 256, 3, 3]), torch.Size([512, 256, 3, 3])]\n",
      "torch.Size([512, 256, 3, 3])\n",
      "[torch.Size([512, 512, 3, 3]), torch.Size([512, 512, 3, 3])]\n",
      "torch.Size([512, 512, 3, 3])\n",
      "[torch.Size([512, 512, 3, 3]), torch.Size([512, 512, 3, 3])]\n",
      "torch.Size([512, 512, 3, 3])\n",
      "[torch.Size([512, 512, 3, 3]), torch.Size([512, 512, 3, 3])]\n",
      "torch.Size([512, 512, 3, 3])\n",
      "[torch.Size([10, 512]), torch.Size([10, 512])]\n",
      "torch.Size([10, 512])\n",
      "in _make_layers [Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), AvgPool2d(kernel_size=1, stride=1, padding=0)]\n",
      "Relu Inplace is  False\n",
      "model parameters are \n",
      " [torch.Size([64, 3, 3, 3]), torch.Size([128, 64, 3, 3]), torch.Size([256, 128, 3, 3]), torch.Size([256, 256, 3, 3]), torch.Size([512, 256, 3, 3]), torch.Size([512, 512, 3, 3]), torch.Size([512, 512, 3, 3]), torch.Size([512, 512, 3, 3]), torch.Size([10, 512])]\n",
      "\n",
      "--------- Testing in global mode ---------\n",
      "size of test_loader dataset:  10000\n",
      "\n",
      "Test set: Avg. loss: 0.0023, Accuracy: 1066/10000 (11%)\n",
      "\n",
      "\n",
      "--------- Testing in global mode ---------\n",
      "size of test_loader dataset:  10000\n",
      "\n",
      "Test set: Avg. loss: 0.0023, Accuracy: 1430/10000 (14%)\n",
      "\n",
      "-------- Retraining the models ---------\n",
      "Retraining model :  model_0\n",
      "num_epochs---------------+++))))))+++ 300\n",
      "num_epochs---------------+++))))))+++ 300\n",
      "lr is  0.05\n",
      "number of epochs would be  30\n",
      "num_epochs-------------- 30\n",
      "Epoch 000\n",
      "/root/miniconda3/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:129: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\"Detected call of `lr_scheduler.step()` before `optimizer.step()`. \"\n",
      "/root/miniconda3/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:154: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "accuracy: {'epoch': 0, 'value': 0.46289999999999987} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 0, 'value': 1.4781726442313183} ({'split': 'train'})\n",
      "accuracy: {'epoch': 0, 'value': 0.6501000344753266} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 0, 'value': 1.1087595582008363} ({'split': 'test'})\n",
      "Epoch 001\n",
      "accuracy: {'epoch': 1, 'value': 0.7291599999999998} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 1, 'value': 0.826286238250733} ({'split': 'train'})\n",
      "accuracy: {'epoch': 1, 'value': 0.7320000410079956} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 1, 'value': 0.8250816404819489} ({'split': 'test'})\n",
      "Epoch 002\n",
      "accuracy: {'epoch': 2, 'value': 0.7880999999999994} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 2, 'value': 0.6528088059806817} ({'split': 'train'})\n",
      "accuracy: {'epoch': 2, 'value': 0.7668000400066376} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 2, 'value': 0.730934739112854} ({'split': 'test'})\n",
      "Epoch 003\n",
      "accuracy: {'epoch': 3, 'value': 0.8120399999999987} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 3, 'value': 0.5733953863716125} ({'split': 'train'})\n",
      "accuracy: {'epoch': 3, 'value': 0.7851000368595124} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 3, 'value': 0.6722977876663209} ({'split': 'test'})\n",
      "Epoch 004\n",
      "accuracy: {'epoch': 4, 'value': 0.8281799999999998} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 4, 'value': 0.522081580562592} ({'split': 'train'})\n",
      "accuracy: {'epoch': 4, 'value': 0.742600041627884} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 4, 'value': 0.7877674579620361} ({'split': 'test'})\n",
      "Epoch 005\n",
      "accuracy: {'epoch': 5, 'value': 0.8413599999999998} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 5, 'value': 0.4835167716407773} ({'split': 'train'})\n",
      "accuracy: {'epoch': 5, 'value': 0.7761000335216522} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 5, 'value': 0.6785974979400635} ({'split': 'test'})\n",
      "Epoch 006\n",
      "accuracy: {'epoch': 6, 'value': 0.8492999999999998} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 6, 'value': 0.45949079609870924} ({'split': 'train'})\n",
      "accuracy: {'epoch': 6, 'value': 0.7769000351428985} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 6, 'value': 0.7717680156230928} ({'split': 'test'})\n",
      "Epoch 007\n",
      "accuracy: {'epoch': 7, 'value': 0.8543799999999994} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 7, 'value': 0.44271955781936656} ({'split': 'train'})\n",
      "accuracy: {'epoch': 7, 'value': 0.7910000324249268} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 7, 'value': 0.6960731267929077} ({'split': 'test'})\n",
      "Epoch 008\n",
      "accuracy: {'epoch': 8, 'value': 0.8610800000000001} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 8, 'value': 0.42570056202888484} ({'split': 'train'})\n",
      "accuracy: {'epoch': 8, 'value': 0.7890000283718108} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 8, 'value': 0.6952049136161804} ({'split': 'test'})\n",
      "Epoch 009\n",
      "accuracy: {'epoch': 9, 'value': 0.8681000000000014} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 9, 'value': 0.4033397150611873} ({'split': 'train'})\n",
      "accuracy: {'epoch': 9, 'value': 0.7845000445842742} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 9, 'value': 0.7178995668888092} ({'split': 'test'})\n",
      "Epoch 010\n",
      "accuracy: {'epoch': 10, 'value': 0.8682400000000006} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 10, 'value': 0.4090823437023163} ({'split': 'train'})\n",
      "accuracy: {'epoch': 10, 'value': 0.7613000392913818} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 10, 'value': 0.767329216003418} ({'split': 'test'})\n",
      "Epoch 011\n",
      "accuracy: {'epoch': 11, 'value': 0.8749000000000005} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 11, 'value': 0.38218344170808766} ({'split': 'train'})\n",
      "accuracy: {'epoch': 11, 'value': 0.789300036430359} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 11, 'value': 0.7229805827140808} ({'split': 'test'})\n",
      "Epoch 012\n",
      "accuracy: {'epoch': 12, 'value': 0.8726600000000002} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 12, 'value': 0.3849793776178355} ({'split': 'train'})\n",
      "accuracy: {'epoch': 12, 'value': 0.7916000425815582} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 12, 'value': 0.6692538261413574} ({'split': 'test'})\n",
      "Epoch 013\n",
      "accuracy: {'epoch': 13, 'value': 0.8781799999999991} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 13, 'value': 0.3750991787767409} ({'split': 'train'})\n",
      "accuracy: {'epoch': 13, 'value': 0.7958000361919403} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 13, 'value': 0.643027800321579} ({'split': 'test'})\n",
      "Epoch 014\n",
      "accuracy: {'epoch': 14, 'value': 0.8838600000000002} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 14, 'value': 0.3563724224233628} ({'split': 'train'})\n",
      "accuracy: {'epoch': 14, 'value': 0.7645000338554382} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 14, 'value': 0.8039357185363771} ({'split': 'test'})\n",
      "Epoch 015\n",
      "accuracy: {'epoch': 15, 'value': 0.8795399999999995} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 15, 'value': 0.3723861938476566} ({'split': 'train'})\n",
      "accuracy: {'epoch': 15, 'value': 0.8037000477313996} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 15, 'value': 0.6595152020454408} ({'split': 'test'})\n",
      "Epoch 016\n",
      "accuracy: {'epoch': 16, 'value': 0.8880200000000007} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 16, 'value': 0.349146549606323} ({'split': 'train'})\n",
      "accuracy: {'epoch': 16, 'value': 0.7815000355243683} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 16, 'value': 0.7728685617446899} ({'split': 'test'})\n",
      "Epoch 017\n",
      "accuracy: {'epoch': 17, 'value': 0.8816200000000005} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 17, 'value': 0.3663059703636166} ({'split': 'train'})\n",
      "accuracy: {'epoch': 17, 'value': 0.810200035572052} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 17, 'value': 0.648629641532898} ({'split': 'test'})\n",
      "Epoch 018\n",
      "accuracy: {'epoch': 18, 'value': 0.8779999999999991} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 18, 'value': 0.37434383252620684} ({'split': 'train'})\n",
      "accuracy: {'epoch': 18, 'value': 0.7528000354766846} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 18, 'value': 0.8281574130058289} ({'split': 'test'})\n",
      "Epoch 019\n",
      "^C\n",
      "Traceback (most recent call last):\n",
      "  File \"main.py\", line 351, in <module>\n",
      "    _, best_retrain_acc = routines.retrain_models(args, [*original_models, geometric_model, naive_model], retrain_loader, test_loader, config, tensorboard_obj=tensorboard_obj, initial_acc=initial_acc, nicks=nicks)\n",
      "  File \"/root/autodl-tmp/OTfusion_pruning/otfusion/routines.py\", line 394, in retrain_models\n",
      "    retrained_network, acc = cifar_train.get_retrained_model(args, retrain_loader, test_loader, old_networks[i], config, output_root_dir, tensorboard_obj=tensorboard_obj, nick=nick, start_acc=initial_acc[i])\n",
      "  File \"/root/autodl-tmp/OTfusion_pruning/otfusion/./cifar/train.py\", line 333, in get_retrained_model\n",
      "    best_acc,model = main(config, output_dir, args.gpu_id, pretrained_model=old_network, pretrained_dataset=(train_loader, test_loader), tensorboard_obj=tensorboard_obj,return_model=True)\n",
      "  File \"/root/autodl-tmp/OTfusion_pruning/otfusion/./cifar/train.py\", line 67, in main\n",
      "    for batch_x, batch_y in training_loader:\n",
      "  File \"/root/miniconda3/lib/python3.8/site-packages/torch/utils/data/dataloader.py\", line 521, in __next__\n",
      "    data = self._next_data()\n",
      "  File \"/root/miniconda3/lib/python3.8/site-packages/torch/utils/data/dataloader.py\", line 561, in _next_data\n",
      "    data = self._dataset_fetcher.fetch(index)  # may raise StopIteration\n",
      "  File \"/root/miniconda3/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py\", line 49, in fetch\n",
      "    data = [self.dataset[idx] for idx in possibly_batched_index]\n",
      "  File \"/root/miniconda3/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py\", line 49, in <listcomp>\n",
      "    data = [self.dataset[idx] for idx in possibly_batched_index]\n",
      "  File \"/root/miniconda3/lib/python3.8/site-packages/torchvision/datasets/cifar.py\", line 121, in __getitem__\n",
      "    img = self.transform(img)\n",
      "  File \"/root/miniconda3/lib/python3.8/site-packages/torchvision/transforms/transforms.py\", line 61, in __call__\n",
      "    img = t(img)\n",
      "  File \"/root/miniconda3/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1102, in _call_impl\n",
      "    return forward_call(*input, **kwargs)\n",
      "  File \"/root/miniconda3/lib/python3.8/site-packages/torchvision/transforms/transforms.py\", line 226, in forward\n",
      "    return F.normalize(tensor, self.mean, self.std, self.inplace)\n",
      "  File \"/root/miniconda3/lib/python3.8/site-packages/torchvision/transforms/functional.py\", line 345, in normalize\n",
      "    if (std == 0).any():\n",
      "KeyboardInterrupt\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m~/miniconda3/lib/python3.8/site-packages/IPython/utils/_process_posix.py\u001b[0m in \u001b[0;36msystem\u001b[0;34m(self, cmd)\u001b[0m\n\u001b[1;32m    161\u001b[0m                 \u001b[0;31m# know whether we've finished (if we matched EOF) or not\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 162\u001b[0;31m                 \u001b[0mres_idx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mchild\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexpect_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpatterns\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_timeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    163\u001b[0m                 \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchild\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbefore\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mout_size\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'replace'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m''\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.8/site-packages/pexpect/spawnbase.py\u001b[0m in \u001b[0;36mexpect_list\u001b[0;34m(self, pattern_list, timeout, searchwindowsize, async_, **kw)\u001b[0m\n\u001b[1;32m    371\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 372\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mexp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexpect_loop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    373\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.8/site-packages/pexpect/expect.py\u001b[0m in \u001b[0;36mexpect_loop\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    168\u001b[0m                 \u001b[0;31m# Still have time left, so read more data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 169\u001b[0;31m                 \u001b[0mincoming\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mspawn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_nonblocking\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mspawn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmaxread\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    170\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mspawn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdelayafterread\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.8/site-packages/pexpect/pty_spawn.py\u001b[0m in \u001b[0;36mread_nonblocking\u001b[0;34m(self, size, timeout)\u001b[0m\n\u001b[1;32m    499\u001b[0m         \u001b[0;31m# (possibly timeout=None), we call select() with a timeout.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 500\u001b[0;31m         \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mselect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    501\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mspawn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_nonblocking\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.8/site-packages/pexpect/pty_spawn.py\u001b[0m in \u001b[0;36mselect\u001b[0;34m(timeout)\u001b[0m\n\u001b[1;32m    449\u001b[0m             \u001b[0;32mdef\u001b[0m \u001b[0mselect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 450\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mselect_ignore_interrupts\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchild_fd\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    451\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.8/site-packages/pexpect/utils.py\u001b[0m in \u001b[0;36mselect_ignore_interrupts\u001b[0;34m(iwtd, owtd, ewtd, timeout)\u001b[0m\n\u001b[1;32m    142\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 143\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mselect\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miwtd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mowtd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mewtd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    144\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mInterruptedError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: ",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_9719/719175872.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msystem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'python main.py --gpu-id 0 --model-name vgg11_nobias --n-epochs 10 --save-result-file sample.csv  --sweep-name exp_sample --correction --ground-metric euclidean --weight-stats  --geom-ensemble-type wts --ground-metric-normalize none --sweep-id 90  --ckpt-type best --dataset Cifar10 --ground-metric-eff --recheck-cifar --activation-seed {activation_seed}  --prelu-acts --past-correction --not-squared --exact --learning-rate 0.0001 --momentum 0.5 --batch-size-train 64 --experiment-dir {experiment_dir} --prunint-rate {prunint_rate} --run-mode {run_mode} --to-download --load-model-dir {load_model_dir} --retrain 30'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/miniconda3/lib/python3.8/site-packages/ipykernel/zmqshell.py\u001b[0m in \u001b[0;36msystem_piped\u001b[0;34m(self, cmd)\u001b[0m\n\u001b[1;32m    635\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muser_ns\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'_exit_code'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msystem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcmd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    636\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 637\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muser_ns\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'_exit_code'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msystem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvar_expand\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcmd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdepth\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    638\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    639\u001b[0m     \u001b[0;31m# Ensure new system_piped implementation is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.8/site-packages/IPython/utils/_process_posix.py\u001b[0m in \u001b[0;36msystem\u001b[0;34m(self, cmd)\u001b[0m\n\u001b[1;32m    184\u001b[0m             \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    185\u001b[0m                 \u001b[0;31m# Ensure the subprocess really is terminated\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 186\u001b[0;31m                 \u001b[0mchild\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mterminate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mforce\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    187\u001b[0m         \u001b[0;31m# add isalive check, to ensure exitstatus is set:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    188\u001b[0m         \u001b[0mchild\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misalive\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.8/site-packages/pexpect/pty_spawn.py\u001b[0m in \u001b[0;36mterminate\u001b[0;34m(self, force)\u001b[0m\n\u001b[1;32m    653\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mforce\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    654\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkill\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msignal\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSIGKILL\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 655\u001b[0;31m                 \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdelayafterterminate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    656\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misalive\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    657\u001b[0m                     \u001b[0;32mreturn\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "!python main.py --gpu-id 0 --model-name vgg11_nobias --n-epochs 10 --save-result-file sample.csv \\\n",
    "--sweep-name exp_sample --correction --ground-metric euclidean --weight-stats \\\n",
    "--geom-ensemble-type wts --ground-metric-normalize none --sweep-id 90 \\\n",
    "--ckpt-type best --dataset Cifar10 --ground-metric-eff --recheck-cifar --activation-seed {activation_seed} \\\n",
    "--prelu-acts --past-correction --not-squared --exact\\\n",
    "--learning-rate 0.0001\\\n",
    "--momentum 0.5\\\n",
    "--batch-size-train 64 --experiment-dir {experiment_dir}\\\n",
    "--prunint-rate {prunint_rate}\\\n",
    "--run-mode {run_mode}\\\n",
    "--to-download\\\n",
    "--load-model-dir {load_model_dir}\\\n",
    "--retrain 30\n",
    "# After Ensemble: 86.07"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a78f1c0-85a7-4629-ba27-fcbf65e981c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "activation_seed=21\n",
    "experiment_dir=\"experiment1\"\n",
    "prunint_rate=0.2\n",
    "run_mode='benchmark'\n",
    "load_model_dir='./cifar_models'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c5b102d6-1a16-4de0-88d5-d56ced2e8d4a",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------- Setting up parameters -------\n",
      "dumping parameters at  /root/autodl-tmp/OTfusion_pruning/otfusion/exp_sample/configurations\n",
      "The parameters are: \n",
      " Namespace(act_bug=False, act_num_samples=100, activation_histograms=False, activation_mode=None, activation_seed=21, alpha=0.7, baseroot='/root/autodl-tmp/OTfusion_pruning/otfusion', batch_size_test=1000, batch_size_train=64, center_acts=False, choice='0 2 4 6 8', cifar_style_data=False, ckpt_type='best', clip_gm=False, clip_max=5, clip_min=0, config_dir='/root/autodl-tmp/OTfusion_pruning/otfusion/exp_sample/configurations', config_file=None, correction=True, csv_dir='/root/autodl-tmp/OTfusion_pruning/otfusion/exp_sample/csv', dataset='Cifar10', debug=False, deprecated=None, deterministic=False, diff_init=False, disable_bias=True, dist_epochs=60, dist_normalize=False, dump_final_models=False, dump_model=False, enable_dropout=False, ensemble_step=0.5, eval_aligned=True, exact=True, exp_name='exp_2023-09-16_10-25-05_499081', experiment_dir='experiment1', geom_ensemble_type='wts', gpu_id=0, gromov=False, gromov_loss='square_loss', ground_metric='euclidean', ground_metric_eff=True, ground_metric_normalize='none', handle_skips=False, importance=None, learning_rate=0.004, load_model_dir='./cifar_models', load_models='', log_interval=100, model_name='vgg11_nobias', momentum=0.5, n_epochs=0, no_random_trainloaders=False, normalize_acts=False, normalize_wts=False, not_squared=True, num_hidden_nodes=400, num_hidden_nodes1=400, num_hidden_nodes2=200, num_hidden_nodes3=100, num_hidden_nodes4=50, num_models=2, options_type='generic', partial_reshape=False, partition_dataloader=-1, partition_type='labels', past_correction=True, personal_class_idx=9, personal_split_frac=0.1, pool_acts=False, pool_relu=False, prediction_wts=False, prelu_acts=True, print_distances=False, proper_marginals=False, prunint_rate=0.2, recheck_acc=False, recheck_cifar=True, reg=0.01, reg_m=0.001, reinit_trainloaders=False, result_dir='/root/autodl-tmp/OTfusion_pruning/otfusion/exp_sample/results', retrain=30, retrain_avg_only=False, retrain_geometric_only=False, retrain_lr_decay=-1, retrain_lr_decay_epochs=None, retrain_lr_decay_factor=None, retrain_seed=-1, rootdir='/root/autodl-tmp/OTfusion_pruning/otfusion/exp_sample', run_mode='benchmark', same_model=-1, save_result_file='sample.csv', second_model_name=None, sinkhorn_type='normal', skip_last_layer=False, skip_last_layer_type='average', skip_personal_idx=False, skip_retrain=-1, softmax_temperature=1, standardize_acts=False, sweep_id=90, sweep_name='exp_sample', temperature=20, tensorboard=False, tensorboard_root='./tensorboard', timestamp='2023-09-16_10-25-05_499081', tmap_stats=False, to_download=True, transform_acts=False, unbalanced=False, update_acts=False, weight_stats=True, width_ratio=1)\n",
      "refactored get_config\n",
      "------- Obtain dataloaders -------\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "------- Training independent models -------\n",
      "QWERTY: Enter Cifar 1\n",
      "number of epochs would b>>>>>>>e  300\n",
      "=======================================================\n",
      "=======================================================\n",
      "=======================================================\n",
      "Print the config {'dataset': 'Cifar10', 'model': 'vgg11_nobias', 'optimizer': 'SGD', 'optimizer_decay_at_epochs': [30, 60, 90, 120, 150, 180, 210, 240, 270], 'optimizer_decay_with_factor': 2.0, 'optimizer_learning_rate': 0.05, 'optimizer_momentum': 0.9, 'optimizer_weight_decay': 0.0005, 'batch_size': 128, 'num_epochs': 300, 'seed': 42}\n",
      "in _make_layers [Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), AvgPool2d(kernel_size=1, stride=1, padding=0)]\n",
      "Relu Inplace is  False\n",
      "model parameters are \n",
      " [torch.Size([64, 3, 3, 3]), torch.Size([128, 64, 3, 3]), torch.Size([256, 128, 3, 3]), torch.Size([256, 256, 3, 3]), torch.Size([512, 256, 3, 3]), torch.Size([512, 512, 3, 3]), torch.Size([512, 512, 3, 3]), torch.Size([512, 512, 3, 3]), torch.Size([10, 512])]\n",
      "Loading model at path ./cifar_models/model_0/best.checkpoint which had accuracy 0.9030999821424489 and at epoch 127\n",
      "QWERTY: Enter Cifar 1\n",
      "number of epochs would b>>>>>>>e  300\n",
      "=======================================================\n",
      "=======================================================\n",
      "=======================================================\n",
      "Print the config {'dataset': 'Cifar10', 'model': 'vgg11_nobias', 'optimizer': 'SGD', 'optimizer_decay_at_epochs': [30, 60, 90, 120, 150, 180, 210, 240, 270], 'optimizer_decay_with_factor': 2.0, 'optimizer_learning_rate': 0.05, 'optimizer_momentum': 0.9, 'optimizer_weight_decay': 0.0005, 'batch_size': 128, 'num_epochs': 300, 'seed': 42}\n",
      "in _make_layers [Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), AvgPool2d(kernel_size=1, stride=1, padding=0)]\n",
      "Relu Inplace is  False\n",
      "model parameters are \n",
      " [torch.Size([64, 3, 3, 3]), torch.Size([128, 64, 3, 3]), torch.Size([256, 128, 3, 3]), torch.Size([256, 256, 3, 3]), torch.Size([512, 256, 3, 3]), torch.Size([512, 512, 3, 3]), torch.Size([512, 512, 3, 3]), torch.Size([512, 512, 3, 3]), torch.Size([10, 512])]\n",
      "Loading model at path ./cifar_models/model_1/best.checkpoint which had accuracy 0.9049999803304669 and at epoch 134\n",
      "num_epochs---------------++++++ 300\n",
      "========================================\n",
      "========================================\n",
      "TEST:args.learning_rate,args.momentum 0.004 0.5\n",
      "========================================\n",
      "========================================\n",
      "========================================\n",
      "\n",
      "--------- Testing in global mode ---------\n",
      "size of test_loader dataset:  10000\n",
      "\n",
      "Test set: Avg. loss: 0.0003, Accuracy: 9019/10000 (90%)\n",
      "\n",
      "========================================\n",
      "========================================\n",
      "TEST:args.learning_rate,args.momentum 0.004 0.5\n",
      "========================================\n",
      "========================================\n",
      "========================================\n",
      "\n",
      "--------- Testing in global mode ---------\n",
      "size of test_loader dataset:  10000\n",
      "\n",
      "Test set: Avg. loss: 0.0003, Accuracy: 9046/10000 (90%)\n",
      "\n",
      "models [VGG(\n",
      "  (features): Sequential(\n",
      "    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    (1): ReLU()\n",
      "    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (3): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    (4): ReLU()\n",
      "    (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (6): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    (7): ReLU()\n",
      "    (8): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    (9): ReLU()\n",
      "    (10): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (11): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    (12): ReLU()\n",
      "    (13): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    (14): ReLU()\n",
      "    (15): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (16): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    (17): ReLU()\n",
      "    (18): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    (19): ReLU()\n",
      "    (20): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (21): AvgPool2d(kernel_size=1, stride=1, padding=0)\n",
      "  )\n",
      "  (classifier): Linear(in_features=512, out_features=10, bias=False)\n",
      "), VGG(\n",
      "  (features): Sequential(\n",
      "    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    (1): ReLU()\n",
      "    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (3): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    (4): ReLU()\n",
      "    (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (6): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    (7): ReLU()\n",
      "    (8): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    (9): ReLU()\n",
      "    (10): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (11): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    (12): ReLU()\n",
      "    (13): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    (14): ReLU()\n",
      "    (15): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (16): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    (17): ReLU()\n",
      "    (18): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    (19): ReLU()\n",
      "    (20): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (21): AvgPool2d(kernel_size=1, stride=1, padding=0)\n",
      "  )\n",
      "  (classifier): Linear(in_features=512, out_features=10, bias=False)\n",
      ")]\n",
      "Apply Unstructured L1 Pruning Globally (all conv layers)\n",
      "Apply Unstructured L1 Pruning Globally (all conv layers)\n",
      "layer features.0.weight has #params  1728\n",
      "layer features.3.weight has #params  73728\n",
      "layer features.6.weight has #params  294912\n",
      "layer features.8.weight has #params  589824\n",
      "layer features.11.weight has #params  1179648\n",
      "layer features.13.weight has #params  2359296\n",
      "layer features.16.weight has #params  2359296\n",
      "layer features.18.weight has #params  2359296\n",
      "layer classifier.weight has #params  5120\n",
      "Activation Timer start\n",
      "Activation Timer ends\n",
      "------- Geometric Ensembling -------\n",
      "Timer start\n",
      "Previous layer shape is  None\n",
      "Processing the coordinates to form ground_metric\n",
      "dont leave off the squaring of the ground metric\n",
      "returns a uniform measure of cardinality:  64\n",
      "returns a uniform measure of cardinality:  64\n",
      "the transport map is  tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')\n",
      "Ratio of trace to the matrix sum:  tensor(0.0469, device='cuda:0')\n",
      "Here, trace is 3.0 and matrix sum is 64.0 \n",
      "this is past correction for weight mode\n",
      "Shape of aligned wt is  torch.Size([64, 3, 9])\n",
      "Shape of fc_layer0_weight_data is  torch.Size([64, 3, 9])\n",
      "in _make_layers [Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), AvgPool2d(kernel_size=1, stride=1, padding=0)]\n",
      "Relu Inplace is  False\n",
      "model parameters are \n",
      " [torch.Size([64, 3, 3, 3]), torch.Size([128, 64, 3, 3]), torch.Size([256, 128, 3, 3]), torch.Size([256, 256, 3, 3]), torch.Size([512, 256, 3, 3]), torch.Size([512, 512, 3, 3]), torch.Size([512, 512, 3, 3]), torch.Size([512, 512, 3, 3]), torch.Size([10, 512])]\n",
      "len of model_state_dict is  9\n",
      "len of new_params is  1\n",
      "updated parameters for layer  features.0.weight\n",
      "\n",
      "--------- Testing in global mode ---------\n",
      "size of test_loader dataset:  10000\n",
      "\n",
      "Test set: Avg. loss: 0.0027, Accuracy: 1249/10000 (12%)\n",
      "\n",
      "accuracy after update is  12.49\n",
      "For layer idx 0, accuracy of the updated model is 12.49\n",
      "Previous layer shape is  torch.Size([64, 3, 3, 3])\n",
      "shape of layer: model 0 torch.Size([128, 64, 9])\n",
      "shape of layer: model 1 torch.Size([128, 64, 9])\n",
      "shape of previous transport map torch.Size([64, 64])\n",
      "Processing the coordinates to form ground_metric\n",
      "dont leave off the squaring of the ground metric\n",
      "returns a uniform measure of cardinality:  128\n",
      "returns a uniform measure of cardinality:  128\n",
      "the transport map is  tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        ...,\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0078],\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],\n",
      "       device='cuda:0')\n",
      "Ratio of trace to the matrix sum:  tensor(0.0078, device='cuda:0')\n",
      "Here, trace is 1.0 and matrix sum is 128.0 \n",
      "this is past correction for weight mode\n",
      "Shape of aligned wt is  torch.Size([128, 64, 9])\n",
      "Shape of fc_layer0_weight_data is  torch.Size([128, 64, 9])\n",
      "in _make_layers [Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), AvgPool2d(kernel_size=1, stride=1, padding=0)]\n",
      "Relu Inplace is  False\n",
      "model parameters are \n",
      " [torch.Size([64, 3, 3, 3]), torch.Size([128, 64, 3, 3]), torch.Size([256, 128, 3, 3]), torch.Size([256, 256, 3, 3]), torch.Size([512, 256, 3, 3]), torch.Size([512, 512, 3, 3]), torch.Size([512, 512, 3, 3]), torch.Size([512, 512, 3, 3]), torch.Size([10, 512])]\n",
      "len of model_state_dict is  9\n",
      "len of new_params is  2\n",
      "updated parameters for layer  features.0.weight\n",
      "updated parameters for layer  features.3.weight\n",
      "\n",
      "--------- Testing in global mode ---------\n",
      "size of test_loader dataset:  10000\n",
      "\n",
      "Test set: Avg. loss: 0.0024, Accuracy: 1036/10000 (10%)\n",
      "\n",
      "accuracy after update is  10.36\n",
      "For layer idx 1, accuracy of the updated model is 10.36\n",
      "Previous layer shape is  torch.Size([128, 64, 3, 3])\n",
      "shape of layer: model 0 torch.Size([256, 128, 9])\n",
      "shape of layer: model 1 torch.Size([256, 128, 9])\n",
      "shape of previous transport map torch.Size([128, 128])\n",
      "Processing the coordinates to form ground_metric\n",
      "dont leave off the squaring of the ground metric\n",
      "returns a uniform measure of cardinality:  256\n",
      "returns a uniform measure of cardinality:  256\n",
      "the transport map is  tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')\n",
      "Ratio of trace to the matrix sum:  tensor(0., device='cuda:0')\n",
      "Here, trace is 0.0 and matrix sum is 256.0 \n",
      "this is past correction for weight mode\n",
      "Shape of aligned wt is  torch.Size([256, 128, 9])\n",
      "Shape of fc_layer0_weight_data is  torch.Size([256, 128, 9])\n",
      "in _make_layers [Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), AvgPool2d(kernel_size=1, stride=1, padding=0)]\n",
      "Relu Inplace is  False\n",
      "model parameters are \n",
      " [torch.Size([64, 3, 3, 3]), torch.Size([128, 64, 3, 3]), torch.Size([256, 128, 3, 3]), torch.Size([256, 256, 3, 3]), torch.Size([512, 256, 3, 3]), torch.Size([512, 512, 3, 3]), torch.Size([512, 512, 3, 3]), torch.Size([512, 512, 3, 3]), torch.Size([10, 512])]\n",
      "len of model_state_dict is  9\n",
      "len of new_params is  3\n",
      "updated parameters for layer  features.0.weight\n",
      "updated parameters for layer  features.3.weight\n",
      "updated parameters for layer  features.6.weight\n",
      "\n",
      "--------- Testing in global mode ---------\n",
      "size of test_loader dataset:  10000\n",
      "\n",
      "Test set: Avg. loss: 0.0023, Accuracy: 1146/10000 (11%)\n",
      "\n",
      "accuracy after update is  11.46\n",
      "For layer idx 2, accuracy of the updated model is 11.46\n",
      "Previous layer shape is  torch.Size([256, 128, 3, 3])\n",
      "shape of layer: model 0 torch.Size([256, 256, 9])\n",
      "shape of layer: model 1 torch.Size([256, 256, 9])\n",
      "shape of previous transport map torch.Size([256, 256])\n",
      "Processing the coordinates to form ground_metric\n",
      "dont leave off the squaring of the ground metric\n",
      "returns a uniform measure of cardinality:  256\n",
      "returns a uniform measure of cardinality:  256\n",
      "the transport map is  tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        ...,\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.0039, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],\n",
      "       device='cuda:0')\n",
      "Ratio of trace to the matrix sum:  tensor(0.0078, device='cuda:0')\n",
      "Here, trace is 2.0 and matrix sum is 256.0 \n",
      "this is past correction for weight mode\n",
      "Shape of aligned wt is  torch.Size([256, 256, 9])\n",
      "Shape of fc_layer0_weight_data is  torch.Size([256, 256, 9])\n",
      "in _make_layers [Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), AvgPool2d(kernel_size=1, stride=1, padding=0)]\n",
      "Relu Inplace is  False\n",
      "model parameters are \n",
      " [torch.Size([64, 3, 3, 3]), torch.Size([128, 64, 3, 3]), torch.Size([256, 128, 3, 3]), torch.Size([256, 256, 3, 3]), torch.Size([512, 256, 3, 3]), torch.Size([512, 512, 3, 3]), torch.Size([512, 512, 3, 3]), torch.Size([512, 512, 3, 3]), torch.Size([10, 512])]\n",
      "len of model_state_dict is  9\n",
      "len of new_params is  4\n",
      "updated parameters for layer  features.0.weight\n",
      "updated parameters for layer  features.3.weight\n",
      "updated parameters for layer  features.6.weight\n",
      "updated parameters for layer  features.8.weight\n",
      "\n",
      "--------- Testing in global mode ---------\n",
      "size of test_loader dataset:  10000\n",
      "\n",
      "Test set: Avg. loss: 0.0024, Accuracy: 1249/10000 (12%)\n",
      "\n",
      "accuracy after update is  12.49\n",
      "For layer idx 3, accuracy of the updated model is 12.49\n",
      "Previous layer shape is  torch.Size([256, 256, 3, 3])\n",
      "shape of layer: model 0 torch.Size([512, 256, 9])\n",
      "shape of layer: model 1 torch.Size([512, 256, 9])\n",
      "shape of previous transport map torch.Size([256, 256])\n",
      "Processing the coordinates to form ground_metric\n",
      "dont leave off the squaring of the ground metric\n",
      "returns a uniform measure of cardinality:  512\n",
      "returns a uniform measure of cardinality:  512\n",
      "the transport map is  tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')\n",
      "Ratio of trace to the matrix sum:  tensor(0.0039, device='cuda:0')\n",
      "Here, trace is 2.0 and matrix sum is 512.0 \n",
      "this is past correction for weight mode\n",
      "Shape of aligned wt is  torch.Size([512, 256, 9])\n",
      "Shape of fc_layer0_weight_data is  torch.Size([512, 256, 9])\n",
      "in _make_layers [Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), AvgPool2d(kernel_size=1, stride=1, padding=0)]\n",
      "Relu Inplace is  False\n",
      "model parameters are \n",
      " [torch.Size([64, 3, 3, 3]), torch.Size([128, 64, 3, 3]), torch.Size([256, 128, 3, 3]), torch.Size([256, 256, 3, 3]), torch.Size([512, 256, 3, 3]), torch.Size([512, 512, 3, 3]), torch.Size([512, 512, 3, 3]), torch.Size([512, 512, 3, 3]), torch.Size([10, 512])]\n",
      "len of model_state_dict is  9\n",
      "len of new_params is  5\n",
      "updated parameters for layer  features.0.weight\n",
      "updated parameters for layer  features.3.weight\n",
      "updated parameters for layer  features.6.weight\n",
      "updated parameters for layer  features.8.weight\n",
      "updated parameters for layer  features.11.weight\n",
      "\n",
      "--------- Testing in global mode ---------\n",
      "size of test_loader dataset:  10000\n",
      "\n",
      "Test set: Avg. loss: 0.0027, Accuracy: 903/10000 (9%)\n",
      "\n",
      "accuracy after update is  9.03\n",
      "For layer idx 4, accuracy of the updated model is 9.03\n",
      "Previous layer shape is  torch.Size([512, 256, 3, 3])\n",
      "shape of layer: model 0 torch.Size([512, 512, 9])\n",
      "shape of layer: model 1 torch.Size([512, 512, 9])\n",
      "shape of previous transport map torch.Size([512, 512])\n",
      "Processing the coordinates to form ground_metric\n",
      "dont leave off the squaring of the ground metric\n",
      "returns a uniform measure of cardinality:  512\n",
      "returns a uniform measure of cardinality:  512\n",
      "the transport map is  tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')\n",
      "Ratio of trace to the matrix sum:  tensor(0.0020, device='cuda:0')\n",
      "Here, trace is 1.0 and matrix sum is 512.0 \n",
      "this is past correction for weight mode\n",
      "Shape of aligned wt is  torch.Size([512, 512, 9])\n",
      "Shape of fc_layer0_weight_data is  torch.Size([512, 512, 9])\n",
      "in _make_layers [Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), AvgPool2d(kernel_size=1, stride=1, padding=0)]\n",
      "Relu Inplace is  False\n",
      "model parameters are \n",
      " [torch.Size([64, 3, 3, 3]), torch.Size([128, 64, 3, 3]), torch.Size([256, 128, 3, 3]), torch.Size([256, 256, 3, 3]), torch.Size([512, 256, 3, 3]), torch.Size([512, 512, 3, 3]), torch.Size([512, 512, 3, 3]), torch.Size([512, 512, 3, 3]), torch.Size([10, 512])]\n",
      "len of model_state_dict is  9\n",
      "len of new_params is  6\n",
      "updated parameters for layer  features.0.weight\n",
      "updated parameters for layer  features.3.weight\n",
      "updated parameters for layer  features.6.weight\n",
      "updated parameters for layer  features.8.weight\n",
      "updated parameters for layer  features.11.weight\n",
      "updated parameters for layer  features.13.weight\n",
      "\n",
      "--------- Testing in global mode ---------\n",
      "size of test_loader dataset:  10000\n",
      "\n",
      "Test set: Avg. loss: 0.0024, Accuracy: 1410/10000 (14%)\n",
      "\n",
      "accuracy after update is  14.1\n",
      "For layer idx 5, accuracy of the updated model is 14.1\n",
      "Previous layer shape is  torch.Size([512, 512, 3, 3])\n",
      "shape of layer: model 0 torch.Size([512, 512, 9])\n",
      "shape of layer: model 1 torch.Size([512, 512, 9])\n",
      "shape of previous transport map torch.Size([512, 512])\n",
      "Processing the coordinates to form ground_metric\n",
      "dont leave off the squaring of the ground metric\n",
      "returns a uniform measure of cardinality:  512\n",
      "returns a uniform measure of cardinality:  512\n",
      "the transport map is  tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')\n",
      "Ratio of trace to the matrix sum:  tensor(0.0020, device='cuda:0')\n",
      "Here, trace is 1.0 and matrix sum is 512.0 \n",
      "this is past correction for weight mode\n",
      "Shape of aligned wt is  torch.Size([512, 512, 9])\n",
      "Shape of fc_layer0_weight_data is  torch.Size([512, 512, 9])\n",
      "in _make_layers [Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), AvgPool2d(kernel_size=1, stride=1, padding=0)]\n",
      "Relu Inplace is  False\n",
      "model parameters are \n",
      " [torch.Size([64, 3, 3, 3]), torch.Size([128, 64, 3, 3]), torch.Size([256, 128, 3, 3]), torch.Size([256, 256, 3, 3]), torch.Size([512, 256, 3, 3]), torch.Size([512, 512, 3, 3]), torch.Size([512, 512, 3, 3]), torch.Size([512, 512, 3, 3]), torch.Size([10, 512])]\n",
      "len of model_state_dict is  9\n",
      "len of new_params is  7\n",
      "updated parameters for layer  features.0.weight\n",
      "updated parameters for layer  features.3.weight\n",
      "updated parameters for layer  features.6.weight\n",
      "updated parameters for layer  features.8.weight\n",
      "updated parameters for layer  features.11.weight\n",
      "updated parameters for layer  features.13.weight\n",
      "updated parameters for layer  features.16.weight\n",
      "\n",
      "--------- Testing in global mode ---------\n",
      "size of test_loader dataset:  10000\n",
      "\n",
      "Test set: Avg. loss: 0.0022, Accuracy: 2217/10000 (22%)\n",
      "\n",
      "accuracy after update is  22.17\n",
      "For layer idx 6, accuracy of the updated model is 22.17\n",
      "Previous layer shape is  torch.Size([512, 512, 3, 3])\n",
      "shape of layer: model 0 torch.Size([512, 512, 9])\n",
      "shape of layer: model 1 torch.Size([512, 512, 9])\n",
      "shape of previous transport map torch.Size([512, 512])\n",
      "Processing the coordinates to form ground_metric\n",
      "dont leave off the squaring of the ground metric\n",
      "returns a uniform measure of cardinality:  512\n",
      "returns a uniform measure of cardinality:  512\n",
      "the transport map is  tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        [0.0020, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        ...,\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],\n",
      "       device='cuda:0')\n",
      "Ratio of trace to the matrix sum:  tensor(0., device='cuda:0')\n",
      "Here, trace is 0.0 and matrix sum is 512.0 \n",
      "this is past correction for weight mode\n",
      "Shape of aligned wt is  torch.Size([512, 512, 9])\n",
      "Shape of fc_layer0_weight_data is  torch.Size([512, 512, 9])\n",
      "in _make_layers [Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), AvgPool2d(kernel_size=1, stride=1, padding=0)]\n",
      "Relu Inplace is  False\n",
      "model parameters are \n",
      " [torch.Size([64, 3, 3, 3]), torch.Size([128, 64, 3, 3]), torch.Size([256, 128, 3, 3]), torch.Size([256, 256, 3, 3]), torch.Size([512, 256, 3, 3]), torch.Size([512, 512, 3, 3]), torch.Size([512, 512, 3, 3]), torch.Size([512, 512, 3, 3]), torch.Size([10, 512])]\n",
      "len of model_state_dict is  9\n",
      "len of new_params is  8\n",
      "updated parameters for layer  features.0.weight\n",
      "updated parameters for layer  features.3.weight\n",
      "updated parameters for layer  features.6.weight\n",
      "updated parameters for layer  features.8.weight\n",
      "updated parameters for layer  features.11.weight\n",
      "updated parameters for layer  features.13.weight\n",
      "updated parameters for layer  features.16.weight\n",
      "updated parameters for layer  features.18.weight\n",
      "\n",
      "--------- Testing in global mode ---------\n",
      "size of test_loader dataset:  10000\n",
      "\n",
      "Test set: Avg. loss: 0.0024, Accuracy: 582/10000 (6%)\n",
      "\n",
      "accuracy after update is  5.82\n",
      "For layer idx 7, accuracy of the updated model is 5.82\n",
      "Previous layer shape is  torch.Size([512, 512, 3, 3])\n",
      "shape of layer: model 0 torch.Size([10, 512])\n",
      "shape of layer: model 1 torch.Size([10, 512])\n",
      "shape of previous transport map torch.Size([512, 512])\n",
      "Processing the coordinates to form ground_metric\n",
      "dont leave off the squaring of the ground metric\n",
      "ground metric is  tensor([[1.4241, 2.4872, 2.7804, 2.8719, 2.7374, 2.8799, 2.7276, 2.6342, 2.5641,\n",
      "         2.5922],\n",
      "        [2.4889, 1.3253, 2.6732, 2.5914, 2.7584, 2.5501, 2.4174, 2.5215, 2.3048,\n",
      "         2.4101],\n",
      "        [2.7600, 2.6711, 1.3832, 2.9465, 2.9065, 2.8701, 2.7276, 2.8385, 2.7485,\n",
      "         2.7866],\n",
      "        [2.8352, 2.6791, 2.8919, 1.5430, 2.8605, 2.9159, 2.7574, 2.7980, 2.8164,\n",
      "         2.6354],\n",
      "        [2.7551, 2.5960, 2.8635, 2.8450, 1.5042, 2.7447, 2.7853, 2.6811, 2.7031,\n",
      "         2.7294],\n",
      "        [2.9190, 2.5396, 2.9469, 2.8416, 2.7272, 1.5502, 2.7781, 2.7303, 2.7977,\n",
      "         2.6697],\n",
      "        [2.6778, 2.4457, 2.7190, 2.9111, 2.6488, 2.8526, 1.3931, 2.6890, 2.4981,\n",
      "         2.5560],\n",
      "        [2.7394, 2.5496, 2.7799, 2.7903, 2.7151, 2.6590, 2.7190, 1.3924, 2.5583,\n",
      "         2.5121],\n",
      "        [2.5972, 2.3660, 2.7693, 2.6872, 2.7026, 2.8280, 2.5325, 2.6366, 1.2999,\n",
      "         2.4041],\n",
      "        [2.5918, 2.2730, 2.7816, 2.7576, 2.6697, 2.7273, 2.5482, 2.5290, 2.5207,\n",
      "         1.2878]], device='cuda:0', grad_fn=<PowBackward0>)\n",
      "returns a uniform measure of cardinality:  10\n",
      "returns a uniform measure of cardinality:  10\n",
      "the transport map is  tensor([[0.1000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0000, 0.1000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0000, 0.0000, 0.1000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.1000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.1000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.1000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.1000, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.1000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.1000,\n",
      "         0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.1000]], device='cuda:0')\n",
      "Ratio of trace to the matrix sum:  tensor(1., device='cuda:0')\n",
      "Here, trace is 9.99755859375 and matrix sum is 9.99755859375 \n",
      "this is past correction for weight mode\n",
      "Shape of aligned wt is  torch.Size([10, 512])\n",
      "Shape of fc_layer0_weight_data is  torch.Size([10, 512])\n",
      "in _make_layers [Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), AvgPool2d(kernel_size=1, stride=1, padding=0)]\n",
      "Relu Inplace is  False\n",
      "model parameters are \n",
      " [torch.Size([64, 3, 3, 3]), torch.Size([128, 64, 3, 3]), torch.Size([256, 128, 3, 3]), torch.Size([256, 256, 3, 3]), torch.Size([512, 256, 3, 3]), torch.Size([512, 512, 3, 3]), torch.Size([512, 512, 3, 3]), torch.Size([512, 512, 3, 3]), torch.Size([10, 512])]\n",
      "len of model_state_dict is  9\n",
      "len of new_params is  9\n",
      "updated parameters for layer  features.0.weight\n",
      "updated parameters for layer  features.3.weight\n",
      "updated parameters for layer  features.6.weight\n",
      "updated parameters for layer  features.8.weight\n",
      "updated parameters for layer  features.11.weight\n",
      "updated parameters for layer  features.13.weight\n",
      "updated parameters for layer  features.16.weight\n",
      "updated parameters for layer  features.18.weight\n",
      "updated parameters for layer  classifier.weight\n",
      "\n",
      "--------- Testing in global mode ---------\n",
      "size of test_loader dataset:  10000\n",
      "\n",
      "Test set: Avg. loss: 0.0003, Accuracy: 9019/10000 (90%)\n",
      "\n",
      "accuracy after update is  90.19\n",
      "For layer idx 8, accuracy of the updated model is 90.19\n",
      "using independent method\n",
      "in _make_layers [Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), AvgPool2d(kernel_size=1, stride=1, padding=0)]\n",
      "Relu Inplace is  False\n",
      "model parameters are \n",
      " [torch.Size([64, 3, 3, 3]), torch.Size([128, 64, 3, 3]), torch.Size([256, 128, 3, 3]), torch.Size([256, 256, 3, 3]), torch.Size([512, 256, 3, 3]), torch.Size([512, 512, 3, 3]), torch.Size([512, 512, 3, 3]), torch.Size([512, 512, 3, 3]), torch.Size([10, 512])]\n",
      "\n",
      "--------- Testing in global mode ---------\n",
      "size of test_loader dataset:  10000\n",
      "\n",
      "Test set: Avg. loss: 0.0023, Accuracy: 1091/10000 (11%)\n",
      "\n",
      "len of model parameters and avg aligned layers is  9 9\n",
      "len of model_state_dict is  9\n",
      "len of param_list is  9\n",
      "\n",
      "--------- Testing in global mode ---------\n",
      "size of test_loader dataset:  10000\n",
      "\n",
      "Test set: Avg. loss: 0.0015, Accuracy: 8544/10000 (85%)\n",
      "\n",
      "===============================================\n",
      "===============================================\n",
      "After Ensemble: 85.44\n",
      "===============================================\n",
      "===============================================\n",
      "Timer ends\n",
      "Time taken for geometric ensembling is 35.47849554568529 seconds\n",
      "------- Prediction based ensembling -------\n",
      "\n",
      "Test set: Avg. loss: 0.0003, Accuracy: 9133/10000 (91%)\n",
      "\n",
      "------- Naive ensembling of weights -------\n",
      "[torch.Size([64, 3, 3, 3]), torch.Size([64, 3, 3, 3])]\n",
      "torch.Size([64, 3, 3, 3])\n",
      "[torch.Size([128, 64, 3, 3]), torch.Size([128, 64, 3, 3])]\n",
      "torch.Size([128, 64, 3, 3])\n",
      "[torch.Size([256, 128, 3, 3]), torch.Size([256, 128, 3, 3])]\n",
      "torch.Size([256, 128, 3, 3])\n",
      "[torch.Size([256, 256, 3, 3]), torch.Size([256, 256, 3, 3])]\n",
      "torch.Size([256, 256, 3, 3])\n",
      "[torch.Size([512, 256, 3, 3]), torch.Size([512, 256, 3, 3])]\n",
      "torch.Size([512, 256, 3, 3])\n",
      "[torch.Size([512, 512, 3, 3]), torch.Size([512, 512, 3, 3])]\n",
      "torch.Size([512, 512, 3, 3])\n",
      "[torch.Size([512, 512, 3, 3]), torch.Size([512, 512, 3, 3])]\n",
      "torch.Size([512, 512, 3, 3])\n",
      "[torch.Size([512, 512, 3, 3]), torch.Size([512, 512, 3, 3])]\n",
      "torch.Size([512, 512, 3, 3])\n",
      "[torch.Size([10, 512]), torch.Size([10, 512])]\n",
      "torch.Size([10, 512])\n",
      "in _make_layers [Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), AvgPool2d(kernel_size=1, stride=1, padding=0)]\n",
      "Relu Inplace is  False\n",
      "model parameters are \n",
      " [torch.Size([64, 3, 3, 3]), torch.Size([128, 64, 3, 3]), torch.Size([256, 128, 3, 3]), torch.Size([256, 256, 3, 3]), torch.Size([512, 256, 3, 3]), torch.Size([512, 512, 3, 3]), torch.Size([512, 512, 3, 3]), torch.Size([512, 512, 3, 3]), torch.Size([10, 512])]\n",
      "\n",
      "--------- Testing in global mode ---------\n",
      "size of test_loader dataset:  10000\n",
      "\n",
      "Test set: Avg. loss: 0.0023, Accuracy: 1003/10000 (10%)\n",
      "\n",
      "\n",
      "--------- Testing in global mode ---------\n",
      "size of test_loader dataset:  10000\n",
      "\n",
      "Test set: Avg. loss: 0.0023, Accuracy: 1689/10000 (17%)\n",
      "\n",
      "-------- Retraining the models ---------\n",
      "Retraining model :  model_0\n",
      "num_epochs---------------+++))))))+++ 300\n",
      "num_epochs---------------+++))))))+++ 300\n",
      "lr is  0.05\n",
      "number of epochs would be  30\n",
      "num_epochs-------------- 30\n",
      "Epoch 000\n",
      "/root/miniconda3/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:129: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\"Detected call of `lr_scheduler.step()` before `optimizer.step()`. \"\n",
      "/root/miniconda3/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:154: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "accuracy: {'epoch': 0, 'value': 0.4336000000000002} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 0, 'value': 1.5597181462955472} ({'split': 'train'})\n",
      "accuracy: {'epoch': 0, 'value': 0.6713000297546385} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 0, 'value': 0.9801202356815338} ({'split': 'test'})\n",
      "Epoch 001\n",
      "accuracy: {'epoch': 1, 'value': 0.7297600000000003} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 1, 'value': 0.8241118642234808} ({'split': 'train'})\n",
      "accuracy: {'epoch': 1, 'value': 0.7458000421524047} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 1, 'value': 0.7581176578998565} ({'split': 'test'})\n",
      "Epoch 002\n",
      "accuracy: {'epoch': 2, 'value': 0.7813599999999998} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 2, 'value': 0.6696514800357815} ({'split': 'train'})\n",
      "accuracy: {'epoch': 2, 'value': 0.7658000349998475} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 2, 'value': 0.7249000668525696} ({'split': 'test'})\n",
      "Epoch 003\n",
      "accuracy: {'epoch': 3, 'value': 0.8145399999999998} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 3, 'value': 0.564621856822967} ({'split': 'train'})\n",
      "accuracy: {'epoch': 3, 'value': 0.7531000256538392} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 3, 'value': 0.7701649069786072} ({'split': 'test'})\n",
      "Epoch 004\n",
      "accuracy: {'epoch': 4, 'value': 0.8280599999999995} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 4, 'value': 0.5265779199981689} ({'split': 'train'})\n",
      "accuracy: {'epoch': 4, 'value': 0.7901000320911407} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 4, 'value': 0.6598710715770723} ({'split': 'test'})\n",
      "Epoch 005\n",
      "accuracy: {'epoch': 5, 'value': 0.8370799999999992} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 5, 'value': 0.49813019565582267} ({'split': 'train'})\n",
      "accuracy: {'epoch': 5, 'value': 0.7936000287532806} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 5, 'value': 0.6691171824932098} ({'split': 'test'})\n",
      "Epoch 006\n",
      "accuracy: {'epoch': 6, 'value': 0.84734} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 6, 'value': 0.46337060040473904} ({'split': 'train'})\n",
      "accuracy: {'epoch': 6, 'value': 0.7907000362873078} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 6, 'value': 0.6734202086925507} ({'split': 'test'})\n",
      "Epoch 007\n",
      "accuracy: {'epoch': 7, 'value': 0.8553199999999999} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 7, 'value': 0.441009681873322} ({'split': 'train'})\n",
      "accuracy: {'epoch': 7, 'value': 0.7926000356674195} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 7, 'value': 0.7215842723846435} ({'split': 'test'})\n",
      "Epoch 008\n",
      "accuracy: {'epoch': 8, 'value': 0.8580200000000007} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 8, 'value': 0.43672133791446677} ({'split': 'train'})\n",
      "accuracy: {'epoch': 8, 'value': 0.7659000396728515} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 8, 'value': 0.7722597718238831} ({'split': 'test'})\n",
      "Epoch 009\n",
      "accuracy: {'epoch': 9, 'value': 0.8680199999999998} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 9, 'value': 0.4041635980033878} ({'split': 'train'})\n",
      "accuracy: {'epoch': 9, 'value': 0.787600040435791} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 9, 'value': 0.6870992124080658} ({'split': 'test'})\n",
      "Epoch 010\n",
      "accuracy: {'epoch': 10, 'value': 0.8661799999999993} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 10, 'value': 0.4053042185401919} ({'split': 'train'})\n",
      "accuracy: {'epoch': 10, 'value': 0.7779000461101532} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 10, 'value': 0.7810189187526704} ({'split': 'test'})\n",
      "Epoch 011\n",
      "accuracy: {'epoch': 11, 'value': 0.8683600000000004} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 11, 'value': 0.4106298659515381} ({'split': 'train'})\n",
      "accuracy: {'epoch': 11, 'value': 0.78340003490448} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 11, 'value': 0.7214223980903626} ({'split': 'test'})\n",
      "Epoch 012\n",
      "accuracy: {'epoch': 12, 'value': 0.86858} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 12, 'value': 0.4060838037776946} ({'split': 'train'})\n",
      "accuracy: {'epoch': 12, 'value': 0.7826000392436981} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 12, 'value': 0.7811884343624116} ({'split': 'test'})\n",
      "Epoch 013\n",
      "accuracy: {'epoch': 13, 'value': 0.8745399999999994} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 13, 'value': 0.38643022662639603} ({'split': 'train'})\n",
      "accuracy: {'epoch': 13, 'value': 0.7889000356197357} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 13, 'value': 0.7206459224224089} ({'split': 'test'})\n",
      "Epoch 014\n",
      "accuracy: {'epoch': 14, 'value': 0.8773999999999998} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 14, 'value': 0.378996676464081} ({'split': 'train'})\n",
      "accuracy: {'epoch': 14, 'value': 0.7685000360012055} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 14, 'value': 0.8538445293903351} ({'split': 'test'})\n",
      "Epoch 015\n",
      "accuracy: {'epoch': 15, 'value': 0.8744800000000001} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 15, 'value': 0.38972430214881854} ({'split': 'train'})\n",
      "accuracy: {'epoch': 15, 'value': 0.7941000342369079} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 15, 'value': 0.6818698287010193} ({'split': 'test'})\n",
      "Epoch 016\n",
      "accuracy: {'epoch': 16, 'value': 0.885179999999999} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 16, 'value': 0.3622493941354754} ({'split': 'train'})\n",
      "accuracy: {'epoch': 16, 'value': 0.796500039100647} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 16, 'value': 0.781831818819046} ({'split': 'test'})\n",
      "Epoch 017\n",
      "accuracy: {'epoch': 17, 'value': 0.8827199999999997} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 17, 'value': 0.36708273889780063} ({'split': 'train'})\n",
      "accuracy: {'epoch': 17, 'value': 0.7708000421524047} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 17, 'value': 0.7427750289440155} ({'split': 'test'})\n",
      "Epoch 018\n",
      "accuracy: {'epoch': 18, 'value': 0.8781000000000007} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 18, 'value': 0.3761559832382206} ({'split': 'train'})\n",
      "accuracy: {'epoch': 18, 'value': 0.7759000360965729} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 18, 'value': 0.7987822651863099} ({'split': 'test'})\n",
      "Epoch 019\n",
      "accuracy: {'epoch': 19, 'value': 0.8807000000000008} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 19, 'value': 0.37083543067932184} ({'split': 'train'})\n",
      "accuracy: {'epoch': 19, 'value': 0.7736000418663025} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 19, 'value': 0.7551369607448578} ({'split': 'test'})\n",
      "Epoch 020\n",
      "accuracy: {'epoch': 20, 'value': 0.8820399999999996} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 20, 'value': 0.361744907941818} ({'split': 'train'})\n",
      "accuracy: {'epoch': 20, 'value': 0.7601000308990478} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 20, 'value': 0.8091262161731719} ({'split': 'test'})\n",
      "Epoch 021\n",
      "accuracy: {'epoch': 21, 'value': 0.8845800000000003} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 21, 'value': 0.35935356058120765} ({'split': 'train'})\n",
      "accuracy: {'epoch': 21, 'value': 0.7866000413894653} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 21, 'value': 0.7499563694000244} ({'split': 'test'})\n",
      "Epoch 022\n",
      "accuracy: {'epoch': 22, 'value': 0.8894200000000007} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 22, 'value': 0.3403456424617768} ({'split': 'train'})\n",
      "accuracy: {'epoch': 22, 'value': 0.7830000340938569} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 22, 'value': 0.7549613058567046} ({'split': 'test'})\n",
      "Epoch 023\n",
      "accuracy: {'epoch': 25, 'value': 0.8904999999999996} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 25, 'value': 0.34193298570632985} ({'split': 'train'})\n",
      "accuracy: {'epoch': 25, 'value': 0.7815000414848328} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 25, 'value': 0.7715516805648803} ({'split': 'test'})\n",
      "Epoch 026\n",
      "accuracy: {'epoch': 26, 'value': 0.8886600000000013} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 26, 'value': 0.3446659360599519} ({'split': 'train'})\n",
      "accuracy: {'epoch': 26, 'value': 0.7733000338077545} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 26, 'value': 0.7918766796588899} ({'split': 'test'})\n",
      "Epoch 027\n",
      "accuracy: {'epoch': 27, 'value': 0.8841799999999997} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 27, 'value': 0.35458068172454826} ({'split': 'train'})\n",
      "accuracy: {'epoch': 27, 'value': 0.8018000304698943} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 27, 'value': 0.6729396283626556} ({'split': 'test'})\n",
      "Epoch 028\n",
      "accuracy: {'epoch': 28, 'value': 0.8891599999999995} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 28, 'value': 0.3437848457622529} ({'split': 'train'})\n",
      "accuracy: {'epoch': 28, 'value': 0.7695000410079956} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 28, 'value': 0.8653207838535308} ({'split': 'test'})\n",
      "Epoch 029\n",
      "accuracy: {'epoch': 29, 'value': 0.8875800000000001} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 29, 'value': 0.35262730140209186} ({'split': 'train'})\n",
      "accuracy: {'epoch': 29, 'value': 0.7907000243663789} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 29, 'value': 0.7512350857257842} ({'split': 'test'})\n",
      "mean_test_accuracy <cifar_utils.accumulators.Mean object at 0x7f2aad59b3d0>\n",
      "QWERTY: Enter Cifar 2, acc 0.8018000304698943\n",
      "Retraining model :  model_1\n",
      "num_epochs---------------+++))))))+++ 30\n",
      "num_epochs---------------+++))))))+++ 30\n",
      "lr is  0.05\n",
      "number of epochs would be  30\n",
      "num_epochs-------------- 30\n",
      "Epoch 000\n",
      "accuracy: {'epoch': 0, 'value': 0.4562399999999997} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 0, 'value': 1.496698154916764} ({'split': 'train'})\n",
      "accuracy: {'epoch': 0, 'value': 0.7115000307559967} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 0, 'value': 0.8756510436534881} ({'split': 'test'})\n",
      "Epoch 001\n",
      "accuracy: {'epoch': 1, 'value': 0.7396399999999997} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 1, 'value': 0.8022125393104566} ({'split': 'train'})\n",
      "accuracy: {'epoch': 1, 'value': 0.761500036716461} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 1, 'value': 0.7302771985530853} ({'split': 'test'})\n",
      "Epoch 002\n",
      "accuracy: {'epoch': 2, 'value': 0.7912800000000003} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 2, 'value': 0.6435718333625792} ({'split': 'train'})\n",
      "accuracy: {'epoch': 2, 'value': 0.7750000417232513} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 2, 'value': 0.7061725318431854} ({'split': 'test'})\n",
      "Epoch 003\n",
      "accuracy: {'epoch': 3, 'value': 0.8162999999999998} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 3, 'value': 0.5576388818359374} ({'split': 'train'})\n",
      "accuracy: {'epoch': 3, 'value': 0.7696000397205353} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 3, 'value': 0.7479754745960236} ({'split': 'test'})\n",
      "Epoch 004\n",
      "accuracy: {'epoch': 4, 'value': 0.8327800000000001} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 4, 'value': 0.5099289708137514} ({'split': 'train'})\n",
      "accuracy: {'epoch': 4, 'value': 0.7567000329494477} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 4, 'value': 0.7603942573070526} ({'split': 'test'})\n",
      "Epoch 005\n",
      "accuracy: {'epoch': 5, 'value': 0.8438000000000007} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 5, 'value': 0.4749817955398563} ({'split': 'train'})\n",
      "accuracy: {'epoch': 5, 'value': 0.7863000333309174} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 5, 'value': 0.6923391103744505} ({'split': 'test'})\n",
      "Epoch 006\n",
      "accuracy: {'epoch': 6, 'value': 0.8524400000000002} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 6, 'value': 0.45001534381866426} ({'split': 'train'})\n",
      "accuracy: {'epoch': 6, 'value': 0.7624000370502473} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 6, 'value': 0.821131807565689} ({'split': 'test'})\n",
      "Epoch 007\n",
      "accuracy: {'epoch': 7, 'value': 0.8577799999999999} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 7, 'value': 0.4336183850860595} ({'split': 'train'})\n",
      "accuracy: {'epoch': 7, 'value': 0.8062000453472138} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 7, 'value': 0.6474069118499756} ({'split': 'test'})\n",
      "Epoch 008\n",
      "accuracy: {'epoch': 8, 'value': 0.8606999999999989} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 8, 'value': 0.4213998666000364} ({'split': 'train'})\n",
      "accuracy: {'epoch': 8, 'value': 0.8039000332355499} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 8, 'value': 0.6299012899398804} ({'split': 'test'})\n",
      "Epoch 009\n",
      "accuracy: {'epoch': 9, 'value': 0.8705800000000001} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 9, 'value': 0.39901255009412734} ({'split': 'train'})\n",
      "accuracy: {'epoch': 9, 'value': 0.7828000247478485} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 9, 'value': 0.7423131942749024} ({'split': 'test'})\n",
      "Epoch 010\n",
      "accuracy: {'epoch': 10, 'value': 0.8684999999999999} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 10, 'value': 0.40051394992291944} ({'split': 'train'})\n",
      "accuracy: {'epoch': 10, 'value': 0.78790003657341} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 10, 'value': 0.7405853033065797} ({'split': 'test'})\n",
      "Epoch 011\n",
      "accuracy: {'epoch': 11, 'value': 0.8790399999999987} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 11, 'value': 0.37526750086784366} ({'split': 'train'})\n",
      "accuracy: {'epoch': 11, 'value': 0.7529000401496887} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 11, 'value': 0.8844749212265015} ({'split': 'test'})\n",
      "Epoch 012\n",
      "accuracy: {'epoch': 12, 'value': 0.8727799999999999} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 12, 'value': 0.3940035326480871} ({'split': 'train'})\n",
      "accuracy: {'epoch': 12, 'value': 0.7744000375270844} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 12, 'value': 0.7863071262836456} ({'split': 'test'})\n",
      "Epoch 013\n",
      "accuracy: {'epoch': 13, 'value': 0.8805999999999999} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 13, 'value': 0.3762404103851319} ({'split': 'train'})\n",
      "accuracy: {'epoch': 13, 'value': 0.7987000286579132} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 13, 'value': 0.6668142974376678} ({'split': 'test'})\n",
      "Epoch 014\n",
      "accuracy: {'epoch': 14, 'value': 0.8776199999999988} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 14, 'value': 0.3790101517868039} ({'split': 'train'})\n",
      "accuracy: {'epoch': 14, 'value': 0.7832000434398652} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 14, 'value': 0.7833970129489899} ({'split': 'test'})\n",
      "Epoch 015\n",
      "accuracy: {'epoch': 15, 'value': 0.8863400000000005} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 15, 'value': 0.3520174220085142} ({'split': 'train'})\n",
      "accuracy: {'epoch': 15, 'value': 0.8015000402927398} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 15, 'value': 0.6719992339611054} ({'split': 'test'})\n",
      "Epoch 016\n",
      "accuracy: {'epoch': 16, 'value': 0.8879400000000001} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 16, 'value': 0.3468151648044589} ({'split': 'train'})\n",
      "accuracy: {'epoch': 16, 'value': 0.8029000401496886} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 16, 'value': 0.7086418151855468} ({'split': 'test'})\n",
      "Epoch 017\n",
      "accuracy: {'epoch': 17, 'value': 0.8865199999999999} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 17, 'value': 0.3528199974060061} ({'split': 'train'})\n",
      "accuracy: {'epoch': 17, 'value': 0.7965000391006469} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 17, 'value': 0.6700470209121704} ({'split': 'test'})\n",
      "Epoch 018\n",
      "accuracy: {'epoch': 18, 'value': 0.8836800000000002} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 18, 'value': 0.36027796088695474} ({'split': 'train'})\n",
      "accuracy: {'epoch': 18, 'value': 0.7464000344276428} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 18, 'value': 0.9345724999904633} ({'split': 'test'})\n",
      "Epoch 019\n",
      "accuracy: {'epoch': 19, 'value': 0.8822200000000003} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 19, 'value': 0.3631704073953623} ({'split': 'train'})\n",
      "accuracy: {'epoch': 19, 'value': 0.7701000392436981} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 19, 'value': 0.8051477551460267} ({'split': 'test'})\n",
      "Epoch 020\n",
      "accuracy: {'epoch': 20, 'value': 0.8887999999999988} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 20, 'value': 0.3430306144428252} ({'split': 'train'})\n",
      "accuracy: {'epoch': 20, 'value': 0.7840000450611114} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 20, 'value': 0.727930200099945} ({'split': 'test'})\n",
      "Epoch 021\n",
      "accuracy: {'epoch': 21, 'value': 0.8862800000000005} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 21, 'value': 0.35263646491050715} ({'split': 'train'})\n",
      "accuracy: {'epoch': 21, 'value': 0.7920000433921814} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 21, 'value': 0.7039123952388763} ({'split': 'test'})\n",
      "Epoch 022\n",
      "accuracy: {'epoch': 22, 'value': 0.8949599999999996} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 22, 'value': 0.32281504945755} ({'split': 'train'})\n",
      "accuracy: {'epoch': 22, 'value': 0.7661000311374664} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 22, 'value': 0.736232316493988} ({'split': 'test'})\n",
      "Epoch 023\n",
      "accuracy: {'epoch': 23, 'value': 0.8886800000000004} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 23, 'value': 0.34536652940750107} ({'split': 'train'})\n",
      "accuracy: {'epoch': 23, 'value': 0.7937000453472137} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 23, 'value': 0.7506037533283233} ({'split': 'test'})\n",
      "Epoch 024\n",
      "accuracy: {'epoch': 24, 'value': 0.8912799999999999} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 24, 'value': 0.33646515978813146} ({'split': 'train'})\n",
      "accuracy: {'epoch': 24, 'value': 0.7760000348091126} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 24, 'value': 0.800137460231781} ({'split': 'test'})\n",
      "Epoch 025\n",
      "accuracy: {'epoch': 25, 'value': 0.891459999999999} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 25, 'value': 0.34031870148658755} ({'split': 'train'})\n",
      "accuracy: {'epoch': 25, 'value': 0.7586000263690948} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 25, 'value': 0.8300036609172821} ({'split': 'test'})\n",
      "Epoch 026\n",
      "accuracy: {'epoch': 26, 'value': 0.8915000000000001} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 26, 'value': 0.33868253534316983} ({'split': 'train'})\n",
      "accuracy: {'epoch': 26, 'value': 0.7952000439167023} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 26, 'value': 0.7199432194232941} ({'split': 'test'})\n",
      "Epoch 027\n",
      "accuracy: {'epoch': 27, 'value': 0.8945200000000003} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 27, 'value': 0.32929059752464246} ({'split': 'train'})\n",
      "accuracy: {'epoch': 27, 'value': 0.7878000378608704} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 27, 'value': 0.7372381269931794} ({'split': 'test'})\n",
      "Epoch 028\n",
      "accuracy: {'epoch': 28, 'value': 0.8939799999999997} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 28, 'value': 0.3328656266784667} ({'split': 'train'})\n",
      "accuracy: {'epoch': 28, 'value': 0.7779000461101531} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 28, 'value': 0.7272949159145355} ({'split': 'test'})\n",
      "Epoch 029\n",
      "accuracy: {'epoch': 29, 'value': 0.8935199999999992} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 29, 'value': 0.33136307847022994} ({'split': 'train'})\n",
      "accuracy: {'epoch': 29, 'value': 0.7899000406265259} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 29, 'value': 0.8140536367893219} ({'split': 'test'})\n",
      "mean_test_accuracy <cifar_utils.accumulators.Mean object at 0x7f2aad59b490>\n",
      "QWERTY: Enter Cifar 2, acc 0.8062000453472138\n",
      "Retraining model :  geometric\n",
      "num_epochs---------------+++))))))+++ 30\n",
      "num_epochs---------------+++))))))+++ 30\n",
      "lr is  0.05\n",
      "number of epochs would be  30\n",
      "num_epochs-------------- 30\n",
      "Epoch 000\n",
      "accuracy: {'epoch': 0, 'value': 0.10548000000000002} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 0, 'value': 2.3047519077301026} ({'split': 'train'})\n",
      "accuracy: {'epoch': 0, 'value': 0.10000000447034836} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 0, 'value': 2.3025848865509033} ({'split': 'test'})\n",
      "Epoch 001\n",
      "accuracy: {'epoch': 1, 'value': 0.10002000000000001} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 1, 'value': 2.302584886703491} ({'split': 'train'})\n",
      "accuracy: {'epoch': 1, 'value': 0.10000000521540642} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 1, 'value': 2.3025848865509033} ({'split': 'test'})\n",
      "Epoch 002\n",
      "accuracy: {'epoch': 2, 'value': 0.09999999999999995} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 2, 'value': 2.302584886703491} ({'split': 'train'})\n",
      "accuracy: {'epoch': 2, 'value': 0.10000000447034835} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 2, 'value': 2.3025848865509033} ({'split': 'test'})\n",
      "Epoch 003\n",
      "accuracy: {'epoch': 3, 'value': 0.09999999999999995} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 3, 'value': 2.302584886703491} ({'split': 'train'})\n",
      "accuracy: {'epoch': 3, 'value': 0.1000000037252903} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 3, 'value': 2.3025848865509033} ({'split': 'test'})\n",
      "Epoch 004\n",
      "accuracy: {'epoch': 4, 'value': 0.09999999999999994} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 4, 'value': 2.302584886703491} ({'split': 'train'})\n",
      "accuracy: {'epoch': 4, 'value': 0.1000000037252903} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 4, 'value': 2.3025848865509033} ({'split': 'test'})\n",
      "Epoch 005\n",
      "accuracy: {'epoch': 5, 'value': 0.10000000000000009} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 5, 'value': 2.302584886703491} ({'split': 'train'})\n",
      "accuracy: {'epoch': 5, 'value': 0.10000000447034836} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 5, 'value': 2.3025848865509033} ({'split': 'test'})\n",
      "Epoch 006\n",
      "accuracy: {'epoch': 6, 'value': 0.09999999999999999} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 6, 'value': 2.302584886703491} ({'split': 'train'})\n",
      "accuracy: {'epoch': 6, 'value': 0.10000000447034835} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 6, 'value': 2.3025848865509033} ({'split': 'test'})\n",
      "Epoch 007\n",
      "accuracy: {'epoch': 7, 'value': 0.09999999999999991} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 7, 'value': 2.302584886703491} ({'split': 'train'})\n",
      "accuracy: {'epoch': 7, 'value': 0.1000000037252903} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 7, 'value': 2.3025848865509033} ({'split': 'test'})\n",
      "Epoch 008\n",
      "accuracy: {'epoch': 8, 'value': 0.09999999999999995} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 8, 'value': 2.302584886703491} ({'split': 'train'})\n",
      "accuracy: {'epoch': 8, 'value': 0.1000000037252903} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 8, 'value': 2.3025848865509033} ({'split': 'test'})\n",
      "Epoch 009\n",
      "accuracy: {'epoch': 9, 'value': 0.09999999999999996} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 9, 'value': 2.302584886703491} ({'split': 'train'})\n",
      "accuracy: {'epoch': 9, 'value': 0.10000000298023225} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 9, 'value': 2.3025848865509033} ({'split': 'test'})\n",
      "Epoch 010\n",
      "accuracy: {'epoch': 10, 'value': 0.09999999999999995} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 10, 'value': 2.302584886703491} ({'split': 'train'})\n",
      "accuracy: {'epoch': 10, 'value': 0.10000000447034836} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 10, 'value': 2.3025848865509033} ({'split': 'test'})\n",
      "Epoch 011\n",
      "accuracy: {'epoch': 11, 'value': 0.10000000000000003} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 11, 'value': 2.302584886703491} ({'split': 'train'})\n",
      "accuracy: {'epoch': 11, 'value': 0.10000000596046449} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 11, 'value': 2.3025848865509033} ({'split': 'test'})\n",
      "Epoch 012\n",
      "accuracy: {'epoch': 12, 'value': 0.10000000000000006} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 12, 'value': 2.302584886703491} ({'split': 'train'})\n",
      "accuracy: {'epoch': 12, 'value': 0.10000000521540643} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 12, 'value': 2.3025848865509033} ({'split': 'test'})\n",
      "Epoch 013\n",
      "accuracy: {'epoch': 13, 'value': 0.09999999999999999} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 13, 'value': 2.302584886703491} ({'split': 'train'})\n",
      "accuracy: {'epoch': 13, 'value': 0.10000000447034837} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 13, 'value': 2.3025848865509033} ({'split': 'test'})\n",
      "Epoch 014\n",
      "accuracy: {'epoch': 14, 'value': 0.09999999999999998} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 14, 'value': 2.302584886703491} ({'split': 'train'})\n",
      "accuracy: {'epoch': 14, 'value': 0.10000000447034836} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 14, 'value': 2.3025848865509033} ({'split': 'test'})\n",
      "Epoch 015\n",
      "accuracy: {'epoch': 15, 'value': 0.09999999999999994} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 15, 'value': 2.302584886703491} ({'split': 'train'})\n",
      "accuracy: {'epoch': 15, 'value': 0.10000000670552253} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 15, 'value': 2.3025848865509033} ({'split': 'test'})\n",
      "Epoch 016\n",
      "accuracy: {'epoch': 16, 'value': 0.10000000000000009} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 16, 'value': 2.302584886703491} ({'split': 'train'})\n",
      "accuracy: {'epoch': 16, 'value': 0.10000000447034836} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 16, 'value': 2.3025848865509033} ({'split': 'test'})\n",
      "Epoch 017\n",
      "accuracy: {'epoch': 17, 'value': 0.10000000000000002} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 17, 'value': 2.302584886703491} ({'split': 'train'})\n",
      "accuracy: {'epoch': 17, 'value': 0.10000000521540642} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 17, 'value': 2.3025848865509033} ({'split': 'test'})\n",
      "Epoch 018\n",
      "accuracy: {'epoch': 18, 'value': 0.09999999999999996} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 18, 'value': 2.302584886703491} ({'split': 'train'})\n",
      "accuracy: {'epoch': 18, 'value': 0.10000000521540642} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 18, 'value': 2.3025848865509033} ({'split': 'test'})\n",
      "Epoch 019\n",
      "accuracy: {'epoch': 19, 'value': 0.1000000000000001} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 19, 'value': 2.302584886703491} ({'split': 'train'})\n",
      "accuracy: {'epoch': 19, 'value': 0.10000000447034836} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 19, 'value': 2.3025848865509033} ({'split': 'test'})\n",
      "Epoch 020\n",
      "accuracy: {'epoch': 20, 'value': 0.1} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 20, 'value': 2.302584886703491} ({'split': 'train'})\n",
      "accuracy: {'epoch': 20, 'value': 0.10000000447034836} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 20, 'value': 2.3025848865509033} ({'split': 'test'})\n",
      "Epoch 021\n",
      "accuracy: {'epoch': 21, 'value': 0.10000000000000002} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 21, 'value': 2.302584886703491} ({'split': 'train'})\n",
      "accuracy: {'epoch': 21, 'value': 0.10000000596046447} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 21, 'value': 2.3025848865509033} ({'split': 'test'})\n",
      "Epoch 022\n",
      "accuracy: {'epoch': 22, 'value': 0.09999999999999998} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 22, 'value': 2.302584886703491} ({'split': 'train'})\n",
      "accuracy: {'epoch': 22, 'value': 0.10000000447034836} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 22, 'value': 2.3025848865509033} ({'split': 'test'})\n",
      "Epoch 023\n",
      "accuracy: {'epoch': 23, 'value': 0.10000000000000013} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 23, 'value': 2.302584886703491} ({'split': 'train'})\n",
      "accuracy: {'epoch': 23, 'value': 0.10000000521540642} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 23, 'value': 2.3025848865509033} ({'split': 'test'})\n",
      "Epoch 024\n",
      "accuracy: {'epoch': 24, 'value': 0.10000000000000003} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 24, 'value': 2.302584886703491} ({'split': 'train'})\n",
      "accuracy: {'epoch': 24, 'value': 0.10000000521540642} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 24, 'value': 2.3025848865509033} ({'split': 'test'})\n",
      "Epoch 025\n",
      "accuracy: {'epoch': 25, 'value': 0.10000000000000005} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 25, 'value': 2.302584886703491} ({'split': 'train'})\n",
      "accuracy: {'epoch': 25, 'value': 0.10000000521540642} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 25, 'value': 2.3025848865509033} ({'split': 'test'})\n",
      "Epoch 026\n",
      "accuracy: {'epoch': 26, 'value': 0.10000000000000002} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 26, 'value': 2.302584886703491} ({'split': 'train'})\n",
      "accuracy: {'epoch': 26, 'value': 0.1000000037252903} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 26, 'value': 2.3025848865509033} ({'split': 'test'})\n",
      "Epoch 027\n",
      "accuracy: {'epoch': 27, 'value': 0.09999999999999995} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 27, 'value': 2.302584886703491} ({'split': 'train'})\n",
      "accuracy: {'epoch': 27, 'value': 0.10000000447034836} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 27, 'value': 2.3025848865509033} ({'split': 'test'})\n",
      "Epoch 028\n",
      "accuracy: {'epoch': 28, 'value': 0.1} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 28, 'value': 2.302584886703491} ({'split': 'train'})\n",
      "accuracy: {'epoch': 28, 'value': 0.10000000447034836} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 28, 'value': 2.3025848865509033} ({'split': 'test'})\n",
      "Epoch 029\n",
      "accuracy: {'epoch': 29, 'value': 0.10000000000000012} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 29, 'value': 2.302584886703491} ({'split': 'train'})\n",
      "accuracy: {'epoch': 29, 'value': 0.10000000596046449} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 29, 'value': 2.3025848865509033} ({'split': 'test'})\n",
      "mean_test_accuracy <cifar_utils.accumulators.Mean object at 0x7f2aad58bd60>\n",
      "QWERTY: Enter Cifar 2, acc 0.10000000670552253\n",
      "Retraining model :  naive_averaging\n",
      "num_epochs---------------+++))))))+++ 30\n",
      "num_epochs---------------+++))))))+++ 30\n",
      "lr is  0.05\n",
      "number of epochs would be  30\n",
      "num_epochs-------------- 30\n",
      "Epoch 000\n",
      "accuracy: {'epoch': 0, 'value': 0.1818000000000001} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 0, 'value': 2.1084258403396605} ({'split': 'train'})\n",
      "accuracy: {'epoch': 0, 'value': 0.29000001251697544} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 0, 'value': 2.00372257232666} ({'split': 'test'})\n",
      "Epoch 001\n",
      "accuracy: {'epoch': 1, 'value': 0.41307999999999967} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 1, 'value': 1.5593513153457639} ({'split': 'train'})\n",
      "accuracy: {'epoch': 1, 'value': 0.4804000228643417} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 1, 'value': 1.436147403717041} ({'split': 'test'})\n",
      "Epoch 002\n",
      "accuracy: {'epoch': 2, 'value': 0.5633599999999995} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 2, 'value': 1.2410577666091918} ({'split': 'train'})\n",
      "accuracy: {'epoch': 2, 'value': 0.6057000219821931} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 2, 'value': 1.1310455203056335} ({'split': 'test'})\n",
      "Epoch 003\n",
      "accuracy: {'epoch': 3, 'value': 0.6448999999999999} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 3, 'value': 1.0355456029510495} ({'split': 'train'})\n",
      "accuracy: {'epoch': 3, 'value': 0.6595000207424163} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 3, 'value': 1.0092097103595734} ({'split': 'test'})\n",
      "Epoch 004\n",
      "accuracy: {'epoch': 4, 'value': 0.6929599999999994} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 4, 'value': 0.9104726698303223} ({'split': 'train'})\n",
      "accuracy: {'epoch': 4, 'value': 0.6862000286579132} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 4, 'value': 0.9560130357742309} ({'split': 'test'})\n",
      "Epoch 005\n",
      "accuracy: {'epoch': 5, 'value': 0.7254400000000005} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 5, 'value': 0.8230020975494388} ({'split': 'train'})\n",
      "accuracy: {'epoch': 5, 'value': 0.7016000390052795} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 5, 'value': 0.931545615196228} ({'split': 'test'})\n",
      "Epoch 006\n",
      "accuracy: {'epoch': 6, 'value': 0.7560800000000003} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 6, 'value': 0.7380233353424076} ({'split': 'train'})\n",
      "accuracy: {'epoch': 6, 'value': 0.7365000367164611} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 6, 'value': 0.7930045664310454} ({'split': 'test'})\n",
      "Epoch 007\n",
      "accuracy: {'epoch': 7, 'value': 0.7677000000000005} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 7, 'value': 0.7053903690338134} ({'split': 'train'})\n",
      "accuracy: {'epoch': 7, 'value': 0.7321000277996064} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 7, 'value': 0.8166541755199432} ({'split': 'test'})\n",
      "Epoch 008\n",
      "accuracy: {'epoch': 8, 'value': 0.7808999999999993} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 8, 'value': 0.670510867290497} ({'split': 'train'})\n",
      "accuracy: {'epoch': 8, 'value': 0.7476000308990479} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 8, 'value': 0.7851340174674988} ({'split': 'test'})\n",
      "Epoch 009\n",
      "accuracy: {'epoch': 9, 'value': 0.7931599999999998} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 9, 'value': 0.6288634703350067} ({'split': 'train'})\n",
      "accuracy: {'epoch': 9, 'value': 0.714900028705597} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 9, 'value': 0.8830189943313599} ({'split': 'test'})\n",
      "Epoch 010\n",
      "accuracy: {'epoch': 10, 'value': 0.8060800000000004} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 10, 'value': 0.5915442943668356} ({'split': 'train'})\n",
      "accuracy: {'epoch': 10, 'value': 0.7544000327587128} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 10, 'value': 0.7752367436885833} ({'split': 'test'})\n",
      "Epoch 011\n",
      "accuracy: {'epoch': 11, 'value': 0.8147400000000006} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 11, 'value': 0.5666357011795046} ({'split': 'train'})\n",
      "accuracy: {'epoch': 11, 'value': 0.7673000276088714} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 11, 'value': 0.7452292919158935} ({'split': 'test'})\n",
      "Epoch 012\n",
      "accuracy: {'epoch': 12, 'value': 0.8215399999999993} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 12, 'value': 0.5385940451908102} ({'split': 'train'})\n",
      "accuracy: {'epoch': 12, 'value': 0.7758000373840332} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 12, 'value': 0.7684906601905823} ({'split': 'test'})\n",
      "Epoch 013\n",
      "accuracy: {'epoch': 13, 'value': 0.8263999999999997} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 13, 'value': 0.5294094088745124} ({'split': 'train'})\n",
      "accuracy: {'epoch': 13, 'value': 0.7405000329017639} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 13, 'value': 0.8507984519004823} ({'split': 'test'})\n",
      "Epoch 014\n",
      "accuracy: {'epoch': 14, 'value': 0.8327999999999992} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 14, 'value': 0.5077233056163789} ({'split': 'train'})\n",
      "accuracy: {'epoch': 14, 'value': 0.7593000471591949} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 14, 'value': 0.8864212810993195} ({'split': 'test'})\n",
      "Epoch 015\n",
      "accuracy: {'epoch': 15, 'value': 0.8390800000000007} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 15, 'value': 0.49282721412658687} ({'split': 'train'})\n",
      "accuracy: {'epoch': 15, 'value': 0.7870000302791595} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 15, 'value': 0.6905732035636902} ({'split': 'test'})\n",
      "Epoch 016\n",
      "accuracy: {'epoch': 16, 'value': 0.8408599999999995} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 16, 'value': 0.48485248830795274} ({'split': 'train'})\n",
      "accuracy: {'epoch': 16, 'value': 0.7647000312805176} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 16, 'value': 0.7677149832248688} ({'split': 'test'})\n",
      "Epoch 017\n",
      "accuracy: {'epoch': 17, 'value': 0.8466000000000005} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 17, 'value': 0.46630572265625014} ({'split': 'train'})\n",
      "accuracy: {'epoch': 17, 'value': 0.7615000307559967} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 17, 'value': 0.774495267868042} ({'split': 'test'})\n",
      "Epoch 018\n",
      "accuracy: {'epoch': 18, 'value': 0.8548000000000004} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 18, 'value': 0.44244194842338586} ({'split': 'train'})\n",
      "accuracy: {'epoch': 18, 'value': 0.7836000323295593} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 18, 'value': 0.7346784889698028} ({'split': 'test'})\n",
      "Epoch 019\n",
      "accuracy: {'epoch': 19, 'value': 0.8513599999999995} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 19, 'value': 0.4528655918788907} ({'split': 'train'})\n",
      "accuracy: {'epoch': 19, 'value': 0.7727000296115876} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 19, 'value': 0.7576559960842133} ({'split': 'test'})\n",
      "Epoch 020\n",
      "accuracy: {'epoch': 20, 'value': 0.8535000000000004} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 20, 'value': 0.4416997969341278} ({'split': 'train'})\n",
      "accuracy: {'epoch': 20, 'value': 0.7704000413417816} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 20, 'value': 0.7616524457931518} ({'split': 'test'})\n",
      "Epoch 021\n",
      "accuracy: {'epoch': 21, 'value': 0.8582400000000001} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 21, 'value': 0.43410323319435107} ({'split': 'train'})\n",
      "accuracy: {'epoch': 21, 'value': 0.7851000368595124} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 21, 'value': 0.7218056201934814} ({'split': 'test'})\n",
      "Epoch 022\n",
      "accuracy: {'epoch': 22, 'value': 0.8590800000000013} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 22, 'value': 0.4304526620197294} ({'split': 'train'})\n",
      "accuracy: {'epoch': 22, 'value': 0.7629000306129455} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 22, 'value': 0.7964264631271363} ({'split': 'test'})\n",
      "Epoch 023\n",
      "accuracy: {'epoch': 23, 'value': 0.8570999999999996} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 23, 'value': 0.43874537193298346} ({'split': 'train'})\n",
      "accuracy: {'epoch': 23, 'value': 0.7834000408649444} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 23, 'value': 0.7295995056629181} ({'split': 'test'})\n",
      "Epoch 024\n",
      "accuracy: {'epoch': 24, 'value': 0.8617} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 24, 'value': 0.4195485930061342} ({'split': 'train'})\n",
      "accuracy: {'epoch': 24, 'value': 0.7871000409126282} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 24, 'value': 0.7368532538414002} ({'split': 'test'})\n",
      "Epoch 025\n",
      "accuracy: {'epoch': 25, 'value': 0.8661000000000009} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 25, 'value': 0.41145368385314945} ({'split': 'train'})\n",
      "accuracy: {'epoch': 25, 'value': 0.7763000369071961} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 25, 'value': 0.7468551576137542} ({'split': 'test'})\n",
      "Epoch 026\n",
      "accuracy: {'epoch': 26, 'value': 0.8684400000000007} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 26, 'value': 0.4110174777984616} ({'split': 'train'})\n",
      "accuracy: {'epoch': 26, 'value': 0.7855000376701355} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 26, 'value': 0.7419838845729827} ({'split': 'test'})\n",
      "Epoch 027\n",
      "accuracy: {'epoch': 27, 'value': 0.8673} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 27, 'value': 0.4019548893642424} ({'split': 'train'})\n",
      "accuracy: {'epoch': 27, 'value': 0.7728000342845918} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 27, 'value': 0.8169683396816254} ({'split': 'test'})\n",
      "Epoch 028\n",
      "accuracy: {'epoch': 28, 'value': 0.8700799999999992} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 28, 'value': 0.40044418381691016} ({'split': 'train'})\n",
      "accuracy: {'epoch': 28, 'value': 0.7898000478744507} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 28, 'value': 0.7550821602344513} ({'split': 'test'})\n",
      "Epoch 029\n",
      "accuracy: {'epoch': 29, 'value': 0.8686200000000005} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 29, 'value': 0.4080447007179266} ({'split': 'train'})\n",
      "accuracy: {'epoch': 29, 'value': 0.7714000344276428} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 29, 'value': 0.8107502162456512} ({'split': 'test'})\n",
      "mean_test_accuracy <cifar_utils.accumulators.Mean object at 0x7f2aad5a0820>\n",
      "QWERTY: Enter Cifar 2, acc 0.7898000478744507\n",
      "----- Saved results at sample.csv ------\n",
      "{'exp_name': 'exp_2023-09-16_10-25-05_499081', 'model0_acc': 90.30999821424489, 'model1_acc': 90.4999980330467, 'geometric_acc': 85.44, 'prediction_acc': 91.33, 'naive_acc': 16.89, 'geometric_gain': -5.059998033046696, 'geometric_gain_%': -5.591158169085267, 'prediction_gain': 0.8300019669533043, 'prediction_gain_%': 0.917129265185424, 'relative_loss_wrt_prediction': 6.508287434270691, 'model0_aligned': 90.19, 'geometric_time': 35.47849554568529, 'retrain_geometric_best': 10.000000670552254, 'retrain_naive_best': 78.98000478744507, 'retrain_model0_best': 80.18000304698944, 'retrain_model1_best': 80.62000453472137, 'retrain_epochs': 30}\n",
      "FYI: the parameters were: \n",
      " Namespace(act_bug=False, act_num_samples=100, activation_histograms=False, activation_mode=None, activation_seed=21, activation_time=2.627447247505188e-05, alpha=0.7, baseroot='/root/autodl-tmp/OTfusion_pruning/otfusion', batch_size_test=1000, batch_size_train=64, center_acts=False, choice='0 2 4 6 8', cifar_style_data=False, ckpt_type='best', clip_gm=False, clip_max=5, clip_min=0, config={'dataset': 'Cifar10', 'model': 'vgg11_nobias', 'optimizer': 'SGD', 'optimizer_decay_at_epochs': [30, 60, 90, 120, 150, 180, 210, 240, 270], 'optimizer_decay_with_factor': 2.0, 'optimizer_learning_rate': 0.05, 'optimizer_momentum': 0.9, 'optimizer_weight_decay': 0.0005, 'batch_size': 128, 'num_epochs': 30, 'seed': 42, 'nick': 'naive_averaging', 'start_acc': 16.89}, config_dir='/root/autodl-tmp/OTfusion_pruning/otfusion/exp_sample/configurations', config_file=None, correction=True, csv_dir='/root/autodl-tmp/OTfusion_pruning/otfusion/exp_sample/csv', dataset='Cifar10', debug=False, deprecated=None, deterministic=False, diff_init=False, disable_bias=True, dist_epochs=60, dist_normalize=False, dump_final_models=False, dump_model=False, enable_dropout=False, ensemble_step=0.5, eval_aligned=True, exact=True, exp_name='exp_2023-09-16_10-25-05_499081', experiment_dir='experiment1', geom_ensemble_type='wts', geometric_time=35.47849554568529, gpu_id=0, gromov=False, gromov_loss='square_loss', ground_metric='euclidean', ground_metric_eff=True, ground_metric_normalize='none', handle_skips=False, importance=None, learning_rate=0.004, load_model_dir='./cifar_models', load_models='', log_interval=100, model0_aligned_acc=90.19, model0_aligned_acc_layer_0=12.49, model0_aligned_acc_layer_1=10.36, model0_aligned_acc_layer_2=11.46, model0_aligned_acc_layer_3=12.49, model0_aligned_acc_layer_4=9.03, model0_aligned_acc_layer_5=14.1, model0_aligned_acc_layer_6=22.17, model0_aligned_acc_layer_7=5.82, model0_aligned_acc_layer_8=90.19, model_name='vgg11_nobias', momentum=0.5, n_epochs=0, no_random_trainloaders=False, normalize_acts=False, normalize_wts=False, not_squared=True, num_hidden_nodes=400, num_hidden_nodes1=400, num_hidden_nodes2=200, num_hidden_nodes3=100, num_hidden_nodes4=50, num_models=2, options_type='generic', params_geometric=9222848, params_model_0=9222848, params_model_1=9222848, partial_reshape=False, partition_dataloader=-1, partition_type='labels', past_correction=True, personal_class_idx=9, personal_split_frac=0.1, pool_acts=False, pool_relu=False, prediction_wts=False, prelu_acts=True, print_distances=False, proper_marginals=False, prunint_rate=0.2, recheck_acc=False, recheck_cifar=True, reg=0.01, reg_m=0.001, reinit_trainloaders=False, result_dir='/root/autodl-tmp/OTfusion_pruning/otfusion/exp_sample/results', retrain=30, retrain_avg_only=False, retrain_geometric_best=0.10000000670552253, retrain_geometric_only=False, retrain_lr_decay=-1, retrain_lr_decay_epochs=None, retrain_lr_decay_factor=None, retrain_model0_best=0.8018000304698943, retrain_model1_best=0.8062000453472138, retrain_naive_best=0.7898000478744507, retrain_seed=-1, rootdir='/root/autodl-tmp/OTfusion_pruning/otfusion/exp_sample', run_mode='benchmark', same_model=-1, save_result_file='sample.csv', second_config={'dataset': 'Cifar10', 'model': 'vgg11_nobias', 'optimizer': 'SGD', 'optimizer_decay_at_epochs': [30, 60, 90, 120, 150, 180, 210, 240, 270], 'optimizer_decay_with_factor': 2.0, 'optimizer_learning_rate': 0.05, 'optimizer_momentum': 0.9, 'optimizer_weight_decay': 0.0005, 'batch_size': 128, 'num_epochs': 30, 'seed': 42, 'nick': 'naive_averaging', 'start_acc': 16.89}, second_model_name=None, sinkhorn_type='normal', skip_last_layer=False, skip_last_layer_type='average', skip_personal_idx=False, skip_retrain=-1, softmax_temperature=1, standardize_acts=False, sweep_id=90, sweep_name='exp_sample', temperature=20, tensorboard=False, tensorboard_root='./tensorboard', timestamp='2023-09-16_10-25-05_499081', tmap_stats=False, to_download=True, transform_acts=False, unbalanced=False, update_acts=False, weight_stats=True, width_ratio=1, **{'trace_sum_ratio_classifier.weight': 1.0, 'trace_sum_ratio_features.0.weight': 0.046875, 'trace_sum_ratio_features.11.weight': 0.00390625, 'trace_sum_ratio_features.13.weight': 0.001953125, 'trace_sum_ratio_features.16.weight': 0.001953125, 'trace_sum_ratio_features.18.weight': 0.0, 'trace_sum_ratio_features.3.weight': 0.0078125, 'trace_sum_ratio_features.6.weight': 0.0, 'trace_sum_ratio_features.8.weight': 0.0078125})\n"
     ]
    }
   ],
   "source": [
    "!python main.py --gpu-id 0 --model-name vgg11_nobias --n-epochs 0 --save-result-file sample.csv \\\n",
    "--sweep-name exp_sample --correction --ground-metric euclidean --weight-stats \\\n",
    "--geom-ensemble-type wts --ground-metric-normalize none --sweep-id 90 \\\n",
    "--ckpt-type best --dataset Cifar10 --ground-metric-eff --recheck-cifar --activation-seed {activation_seed} \\\n",
    "--prelu-acts --past-correction --not-squared --exact\\\n",
    "--learning-rate 0.004\\\n",
    "--momentum 0.5\\\n",
    "--batch-size-train 64 --experiment-dir {experiment_dir}\\\n",
    "--prunint-rate {prunint_rate}\\\n",
    "--run-mode {run_mode}\\\n",
    "--to-download\\\n",
    "--load-model-dir {load_model_dir}\\\n",
    "--retrain 30\\\n",
    "--eval-aligned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c4db3b58-04ae-4bdf-b853-68b86e28644a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#剪枝实验1:prune without finetune\n",
    "activation_seed=21\n",
    "experiment_dir=\"experiment1\"\n",
    "prunint_rate=0.2\n",
    "run_mode='prune'\n",
    "load_model_dir='./cifar_models'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "acf10f3e-1c4a-40b3-85af-c6fc57512703",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------- Setting up parameters -------\n",
      "dumping parameters at  /root/autodl-tmp/OTfusion_pruning/otfusion/exp_sample/configurations\n",
      "The parameters are: \n",
      " Namespace(act_bug=False, act_num_samples=100, activation_histograms=False, activation_mode=None, activation_seed=21, alpha=0.7, baseroot='/root/autodl-tmp/OTfusion_pruning/otfusion', batch_size_test=1000, batch_size_train=64, center_acts=False, choice='0 2 4 6 8', cifar_style_data=False, ckpt_type='best', clip_gm=False, clip_max=5, clip_min=0, config_dir='/root/autodl-tmp/OTfusion_pruning/otfusion/exp_sample/configurations', config_file=None, correction=True, csv_dir='/root/autodl-tmp/OTfusion_pruning/otfusion/exp_sample/csv', dataset='Cifar10', debug=False, deprecated=None, deterministic=False, diff_init=False, disable_bias=True, dist_epochs=60, dist_normalize=False, dump_final_models=False, dump_model=False, enable_dropout=False, ensemble_step=0.5, eval_aligned=False, exact=True, exp_name='exp_2023-09-16_14-53-20_717630', experiment_dir='experiment1', geom_ensemble_type='wts', gpu_id=0, gromov=False, gromov_loss='square_loss', ground_metric='euclidean', ground_metric_eff=True, ground_metric_normalize='none', handle_skips=False, importance=None, learning_rate=0.08, load_model_dir='./cifar_models', load_models='', log_interval=100, model_name='vgg11_nobias', momentum=0.5, n_epochs=90, no_random_trainloaders=False, normalize_acts=False, normalize_wts=False, not_squared=True, num_hidden_nodes=400, num_hidden_nodes1=400, num_hidden_nodes2=200, num_hidden_nodes3=100, num_hidden_nodes4=50, num_models=2, options_type='generic', partial_reshape=False, partition_dataloader=-1, partition_type='labels', past_correction=True, personal_class_idx=9, personal_split_frac=0.1, pool_acts=False, pool_relu=False, prediction_wts=False, prelu_acts=True, print_distances=False, proper_marginals=False, prunint_rate=0.2, recheck_acc=False, recheck_cifar=True, reg=0.01, reg_m=0.001, reinit_trainloaders=False, result_dir='/root/autodl-tmp/OTfusion_pruning/otfusion/exp_sample/results', retrain=30, retrain_avg_only=False, retrain_geometric_only=False, retrain_lr_decay=-1, retrain_lr_decay_epochs=None, retrain_lr_decay_factor=None, retrain_seed=-1, rootdir='/root/autodl-tmp/OTfusion_pruning/otfusion/exp_sample', run_mode='prune', same_model=-1, save_result_file='sample.csv', second_model_name=None, sinkhorn_type='normal', skip_last_layer=False, skip_last_layer_type='average', skip_personal_idx=False, skip_retrain=-1, softmax_temperature=1, standardize_acts=False, sweep_id=90, sweep_name='exp_sample', temperature=20, tensorboard=False, tensorboard_root='./tensorboard', timestamp='2023-09-16_14-53-20_717630', tmap_stats=False, to_download=True, transform_acts=False, unbalanced=False, update_acts=False, weight_stats=True, width_ratio=1)\n",
      "refactored get_config\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "in _make_layers [Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), AvgPool2d(kernel_size=1, stride=1, padding=0)]\n",
      "Relu Inplace is  False\n",
      "model parameters are \n",
      " [torch.Size([64, 3, 3, 3]), torch.Size([128, 64, 3, 3]), torch.Size([256, 128, 3, 3]), torch.Size([256, 256, 3, 3]), torch.Size([512, 256, 3, 3]), torch.Size([512, 512, 3, 3]), torch.Size([512, 512, 3, 3]), torch.Size([512, 512, 3, 3]), torch.Size([10, 512])]\n",
      "in _make_layers [Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), AvgPool2d(kernel_size=1, stride=1, padding=0)]\n",
      "Relu Inplace is  False\n",
      "model parameters are \n",
      " [torch.Size([64, 3, 3, 3]), torch.Size([128, 64, 3, 3]), torch.Size([256, 128, 3, 3]), torch.Size([256, 256, 3, 3]), torch.Size([512, 256, 3, 3]), torch.Size([512, 512, 3, 3]), torch.Size([512, 512, 3, 3]), torch.Size([512, 512, 3, 3]), torch.Size([10, 512])]\n",
      "in _make_layers [Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), AvgPool2d(kernel_size=1, stride=1, padding=0)]\n",
      "Relu Inplace is  False\n",
      "model parameters are \n",
      " [torch.Size([64, 3, 3, 3]), torch.Size([128, 64, 3, 3]), torch.Size([256, 128, 3, 3]), torch.Size([256, 256, 3, 3]), torch.Size([512, 256, 3, 3]), torch.Size([512, 512, 3, 3]), torch.Size([512, 512, 3, 3]), torch.Size([512, 512, 3, 3]), torch.Size([10, 512])]\n",
      "in _make_layers [Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), AvgPool2d(kernel_size=1, stride=1, padding=0)]\n",
      "Relu Inplace is  False\n",
      "model parameters are \n",
      " [torch.Size([64, 3, 3, 3]), torch.Size([128, 64, 3, 3]), torch.Size([256, 128, 3, 3]), torch.Size([256, 256, 3, 3]), torch.Size([512, 256, 3, 3]), torch.Size([512, 512, 3, 3]), torch.Size([512, 512, 3, 3]), torch.Size([512, 512, 3, 3]), torch.Size([10, 512])]\n",
      "Pruning with custom mask (all conv layers)\n",
      "Can not find [features.0.weight] in mask_dict\n",
      "Can not find [features.3.weight] in mask_dict\n",
      "Can not find [features.6.weight] in mask_dict\n",
      "Can not find [features.8.weight] in mask_dict\n",
      "Can not find [features.11.weight] in mask_dict\n",
      "Can not find [features.13.weight] in mask_dict\n",
      "Can not find [features.16.weight] in mask_dict\n",
      "Can not find [features.18.weight] in mask_dict\n",
      "\n",
      "--------- Testing in global mode ---------\n",
      "size of test_loader dataset:  10000\n",
      "\n",
      "Test set: Avg. loss: 0.0003, Accuracy: 9019/10000 (90%)\n",
      "\n",
      "Pruning with custom mask (all conv layers)\n",
      "Can not find [features.0.weight] in mask_dict\n",
      "Can not find [features.3.weight] in mask_dict\n",
      "Can not find [features.6.weight] in mask_dict\n",
      "Can not find [features.8.weight] in mask_dict\n",
      "Can not find [features.11.weight] in mask_dict\n",
      "Can not find [features.13.weight] in mask_dict\n",
      "Can not find [features.16.weight] in mask_dict\n",
      "Can not find [features.18.weight] in mask_dict\n",
      "\n",
      "--------- Testing in global mode ---------\n",
      "size of test_loader dataset:  10000\n",
      "\n",
      "Test set: Avg. loss: 0.0003, Accuracy: 9046/10000 (90%)\n",
      "\n",
      "layer features.0.weight has #params  1728\n",
      "layer features.3.weight has #params  73728\n",
      "layer features.6.weight has #params  294912\n",
      "layer features.8.weight has #params  589824\n",
      "layer features.11.weight has #params  1179648\n",
      "layer features.13.weight has #params  2359296\n",
      "layer features.16.weight has #params  2359296\n",
      "layer features.18.weight has #params  2359296\n",
      "layer classifier.weight has #params  5120\n",
      "Activation Timer start\n",
      "Activation Timer ends\n",
      "------- Geometric Ensembling -------\n",
      "Timer start\n",
      "Previous layer shape is  None\n",
      "Processing the coordinates to form ground_metric\n",
      "dont leave off the squaring of the ground metric\n",
      "returns a uniform measure of cardinality:  64\n",
      "returns a uniform measure of cardinality:  64\n",
      "the transport map is  tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')\n",
      "Ratio of trace to the matrix sum:  tensor(0.0469, device='cuda:0')\n",
      "Here, trace is 3.0 and matrix sum is 64.0 \n",
      "this is past correction for weight mode\n",
      "Shape of aligned wt is  torch.Size([64, 3, 9])\n",
      "Shape of fc_layer0_weight_data is  torch.Size([64, 3, 9])\n",
      "Previous layer shape is  torch.Size([64, 3, 3, 3])\n",
      "shape of layer: model 0 torch.Size([128, 64, 9])\n",
      "shape of layer: model 1 torch.Size([128, 64, 9])\n",
      "shape of previous transport map torch.Size([64, 64])\n",
      "Processing the coordinates to form ground_metric\n",
      "dont leave off the squaring of the ground metric\n",
      "returns a uniform measure of cardinality:  128\n",
      "returns a uniform measure of cardinality:  128\n",
      "the transport map is  tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        ...,\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0078],\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],\n",
      "       device='cuda:0')\n",
      "Ratio of trace to the matrix sum:  tensor(0.0078, device='cuda:0')\n",
      "Here, trace is 1.0 and matrix sum is 128.0 \n",
      "this is past correction for weight mode\n",
      "Shape of aligned wt is  torch.Size([128, 64, 9])\n",
      "Shape of fc_layer0_weight_data is  torch.Size([128, 64, 9])\n",
      "Previous layer shape is  torch.Size([128, 64, 3, 3])\n",
      "shape of layer: model 0 torch.Size([256, 128, 9])\n",
      "shape of layer: model 1 torch.Size([256, 128, 9])\n",
      "shape of previous transport map torch.Size([128, 128])\n",
      "Processing the coordinates to form ground_metric\n",
      "dont leave off the squaring of the ground metric\n",
      "returns a uniform measure of cardinality:  256\n",
      "returns a uniform measure of cardinality:  256\n",
      "the transport map is  tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')\n",
      "Ratio of trace to the matrix sum:  tensor(0., device='cuda:0')\n",
      "Here, trace is 0.0 and matrix sum is 256.0 \n",
      "this is past correction for weight mode\n",
      "Shape of aligned wt is  torch.Size([256, 128, 9])\n",
      "Shape of fc_layer0_weight_data is  torch.Size([256, 128, 9])\n",
      "Previous layer shape is  torch.Size([256, 128, 3, 3])\n",
      "shape of layer: model 0 torch.Size([256, 256, 9])\n",
      "shape of layer: model 1 torch.Size([256, 256, 9])\n",
      "shape of previous transport map torch.Size([256, 256])\n",
      "Processing the coordinates to form ground_metric\n",
      "dont leave off the squaring of the ground metric\n",
      "returns a uniform measure of cardinality:  256\n",
      "returns a uniform measure of cardinality:  256\n",
      "the transport map is  tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        ...,\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.0039, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],\n",
      "       device='cuda:0')\n",
      "Ratio of trace to the matrix sum:  tensor(0.0078, device='cuda:0')\n",
      "Here, trace is 2.0 and matrix sum is 256.0 \n",
      "this is past correction for weight mode\n",
      "Shape of aligned wt is  torch.Size([256, 256, 9])\n",
      "Shape of fc_layer0_weight_data is  torch.Size([256, 256, 9])\n",
      "Previous layer shape is  torch.Size([256, 256, 3, 3])\n",
      "shape of layer: model 0 torch.Size([512, 256, 9])\n",
      "shape of layer: model 1 torch.Size([512, 256, 9])\n",
      "shape of previous transport map torch.Size([256, 256])\n",
      "Processing the coordinates to form ground_metric\n",
      "dont leave off the squaring of the ground metric\n",
      "returns a uniform measure of cardinality:  512\n",
      "returns a uniform measure of cardinality:  512\n",
      "the transport map is  tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')\n",
      "Ratio of trace to the matrix sum:  tensor(0.0039, device='cuda:0')\n",
      "Here, trace is 2.0 and matrix sum is 512.0 \n",
      "this is past correction for weight mode\n",
      "Shape of aligned wt is  torch.Size([512, 256, 9])\n",
      "Shape of fc_layer0_weight_data is  torch.Size([512, 256, 9])\n",
      "Previous layer shape is  torch.Size([512, 256, 3, 3])\n",
      "shape of layer: model 0 torch.Size([512, 512, 9])\n",
      "shape of layer: model 1 torch.Size([512, 512, 9])\n",
      "shape of previous transport map torch.Size([512, 512])\n",
      "Processing the coordinates to form ground_metric\n",
      "dont leave off the squaring of the ground metric\n",
      "returns a uniform measure of cardinality:  512\n",
      "returns a uniform measure of cardinality:  512\n",
      "the transport map is  tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')\n",
      "Ratio of trace to the matrix sum:  tensor(0.0020, device='cuda:0')\n",
      "Here, trace is 1.0 and matrix sum is 512.0 \n",
      "this is past correction for weight mode\n",
      "Shape of aligned wt is  torch.Size([512, 512, 9])\n",
      "Shape of fc_layer0_weight_data is  torch.Size([512, 512, 9])\n",
      "Previous layer shape is  torch.Size([512, 512, 3, 3])\n",
      "shape of layer: model 0 torch.Size([512, 512, 9])\n",
      "shape of layer: model 1 torch.Size([512, 512, 9])\n",
      "shape of previous transport map torch.Size([512, 512])\n",
      "Processing the coordinates to form ground_metric\n",
      "dont leave off the squaring of the ground metric\n",
      "returns a uniform measure of cardinality:  512\n",
      "returns a uniform measure of cardinality:  512\n",
      "the transport map is  tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')\n",
      "Ratio of trace to the matrix sum:  tensor(0.0020, device='cuda:0')\n",
      "Here, trace is 1.0 and matrix sum is 512.0 \n",
      "this is past correction for weight mode\n",
      "Shape of aligned wt is  torch.Size([512, 512, 9])\n",
      "Shape of fc_layer0_weight_data is  torch.Size([512, 512, 9])\n",
      "Previous layer shape is  torch.Size([512, 512, 3, 3])\n",
      "shape of layer: model 0 torch.Size([512, 512, 9])\n",
      "shape of layer: model 1 torch.Size([512, 512, 9])\n",
      "shape of previous transport map torch.Size([512, 512])\n",
      "Processing the coordinates to form ground_metric\n",
      "dont leave off the squaring of the ground metric\n",
      "returns a uniform measure of cardinality:  512\n",
      "returns a uniform measure of cardinality:  512\n",
      "the transport map is  tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        [0.0020, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        ...,\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],\n",
      "       device='cuda:0')\n",
      "Ratio of trace to the matrix sum:  tensor(0., device='cuda:0')\n",
      "Here, trace is 0.0 and matrix sum is 512.0 \n",
      "this is past correction for weight mode\n",
      "Shape of aligned wt is  torch.Size([512, 512, 9])\n",
      "Shape of fc_layer0_weight_data is  torch.Size([512, 512, 9])\n",
      "Previous layer shape is  torch.Size([512, 512, 3, 3])\n",
      "shape of layer: model 0 torch.Size([10, 512])\n",
      "shape of layer: model 1 torch.Size([10, 512])\n",
      "shape of previous transport map torch.Size([512, 512])\n",
      "Processing the coordinates to form ground_metric\n",
      "dont leave off the squaring of the ground metric\n",
      "ground metric is  tensor([[1.4241, 2.4872, 2.7804, 2.8719, 2.7374, 2.8799, 2.7276, 2.6342, 2.5641,\n",
      "         2.5922],\n",
      "        [2.4889, 1.3253, 2.6732, 2.5914, 2.7584, 2.5501, 2.4174, 2.5215, 2.3048,\n",
      "         2.4101],\n",
      "        [2.7600, 2.6711, 1.3832, 2.9465, 2.9065, 2.8701, 2.7276, 2.8385, 2.7485,\n",
      "         2.7866],\n",
      "        [2.8352, 2.6791, 2.8919, 1.5430, 2.8605, 2.9159, 2.7574, 2.7980, 2.8164,\n",
      "         2.6354],\n",
      "        [2.7551, 2.5960, 2.8635, 2.8450, 1.5042, 2.7447, 2.7853, 2.6811, 2.7031,\n",
      "         2.7294],\n",
      "        [2.9190, 2.5396, 2.9469, 2.8416, 2.7272, 1.5502, 2.7781, 2.7303, 2.7977,\n",
      "         2.6697],\n",
      "        [2.6778, 2.4457, 2.7190, 2.9111, 2.6488, 2.8526, 1.3931, 2.6890, 2.4981,\n",
      "         2.5560],\n",
      "        [2.7394, 2.5496, 2.7799, 2.7903, 2.7151, 2.6590, 2.7190, 1.3924, 2.5583,\n",
      "         2.5121],\n",
      "        [2.5972, 2.3660, 2.7693, 2.6872, 2.7026, 2.8280, 2.5325, 2.6366, 1.2999,\n",
      "         2.4041],\n",
      "        [2.5918, 2.2730, 2.7816, 2.7576, 2.6697, 2.7273, 2.5482, 2.5290, 2.5207,\n",
      "         1.2878]], device='cuda:0', grad_fn=<PowBackward0>)\n",
      "returns a uniform measure of cardinality:  10\n",
      "returns a uniform measure of cardinality:  10\n",
      "the transport map is  tensor([[0.1000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0000, 0.1000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0000, 0.0000, 0.1000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.1000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.1000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.1000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.1000, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.1000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.1000,\n",
      "         0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.1000]], device='cuda:0')\n",
      "Ratio of trace to the matrix sum:  tensor(1., device='cuda:0')\n",
      "Here, trace is 9.99755859375 and matrix sum is 9.99755859375 \n",
      "this is past correction for weight mode\n",
      "Shape of aligned wt is  torch.Size([10, 512])\n",
      "Shape of fc_layer0_weight_data is  torch.Size([10, 512])\n",
      "using independent method\n",
      "in _make_layers [Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), AvgPool2d(kernel_size=1, stride=1, padding=0)]\n",
      "Relu Inplace is  False\n",
      "model parameters are \n",
      " [torch.Size([64, 3, 3, 3]), torch.Size([128, 64, 3, 3]), torch.Size([256, 128, 3, 3]), torch.Size([256, 256, 3, 3]), torch.Size([512, 256, 3, 3]), torch.Size([512, 512, 3, 3]), torch.Size([512, 512, 3, 3]), torch.Size([512, 512, 3, 3]), torch.Size([10, 512])]\n",
      "\n",
      "--------- Testing in global mode ---------\n",
      "size of test_loader dataset:  10000\n",
      "\n",
      "Test set: Avg. loss: 0.0023, Accuracy: 1019/10000 (10%)\n",
      "\n",
      "len of model parameters and avg aligned layers is  9 9\n",
      "len of model_state_dict is  9\n",
      "len of param_list is  9\n",
      "\n",
      "--------- Testing in global mode ---------\n",
      "size of test_loader dataset:  10000\n",
      "\n",
      "Test set: Avg. loss: 0.0015, Accuracy: 8545/10000 (85%)\n",
      "\n",
      "===============================================\n",
      "===============================================\n",
      "After Ensemble: 85.45\n",
      "===============================================\n",
      "===============================================\n",
      "Timer ends\n",
      "Time taken for geometric ensembling is 3.620155781507492 seconds\n",
      "------- Prediction based ensembling -------\n",
      "\n",
      "Test set: Avg. loss: 0.0003, Accuracy: 9133/10000 (91%)\n",
      "\n",
      "------- Naive ensembling of weights -------\n",
      "[torch.Size([64, 3, 3, 3]), torch.Size([64, 3, 3, 3])]\n",
      "torch.Size([64, 3, 3, 3])\n",
      "[torch.Size([128, 64, 3, 3]), torch.Size([128, 64, 3, 3])]\n",
      "torch.Size([128, 64, 3, 3])\n",
      "[torch.Size([256, 128, 3, 3]), torch.Size([256, 128, 3, 3])]\n",
      "torch.Size([256, 128, 3, 3])\n",
      "[torch.Size([256, 256, 3, 3]), torch.Size([256, 256, 3, 3])]\n",
      "torch.Size([256, 256, 3, 3])\n",
      "[torch.Size([512, 256, 3, 3]), torch.Size([512, 256, 3, 3])]\n",
      "torch.Size([512, 256, 3, 3])\n",
      "[torch.Size([512, 512, 3, 3]), torch.Size([512, 512, 3, 3])]\n",
      "torch.Size([512, 512, 3, 3])\n",
      "[torch.Size([512, 512, 3, 3]), torch.Size([512, 512, 3, 3])]\n",
      "torch.Size([512, 512, 3, 3])\n",
      "[torch.Size([512, 512, 3, 3]), torch.Size([512, 512, 3, 3])]\n",
      "torch.Size([512, 512, 3, 3])\n",
      "[torch.Size([10, 512]), torch.Size([10, 512])]\n",
      "torch.Size([10, 512])\n",
      "in _make_layers [Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), AvgPool2d(kernel_size=1, stride=1, padding=0)]\n",
      "Relu Inplace is  False\n",
      "model parameters are \n",
      " [torch.Size([64, 3, 3, 3]), torch.Size([128, 64, 3, 3]), torch.Size([256, 128, 3, 3]), torch.Size([256, 256, 3, 3]), torch.Size([512, 256, 3, 3]), torch.Size([512, 512, 3, 3]), torch.Size([512, 512, 3, 3]), torch.Size([512, 512, 3, 3]), torch.Size([10, 512])]\n",
      "\n",
      "--------- Testing in global mode ---------\n",
      "size of test_loader dataset:  10000\n",
      "\n",
      "Test set: Avg. loss: 0.0023, Accuracy: 1143/10000 (11%)\n",
      "\n",
      "\n",
      "--------- Testing in global mode ---------\n",
      "size of test_loader dataset:  10000\n",
      "\n",
      "Test set: Avg. loss: 0.0023, Accuracy: 1689/10000 (17%)\n",
      "\n",
      "-------- Retraining the models ---------\n",
      "Retraining model :  model_0\n",
      "num_epochs---------------+++))))))+++ 300\n",
      "num_epochs---------------+++))))))+++ 300\n",
      "lr is  0.05\n",
      "number of epochs would be  30\n",
      "num_epochs-------------- 30\n",
      "Epoch 000\n",
      "/root/miniconda3/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:129: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\"Detected call of `lr_scheduler.step()` before `optimizer.step()`. \"\n",
      "/root/miniconda3/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:154: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "accuracy: {'epoch': 0, 'value': 0.5033000000000002} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 0, 'value': 1.4018298149490362} ({'split': 'train'})\n",
      "accuracy: {'epoch': 0, 'value': 0.7034000277519227} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 0, 'value': 0.9205841004848481} ({'split': 'test'})\n",
      "Epoch 001\n",
      "^C\n",
      "Traceback (most recent call last):\n",
      "  File \"main.py\", line 338, in <module>\n",
      "    _, best_retrain_acc = routines.retrain_models(args, [*original_models, geometric_model, naive_model], retrain_loader, test_loader, config, tensorboard_obj=tensorboard_obj, initial_acc=initial_acc, nicks=nicks)\n",
      "  File \"/root/autodl-tmp/OTfusion_pruning/otfusion/routines.py\", line 392, in retrain_models\n",
      "    retrained_network, acc = cifar_train.get_retrained_model(args, retrain_loader, test_loader, old_networks[i], config, output_root_dir, tensorboard_obj=tensorboard_obj, nick=nick, start_acc=initial_acc[i])\n",
      "  File \"/root/autodl-tmp/OTfusion_pruning/otfusion/./cifar/train.py\", line 333, in get_retrained_model\n",
      "    best_acc,model = main(config, output_dir, args.gpu_id, pretrained_model=old_network, pretrained_dataset=(train_loader, test_loader), tensorboard_obj=tensorboard_obj,return_model=True)\n",
      "  File \"/root/autodl-tmp/OTfusion_pruning/otfusion/./cifar/train.py\", line 75, in main\n",
      "    loss.backward()\n",
      "  File \"/root/miniconda3/lib/python3.8/site-packages/torch/_tensor.py\", line 307, in backward\n",
      "    torch.autograd.backward(self, gradient, retain_graph, create_graph, inputs=inputs)\n",
      "  File \"/root/miniconda3/lib/python3.8/site-packages/torch/autograd/__init__.py\", line 154, in backward\n",
      "    Variable._execution_engine.run_backward(\n",
      "KeyboardInterrupt\n"
     ]
    }
   ],
   "source": [
    "!python main.py --gpu-id 0 --model-name vgg11_nobias --n-epochs 90 --save-result-file sample.csv \\\n",
    "--sweep-name exp_sample --correction --ground-metric euclidean --weight-stats \\\n",
    "--geom-ensemble-type wts --ground-metric-normalize none --sweep-id 90 \\\n",
    "--ckpt-type best --dataset Cifar10 --ground-metric-eff --recheck-cifar --activation-seed {activation_seed} \\\n",
    "--prelu-acts --past-correction --not-squared --exact\\\n",
    "--learning-rate 0.08\\\n",
    "--momentum 0.5\\\n",
    "--batch-size-train 64 --experiment-dir {experiment_dir}\\\n",
    "--prunint-rate {prunint_rate}\\\n",
    "--run-mode {run_mode}\\\n",
    "--to-download\\\n",
    "--load-model-dir {load_model_dir}\\\n",
    "--retrain 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f467a3e4-8ba5-45f2-b0e9-8c80dd5fe3e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#剪枝实验2:prune finetune\n",
    "activation_seed=21\n",
    "experiment_dir=\"experiment1\"\n",
    "prunint_rate=0.2\n",
    "run_mode='finetune'\n",
    "load_model_dir='./cifar_models'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6e7e97b-c2e3-453d-8c13-88d3c93fad93",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python main.py --gpu-id 0 --model-name vgg11_nobias --n-epochs 90 --save-result-file sample.csv \\\n",
    "--sweep-name exp_sample --correction --ground-metric euclidean --weight-stats \\\n",
    "--geom-ensemble-type wts --ground-metric-normalize none --sweep-id 90 \\\n",
    "--ckpt-type best --dataset Cifar10 --ground-metric-eff --recheck-cifar --activation-seed {activation_seed} \\\n",
    "--prelu-acts --past-correction --not-squared --exact\\\n",
    "--learning-rate 0.08\\\n",
    "--momentum 0.5\\\n",
    "--batch-size-train 64 --experiment-dir {experiment_dir}\\\n",
    "--prunint-rate {prunint_rate}\\\n",
    "--run-mode {run_mode}\\\n",
    "--to-download\\\n",
    "--load-model-dir {load_model_dir}\\\n",
    "--retrain 30"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
