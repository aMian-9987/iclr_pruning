{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9548f487-ef4e-48bc-b034-06d3a5938736",
   "metadata": {},
   "outputs": [],
   "source": [
    "activation_seed=21\n",
    "experiment_dir=\"experiment0.5\"\n",
    "prunint_rate=0.1\n",
    "run_mode='benchmark'\n",
    "load_model_dir='./cifar_models'\n",
    "#85.44"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6e80ead-659e-47bc-bf87-d87c13b16229",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------- Setting up parameters -------\n",
      "dumping parameters at  /storage/coda1/p-jli3175/0/mwu385/OTfusion_pruning_final/otfusion/exp_sample/configurations\n",
      "The parameters are: \n",
      " Namespace(experiment_dir='experiment0.5', prunint_rate=0.1, run_mode='benchmark', load_model_dir='./cifar_models', prune_times=4, finetunetimes_benchmark=0, n_epochs=0, batch_size_train=64, batch_size_test=1000, learning_rate=0.004, momentum=0.5, log_interval=100, to_download=True, disable_bias=True, dataset='Cifar10', num_models=2, model_name='vgg11_nobias', config_file=None, config_dir='/storage/coda1/p-jli3175/0/mwu385/OTfusion_pruning_final/otfusion/exp_sample/configurations', num_hidden_nodes=400, num_hidden_nodes1=400, num_hidden_nodes2=200, num_hidden_nodes3=100, num_hidden_nodes4=50, sweep_id=90, gpu_id=0, skip_last_layer=False, skip_last_layer_type='average', debug=False, cifar_style_data=False, activation_histograms=False, act_num_samples=100, softmax_temperature=1, activation_mode=None, options_type='generic', deprecated=None, save_result_file='sample.csv', sweep_name='exp_sample', reg=0.01, reg_m=0.001, ground_metric='euclidean', ground_metric_normalize='none', not_squared=True, clip_gm=False, clip_min=0, clip_max=5, tmap_stats=False, ensemble_step=0.5, ground_metric_eff=True, retrain=30, retrain_lr_decay=-1, retrain_lr_decay_factor=None, retrain_lr_decay_epochs=None, retrain_avg_only=False, retrain_geometric_only=False, load_models='', ckpt_type='best', recheck_cifar=True, recheck_acc=False, eval_aligned=True, enable_dropout=False, dump_model=False, dump_final_models=False, correction=True, activation_seed=21, weight_stats=True, sinkhorn_type='normal', geom_ensemble_type='wts', act_bug=False, standardize_acts=False, transform_acts=False, center_acts=False, prelu_acts=True, pool_acts=False, pool_relu=False, normalize_acts=False, normalize_wts=False, gromov=False, gromov_loss='square_loss', tensorboard_root='./tensorboard', tensorboard=False, same_model=-1, dist_normalize=False, update_acts=False, past_correction=True, partial_reshape=False, choice='0 2 4 6 8', diff_init=False, partition_type='labels', personal_class_idx=9, partition_dataloader=-1, personal_split_frac=0.1, exact=True, skip_personal_idx=False, prediction_wts=False, width_ratio=1, proper_marginals=False, retrain_seed=-1, no_random_trainloaders=False, reinit_trainloaders=False, second_model_name=None, print_distances=False, deterministic=False, skip_retrain=-1, importance=None, unbalanced=False, temperature=20, alpha=0.7, dist_epochs=60, handle_skips=False, timestamp='2023-09-18_23-38-16_025861', rootdir='/storage/coda1/p-jli3175/0/mwu385/OTfusion_pruning_final/otfusion/exp_sample', baseroot='/storage/coda1/p-jli3175/0/mwu385/OTfusion_pruning_final/otfusion', result_dir='/storage/coda1/p-jli3175/0/mwu385/OTfusion_pruning_final/otfusion/exp_sample/results', exp_name='exp_2023-09-18_23-38-16_025861', csv_dir='/storage/coda1/p-jli3175/0/mwu385/OTfusion_pruning_final/otfusion/exp_sample/csv')\n",
      "refactored get_config\n",
      "------- Obtain dataloaders -------\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "------- Training independent models -------\n",
      "QWERTY: Enter Cifar 1\n",
      "number of epochs would b>>>>>>>e  300\n",
      "in _make_layers [Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), AvgPool2d(kernel_size=1, stride=1, padding=0)]\n",
      "Relu Inplace is  False\n",
      "model parameters are \n",
      " [torch.Size([64, 3, 3, 3]), torch.Size([128, 64, 3, 3]), torch.Size([256, 128, 3, 3]), torch.Size([256, 256, 3, 3]), torch.Size([512, 256, 3, 3]), torch.Size([512, 512, 3, 3]), torch.Size([512, 512, 3, 3]), torch.Size([512, 512, 3, 3]), torch.Size([10, 512])]\n",
      "Loading model at path ./cifar_models/model_0/best.checkpoint which had accuracy 0.9030999821424489 and at epoch 127\n",
      "\n",
      "--------- Testing in global mode ---------\n",
      "size of test_loader dataset:  10000\n",
      "\n",
      "Test set: Avg. loss: 0.0003, Accuracy: 9019/10000 (90%)\n",
      "\n",
      "He says accury is  90.30999821424489 but actually 90.19\n",
      "QWERTY: Enter Cifar 1\n",
      "number of epochs would b>>>>>>>e  300\n",
      "in _make_layers [Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), AvgPool2d(kernel_size=1, stride=1, padding=0)]\n",
      "Relu Inplace is  False\n",
      "model parameters are \n",
      " [torch.Size([64, 3, 3, 3]), torch.Size([128, 64, 3, 3]), torch.Size([256, 128, 3, 3]), torch.Size([256, 256, 3, 3]), torch.Size([512, 256, 3, 3]), torch.Size([512, 512, 3, 3]), torch.Size([512, 512, 3, 3]), torch.Size([512, 512, 3, 3]), torch.Size([10, 512])]\n",
      "Loading model at path ./cifar_models/model_1/best.checkpoint which had accuracy 0.9049999803304669 and at epoch 134\n",
      "\n",
      "--------- Testing in global mode ---------\n",
      "size of test_loader dataset:  10000\n",
      "\n",
      "Test set: Avg. loss: 0.0003, Accuracy: 9046/10000 (90%)\n",
      "\n",
      "He says accury is  90.4999980330467 but actually 90.46\n",
      "========================================\n",
      "========================================\n",
      "TEST:args.learning_rate,args.momentum 0.004 0.5\n",
      "========================================\n",
      "========================================\n",
      "========================================\n",
      "\n",
      "--------- Testing in global mode ---------\n",
      "size of test_loader dataset:  10000\n",
      "\n",
      "Test set: Avg. loss: 0.0003, Accuracy: 9019/10000 (90%)\n",
      "\n",
      "========================================\n",
      "========================================\n",
      "TEST:args.learning_rate,args.momentum 0.004 0.5\n",
      "========================================\n",
      "========================================\n",
      "========================================\n",
      "\n",
      "--------- Testing in global mode ---------\n",
      "size of test_loader dataset:  10000\n",
      "\n",
      "Test set: Avg. loss: 0.0003, Accuracy: 9046/10000 (90%)\n",
      "\n",
      "===============================================\n",
      "===============================================\n",
      "before Ensemble,acc: [90.19, 90.46] loss [0.0003167657166719437, 0.00032148256003856657]\n",
      "===============================================\n",
      "===============================================\n",
      "check zero rate for model 0\n",
      "odict_keys(['features.0.weight', 'features.3.weight', 'features.6.weight', 'features.8.weight', 'features.11.weight', 'features.13.weight', 'features.16.weight', 'features.18.weight', 'classifier.weight'])\n",
      "the ratio of zero for features.0.weight\n",
      "tensor(0., device='cuda:0')\n",
      "the ratio of zero for features.3.weight\n",
      "tensor(0., device='cuda:0')\n",
      "the ratio of zero for features.6.weight\n",
      "tensor(0., device='cuda:0')\n",
      "the ratio of zero for features.8.weight\n",
      "tensor(0., device='cuda:0')\n",
      "the ratio of zero for features.11.weight\n",
      "tensor(0., device='cuda:0')\n",
      "the ratio of zero for features.13.weight\n",
      "tensor(0., device='cuda:0')\n",
      "the ratio of zero for features.16.weight\n",
      "tensor(0., device='cuda:0')\n",
      "the ratio of zero for features.18.weight\n",
      "tensor(0., device='cuda:0')\n",
      "the ratio of zero for classifier.weight\n",
      "tensor(0., device='cuda:0')\n",
      "check zero rate for model 1\n",
      "odict_keys(['features.0.weight', 'features.3.weight', 'features.6.weight', 'features.8.weight', 'features.11.weight', 'features.13.weight', 'features.16.weight', 'features.18.weight', 'classifier.weight'])\n",
      "the ratio of zero for features.0.weight\n",
      "tensor(0., device='cuda:0')\n",
      "the ratio of zero for features.3.weight\n",
      "tensor(0., device='cuda:0')\n",
      "the ratio of zero for features.6.weight\n",
      "tensor(0., device='cuda:0')\n",
      "the ratio of zero for features.8.weight\n",
      "tensor(0., device='cuda:0')\n",
      "the ratio of zero for features.11.weight\n",
      "tensor(0., device='cuda:0')\n",
      "the ratio of zero for features.13.weight\n",
      "tensor(0., device='cuda:0')\n",
      "the ratio of zero for features.16.weight\n",
      "tensor(0., device='cuda:0')\n",
      "the ratio of zero for features.18.weight\n",
      "tensor(0., device='cuda:0')\n",
      "the ratio of zero for classifier.weight\n",
      "tensor(0., device='cuda:0')\n",
      "layer features.0.weight has #params  1728\n",
      "layer features.3.weight has #params  73728\n",
      "layer features.6.weight has #params  294912\n",
      "layer features.8.weight has #params  589824\n",
      "layer features.11.weight has #params  1179648\n",
      "layer features.13.weight has #params  2359296\n",
      "layer features.16.weight has #params  2359296\n",
      "layer features.18.weight has #params  2359296\n",
      "layer classifier.weight has #params  5120\n",
      "Activation Timer start\n",
      "Activation Timer ends\n",
      "------- Geometric Ensembling -------\n",
      "Timer start\n",
      "Previous layer shape is  None\n",
      "Processing the coordinates to form ground_metric\n",
      "dont leave off the squaring of the ground metric\n",
      "returns a uniform measure of cardinality:  64\n",
      "returns a uniform measure of cardinality:  64\n",
      "the transport map is  tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')\n",
      "Ratio of trace to the matrix sum:  tensor(0.0469, device='cuda:0')\n",
      "Here, trace is 2.9999806880950928 and matrix sum is 63.99958801269531 \n",
      "this is past correction for weight mode\n",
      "Shape of aligned wt is  torch.Size([64, 3, 9])\n",
      "Shape of fc_layer0_weight_data is  torch.Size([64, 3, 9])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "in _make_layers [Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), AvgPool2d(kernel_size=1, stride=1, padding=0)]\n",
      "Relu Inplace is  False\n",
      "model parameters are \n",
      " [torch.Size([64, 3, 3, 3]), torch.Size([128, 64, 3, 3]), torch.Size([256, 128, 3, 3]), torch.Size([256, 256, 3, 3]), torch.Size([512, 256, 3, 3]), torch.Size([512, 512, 3, 3]), torch.Size([512, 512, 3, 3]), torch.Size([512, 512, 3, 3]), torch.Size([10, 512])]\n",
      "len of model_state_dict is  9\n",
      "len of new_params is  1\n",
      "updated parameters for layer  features.0.weight\n",
      "\n",
      "--------- Testing in global mode ---------\n",
      "size of test_loader dataset:  10000\n",
      "\n",
      "Test set: Avg. loss: 0.0027, Accuracy: 1250/10000 (12%)\n",
      "\n",
      "accuracy after update is  12.5\n",
      "For layer idx 0, accuracy of the updated model is 12.5\n",
      "Previous layer shape is  torch.Size([64, 3, 3, 3])\n",
      "shape of layer: model 0 torch.Size([128, 64, 9])\n",
      "shape of layer: model 1 torch.Size([128, 64, 9])\n",
      "shape of previous transport map torch.Size([64, 64])\n",
      "Processing the coordinates to form ground_metric\n",
      "dont leave off the squaring of the ground metric\n",
      "returns a uniform measure of cardinality:  128\n",
      "returns a uniform measure of cardinality:  128\n",
      "the transport map is  tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        ...,\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0078],\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],\n",
      "       device='cuda:0')\n",
      "Ratio of trace to the matrix sum:  tensor(0.0078, device='cuda:0')\n",
      "Here, trace is 0.9999872446060181 and matrix sum is 127.99836730957031 \n",
      "this is past correction for weight mode\n",
      "Shape of aligned wt is  torch.Size([128, 64, 9])\n",
      "Shape of fc_layer0_weight_data is  torch.Size([128, 64, 9])\n",
      "in _make_layers [Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), AvgPool2d(kernel_size=1, stride=1, padding=0)]\n",
      "Relu Inplace is  False\n",
      "model parameters are \n",
      " [torch.Size([64, 3, 3, 3]), torch.Size([128, 64, 3, 3]), torch.Size([256, 128, 3, 3]), torch.Size([256, 256, 3, 3]), torch.Size([512, 256, 3, 3]), torch.Size([512, 512, 3, 3]), torch.Size([512, 512, 3, 3]), torch.Size([512, 512, 3, 3]), torch.Size([10, 512])]\n",
      "len of model_state_dict is  9\n",
      "len of new_params is  2\n",
      "updated parameters for layer  features.0.weight\n",
      "updated parameters for layer  features.3.weight\n",
      "\n",
      "--------- Testing in global mode ---------\n",
      "size of test_loader dataset:  10000\n",
      "\n",
      "Test set: Avg. loss: 0.0024, Accuracy: 1036/10000 (10%)\n",
      "\n",
      "accuracy after update is  10.36\n",
      "For layer idx 1, accuracy of the updated model is 10.36\n",
      "Previous layer shape is  torch.Size([128, 64, 3, 3])\n",
      "shape of layer: model 0 torch.Size([256, 128, 9])\n",
      "shape of layer: model 1 torch.Size([256, 128, 9])\n",
      "shape of previous transport map torch.Size([128, 128])\n",
      "Processing the coordinates to form ground_metric\n",
      "dont leave off the squaring of the ground metric\n",
      "returns a uniform measure of cardinality:  256\n",
      "returns a uniform measure of cardinality:  256\n",
      "the transport map is  tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')\n",
      "Ratio of trace to the matrix sum:  tensor(0., device='cuda:0')\n",
      "Here, trace is 0.0 and matrix sum is 255.99343872070312 \n",
      "this is past correction for weight mode\n",
      "Shape of aligned wt is  torch.Size([256, 128, 9])\n",
      "Shape of fc_layer0_weight_data is  torch.Size([256, 128, 9])\n",
      "in _make_layers [Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), AvgPool2d(kernel_size=1, stride=1, padding=0)]\n",
      "Relu Inplace is  False\n",
      "model parameters are \n",
      " [torch.Size([64, 3, 3, 3]), torch.Size([128, 64, 3, 3]), torch.Size([256, 128, 3, 3]), torch.Size([256, 256, 3, 3]), torch.Size([512, 256, 3, 3]), torch.Size([512, 512, 3, 3]), torch.Size([512, 512, 3, 3]), torch.Size([512, 512, 3, 3]), torch.Size([10, 512])]\n",
      "len of model_state_dict is  9\n",
      "len of new_params is  3\n",
      "updated parameters for layer  features.0.weight\n",
      "updated parameters for layer  features.3.weight\n",
      "updated parameters for layer  features.6.weight\n",
      "\n",
      "--------- Testing in global mode ---------\n",
      "size of test_loader dataset:  10000\n",
      "\n",
      "Test set: Avg. loss: 0.0023, Accuracy: 1148/10000 (11%)\n",
      "\n",
      "accuracy after update is  11.48\n",
      "For layer idx 2, accuracy of the updated model is 11.48\n",
      "Previous layer shape is  torch.Size([256, 128, 3, 3])\n",
      "shape of layer: model 0 torch.Size([256, 256, 9])\n",
      "shape of layer: model 1 torch.Size([256, 256, 9])\n",
      "shape of previous transport map torch.Size([256, 256])\n",
      "Processing the coordinates to form ground_metric\n",
      "dont leave off the squaring of the ground metric\n",
      "returns a uniform measure of cardinality:  256\n",
      "returns a uniform measure of cardinality:  256\n",
      "the transport map is  tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        ...,\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.0039, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],\n",
      "       device='cuda:0')\n",
      "Ratio of trace to the matrix sum:  tensor(0.0078, device='cuda:0')\n",
      "Here, trace is 1.9999487400054932 and matrix sum is 255.99343872070312 \n",
      "this is past correction for weight mode\n",
      "Shape of aligned wt is  torch.Size([256, 256, 9])\n",
      "Shape of fc_layer0_weight_data is  torch.Size([256, 256, 9])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "in _make_layers [Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), AvgPool2d(kernel_size=1, stride=1, padding=0)]\n",
      "Relu Inplace is  False\n",
      "model parameters are \n",
      " [torch.Size([64, 3, 3, 3]), torch.Size([128, 64, 3, 3]), torch.Size([256, 128, 3, 3]), torch.Size([256, 256, 3, 3]), torch.Size([512, 256, 3, 3]), torch.Size([512, 512, 3, 3]), torch.Size([512, 512, 3, 3]), torch.Size([512, 512, 3, 3]), torch.Size([10, 512])]\n",
      "len of model_state_dict is  9\n",
      "len of new_params is  4\n",
      "updated parameters for layer  features.0.weight\n",
      "updated parameters for layer  features.3.weight\n",
      "updated parameters for layer  features.6.weight\n",
      "updated parameters for layer  features.8.weight\n",
      "\n",
      "--------- Testing in global mode ---------\n",
      "size of test_loader dataset:  10000\n",
      "\n",
      "Test set: Avg. loss: 0.0024, Accuracy: 1248/10000 (12%)\n",
      "\n",
      "accuracy after update is  12.48\n",
      "For layer idx 3, accuracy of the updated model is 12.48\n",
      "Previous layer shape is  torch.Size([256, 256, 3, 3])\n",
      "shape of layer: model 0 torch.Size([512, 256, 9])\n",
      "shape of layer: model 1 torch.Size([512, 256, 9])\n",
      "shape of previous transport map torch.Size([256, 256])\n",
      "Processing the coordinates to form ground_metric\n",
      "dont leave off the squaring of the ground metric\n",
      "returns a uniform measure of cardinality:  512\n",
      "returns a uniform measure of cardinality:  512\n",
      "the transport map is  tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')\n",
      "Ratio of trace to the matrix sum:  tensor(0.0039, device='cuda:0')\n",
      "Here, trace is 1.9998977184295654 and matrix sum is 511.97381591796875 \n",
      "this is past correction for weight mode\n",
      "Shape of aligned wt is  torch.Size([512, 256, 9])\n",
      "Shape of fc_layer0_weight_data is  torch.Size([512, 256, 9])\n",
      "in _make_layers [Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), AvgPool2d(kernel_size=1, stride=1, padding=0)]\n",
      "Relu Inplace is  False\n",
      "model parameters are \n",
      " [torch.Size([64, 3, 3, 3]), torch.Size([128, 64, 3, 3]), torch.Size([256, 128, 3, 3]), torch.Size([256, 256, 3, 3]), torch.Size([512, 256, 3, 3]), torch.Size([512, 512, 3, 3]), torch.Size([512, 512, 3, 3]), torch.Size([512, 512, 3, 3]), torch.Size([10, 512])]\n",
      "len of model_state_dict is  9\n",
      "len of new_params is  5\n",
      "updated parameters for layer  features.0.weight\n",
      "updated parameters for layer  features.3.weight\n",
      "updated parameters for layer  features.6.weight\n",
      "updated parameters for layer  features.8.weight\n",
      "updated parameters for layer  features.11.weight\n",
      "\n",
      "--------- Testing in global mode ---------\n",
      "size of test_loader dataset:  10000\n",
      "\n",
      "Test set: Avg. loss: 0.0027, Accuracy: 904/10000 (9%)\n",
      "\n",
      "accuracy after update is  9.04\n",
      "For layer idx 4, accuracy of the updated model is 9.04\n",
      "Previous layer shape is  torch.Size([512, 256, 3, 3])\n",
      "shape of layer: model 0 torch.Size([512, 512, 9])\n",
      "shape of layer: model 1 torch.Size([512, 512, 9])\n",
      "shape of previous transport map torch.Size([512, 512])\n",
      "Processing the coordinates to form ground_metric\n",
      "dont leave off the squaring of the ground metric\n",
      "returns a uniform measure of cardinality:  512\n",
      "returns a uniform measure of cardinality:  512\n",
      "the transport map is  tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')\n",
      "Ratio of trace to the matrix sum:  tensor(0.0020, device='cuda:0')\n",
      "Here, trace is 0.9999488592147827 and matrix sum is 511.97381591796875 \n",
      "this is past correction for weight mode\n",
      "Shape of aligned wt is  torch.Size([512, 512, 9])\n",
      "Shape of fc_layer0_weight_data is  torch.Size([512, 512, 9])\n",
      "in _make_layers [Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), AvgPool2d(kernel_size=1, stride=1, padding=0)]\n",
      "Relu Inplace is  False\n",
      "model parameters are \n",
      " [torch.Size([64, 3, 3, 3]), torch.Size([128, 64, 3, 3]), torch.Size([256, 128, 3, 3]), torch.Size([256, 256, 3, 3]), torch.Size([512, 256, 3, 3]), torch.Size([512, 512, 3, 3]), torch.Size([512, 512, 3, 3]), torch.Size([512, 512, 3, 3]), torch.Size([10, 512])]\n",
      "len of model_state_dict is  9\n",
      "len of new_params is  6\n",
      "updated parameters for layer  features.0.weight\n",
      "updated parameters for layer  features.3.weight\n",
      "updated parameters for layer  features.6.weight\n",
      "updated parameters for layer  features.8.weight\n",
      "updated parameters for layer  features.11.weight\n",
      "updated parameters for layer  features.13.weight\n",
      "\n",
      "--------- Testing in global mode ---------\n",
      "size of test_loader dataset:  10000\n",
      "\n",
      "Test set: Avg. loss: 0.0024, Accuracy: 1411/10000 (14%)\n",
      "\n",
      "accuracy after update is  14.11\n",
      "For layer idx 5, accuracy of the updated model is 14.11\n",
      "Previous layer shape is  torch.Size([512, 512, 3, 3])\n",
      "shape of layer: model 0 torch.Size([512, 512, 9])\n",
      "shape of layer: model 1 torch.Size([512, 512, 9])\n",
      "shape of previous transport map torch.Size([512, 512])\n",
      "Processing the coordinates to form ground_metric\n",
      "dont leave off the squaring of the ground metric\n",
      "returns a uniform measure of cardinality:  512\n",
      "returns a uniform measure of cardinality:  512\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the transport map is  tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')\n",
      "Ratio of trace to the matrix sum:  tensor(0.0020, device='cuda:0')\n",
      "Here, trace is 0.9999488592147827 and matrix sum is 511.97381591796875 \n",
      "this is past correction for weight mode\n",
      "Shape of aligned wt is  torch.Size([512, 512, 9])\n",
      "Shape of fc_layer0_weight_data is  torch.Size([512, 512, 9])\n",
      "in _make_layers [Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), AvgPool2d(kernel_size=1, stride=1, padding=0)]\n",
      "Relu Inplace is  False\n",
      "model parameters are \n",
      " [torch.Size([64, 3, 3, 3]), torch.Size([128, 64, 3, 3]), torch.Size([256, 128, 3, 3]), torch.Size([256, 256, 3, 3]), torch.Size([512, 256, 3, 3]), torch.Size([512, 512, 3, 3]), torch.Size([512, 512, 3, 3]), torch.Size([512, 512, 3, 3]), torch.Size([10, 512])]\n",
      "len of model_state_dict is  9\n",
      "len of new_params is  7\n",
      "updated parameters for layer  features.0.weight\n",
      "updated parameters for layer  features.3.weight\n",
      "updated parameters for layer  features.6.weight\n",
      "updated parameters for layer  features.8.weight\n",
      "updated parameters for layer  features.11.weight\n",
      "updated parameters for layer  features.13.weight\n",
      "updated parameters for layer  features.16.weight\n",
      "\n",
      "--------- Testing in global mode ---------\n",
      "size of test_loader dataset:  10000\n",
      "\n",
      "Test set: Avg. loss: 0.0022, Accuracy: 2219/10000 (22%)\n",
      "\n",
      "accuracy after update is  22.19\n",
      "For layer idx 6, accuracy of the updated model is 22.19\n",
      "Previous layer shape is  torch.Size([512, 512, 3, 3])\n",
      "shape of layer: model 0 torch.Size([512, 512, 9])\n",
      "shape of layer: model 1 torch.Size([512, 512, 9])\n",
      "shape of previous transport map torch.Size([512, 512])\n",
      "Processing the coordinates to form ground_metric\n",
      "dont leave off the squaring of the ground metric\n",
      "returns a uniform measure of cardinality:  512\n",
      "returns a uniform measure of cardinality:  512\n",
      "the transport map is  tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        [0.0020, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        ...,\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],\n",
      "       device='cuda:0')\n",
      "Ratio of trace to the matrix sum:  tensor(0., device='cuda:0')\n",
      "Here, trace is 0.0 and matrix sum is 511.97381591796875 \n",
      "this is past correction for weight mode\n",
      "Shape of aligned wt is  torch.Size([512, 512, 9])\n",
      "Shape of fc_layer0_weight_data is  torch.Size([512, 512, 9])\n",
      "in _make_layers [Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), AvgPool2d(kernel_size=1, stride=1, padding=0)]\n",
      "Relu Inplace is  False\n",
      "model parameters are \n",
      " [torch.Size([64, 3, 3, 3]), torch.Size([128, 64, 3, 3]), torch.Size([256, 128, 3, 3]), torch.Size([256, 256, 3, 3]), torch.Size([512, 256, 3, 3]), torch.Size([512, 512, 3, 3]), torch.Size([512, 512, 3, 3]), torch.Size([512, 512, 3, 3]), torch.Size([10, 512])]\n",
      "len of model_state_dict is  9\n",
      "len of new_params is  8\n",
      "updated parameters for layer  features.0.weight\n",
      "updated parameters for layer  features.3.weight\n",
      "updated parameters for layer  features.6.weight\n",
      "updated parameters for layer  features.8.weight\n",
      "updated parameters for layer  features.11.weight\n",
      "updated parameters for layer  features.13.weight\n",
      "updated parameters for layer  features.16.weight\n",
      "updated parameters for layer  features.18.weight\n",
      "\n",
      "--------- Testing in global mode ---------\n",
      "size of test_loader dataset:  10000\n",
      "\n",
      "Test set: Avg. loss: 0.0024, Accuracy: 582/10000 (6%)\n",
      "\n",
      "accuracy after update is  5.82\n",
      "For layer idx 7, accuracy of the updated model is 5.82\n",
      "Previous layer shape is  torch.Size([512, 512, 3, 3])\n",
      "shape of layer: model 0 torch.Size([10, 512])\n",
      "shape of layer: model 1 torch.Size([10, 512])\n",
      "shape of previous transport map torch.Size([512, 512])\n",
      "Processing the coordinates to form ground_metric\n",
      "dont leave off the squaring of the ground metric\n",
      "ground metric is  tensor([[1.4240, 2.4871, 2.7803, 2.8718, 2.7374, 2.8798, 2.7275, 2.6341, 2.5641,\n",
      "         2.5921],\n",
      "        [2.4888, 1.3252, 2.6731, 2.5913, 2.7583, 2.5500, 2.4173, 2.5214, 2.3047,\n",
      "         2.4100],\n",
      "        [2.7599, 2.6711, 1.3831, 2.9464, 2.9064, 2.8700, 2.7276, 2.8384, 2.7484,\n",
      "         2.7866],\n",
      "        [2.8351, 2.6790, 2.8918, 1.5430, 2.8604, 2.9158, 2.7573, 2.7979, 2.8163,\n",
      "         2.6353],\n",
      "        [2.7550, 2.5959, 2.8634, 2.8449, 1.5042, 2.7447, 2.7852, 2.6810, 2.7030,\n",
      "         2.7293],\n",
      "        [2.9189, 2.5394, 2.9468, 2.8414, 2.7271, 1.5501, 2.7780, 2.7302, 2.7976,\n",
      "         2.6695],\n",
      "        [2.6778, 2.4456, 2.7189, 2.9110, 2.6487, 2.8525, 1.3930, 2.6890, 2.4980,\n",
      "         2.5559],\n",
      "        [2.7393, 2.5495, 2.7799, 2.7902, 2.7150, 2.6589, 2.7189, 1.3924, 2.5582,\n",
      "         2.5120],\n",
      "        [2.5971, 2.3659, 2.7693, 2.6871, 2.7025, 2.8279, 2.5324, 2.6365, 1.2999,\n",
      "         2.4040],\n",
      "        [2.5918, 2.2730, 2.7815, 2.7576, 2.6696, 2.7272, 2.5482, 2.5289, 2.5207,\n",
      "         1.2878]], device='cuda:0', grad_fn=<PowBackward0>)\n",
      "returns a uniform measure of cardinality:  10\n",
      "returns a uniform measure of cardinality:  10\n",
      "the transport map is  tensor([[0.1000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0000, 0.1000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0000, 0.0000, 0.1000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.1000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.1000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.1000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.1000, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.1000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.1000,\n",
      "         0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.1000]], device='cuda:0')\n",
      "Ratio of trace to the matrix sum:  tensor(1., device='cuda:0')\n",
      "Here, trace is 9.999990463256836 and matrix sum is 9.999990463256836 \n",
      "this is past correction for weight mode\n",
      "Shape of aligned wt is  torch.Size([10, 512])\n",
      "Shape of fc_layer0_weight_data is  torch.Size([10, 512])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "in _make_layers [Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), AvgPool2d(kernel_size=1, stride=1, padding=0)]\n",
      "Relu Inplace is  False\n",
      "model parameters are \n",
      " [torch.Size([64, 3, 3, 3]), torch.Size([128, 64, 3, 3]), torch.Size([256, 128, 3, 3]), torch.Size([256, 256, 3, 3]), torch.Size([512, 256, 3, 3]), torch.Size([512, 512, 3, 3]), torch.Size([512, 512, 3, 3]), torch.Size([512, 512, 3, 3]), torch.Size([10, 512])]\n",
      "len of model_state_dict is  9\n",
      "len of new_params is  9\n",
      "updated parameters for layer  features.0.weight\n",
      "updated parameters for layer  features.3.weight\n",
      "updated parameters for layer  features.6.weight\n",
      "updated parameters for layer  features.8.weight\n",
      "updated parameters for layer  features.11.weight\n",
      "updated parameters for layer  features.13.weight\n",
      "updated parameters for layer  features.16.weight\n",
      "updated parameters for layer  features.18.weight\n",
      "updated parameters for layer  classifier.weight\n",
      "\n",
      "--------- Testing in global mode ---------\n",
      "size of test_loader dataset:  10000\n",
      "\n",
      "Test set: Avg. loss: 0.0003, Accuracy: 9019/10000 (90%)\n",
      "\n",
      "accuracy after update is  90.19\n",
      "For layer idx 8, accuracy of the updated model is 90.19\n",
      "using independent method\n",
      "in _make_layers [Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), AvgPool2d(kernel_size=1, stride=1, padding=0)]\n",
      "Relu Inplace is  False\n",
      "model parameters are \n",
      " [torch.Size([64, 3, 3, 3]), torch.Size([128, 64, 3, 3]), torch.Size([256, 128, 3, 3]), torch.Size([256, 256, 3, 3]), torch.Size([512, 256, 3, 3]), torch.Size([512, 512, 3, 3]), torch.Size([512, 512, 3, 3]), torch.Size([512, 512, 3, 3]), torch.Size([10, 512])]\n",
      "\n",
      "--------- Testing in global mode ---------\n",
      "size of test_loader dataset:  10000\n",
      "\n",
      "Test set: Avg. loss: 0.0023, Accuracy: 1090/10000 (11%)\n",
      "\n",
      "len of model parameters and avg aligned layers is  9 9\n",
      "len of model_state_dict is  9\n",
      "len of param_list is  9\n",
      "\n",
      "--------- Testing in global mode ---------\n",
      "size of test_loader dataset:  10000\n",
      "\n",
      "Test set: Avg. loss: 0.0015, Accuracy: 8544/10000 (85%)\n",
      "\n",
      "===============================================\n",
      "===============================================\n",
      "After Ensemble,acc: 85.44 loss 0.001490088927745819\n",
      "===============================================\n",
      "===============================================\n",
      "Timer ends\n",
      "Time taken for geometric ensembling is 39.17906619282439 seconds\n",
      "------- Prediction based ensembling -------\n",
      "\n",
      "Test set: Avg. loss: 0.0003, Accuracy: 9134/10000 (91%)\n",
      "\n",
      "------- Naive ensembling of weights -------\n",
      "[torch.Size([64, 3, 3, 3]), torch.Size([64, 3, 3, 3])]\n",
      "torch.Size([64, 3, 3, 3])\n",
      "[torch.Size([128, 64, 3, 3]), torch.Size([128, 64, 3, 3])]\n",
      "torch.Size([128, 64, 3, 3])\n",
      "[torch.Size([256, 128, 3, 3]), torch.Size([256, 128, 3, 3])]\n",
      "torch.Size([256, 128, 3, 3])\n",
      "[torch.Size([256, 256, 3, 3]), torch.Size([256, 256, 3, 3])]\n",
      "torch.Size([256, 256, 3, 3])\n",
      "[torch.Size([512, 256, 3, 3]), torch.Size([512, 256, 3, 3])]\n",
      "torch.Size([512, 256, 3, 3])\n",
      "[torch.Size([512, 512, 3, 3]), torch.Size([512, 512, 3, 3])]\n",
      "torch.Size([512, 512, 3, 3])\n",
      "[torch.Size([512, 512, 3, 3]), torch.Size([512, 512, 3, 3])]\n",
      "torch.Size([512, 512, 3, 3])\n",
      "[torch.Size([512, 512, 3, 3]), torch.Size([512, 512, 3, 3])]\n",
      "torch.Size([512, 512, 3, 3])\n",
      "[torch.Size([10, 512]), torch.Size([10, 512])]\n",
      "torch.Size([10, 512])\n",
      "in _make_layers [Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), AvgPool2d(kernel_size=1, stride=1, padding=0)]\n",
      "Relu Inplace is  False\n",
      "model parameters are \n",
      " [torch.Size([64, 3, 3, 3]), torch.Size([128, 64, 3, 3]), torch.Size([256, 128, 3, 3]), torch.Size([256, 256, 3, 3]), torch.Size([512, 256, 3, 3]), torch.Size([512, 512, 3, 3]), torch.Size([512, 512, 3, 3]), torch.Size([512, 512, 3, 3]), torch.Size([10, 512])]\n",
      "\n",
      "--------- Testing in global mode ---------\n",
      "size of test_loader dataset:  10000\n",
      "\n",
      "Test set: Avg. loss: 0.0023, Accuracy: 1003/10000 (10%)\n",
      "\n",
      "\n",
      "--------- Testing in global mode ---------\n",
      "size of test_loader dataset:  10000\n",
      "\n",
      "Test set: Avg. loss: 0.0023, Accuracy: 1689/10000 (17%)\n",
      "\n",
      "-------- Retraining the models ---------\n",
      "Retraining model :  model_0\n",
      "num_epochs---------------+++))))))+++ 300\n",
      "num_epochs---------------+++))))))+++ 300\n",
      "lr is  0.05\n",
      "number of epochs would be  30\n",
      "num_epochs-------------- 30\n",
      "Epoch 000\n",
      "/usr/local/pace-apps/manual/packages/pytorch/1.12.0/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:131: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\"Detected call of `lr_scheduler.step()` before `optimizer.step()`. \"\n",
      "/usr/local/pace-apps/manual/packages/pytorch/1.12.0/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:156: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: {'epoch': 0, 'value': 0.32661999999999997} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 0, 'value': 1.7951463354825983} ({'split': 'train'})\n",
      "accuracy: {'epoch': 0, 'value': 0.4468000173568726} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 0, 'value': 1.6127035140991213} ({'split': 'test'})\n",
      "Epoch 001\n",
      "accuracy: {'epoch': 1, 'value': 0.61872} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 1, 'value': 1.1192380926132195} ({'split': 'train'})\n",
      "accuracy: {'epoch': 1, 'value': 0.6669000327587128} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 1, 'value': 0.9896149754524232} ({'split': 'test'})\n",
      "Epoch 002\n",
      "accuracy: {'epoch': 2, 'value': 0.7170200000000008} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 2, 'value': 0.8508917509841916} ({'split': 'train'})\n",
      "accuracy: {'epoch': 2, 'value': 0.7598000347614289} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 2, 'value': 0.7539094805717468} ({'split': 'test'})\n",
      "Epoch 003\n",
      "accuracy: {'epoch': 3, 'value': 0.7648800000000001} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 3, 'value': 0.7084888742065429} ({'split': 'train'})\n",
      "accuracy: {'epoch': 3, 'value': 0.7596000373363494} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 3, 'value': 0.7555541694164276} ({'split': 'test'})\n",
      "Epoch 004\n",
      "accuracy: {'epoch': 4, 'value': 0.7860000000000006} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 4, 'value': 0.6431966521453859} ({'split': 'train'})\n",
      "accuracy: {'epoch': 4, 'value': 0.7735000371932984} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 4, 'value': 0.7190211236476898} ({'split': 'test'})\n",
      "Epoch 005\n",
      "accuracy: {'epoch': 5, 'value': 0.8074200000000001} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 5, 'value': 0.5843539005661013} ({'split': 'train'})\n",
      "accuracy: {'epoch': 5, 'value': 0.7666000306606293} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 5, 'value': 0.7150197565555573} ({'split': 'test'})\n",
      "Epoch 006\n",
      "accuracy: {'epoch': 6, 'value': 0.8217600000000007} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 6, 'value': 0.5457212944412239} ({'split': 'train'})\n",
      "accuracy: {'epoch': 6, 'value': 0.7734000444412232} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 6, 'value': 0.7455655336380005} ({'split': 'test'})\n",
      "Epoch 007\n",
      "accuracy: {'epoch': 7, 'value': 0.8297599999999998} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 7, 'value': 0.5171119902610777} ({'split': 'train'})\n",
      "accuracy: {'epoch': 7, 'value': 0.7826000332832337} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 7, 'value': 0.7372263729572296} ({'split': 'test'})\n",
      "Epoch 008\n"
     ]
    }
   ],
   "source": [
    "!python main.py --gpu-id 0 --model-name vgg11_nobias --n-epochs 0 --save-result-file sample.csv \\\n",
    "--sweep-name exp_sample --correction --ground-metric euclidean --weight-stats \\\n",
    "--geom-ensemble-type wts --ground-metric-normalize none --sweep-id 90 \\\n",
    "--ckpt-type best --dataset Cifar10 --ground-metric-eff --recheck-cifar --activation-seed {activation_seed} \\\n",
    "--prelu-acts --past-correction --not-squared --exact\\\n",
    "--learning-rate 0.004\\\n",
    "--momentum 0.5\\\n",
    "--batch-size-train 64 --experiment-dir {experiment_dir}\\\n",
    "--prunint-rate {prunint_rate}\\\n",
    "--run-mode {run_mode}\\\n",
    "--to-download\\\n",
    "--load-model-dir {load_model_dir}\\\n",
    "--retrain 30\\\n",
    "--eval-aligned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b8f3bf3a-6bea-412d-bd6b-a461aeb2e468",
   "metadata": {},
   "outputs": [],
   "source": [
    "#剪枝实验3:prune times without finetune\n",
    "#86.67\n",
    "activation_seed=21\n",
    "experiment_dir=\"experiment0.5\"\n",
    "prunint_rate=0.1\n",
    "run_mode='prune_times'\n",
    "load_model_dir='./cifar_models'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f9be469b-fcc3-4d81-a058-912fc2510ce3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------- Setting up parameters -------\n",
      "dumping parameters at  /storage/coda1/p-jli3175/0/mwu385/OTfusion_pruning_final/otfusion/exp_sample/configurations\n",
      "The parameters are: \n",
      " Namespace(experiment_dir='experiment0.5', prunint_rate=0.1, run_mode='prune_times', load_model_dir='./cifar_models', prune_times=3, finetunetimes_benchmark=0, n_epochs=10, batch_size_train=256, batch_size_test=1000, learning_rate=0.0001, momentum=0.5, log_interval=100, to_download=True, disable_bias=True, dataset='Cifar10', num_models=2, model_name='vgg11_nobias', config_file=None, config_dir='/storage/coda1/p-jli3175/0/mwu385/OTfusion_pruning_final/otfusion/exp_sample/configurations', num_hidden_nodes=400, num_hidden_nodes1=400, num_hidden_nodes2=200, num_hidden_nodes3=100, num_hidden_nodes4=50, sweep_id=90, gpu_id=0, skip_last_layer=False, skip_last_layer_type='average', debug=False, cifar_style_data=False, activation_histograms=False, act_num_samples=100, softmax_temperature=1, activation_mode=None, options_type='generic', deprecated=None, save_result_file='sample.csv', sweep_name='exp_sample', reg=0.01, reg_m=0.001, ground_metric='euclidean', ground_metric_normalize='none', not_squared=True, clip_gm=False, clip_min=0, clip_max=5, tmap_stats=False, ensemble_step=0.5, ground_metric_eff=True, retrain=30, retrain_lr_decay=-1, retrain_lr_decay_factor=None, retrain_lr_decay_epochs=None, retrain_avg_only=False, retrain_geometric_only=False, load_models='', ckpt_type='best', recheck_cifar=True, recheck_acc=False, eval_aligned=False, enable_dropout=False, dump_model=False, dump_final_models=False, correction=True, activation_seed=21, weight_stats=True, sinkhorn_type='normal', geom_ensemble_type='wts', act_bug=False, standardize_acts=False, transform_acts=False, center_acts=False, prelu_acts=True, pool_acts=False, pool_relu=False, normalize_acts=False, normalize_wts=False, gromov=False, gromov_loss='square_loss', tensorboard_root='./tensorboard', tensorboard=False, same_model=-1, dist_normalize=False, update_acts=False, past_correction=True, partial_reshape=False, choice='0 2 4 6 8', diff_init=False, partition_type='labels', personal_class_idx=9, partition_dataloader=-1, personal_split_frac=0.1, exact=True, skip_personal_idx=False, prediction_wts=False, width_ratio=1, proper_marginals=False, retrain_seed=-1, no_random_trainloaders=False, reinit_trainloaders=False, second_model_name=None, print_distances=False, deterministic=False, skip_retrain=-1, importance=None, unbalanced=False, temperature=20, alpha=0.7, dist_epochs=60, handle_skips=False, timestamp='2023-09-18_23-48-00_042146', rootdir='/storage/coda1/p-jli3175/0/mwu385/OTfusion_pruning_final/otfusion/exp_sample', baseroot='/storage/coda1/p-jli3175/0/mwu385/OTfusion_pruning_final/otfusion', result_dir='/storage/coda1/p-jli3175/0/mwu385/OTfusion_pruning_final/otfusion/exp_sample/results', exp_name='exp_2023-09-18_23-48-00_042146', csv_dir='/storage/coda1/p-jli3175/0/mwu385/OTfusion_pruning_final/otfusion/exp_sample/csv')\n",
      "refactored get_config\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "in _make_layers [Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), AvgPool2d(kernel_size=1, stride=1, padding=0)]\n",
      "Relu Inplace is  False\n",
      "model parameters are \n",
      " [torch.Size([64, 3, 3, 3]), torch.Size([128, 64, 3, 3]), torch.Size([256, 128, 3, 3]), torch.Size([256, 256, 3, 3]), torch.Size([512, 256, 3, 3]), torch.Size([512, 512, 3, 3]), torch.Size([512, 512, 3, 3]), torch.Size([512, 512, 3, 3]), torch.Size([10, 512])]\n",
      "in _make_layers [Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), AvgPool2d(kernel_size=1, stride=1, padding=0)]\n",
      "Relu Inplace is  False\n",
      "model parameters are \n",
      " [torch.Size([64, 3, 3, 3]), torch.Size([128, 64, 3, 3]), torch.Size([256, 128, 3, 3]), torch.Size([256, 256, 3, 3]), torch.Size([512, 256, 3, 3]), torch.Size([512, 512, 3, 3]), torch.Size([512, 512, 3, 3]), torch.Size([512, 512, 3, 3]), torch.Size([10, 512])]\n",
      "in _make_layers [Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), AvgPool2d(kernel_size=1, stride=1, padding=0)]\n",
      "Relu Inplace is  False\n",
      "model parameters are \n",
      " [torch.Size([64, 3, 3, 3]), torch.Size([128, 64, 3, 3]), torch.Size([256, 128, 3, 3]), torch.Size([256, 256, 3, 3]), torch.Size([512, 256, 3, 3]), torch.Size([512, 512, 3, 3]), torch.Size([512, 512, 3, 3]), torch.Size([512, 512, 3, 3]), torch.Size([10, 512])]\n",
      "in _make_layers [Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), AvgPool2d(kernel_size=1, stride=1, padding=0)]\n",
      "Relu Inplace is  False\n",
      "model parameters are \n",
      " [torch.Size([64, 3, 3, 3]), torch.Size([128, 64, 3, 3]), torch.Size([256, 128, 3, 3]), torch.Size([256, 256, 3, 3]), torch.Size([512, 256, 3, 3]), torch.Size([512, 512, 3, 3]), torch.Size([512, 512, 3, 3]), torch.Size([512, 512, 3, 3]), torch.Size([10, 512])]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "enter prune_times, the times is 3,ratio:0.03333333333333333\n",
      "Train Epoch: 1 [0/50000 (0%)]\tLoss: 0.101789\n",
      "Train Epoch: 1 [25600/50000 (51%)]\tLoss: 0.094688\n",
      "\n",
      "--------- Testing in global mode ---------\n",
      "size of test_loader dataset:  10000\n",
      "\n",
      "Test set: Avg. loss: 0.0003, Accuracy: 9025/10000 (90%)\n",
      "\n",
      "Train Epoch: 2 [0/50000 (0%)]\tLoss: 0.072084\n",
      "Train Epoch: 2 [25600/50000 (51%)]\tLoss: 0.088860\n",
      "\n",
      "--------- Testing in global mode ---------\n",
      "size of test_loader dataset:  10000\n",
      "\n",
      "Test set: Avg. loss: 0.0003, Accuracy: 9044/10000 (90%)\n",
      "\n",
      "Train Epoch: 3 [0/50000 (0%)]\tLoss: 0.064542\n",
      "Train Epoch: 3 [25600/50000 (51%)]\tLoss: 0.059761\n",
      "\n",
      "--------- Testing in global mode ---------\n",
      "size of test_loader dataset:  10000\n",
      "\n",
      "Test set: Avg. loss: 0.0003, Accuracy: 9049/10000 (90%)\n",
      "\n",
      "Train Epoch: 4 [0/50000 (0%)]\tLoss: 0.069622\n",
      "Train Epoch: 4 [25600/50000 (51%)]\tLoss: 0.055398\n",
      "\n",
      "--------- Testing in global mode ---------\n",
      "size of test_loader dataset:  10000\n",
      "\n",
      "Test set: Avg. loss: 0.0003, Accuracy: 9051/10000 (91%)\n",
      "\n",
      "Train Epoch: 5 [0/50000 (0%)]\tLoss: 0.066763\n",
      "Train Epoch: 5 [25600/50000 (51%)]\tLoss: 0.069572\n",
      "\n",
      "--------- Testing in global mode ---------\n",
      "size of test_loader dataset:  10000\n",
      "\n",
      "Test set: Avg. loss: 0.0003, Accuracy: 9052/10000 (91%)\n",
      "\n",
      "Train Epoch: 6 [0/50000 (0%)]\tLoss: 0.062392\n",
      "Train Epoch: 6 [25600/50000 (51%)]\tLoss: 0.061215\n",
      "\n",
      "--------- Testing in global mode ---------\n",
      "size of test_loader dataset:  10000\n",
      "\n",
      "Test set: Avg. loss: 0.0003, Accuracy: 9047/10000 (90%)\n",
      "\n",
      "Train Epoch: 7 [0/50000 (0%)]\tLoss: 0.068954\n",
      "Train Epoch: 7 [25600/50000 (51%)]\tLoss: 0.073793\n",
      "\n",
      "--------- Testing in global mode ---------\n",
      "size of test_loader dataset:  10000\n",
      "\n",
      "Test set: Avg. loss: 0.0003, Accuracy: 9049/10000 (90%)\n",
      "\n",
      "Train Epoch: 8 [0/50000 (0%)]\tLoss: 0.054324\n",
      "Train Epoch: 8 [25600/50000 (51%)]\tLoss: 0.071218\n",
      "\n",
      "--------- Testing in global mode ---------\n",
      "size of test_loader dataset:  10000\n",
      "\n",
      "Test set: Avg. loss: 0.0003, Accuracy: 9051/10000 (91%)\n",
      "\n",
      "Train Epoch: 9 [0/50000 (0%)]\tLoss: 0.056769\n",
      "Train Epoch: 9 [25600/50000 (51%)]\tLoss: 0.064998\n",
      "\n",
      "--------- Testing in global mode ---------\n",
      "size of test_loader dataset:  10000\n",
      "\n",
      "Test set: Avg. loss: 0.0003, Accuracy: 9050/10000 (90%)\n",
      "\n",
      "Train Epoch: 10 [0/50000 (0%)]\tLoss: 0.059847\n",
      "Train Epoch: 10 [25600/50000 (51%)]\tLoss: 0.060106\n",
      "\n",
      "--------- Testing in global mode ---------\n",
      "size of test_loader dataset:  10000\n",
      "\n",
      "Test set: Avg. loss: 0.0003, Accuracy: 9047/10000 (90%)\n",
      "\n",
      "Train Epoch: 1 [0/50000 (0%)]\tLoss: 0.098142\n",
      "Train Epoch: 1 [25600/50000 (51%)]\tLoss: 0.077978\n",
      "\n",
      "--------- Testing in global mode ---------\n",
      "size of test_loader dataset:  10000\n",
      "\n",
      "Test set: Avg. loss: 0.0003, Accuracy: 9067/10000 (91%)\n",
      "\n",
      "Train Epoch: 2 [0/50000 (0%)]\tLoss: 0.069018\n",
      "Train Epoch: 2 [25600/50000 (51%)]\tLoss: 0.094171\n",
      "\n",
      "--------- Testing in global mode ---------\n",
      "size of test_loader dataset:  10000\n",
      "\n",
      "Test set: Avg. loss: 0.0003, Accuracy: 9073/10000 (91%)\n",
      "\n",
      "Train Epoch: 3 [0/50000 (0%)]\tLoss: 0.072141\n",
      "Train Epoch: 3 [25600/50000 (51%)]\tLoss: 0.072218\n",
      "\n",
      "--------- Testing in global mode ---------\n",
      "size of test_loader dataset:  10000\n",
      "\n",
      "Test set: Avg. loss: 0.0003, Accuracy: 9069/10000 (91%)\n",
      "\n",
      "Train Epoch: 4 [0/50000 (0%)]\tLoss: 0.084318\n",
      "Train Epoch: 4 [25600/50000 (51%)]\tLoss: 0.065918\n",
      "\n",
      "--------- Testing in global mode ---------\n",
      "size of test_loader dataset:  10000\n",
      "\n",
      "Test set: Avg. loss: 0.0003, Accuracy: 9063/10000 (91%)\n",
      "\n",
      "Train Epoch: 5 [0/50000 (0%)]\tLoss: 0.063841\n",
      "Train Epoch: 5 [25600/50000 (51%)]\tLoss: 0.087531\n",
      "\n",
      "--------- Testing in global mode ---------\n",
      "size of test_loader dataset:  10000\n",
      "\n",
      "Test set: Avg. loss: 0.0003, Accuracy: 9058/10000 (91%)\n",
      "\n",
      "Train Epoch: 6 [0/50000 (0%)]\tLoss: 0.062021\n",
      "Train Epoch: 6 [25600/50000 (51%)]\tLoss: 0.059345\n",
      "\n",
      "--------- Testing in global mode ---------\n",
      "size of test_loader dataset:  10000\n",
      "\n",
      "Test set: Avg. loss: 0.0003, Accuracy: 9058/10000 (91%)\n",
      "\n",
      "Train Epoch: 7 [0/50000 (0%)]\tLoss: 0.063854\n",
      "Train Epoch: 7 [25600/50000 (51%)]\tLoss: 0.051740\n",
      "\n",
      "--------- Testing in global mode ---------\n",
      "size of test_loader dataset:  10000\n",
      "\n",
      "Test set: Avg. loss: 0.0003, Accuracy: 9054/10000 (91%)\n",
      "\n",
      "Train Epoch: 8 [0/50000 (0%)]\tLoss: 0.051275\n",
      "Train Epoch: 8 [25600/50000 (51%)]\tLoss: 0.062114\n",
      "\n",
      "--------- Testing in global mode ---------\n",
      "size of test_loader dataset:  10000\n",
      "\n",
      "Test set: Avg. loss: 0.0003, Accuracy: 9054/10000 (91%)\n",
      "\n",
      "Train Epoch: 9 [0/50000 (0%)]\tLoss: 0.058407\n",
      "Train Epoch: 9 [25600/50000 (51%)]\tLoss: 0.059580\n",
      "\n",
      "--------- Testing in global mode ---------\n",
      "size of test_loader dataset:  10000\n",
      "\n",
      "Test set: Avg. loss: 0.0003, Accuracy: 9056/10000 (91%)\n",
      "\n",
      "Train Epoch: 10 [0/50000 (0%)]\tLoss: 0.054807\n",
      "Train Epoch: 10 [25600/50000 (51%)]\tLoss: 0.051913\n",
      "\n",
      "--------- Testing in global mode ---------\n",
      "size of test_loader dataset:  10000\n",
      "\n",
      "Test set: Avg. loss: 0.0003, Accuracy: 9062/10000 (91%)\n",
      "\n",
      "in times 0, now cumm_ratio 0.03333333333333333,accuracies:[90.47, 90.62],losses:[0.0002988600790500641, 0.00029559371024370193]\n",
      "Train Epoch: 1 [0/50000 (0%)]\tLoss: 0.041041\n",
      "Train Epoch: 1 [25600/50000 (51%)]\tLoss: 0.058511\n",
      "\n",
      "--------- Testing in global mode ---------\n",
      "size of test_loader dataset:  10000\n",
      "\n",
      "Test set: Avg. loss: 0.0003, Accuracy: 9046/10000 (90%)\n",
      "\n",
      "Train Epoch: 2 [0/50000 (0%)]\tLoss: 0.066889\n",
      "Train Epoch: 2 [25600/50000 (51%)]\tLoss: 0.048299\n",
      "\n",
      "--------- Testing in global mode ---------\n",
      "size of test_loader dataset:  10000\n",
      "\n",
      "Test set: Avg. loss: 0.0003, Accuracy: 9047/10000 (90%)\n",
      "\n",
      "Train Epoch: 3 [0/50000 (0%)]\tLoss: 0.048745\n",
      "Train Epoch: 3 [25600/50000 (51%)]\tLoss: 0.054344\n",
      "\n",
      "--------- Testing in global mode ---------\n",
      "size of test_loader dataset:  10000\n",
      "\n",
      "Test set: Avg. loss: 0.0003, Accuracy: 9046/10000 (90%)\n",
      "\n",
      "Train Epoch: 4 [0/50000 (0%)]\tLoss: 0.040326\n",
      "Train Epoch: 4 [25600/50000 (51%)]\tLoss: 0.049172\n",
      "\n",
      "--------- Testing in global mode ---------\n",
      "size of test_loader dataset:  10000\n",
      "\n",
      "Test set: Avg. loss: 0.0003, Accuracy: 9048/10000 (90%)\n",
      "\n",
      "Train Epoch: 5 [0/50000 (0%)]\tLoss: 0.039351\n",
      "Train Epoch: 5 [25600/50000 (51%)]\tLoss: 0.043498\n",
      "\n",
      "--------- Testing in global mode ---------\n",
      "size of test_loader dataset:  10000\n",
      "\n",
      "Test set: Avg. loss: 0.0003, Accuracy: 9050/10000 (90%)\n",
      "\n",
      "Train Epoch: 6 [0/50000 (0%)]\tLoss: 0.051964\n",
      "Train Epoch: 6 [25600/50000 (51%)]\tLoss: 0.039037\n",
      "\n",
      "--------- Testing in global mode ---------\n",
      "size of test_loader dataset:  10000\n",
      "\n",
      "Test set: Avg. loss: 0.0003, Accuracy: 9048/10000 (90%)\n",
      "\n",
      "Train Epoch: 7 [0/50000 (0%)]\tLoss: 0.034801\n",
      "Train Epoch: 7 [25600/50000 (51%)]\tLoss: 0.053229\n",
      "\n",
      "--------- Testing in global mode ---------\n",
      "size of test_loader dataset:  10000\n",
      "\n",
      "Test set: Avg. loss: 0.0003, Accuracy: 9045/10000 (90%)\n",
      "\n",
      "Train Epoch: 8 [0/50000 (0%)]\tLoss: 0.047877\n",
      "Train Epoch: 8 [25600/50000 (51%)]\tLoss: 0.040826\n",
      "\n",
      "--------- Testing in global mode ---------\n",
      "size of test_loader dataset:  10000\n",
      "\n",
      "Test set: Avg. loss: 0.0003, Accuracy: 9047/10000 (90%)\n",
      "\n",
      "Train Epoch: 9 [0/50000 (0%)]\tLoss: 0.046165\n",
      "Train Epoch: 9 [25600/50000 (51%)]\tLoss: 0.042099\n",
      "\n",
      "--------- Testing in global mode ---------\n",
      "size of test_loader dataset:  10000\n",
      "\n",
      "Test set: Avg. loss: 0.0003, Accuracy: 9045/10000 (90%)\n",
      "\n",
      "Train Epoch: 10 [0/50000 (0%)]\tLoss: 0.054572\n",
      "Train Epoch: 10 [25600/50000 (51%)]\tLoss: 0.042057\n",
      "\n",
      "--------- Testing in global mode ---------\n",
      "size of test_loader dataset:  10000\n",
      "\n",
      "Test set: Avg. loss: 0.0003, Accuracy: 9046/10000 (90%)\n",
      "\n",
      "Train Epoch: 1 [0/50000 (0%)]\tLoss: 0.046947\n",
      "Train Epoch: 1 [25600/50000 (51%)]\tLoss: 0.046673\n",
      "\n",
      "--------- Testing in global mode ---------\n",
      "size of test_loader dataset:  10000\n",
      "\n",
      "Test set: Avg. loss: 0.0003, Accuracy: 9055/10000 (91%)\n",
      "\n",
      "Train Epoch: 2 [0/50000 (0%)]\tLoss: 0.053007\n",
      "Train Epoch: 2 [25600/50000 (51%)]\tLoss: 0.043337\n",
      "\n",
      "--------- Testing in global mode ---------\n",
      "size of test_loader dataset:  10000\n",
      "\n",
      "Test set: Avg. loss: 0.0003, Accuracy: 9057/10000 (91%)\n",
      "\n",
      "Train Epoch: 3 [0/50000 (0%)]\tLoss: 0.053500\n",
      "Train Epoch: 3 [25600/50000 (51%)]\tLoss: 0.047302\n",
      "\n",
      "--------- Testing in global mode ---------\n",
      "size of test_loader dataset:  10000\n",
      "\n",
      "Test set: Avg. loss: 0.0003, Accuracy: 9055/10000 (91%)\n",
      "\n",
      "Train Epoch: 4 [0/50000 (0%)]\tLoss: 0.054545\n",
      "Train Epoch: 4 [25600/50000 (51%)]\tLoss: 0.043929\n",
      "\n",
      "--------- Testing in global mode ---------\n",
      "size of test_loader dataset:  10000\n",
      "\n",
      "Test set: Avg. loss: 0.0003, Accuracy: 9054/10000 (91%)\n",
      "\n",
      "Train Epoch: 5 [0/50000 (0%)]\tLoss: 0.044912\n",
      "Train Epoch: 5 [25600/50000 (51%)]\tLoss: 0.056330\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--------- Testing in global mode ---------\n",
      "size of test_loader dataset:  10000\n",
      "\n",
      "Test set: Avg. loss: 0.0003, Accuracy: 9051/10000 (91%)\n",
      "\n",
      "Train Epoch: 6 [0/50000 (0%)]\tLoss: 0.050751\n",
      "Train Epoch: 6 [25600/50000 (51%)]\tLoss: 0.039593\n",
      "\n",
      "--------- Testing in global mode ---------\n",
      "size of test_loader dataset:  10000\n",
      "\n",
      "Test set: Avg. loss: 0.0003, Accuracy: 9050/10000 (90%)\n",
      "\n",
      "Train Epoch: 7 [0/50000 (0%)]\tLoss: 0.033185\n",
      "Train Epoch: 7 [25600/50000 (51%)]\tLoss: 0.041040\n",
      "\n",
      "--------- Testing in global mode ---------\n",
      "size of test_loader dataset:  10000\n",
      "\n",
      "Test set: Avg. loss: 0.0003, Accuracy: 9049/10000 (90%)\n",
      "\n",
      "Train Epoch: 8 [0/50000 (0%)]\tLoss: 0.047069\n",
      "Train Epoch: 8 [25600/50000 (51%)]\tLoss: 0.049885\n",
      "\n",
      "--------- Testing in global mode ---------\n",
      "size of test_loader dataset:  10000\n",
      "\n",
      "Test set: Avg. loss: 0.0003, Accuracy: 9046/10000 (90%)\n",
      "\n",
      "Train Epoch: 9 [0/50000 (0%)]\tLoss: 0.051880\n",
      "Train Epoch: 9 [25600/50000 (51%)]\tLoss: 0.043798\n",
      "\n",
      "--------- Testing in global mode ---------\n",
      "size of test_loader dataset:  10000\n",
      "\n",
      "Test set: Avg. loss: 0.0003, Accuracy: 9045/10000 (90%)\n",
      "\n",
      "Train Epoch: 10 [0/50000 (0%)]\tLoss: 0.041182\n",
      "Train Epoch: 10 [25600/50000 (51%)]\tLoss: 0.052047\n",
      "\n",
      "--------- Testing in global mode ---------\n",
      "size of test_loader dataset:  10000\n",
      "\n",
      "Test set: Avg. loss: 0.0003, Accuracy: 9043/10000 (90%)\n",
      "\n",
      "in times 1, now cumm_ratio 0.06666666666666667,accuracies:[90.46, 90.43],losses:[0.000300465926527977, 0.000296786130964756]\n",
      "Train Epoch: 1 [0/50000 (0%)]\tLoss: 0.038749\n",
      "Train Epoch: 1 [25600/50000 (51%)]\tLoss: 0.046455\n",
      "\n",
      "--------- Testing in global mode ---------\n",
      "size of test_loader dataset:  10000\n",
      "\n",
      "Test set: Avg. loss: 0.0003, Accuracy: 9045/10000 (90%)\n",
      "\n",
      "Train Epoch: 2 [0/50000 (0%)]\tLoss: 0.038959\n",
      "Train Epoch: 2 [25600/50000 (51%)]\tLoss: 0.041183\n",
      "\n",
      "--------- Testing in global mode ---------\n",
      "size of test_loader dataset:  10000\n",
      "\n",
      "Test set: Avg. loss: 0.0003, Accuracy: 9046/10000 (90%)\n",
      "\n",
      "Train Epoch: 3 [0/50000 (0%)]\tLoss: 0.054732\n",
      "Train Epoch: 3 [25600/50000 (51%)]\tLoss: 0.042637\n",
      "\n",
      "--------- Testing in global mode ---------\n",
      "size of test_loader dataset:  10000\n",
      "\n",
      "Test set: Avg. loss: 0.0003, Accuracy: 9048/10000 (90%)\n",
      "\n",
      "Train Epoch: 4 [0/50000 (0%)]\tLoss: 0.045500\n",
      "Train Epoch: 4 [25600/50000 (51%)]\tLoss: 0.041297\n",
      "\n",
      "--------- Testing in global mode ---------\n",
      "size of test_loader dataset:  10000\n",
      "\n",
      "Test set: Avg. loss: 0.0003, Accuracy: 9046/10000 (90%)\n",
      "\n",
      "Train Epoch: 5 [0/50000 (0%)]\tLoss: 0.043949\n",
      "Train Epoch: 5 [25600/50000 (51%)]\tLoss: 0.038178\n",
      "\n",
      "--------- Testing in global mode ---------\n",
      "size of test_loader dataset:  10000\n",
      "\n",
      "Test set: Avg. loss: 0.0003, Accuracy: 9041/10000 (90%)\n",
      "\n",
      "Train Epoch: 6 [0/50000 (0%)]\tLoss: 0.044087\n",
      "Train Epoch: 6 [25600/50000 (51%)]\tLoss: 0.036624\n",
      "\n",
      "--------- Testing in global mode ---------\n",
      "size of test_loader dataset:  10000\n",
      "\n",
      "Test set: Avg. loss: 0.0003, Accuracy: 9040/10000 (90%)\n",
      "\n",
      "Train Epoch: 7 [0/50000 (0%)]\tLoss: 0.024740\n",
      "Train Epoch: 7 [25600/50000 (51%)]\tLoss: 0.041686\n",
      "\n",
      "--------- Testing in global mode ---------\n",
      "size of test_loader dataset:  10000\n",
      "\n",
      "Test set: Avg. loss: 0.0003, Accuracy: 9038/10000 (90%)\n",
      "\n",
      "Train Epoch: 8 [0/50000 (0%)]\tLoss: 0.044529\n",
      "Train Epoch: 8 [25600/50000 (51%)]\tLoss: 0.032994\n",
      "\n",
      "--------- Testing in global mode ---------\n",
      "size of test_loader dataset:  10000\n",
      "\n",
      "Test set: Avg. loss: 0.0003, Accuracy: 9038/10000 (90%)\n",
      "\n",
      "Train Epoch: 9 [0/50000 (0%)]\tLoss: 0.033306\n",
      "Train Epoch: 9 [25600/50000 (51%)]\tLoss: 0.034491\n",
      "\n",
      "--------- Testing in global mode ---------\n",
      "size of test_loader dataset:  10000\n",
      "\n",
      "Test set: Avg. loss: 0.0003, Accuracy: 9038/10000 (90%)\n",
      "\n",
      "Train Epoch: 10 [0/50000 (0%)]\tLoss: 0.033253\n",
      "Train Epoch: 10 [25600/50000 (51%)]\tLoss: 0.043178\n",
      "\n",
      "--------- Testing in global mode ---------\n",
      "size of test_loader dataset:  10000\n",
      "\n",
      "Test set: Avg. loss: 0.0003, Accuracy: 9036/10000 (90%)\n",
      "\n",
      "Train Epoch: 1 [0/50000 (0%)]\tLoss: 0.038899\n",
      "Train Epoch: 1 [25600/50000 (51%)]\tLoss: 0.041474\n",
      "\n",
      "--------- Testing in global mode ---------\n",
      "size of test_loader dataset:  10000\n",
      "\n",
      "Test set: Avg. loss: 0.0003, Accuracy: 9044/10000 (90%)\n",
      "\n",
      "Train Epoch: 2 [0/50000 (0%)]\tLoss: 0.033870\n",
      "Train Epoch: 2 [25600/50000 (51%)]\tLoss: 0.043790\n",
      "\n",
      "--------- Testing in global mode ---------\n",
      "size of test_loader dataset:  10000\n",
      "\n",
      "Test set: Avg. loss: 0.0003, Accuracy: 9045/10000 (90%)\n",
      "\n",
      "Train Epoch: 3 [0/50000 (0%)]\tLoss: 0.050958\n",
      "Train Epoch: 3 [25600/50000 (51%)]\tLoss: 0.035053\n",
      "\n",
      "--------- Testing in global mode ---------\n",
      "size of test_loader dataset:  10000\n",
      "\n",
      "Test set: Avg. loss: 0.0003, Accuracy: 9046/10000 (90%)\n",
      "\n",
      "Train Epoch: 4 [0/50000 (0%)]\tLoss: 0.030436\n",
      "Train Epoch: 4 [25600/50000 (51%)]\tLoss: 0.041229\n",
      "\n",
      "--------- Testing in global mode ---------\n",
      "size of test_loader dataset:  10000\n",
      "\n",
      "Test set: Avg. loss: 0.0003, Accuracy: 9045/10000 (90%)\n",
      "\n",
      "Train Epoch: 5 [0/50000 (0%)]\tLoss: 0.042111\n",
      "Train Epoch: 5 [25600/50000 (51%)]\tLoss: 0.038093\n",
      "\n",
      "--------- Testing in global mode ---------\n",
      "size of test_loader dataset:  10000\n",
      "\n",
      "Test set: Avg. loss: 0.0003, Accuracy: 9041/10000 (90%)\n",
      "\n",
      "Train Epoch: 6 [0/50000 (0%)]\tLoss: 0.034861\n",
      "Train Epoch: 6 [25600/50000 (51%)]\tLoss: 0.037804\n",
      "\n",
      "--------- Testing in global mode ---------\n",
      "size of test_loader dataset:  10000\n",
      "\n",
      "Test set: Avg. loss: 0.0003, Accuracy: 9036/10000 (90%)\n",
      "\n",
      "Train Epoch: 7 [0/50000 (0%)]\tLoss: 0.043934\n",
      "Train Epoch: 7 [25600/50000 (51%)]\tLoss: 0.045628\n",
      "\n",
      "--------- Testing in global mode ---------\n",
      "size of test_loader dataset:  10000\n",
      "\n",
      "Test set: Avg. loss: 0.0003, Accuracy: 9034/10000 (90%)\n",
      "\n",
      "Train Epoch: 8 [0/50000 (0%)]\tLoss: 0.040872\n",
      "Train Epoch: 8 [25600/50000 (51%)]\tLoss: 0.036776\n",
      "\n",
      "--------- Testing in global mode ---------\n",
      "size of test_loader dataset:  10000\n",
      "\n",
      "Test set: Avg. loss: 0.0003, Accuracy: 9034/10000 (90%)\n",
      "\n",
      "Train Epoch: 9 [0/50000 (0%)]\tLoss: 0.034741\n",
      "Train Epoch: 9 [25600/50000 (51%)]\tLoss: 0.045739\n",
      "\n",
      "--------- Testing in global mode ---------\n",
      "size of test_loader dataset:  10000\n",
      "\n",
      "Test set: Avg. loss: 0.0003, Accuracy: 9031/10000 (90%)\n",
      "\n",
      "Train Epoch: 10 [0/50000 (0%)]\tLoss: 0.032746\n",
      "Train Epoch: 10 [25600/50000 (51%)]\tLoss: 0.041258\n",
      "\n",
      "--------- Testing in global mode ---------\n",
      "size of test_loader dataset:  10000\n",
      "\n",
      "Test set: Avg. loss: 0.0003, Accuracy: 9030/10000 (90%)\n",
      "\n",
      "in times 2, now cumm_ratio 0.1,accuracies:[90.36, 90.3],losses:[0.00030356852412223817, 0.000299582214653492]\n",
      "===============================================\n",
      "===============================================\n",
      "before Ensemble,acc: [90.36, 90.3] loss [0.00030356852412223817, 0.000299582214653492]\n",
      "===============================================\n",
      "===============================================\n",
      "check zero rate for model 0\n",
      "odict_keys(['features.0.weight', 'features.3.weight', 'features.6.weight', 'features.8.weight', 'features.11.weight', 'features.13.weight', 'features.16.weight', 'features.18.weight', 'classifier.weight'])\n",
      "the ratio of zero for features.0.weight\n",
      "tensor(0.1001, device='cuda:0')\n",
      "the ratio of zero for features.3.weight\n",
      "tensor(0.1000, device='cuda:0')\n",
      "the ratio of zero for features.6.weight\n",
      "tensor(0.1000, device='cuda:0')\n",
      "the ratio of zero for features.8.weight\n",
      "tensor(0.1000, device='cuda:0')\n",
      "the ratio of zero for features.11.weight\n",
      "tensor(0.1000, device='cuda:0')\n",
      "the ratio of zero for features.13.weight\n",
      "tensor(0.1000, device='cuda:0')\n",
      "the ratio of zero for features.16.weight\n",
      "tensor(0.1000, device='cuda:0')\n",
      "the ratio of zero for features.18.weight\n",
      "tensor(0.1000, device='cuda:0')\n",
      "the ratio of zero for classifier.weight\n",
      "tensor(0.1000, device='cuda:0')\n",
      "check zero rate for model 1\n",
      "odict_keys(['features.0.weight', 'features.3.weight', 'features.6.weight', 'features.8.weight', 'features.11.weight', 'features.13.weight', 'features.16.weight', 'features.18.weight', 'classifier.weight'])\n",
      "the ratio of zero for features.0.weight\n",
      "tensor(0.1001, device='cuda:0')\n",
      "the ratio of zero for features.3.weight\n",
      "tensor(0.1000, device='cuda:0')\n",
      "the ratio of zero for features.6.weight\n",
      "tensor(0.1000, device='cuda:0')\n",
      "the ratio of zero for features.8.weight\n",
      "tensor(0.1000, device='cuda:0')\n",
      "the ratio of zero for features.11.weight\n",
      "tensor(0.1000, device='cuda:0')\n",
      "the ratio of zero for features.13.weight\n",
      "tensor(0.1000, device='cuda:0')\n",
      "the ratio of zero for features.16.weight\n",
      "tensor(0.1000, device='cuda:0')\n",
      "the ratio of zero for features.18.weight\n",
      "tensor(0.1000, device='cuda:0')\n",
      "the ratio of zero for classifier.weight\n",
      "tensor(0.1000, device='cuda:0')\n",
      "layer features.0.weight has #params  1728\n",
      "layer features.3.weight has #params  73728\n",
      "layer features.6.weight has #params  294912\n",
      "layer features.8.weight has #params  589824\n",
      "layer features.11.weight has #params  1179648\n",
      "layer features.13.weight has #params  2359296\n",
      "layer features.16.weight has #params  2359296\n",
      "layer features.18.weight has #params  2359296\n",
      "layer classifier.weight has #params  5120\n",
      "Activation Timer start\n",
      "Activation Timer ends\n",
      "------- Geometric Ensembling -------\n",
      "Timer start\n",
      "Previous layer shape is  None\n",
      "Processing the coordinates to form ground_metric\n",
      "dont leave off the squaring of the ground metric\n",
      "returns a uniform measure of cardinality:  64\n",
      "returns a uniform measure of cardinality:  64\n",
      "the transport map is  tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')\n",
      "Ratio of trace to the matrix sum:  tensor(0.0469, device='cuda:0')\n",
      "Here, trace is 2.9999806880950928 and matrix sum is 63.99958801269531 \n",
      "this is past correction for weight mode\n",
      "Shape of aligned wt is  torch.Size([64, 3, 9])\n",
      "Shape of fc_layer0_weight_data is  torch.Size([64, 3, 9])\n",
      "Previous layer shape is  torch.Size([64, 3, 3, 3])\n",
      "shape of layer: model 0 torch.Size([128, 64, 9])\n",
      "shape of layer: model 1 torch.Size([128, 64, 9])\n",
      "shape of previous transport map torch.Size([64, 64])\n",
      "Processing the coordinates to form ground_metric\n",
      "dont leave off the squaring of the ground metric\n",
      "returns a uniform measure of cardinality:  128\n",
      "returns a uniform measure of cardinality:  128\n",
      "the transport map is  tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        ...,\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0078],\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],\n",
      "       device='cuda:0')\n",
      "Ratio of trace to the matrix sum:  tensor(0.0078, device='cuda:0')\n",
      "Here, trace is 0.9999872446060181 and matrix sum is 127.99836730957031 \n",
      "this is past correction for weight mode\n",
      "Shape of aligned wt is  torch.Size([128, 64, 9])\n",
      "Shape of fc_layer0_weight_data is  torch.Size([128, 64, 9])\n",
      "Previous layer shape is  torch.Size([128, 64, 3, 3])\n",
      "shape of layer: model 0 torch.Size([256, 128, 9])\n",
      "shape of layer: model 1 torch.Size([256, 128, 9])\n",
      "shape of previous transport map torch.Size([128, 128])\n",
      "Processing the coordinates to form ground_metric\n",
      "dont leave off the squaring of the ground metric\n",
      "returns a uniform measure of cardinality:  256\n",
      "returns a uniform measure of cardinality:  256\n",
      "the transport map is  tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')\n",
      "Ratio of trace to the matrix sum:  tensor(0., device='cuda:0')\n",
      "Here, trace is 0.0 and matrix sum is 255.99343872070312 \n",
      "this is past correction for weight mode\n",
      "Shape of aligned wt is  torch.Size([256, 128, 9])\n",
      "Shape of fc_layer0_weight_data is  torch.Size([256, 128, 9])\n",
      "Previous layer shape is  torch.Size([256, 128, 3, 3])\n",
      "shape of layer: model 0 torch.Size([256, 256, 9])\n",
      "shape of layer: model 1 torch.Size([256, 256, 9])\n",
      "shape of previous transport map torch.Size([256, 256])\n",
      "Processing the coordinates to form ground_metric\n",
      "dont leave off the squaring of the ground metric\n",
      "returns a uniform measure of cardinality:  256\n",
      "returns a uniform measure of cardinality:  256\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the transport map is  tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        ...,\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.0039, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],\n",
      "       device='cuda:0')\n",
      "Ratio of trace to the matrix sum:  tensor(0.0078, device='cuda:0')\n",
      "Here, trace is 1.9999487400054932 and matrix sum is 255.99343872070312 \n",
      "this is past correction for weight mode\n",
      "Shape of aligned wt is  torch.Size([256, 256, 9])\n",
      "Shape of fc_layer0_weight_data is  torch.Size([256, 256, 9])\n",
      "Previous layer shape is  torch.Size([256, 256, 3, 3])\n",
      "shape of layer: model 0 torch.Size([512, 256, 9])\n",
      "shape of layer: model 1 torch.Size([512, 256, 9])\n",
      "shape of previous transport map torch.Size([256, 256])\n",
      "Processing the coordinates to form ground_metric\n",
      "dont leave off the squaring of the ground metric\n",
      "returns a uniform measure of cardinality:  512\n",
      "returns a uniform measure of cardinality:  512\n",
      "the transport map is  tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')\n",
      "Ratio of trace to the matrix sum:  tensor(0.0039, device='cuda:0')\n",
      "Here, trace is 1.9998977184295654 and matrix sum is 511.97381591796875 \n",
      "this is past correction for weight mode\n",
      "Shape of aligned wt is  torch.Size([512, 256, 9])\n",
      "Shape of fc_layer0_weight_data is  torch.Size([512, 256, 9])\n",
      "Previous layer shape is  torch.Size([512, 256, 3, 3])\n",
      "shape of layer: model 0 torch.Size([512, 512, 9])\n",
      "shape of layer: model 1 torch.Size([512, 512, 9])\n",
      "shape of previous transport map torch.Size([512, 512])\n",
      "Processing the coordinates to form ground_metric\n",
      "dont leave off the squaring of the ground metric\n",
      "returns a uniform measure of cardinality:  512\n",
      "returns a uniform measure of cardinality:  512\n",
      "the transport map is  tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')\n",
      "Ratio of trace to the matrix sum:  tensor(0.0020, device='cuda:0')\n",
      "Here, trace is 0.9999488592147827 and matrix sum is 511.97381591796875 \n",
      "this is past correction for weight mode\n",
      "Shape of aligned wt is  torch.Size([512, 512, 9])\n",
      "Shape of fc_layer0_weight_data is  torch.Size([512, 512, 9])\n",
      "Previous layer shape is  torch.Size([512, 512, 3, 3])\n",
      "shape of layer: model 0 torch.Size([512, 512, 9])\n",
      "shape of layer: model 1 torch.Size([512, 512, 9])\n",
      "shape of previous transport map torch.Size([512, 512])\n",
      "Processing the coordinates to form ground_metric\n",
      "dont leave off the squaring of the ground metric\n",
      "returns a uniform measure of cardinality:  512\n",
      "returns a uniform measure of cardinality:  512\n",
      "the transport map is  tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')\n",
      "Ratio of trace to the matrix sum:  tensor(0.0039, device='cuda:0')\n",
      "Here, trace is 1.9998977184295654 and matrix sum is 511.97381591796875 \n",
      "this is past correction for weight mode\n",
      "Shape of aligned wt is  torch.Size([512, 512, 9])\n",
      "Shape of fc_layer0_weight_data is  torch.Size([512, 512, 9])\n",
      "Previous layer shape is  torch.Size([512, 512, 3, 3])\n",
      "shape of layer: model 0 torch.Size([512, 512, 9])\n",
      "shape of layer: model 1 torch.Size([512, 512, 9])\n",
      "shape of previous transport map torch.Size([512, 512])\n",
      "Processing the coordinates to form ground_metric\n",
      "dont leave off the squaring of the ground metric\n",
      "returns a uniform measure of cardinality:  512\n",
      "returns a uniform measure of cardinality:  512\n",
      "the transport map is  tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        [0.0020, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        ...,\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],\n",
      "       device='cuda:0')\n",
      "Ratio of trace to the matrix sum:  tensor(0., device='cuda:0')\n",
      "Here, trace is 0.0 and matrix sum is 511.97381591796875 \n",
      "this is past correction for weight mode\n",
      "Shape of aligned wt is  torch.Size([512, 512, 9])\n",
      "Shape of fc_layer0_weight_data is  torch.Size([512, 512, 9])\n",
      "Previous layer shape is  torch.Size([512, 512, 3, 3])\n",
      "shape of layer: model 0 torch.Size([10, 512])\n",
      "shape of layer: model 1 torch.Size([10, 512])\n",
      "shape of previous transport map torch.Size([512, 512])\n",
      "Processing the coordinates to form ground_metric\n",
      "dont leave off the squaring of the ground metric\n",
      "ground metric is  tensor([[1.4370, 2.5170, 2.7965, 2.8902, 2.7202, 2.9037, 2.7234, 2.6404, 2.5880,\n",
      "         2.6038],\n",
      "        [2.4790, 1.3099, 2.7099, 2.5984, 2.7687, 2.5522, 2.4336, 2.5396, 2.3124,\n",
      "         2.4238],\n",
      "        [2.7866, 2.6485, 1.3376, 2.9705, 2.9295, 2.8846, 2.7586, 2.8667, 2.7685,\n",
      "         2.7801],\n",
      "        [2.8154, 2.6777, 2.9166, 1.5557, 2.8383, 2.9819, 2.7661, 2.8123, 2.8054,\n",
      "         2.6870],\n",
      "        [2.8038, 2.5750, 2.8802, 2.8660, 1.5588, 2.7412, 2.7859, 2.6901, 2.7019,\n",
      "         2.7343],\n",
      "        [2.9356, 2.5723, 2.9471, 2.8306, 2.7437, 1.5425, 2.7796, 2.7301, 2.8238,\n",
      "         2.7095],\n",
      "        [2.7021, 2.4642, 2.7190, 2.9199, 2.6661, 2.8698, 1.3875, 2.7161, 2.5033,\n",
      "         2.5364],\n",
      "        [2.7519, 2.5755, 2.7697, 2.8235, 2.7236, 2.6818, 2.7160, 1.3764, 2.5775,\n",
      "         2.5130],\n",
      "        [2.5759, 2.4190, 2.7833, 2.7037, 2.7420, 2.8243, 2.5606, 2.6471, 1.2724,\n",
      "         2.3830],\n",
      "        [2.6126, 2.2658, 2.8247, 2.7498, 2.6714, 2.7021, 2.5691, 2.5257, 2.5505,\n",
      "         1.3190]], device='cuda:0', grad_fn=<PowBackward0>)\n",
      "returns a uniform measure of cardinality:  10\n",
      "returns a uniform measure of cardinality:  10\n",
      "the transport map is  tensor([[0.1000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0000, 0.1000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0000, 0.0000, 0.1000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.1000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.1000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.1000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.1000, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.1000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.1000,\n",
      "         0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.1000]], device='cuda:0')\n",
      "Ratio of trace to the matrix sum:  tensor(1., device='cuda:0')\n",
      "Here, trace is 9.999990463256836 and matrix sum is 9.999990463256836 \n",
      "this is past correction for weight mode\n",
      "Shape of aligned wt is  torch.Size([10, 512])\n",
      "Shape of fc_layer0_weight_data is  torch.Size([10, 512])\n",
      "using independent method\n",
      "in _make_layers [Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), AvgPool2d(kernel_size=1, stride=1, padding=0)]\n",
      "Relu Inplace is  False\n",
      "model parameters are \n",
      " [torch.Size([64, 3, 3, 3]), torch.Size([128, 64, 3, 3]), torch.Size([256, 128, 3, 3]), torch.Size([256, 256, 3, 3]), torch.Size([512, 256, 3, 3]), torch.Size([512, 512, 3, 3]), torch.Size([512, 512, 3, 3]), torch.Size([512, 512, 3, 3]), torch.Size([10, 512])]\n",
      "\n",
      "--------- Testing in global mode ---------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "size of test_loader dataset:  10000\n",
      "\n",
      "Test set: Avg. loss: 0.0023, Accuracy: 1000/10000 (10%)\n",
      "\n",
      "len of model parameters and avg aligned layers is  9 9\n",
      "len of model_state_dict is  9\n",
      "len of param_list is  9\n",
      "\n",
      "--------- Testing in global mode ---------\n",
      "size of test_loader dataset:  10000\n",
      "\n",
      "Test set: Avg. loss: 0.0012, Accuracy: 8667/10000 (87%)\n",
      "\n",
      "===============================================\n",
      "===============================================\n",
      "After Ensemble,acc: 86.67 loss 0.0012457099914550782\n",
      "===============================================\n",
      "===============================================\n",
      "Timer ends\n",
      "Time taken for geometric ensembling is 3.384374941000715 seconds\n",
      "------- Prediction based ensembling -------\n",
      "\n",
      "Test set: Avg. loss: 0.0003, Accuracy: 9118/10000 (91%)\n",
      "\n",
      "------- Naive ensembling of weights -------\n",
      "[torch.Size([64, 3, 3, 3]), torch.Size([64, 3, 3, 3])]\n",
      "torch.Size([64, 3, 3, 3])\n",
      "[torch.Size([128, 64, 3, 3]), torch.Size([128, 64, 3, 3])]\n",
      "torch.Size([128, 64, 3, 3])\n",
      "[torch.Size([256, 128, 3, 3]), torch.Size([256, 128, 3, 3])]\n",
      "torch.Size([256, 128, 3, 3])\n",
      "[torch.Size([256, 256, 3, 3]), torch.Size([256, 256, 3, 3])]\n",
      "torch.Size([256, 256, 3, 3])\n",
      "[torch.Size([512, 256, 3, 3]), torch.Size([512, 256, 3, 3])]\n",
      "torch.Size([512, 256, 3, 3])\n",
      "[torch.Size([512, 512, 3, 3]), torch.Size([512, 512, 3, 3])]\n",
      "torch.Size([512, 512, 3, 3])\n",
      "[torch.Size([512, 512, 3, 3]), torch.Size([512, 512, 3, 3])]\n",
      "torch.Size([512, 512, 3, 3])\n",
      "[torch.Size([512, 512, 3, 3]), torch.Size([512, 512, 3, 3])]\n",
      "torch.Size([512, 512, 3, 3])\n",
      "[torch.Size([10, 512]), torch.Size([10, 512])]\n",
      "torch.Size([10, 512])\n",
      "in _make_layers [Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), AvgPool2d(kernel_size=1, stride=1, padding=0)]\n",
      "Relu Inplace is  False\n",
      "model parameters are \n",
      " [torch.Size([64, 3, 3, 3]), torch.Size([128, 64, 3, 3]), torch.Size([256, 128, 3, 3]), torch.Size([256, 256, 3, 3]), torch.Size([512, 256, 3, 3]), torch.Size([512, 512, 3, 3]), torch.Size([512, 512, 3, 3]), torch.Size([512, 512, 3, 3]), torch.Size([10, 512])]\n",
      "\n",
      "--------- Testing in global mode ---------\n",
      "size of test_loader dataset:  10000\n",
      "\n",
      "Test set: Avg. loss: 0.0023, Accuracy: 904/10000 (9%)\n",
      "\n",
      "\n",
      "--------- Testing in global mode ---------\n",
      "size of test_loader dataset:  10000\n",
      "\n",
      "Test set: Avg. loss: 0.0023, Accuracy: 1445/10000 (14%)\n",
      "\n",
      "-------- Retraining the models ---------\n",
      "Retraining model :  model_0\n",
      "num_epochs---------------+++))))))+++ 300\n",
      "num_epochs---------------+++))))))+++ 300\n",
      "lr is  0.05\n",
      "number of epochs would be  30\n",
      "num_epochs-------------- 30\n",
      "Epoch 000\n",
      "/usr/local/pace-apps/manual/packages/pytorch/1.12.0/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:131: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\"Detected call of `lr_scheduler.step()` before `optimizer.step()`. \"\n",
      "/usr/local/pace-apps/manual/packages/pytorch/1.12.0/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:156: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "accuracy: {'epoch': 0, 'value': 0.9041399999809262} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 0, 'value': 0.28669519910335545} ({'split': 'train'})\n",
      "accuracy: {'epoch': 0, 'value': 0.785200035572052} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 0, 'value': 0.7005681455135345} ({'split': 'test'})\n",
      "Epoch 001\n",
      "accuracy: {'epoch': 1, 'value': 0.9273800000572207} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 1, 'value': 0.21586352387905125} ({'split': 'train'})\n",
      "accuracy: {'epoch': 1, 'value': 0.8405000388622283} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 1, 'value': 0.5108964920043946} ({'split': 'test'})\n",
      "Epoch 002\n",
      "accuracy: {'epoch': 2, 'value': 0.9498200000190735} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 2, 'value': 0.14992135113000865} ({'split': 'train'})\n",
      "accuracy: {'epoch': 2, 'value': 0.8540000438690185} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 2, 'value': 0.49704589247703557} ({'split': 'test'})\n",
      "Epoch 003\n",
      "accuracy: {'epoch': 3, 'value': 0.9645400000000005} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 3, 'value': 0.10720644918918612} ({'split': 'train'})\n",
      "accuracy: {'epoch': 3, 'value': 0.8466000437736512} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 3, 'value': 0.5993120431900024} ({'split': 'test'})\n",
      "Epoch 004\n",
      "accuracy: {'epoch': 4, 'value': 0.9739800000190737} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 4, 'value': 0.0801488254261017} ({'split': 'train'})\n",
      "accuracy: {'epoch': 4, 'value': 0.8577000439167023} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 4, 'value': 0.5977935373783111} ({'split': 'test'})\n",
      "Epoch 005\n",
      "accuracy: {'epoch': 5, 'value': 0.9739999999809265} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 5, 'value': 0.08260964218616489} ({'split': 'train'})\n",
      "accuracy: {'epoch': 5, 'value': 0.8455000340938568} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 5, 'value': 0.5478155165910721} ({'split': 'test'})\n",
      "Epoch 006\n",
      "accuracy: {'epoch': 6, 'value': 0.9815399999999999} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 6, 'value': 0.05657782359242438} ({'split': 'train'})\n",
      "accuracy: {'epoch': 6, 'value': 0.8633000373840332} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 6, 'value': 0.5162014275789262} ({'split': 'test'})\n",
      "Epoch 007\n",
      "accuracy: {'epoch': 7, 'value': 0.9785600000572204} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 7, 'value': 0.06542012962818143} ({'split': 'train'})\n",
      "accuracy: {'epoch': 7, 'value': 0.8551000475883483} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 7, 'value': 0.5810924649238586} ({'split': 'test'})\n",
      "Epoch 008\n",
      "accuracy: {'epoch': 8, 'value': 0.9802000000190735} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 8, 'value': 0.059666771401166926} ({'split': 'train'})\n",
      "accuracy: {'epoch': 8, 'value': 0.8568000376224519} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 8, 'value': 0.5835742086172104} ({'split': 'test'})\n",
      "Epoch 009\n",
      "accuracy: {'epoch': 9, 'value': 0.985840000038147} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 9, 'value': 0.04381831169962885} ({'split': 'train'})\n",
      "accuracy: {'epoch': 9, 'value': 0.8572000384330749} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 9, 'value': 0.5706169664859773} ({'split': 'test'})\n",
      "Epoch 010\n",
      "accuracy: {'epoch': 10, 'value': 0.9861600000381469} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 10, 'value': 0.04470444669246673} ({'split': 'train'})\n",
      "accuracy: {'epoch': 10, 'value': 0.8613000392913818} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 10, 'value': 0.5418256282806397} ({'split': 'test'})\n",
      "Epoch 011\n",
      "accuracy: {'epoch': 11, 'value': 0.9839599999809263} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 11, 'value': 0.04894669854164124} ({'split': 'train'})\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: {'epoch': 11, 'value': 0.868600034713745} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 11, 'value': 0.5321995973587036} ({'split': 'test'})\n",
      "Epoch 012\n",
      "accuracy: {'epoch': 12, 'value': 0.9875000000190737} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 12, 'value': 0.041528320365548134} ({'split': 'train'})\n",
      "accuracy: {'epoch': 12, 'value': 0.8638000428676607} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 12, 'value': 0.5639681100845337} ({'split': 'test'})\n",
      "Epoch 013\n",
      "accuracy: {'epoch': 13, 'value': 0.9885000000190733} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 13, 'value': 0.03524789658308029} ({'split': 'train'})\n",
      "accuracy: {'epoch': 13, 'value': 0.8639000415802002} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 13, 'value': 0.5752758264541625} ({'split': 'test'})\n",
      "Epoch 014\n",
      "accuracy: {'epoch': 14, 'value': 0.9886799999999999} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 14, 'value': 0.03680704001188278} ({'split': 'train'})\n",
      "accuracy: {'epoch': 14, 'value': 0.8611000418663025} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 14, 'value': 0.5568656235933304} ({'split': 'test'})\n",
      "Epoch 015\n",
      "accuracy: {'epoch': 15, 'value': 0.9903400000190737} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 15, 'value': 0.031033228895664226} ({'split': 'train'})\n",
      "accuracy: {'epoch': 15, 'value': 0.8603000402450561} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 15, 'value': 0.6231871545314789} ({'split': 'test'})\n",
      "Epoch 016\n",
      "accuracy: {'epoch': 16, 'value': 0.9865200000381471} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 16, 'value': 0.042344290130138365} ({'split': 'train'})\n",
      "accuracy: {'epoch': 16, 'value': 0.8511000394821167} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 16, 'value': 0.6502823412418365} ({'split': 'test'})\n",
      "Epoch 017\n",
      "accuracy: {'epoch': 17, 'value': 0.9848} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 17, 'value': 0.04950719430446625} ({'split': 'train'})\n",
      "accuracy: {'epoch': 17, 'value': 0.8571000456809998} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 17, 'value': 0.5751060009002686} ({'split': 'test'})\n",
      "Epoch 018\n",
      "accuracy: {'epoch': 18, 'value': 0.9899599999999998} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 18, 'value': 0.03186043645262718} ({'split': 'train'})\n",
      "accuracy: {'epoch': 18, 'value': 0.8420000255107879} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 18, 'value': 0.5996732831001282} ({'split': 'test'})\n",
      "Epoch 019\n",
      "accuracy: {'epoch': 19, 'value': 0.98684} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 19, 'value': 0.04113880061149597} ({'split': 'train'})\n",
      "accuracy: {'epoch': 19, 'value': 0.8557000339031221} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 19, 'value': 0.6007565498352052} ({'split': 'test'})\n",
      "Epoch 020\n",
      "accuracy: {'epoch': 20, 'value': 0.9902000000190735} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 20, 'value': 0.03129692400336267} ({'split': 'train'})\n",
      "accuracy: {'epoch': 20, 'value': 0.8536000430583954} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 20, 'value': 0.6288745880126954} ({'split': 'test'})\n",
      "Epoch 021\n",
      "accuracy: {'epoch': 21, 'value': 0.9851600000381471} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 21, 'value': 0.04735606542348866} ({'split': 'train'})\n",
      "accuracy: {'epoch': 21, 'value': 0.8533000409603119} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 21, 'value': 0.5529408246278763} ({'split': 'test'})\n",
      "Epoch 022\n",
      "accuracy: {'epoch': 22, 'value': 0.9881000000572202} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 22, 'value': 0.039681040022373194} ({'split': 'train'})\n",
      "accuracy: {'epoch': 22, 'value': 0.8478000283241273} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 22, 'value': 0.6483450472354888} ({'split': 'test'})\n",
      "Epoch 023\n",
      "accuracy: {'epoch': 23, 'value': 0.9842800000381471} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 23, 'value': 0.05020188857793811} ({'split': 'train'})\n",
      "accuracy: {'epoch': 23, 'value': 0.8596000432968139} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 23, 'value': 0.5691728562116624} ({'split': 'test'})\n",
      "Epoch 024\n",
      "accuracy: {'epoch': 24, 'value': 0.9889800000381473} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 24, 'value': 0.03518414702296258} ({'split': 'train'})\n",
      "accuracy: {'epoch': 24, 'value': 0.8524000287055968} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 24, 'value': 0.6272675931453705} ({'split': 'test'})\n",
      "Epoch 025\n",
      "accuracy: {'epoch': 25, 'value': 0.9872800000190738} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 25, 'value': 0.04031708944618703} ({'split': 'train'})\n",
      "accuracy: {'epoch': 25, 'value': 0.8609000384807588} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 25, 'value': 0.5953854739665986} ({'split': 'test'})\n",
      "Epoch 026\n",
      "accuracy: {'epoch': 26, 'value': 0.9887000000000007} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 26, 'value': 0.036514916763305684} ({'split': 'train'})\n",
      "accuracy: {'epoch': 26, 'value': 0.8568000435829163} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 26, 'value': 0.5933032631874084} ({'split': 'test'})\n",
      "Epoch 027\n",
      "accuracy: {'epoch': 27, 'value': 0.9885600000381467} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 27, 'value': 0.03677967059612274} ({'split': 'train'})\n",
      "accuracy: {'epoch': 27, 'value': 0.8428000390529632} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 27, 'value': 0.6193886876106262} ({'split': 'test'})\n",
      "Epoch 028\n",
      "accuracy: {'epoch': 28, 'value': 0.9864799999809262} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 28, 'value': 0.043895746393203726} ({'split': 'train'})\n",
      "accuracy: {'epoch': 28, 'value': 0.8438000380992888} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 28, 'value': 0.6777532935142516} ({'split': 'test'})\n",
      "Epoch 029\n",
      "accuracy: {'epoch': 29, 'value': 0.9850400000572211} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 29, 'value': 0.04914155118465423} ({'split': 'train'})\n",
      "accuracy: {'epoch': 29, 'value': 0.8384000360965729} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 29, 'value': 0.6994359195232391} ({'split': 'test'})\n",
      "mean_test_accuracy <cifar_utils.accumulators.Mean object at 0x2aab8d7b42b0>\n",
      "QWERTY: Enter Cifar 2, acc 0.868600034713745\n",
      "Retraining model :  model_1\n",
      "num_epochs---------------+++))))))+++ 30\n",
      "num_epochs---------------+++))))))+++ 30\n",
      "lr is  0.05\n",
      "number of epochs would be  30\n",
      "num_epochs-------------- 30\n",
      "Epoch 000\n",
      "accuracy: {'epoch': 0, 'value': 0.9067600000572205} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 0, 'value': 0.28292365478992465} ({'split': 'train'})\n",
      "accuracy: {'epoch': 0, 'value': 0.8085000395774842} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 0, 'value': 0.6188137710094452} ({'split': 'test'})\n",
      "Epoch 001\n",
      "accuracy: {'epoch': 1, 'value': 0.9257599999809268} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 1, 'value': 0.21827882609367377} ({'split': 'train'})\n",
      "accuracy: {'epoch': 1, 'value': 0.8292000353336334} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 1, 'value': 0.5813724726438523} ({'split': 'test'})\n",
      "Epoch 002\n",
      "^C\n",
      "Traceback (most recent call last):\n",
      "  File \"/storage/coda1/p-jli3175/0/mwu385/OTfusion_pruning_final/otfusion/main.py\", line 460, in <module>\n",
      "    _, best_retrain_acc = routines.retrain_models(args, [*original_models, geometric_model, naive_model], retrain_loader, test_loader, config, tensorboard_obj=tensorboard_obj, initial_acc=initial_acc, nicks=nicks)\n",
      "  File \"/storage/coda1/p-jli3175/0/mwu385/OTfusion_pruning_final/otfusion/routines.py\", line 394, in retrain_models\n",
      "    retrained_network, acc = cifar_train.get_retrained_model(args, retrain_loader, test_loader, old_networks[i], config, output_root_dir, tensorboard_obj=tensorboard_obj, nick=nick, start_acc=initial_acc[i])\n",
      "  File \"/storage/coda1/p-jli3175/0/mwu385/OTfusion_pruning_final/otfusion/./cifar/train.py\", line 333, in get_retrained_model\n",
      "    best_acc,model = main(config, output_dir, args.gpu_id, pretrained_model=old_network, pretrained_dataset=(train_loader, test_loader), tensorboard_obj=tensorboard_obj,return_model=True)\n",
      "  File \"/storage/coda1/p-jli3175/0/mwu385/OTfusion_pruning_final/otfusion/./cifar/train.py\", line 67, in main\n",
      "    for batch_x, batch_y in training_loader:\n",
      "  File \"/usr/local/pace-apps/manual/packages/pytorch/1.12.0/lib/python3.9/site-packages/torch/utils/data/dataloader.py\", line 652, in __next__\n",
      "    data = self._next_data()\n",
      "  File \"/usr/local/pace-apps/manual/packages/pytorch/1.12.0/lib/python3.9/site-packages/torch/utils/data/dataloader.py\", line 692, in _next_data\n",
      "    data = self._dataset_fetcher.fetch(index)  # may raise StopIteration\n",
      "  File \"/usr/local/pace-apps/manual/packages/pytorch/1.12.0/lib/python3.9/site-packages/torch/utils/data/_utils/fetch.py\", line 49, in fetch\n",
      "    data = [self.dataset[idx] for idx in possibly_batched_index]\n",
      "  File \"/usr/local/pace-apps/manual/packages/pytorch/1.12.0/lib/python3.9/site-packages/torch/utils/data/_utils/fetch.py\", line 49, in <listcomp>\n",
      "    data = [self.dataset[idx] for idx in possibly_batched_index]\n",
      "  File \"/usr/local/pace-apps/manual/packages/pytorch/1.12.0/lib/python3.9/site-packages/torchvision/datasets/cifar.py\", line 115, in __getitem__\n",
      "    img = Image.fromarray(img)\n",
      "  File \"/usr/local/pace-apps/manual/packages/pytorch/1.12.0/lib/python3.9/site-packages/PIL/Image.py\", line 2970, in fromarray\n",
      "    obj = obj.tobytes()\n",
      "KeyboardInterrupt\n"
     ]
    }
   ],
   "source": [
    "!python main.py --gpu-id 0 --model-name vgg11_nobias --n-epochs 10 --save-result-file sample.csv \\\n",
    "--sweep-name exp_sample --correction --ground-metric euclidean --weight-stats \\\n",
    "--geom-ensemble-type wts --ground-metric-normalize none --sweep-id 90 \\\n",
    "--ckpt-type best --dataset Cifar10 --ground-metric-eff --recheck-cifar --activation-seed {activation_seed} \\\n",
    "--prelu-acts --past-correction --not-squared --exact\\\n",
    "--learning-rate 0.0001\\\n",
    "--momentum 0.5\\\n",
    "--batch-size-train 256 --experiment-dir {experiment_dir}\\\n",
    "--prunint-rate {prunint_rate}\\\n",
    "--run-mode {run_mode}\\\n",
    "--to-download\\\n",
    "--load-model-dir {load_model_dir}\\\n",
    "--retrain 30\\\n",
    "--prune_times 3\n",
    "# 256 0.0001 86.67"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "01c8cb70",
   "metadata": {},
   "outputs": [],
   "source": [
    "#剪枝实验3:prune times without finetune\n",
    "#86.67\n",
    "activation_seed=21\n",
    "experiment_dir=\"experiment0.5\"\n",
    "prunint_rate=0.1\n",
    "run_mode='prune_times'\n",
    "load_model_dir='./cifar_models'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c572d888",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------- Setting up parameters -------\n",
      "dumping parameters at  /storage/coda1/p-jli3175/0/mwu385/OTfusion_pruning_final/otfusion/exp_sample/configurations\n",
      "The parameters are: \n",
      " Namespace(experiment_dir='experiment0.5', prunint_rate=0.1, run_mode='prune_times', load_model_dir='./cifar_models', prune_times=3, finetunetimes_benchmark=0, n_epochs=10, batch_size_train=256, batch_size_test=1000, learning_rate=3e-05, momentum=0.5, log_interval=100, to_download=True, disable_bias=True, dataset='Cifar10', num_models=2, model_name='vgg11_nobias', config_file=None, config_dir='/storage/coda1/p-jli3175/0/mwu385/OTfusion_pruning_final/otfusion/exp_sample/configurations', num_hidden_nodes=400, num_hidden_nodes1=400, num_hidden_nodes2=200, num_hidden_nodes3=100, num_hidden_nodes4=50, sweep_id=90, gpu_id=0, skip_last_layer=False, skip_last_layer_type='average', debug=False, cifar_style_data=False, activation_histograms=False, act_num_samples=100, softmax_temperature=1, activation_mode=None, options_type='generic', deprecated=None, save_result_file='sample.csv', sweep_name='exp_sample', reg=0.01, reg_m=0.001, ground_metric='euclidean', ground_metric_normalize='none', not_squared=True, clip_gm=False, clip_min=0, clip_max=5, tmap_stats=False, ensemble_step=0.5, ground_metric_eff=True, retrain=30, retrain_lr_decay=-1, retrain_lr_decay_factor=None, retrain_lr_decay_epochs=None, retrain_avg_only=False, retrain_geometric_only=False, load_models='', ckpt_type='best', recheck_cifar=True, recheck_acc=False, eval_aligned=False, enable_dropout=False, dump_model=False, dump_final_models=False, correction=True, activation_seed=21, weight_stats=True, sinkhorn_type='normal', geom_ensemble_type='wts', act_bug=False, standardize_acts=False, transform_acts=False, center_acts=False, prelu_acts=True, pool_acts=False, pool_relu=False, normalize_acts=False, normalize_wts=False, gromov=False, gromov_loss='square_loss', tensorboard_root='./tensorboard', tensorboard=False, same_model=-1, dist_normalize=False, update_acts=False, past_correction=True, partial_reshape=False, choice='0 2 4 6 8', diff_init=False, partition_type='labels', personal_class_idx=9, partition_dataloader=-1, personal_split_frac=0.1, exact=True, skip_personal_idx=False, prediction_wts=False, width_ratio=1, proper_marginals=False, retrain_seed=-1, no_random_trainloaders=False, reinit_trainloaders=False, second_model_name=None, print_distances=False, deterministic=False, skip_retrain=-1, importance=None, unbalanced=False, temperature=20, alpha=0.7, dist_epochs=60, handle_skips=False, timestamp='2023-09-19_01-02-00_569149', rootdir='/storage/coda1/p-jli3175/0/mwu385/OTfusion_pruning_final/otfusion/exp_sample', baseroot='/storage/coda1/p-jli3175/0/mwu385/OTfusion_pruning_final/otfusion', result_dir='/storage/coda1/p-jli3175/0/mwu385/OTfusion_pruning_final/otfusion/exp_sample/results', exp_name='exp_2023-09-19_01-02-00_569149', csv_dir='/storage/coda1/p-jli3175/0/mwu385/OTfusion_pruning_final/otfusion/exp_sample/csv')\n",
      "refactored get_config\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "in _make_layers [Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), AvgPool2d(kernel_size=1, stride=1, padding=0)]\n",
      "Relu Inplace is  False\n",
      "model parameters are \n",
      " [torch.Size([64, 3, 3, 3]), torch.Size([128, 64, 3, 3]), torch.Size([256, 128, 3, 3]), torch.Size([256, 256, 3, 3]), torch.Size([512, 256, 3, 3]), torch.Size([512, 512, 3, 3]), torch.Size([512, 512, 3, 3]), torch.Size([512, 512, 3, 3]), torch.Size([10, 512])]\n",
      "in _make_layers [Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), AvgPool2d(kernel_size=1, stride=1, padding=0)]\n",
      "Relu Inplace is  False\n",
      "model parameters are \n",
      " [torch.Size([64, 3, 3, 3]), torch.Size([128, 64, 3, 3]), torch.Size([256, 128, 3, 3]), torch.Size([256, 256, 3, 3]), torch.Size([512, 256, 3, 3]), torch.Size([512, 512, 3, 3]), torch.Size([512, 512, 3, 3]), torch.Size([512, 512, 3, 3]), torch.Size([10, 512])]\n",
      "in _make_layers [Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), AvgPool2d(kernel_size=1, stride=1, padding=0)]\n",
      "Relu Inplace is  False\n",
      "model parameters are \n",
      " [torch.Size([64, 3, 3, 3]), torch.Size([128, 64, 3, 3]), torch.Size([256, 128, 3, 3]), torch.Size([256, 256, 3, 3]), torch.Size([512, 256, 3, 3]), torch.Size([512, 512, 3, 3]), torch.Size([512, 512, 3, 3]), torch.Size([512, 512, 3, 3]), torch.Size([10, 512])]\n",
      "in _make_layers [Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), AvgPool2d(kernel_size=1, stride=1, padding=0)]\n",
      "Relu Inplace is  False\n",
      "model parameters are \n",
      " [torch.Size([64, 3, 3, 3]), torch.Size([128, 64, 3, 3]), torch.Size([256, 128, 3, 3]), torch.Size([256, 256, 3, 3]), torch.Size([512, 256, 3, 3]), torch.Size([512, 512, 3, 3]), torch.Size([512, 512, 3, 3]), torch.Size([512, 512, 3, 3]), torch.Size([10, 512])]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "enter prune_times, the times is 3,ratio:0.03333333333333333\n",
      "Train Epoch: 1 [0/50000 (0%)]\tLoss: 0.101789\n",
      "Train Epoch: 1 [25600/50000 (51%)]\tLoss: 0.097934\n",
      "\n",
      "--------- Testing in global mode ---------\n",
      "size of test_loader dataset:  10000\n",
      "\n",
      "Test set: Avg. loss: 0.0003, Accuracy: 9017/10000 (90%)\n",
      "\n",
      "Train Epoch: 2 [0/50000 (0%)]\tLoss: 0.075346\n",
      "Train Epoch: 2 [25600/50000 (51%)]\tLoss: 0.095787\n",
      "\n",
      "--------- Testing in global mode ---------\n",
      "size of test_loader dataset:  10000\n",
      "\n",
      "Test set: Avg. loss: 0.0003, Accuracy: 9022/10000 (90%)\n",
      "\n",
      "Train Epoch: 3 [0/50000 (0%)]\tLoss: 0.071288\n",
      "Train Epoch: 3 [25600/50000 (51%)]\tLoss: 0.067986\n",
      "\n",
      "--------- Testing in global mode ---------\n",
      "size of test_loader dataset:  10000\n",
      "\n",
      "Test set: Avg. loss: 0.0003, Accuracy: 9027/10000 (90%)\n",
      "\n",
      "Train Epoch: 4 [0/50000 (0%)]\tLoss: 0.078733\n",
      "Train Epoch: 4 [25600/50000 (51%)]\tLoss: 0.065081\n",
      "\n",
      "--------- Testing in global mode ---------\n",
      "size of test_loader dataset:  10000\n",
      "\n",
      "Test set: Avg. loss: 0.0003, Accuracy: 9030/10000 (90%)\n",
      "\n",
      "Train Epoch: 5 [0/50000 (0%)]\tLoss: 0.075530\n",
      "Train Epoch: 5 [25600/50000 (51%)]\tLoss: 0.083298\n",
      "\n",
      "--------- Testing in global mode ---------\n",
      "size of test_loader dataset:  10000\n",
      "\n",
      "Test set: Avg. loss: 0.0003, Accuracy: 9035/10000 (90%)\n",
      "\n",
      "Train Epoch: 6 [0/50000 (0%)]\tLoss: 0.072795\n",
      "Train Epoch: 6 [25600/50000 (51%)]\tLoss: 0.073299\n",
      "\n",
      "--------- Testing in global mode ---------\n",
      "size of test_loader dataset:  10000\n",
      "\n",
      "Test set: Avg. loss: 0.0003, Accuracy: 9039/10000 (90%)\n",
      "\n",
      "Train Epoch: 7 [0/50000 (0%)]\tLoss: 0.085583\n",
      "Train Epoch: 7 [25600/50000 (51%)]\tLoss: 0.086762\n",
      "\n",
      "--------- Testing in global mode ---------\n",
      "size of test_loader dataset:  10000\n",
      "\n",
      "Test set: Avg. loss: 0.0003, Accuracy: 9047/10000 (90%)\n",
      "\n",
      "Train Epoch: 8 [0/50000 (0%)]\tLoss: 0.068078\n",
      "Train Epoch: 8 [25600/50000 (51%)]\tLoss: 0.085875\n",
      "\n",
      "--------- Testing in global mode ---------\n",
      "size of test_loader dataset:  10000\n",
      "\n",
      "Test set: Avg. loss: 0.0003, Accuracy: 9044/10000 (90%)\n",
      "\n",
      "Train Epoch: 9 [0/50000 (0%)]\tLoss: 0.073866\n",
      "Train Epoch: 9 [25600/50000 (51%)]\tLoss: 0.078427\n",
      "\n",
      "--------- Testing in global mode ---------\n",
      "size of test_loader dataset:  10000\n",
      "\n",
      "Test set: Avg. loss: 0.0003, Accuracy: 9050/10000 (90%)\n",
      "\n",
      "Train Epoch: 10 [0/50000 (0%)]\tLoss: 0.075604\n",
      "Train Epoch: 10 [25600/50000 (51%)]\tLoss: 0.076298\n",
      "\n",
      "--------- Testing in global mode ---------\n",
      "size of test_loader dataset:  10000\n",
      "\n",
      "Test set: Avg. loss: 0.0003, Accuracy: 9051/10000 (91%)\n",
      "\n",
      "Train Epoch: 1 [0/50000 (0%)]\tLoss: 0.098142\n",
      "Train Epoch: 1 [25600/50000 (51%)]\tLoss: 0.081019\n",
      "\n",
      "--------- Testing in global mode ---------\n",
      "size of test_loader dataset:  10000\n",
      "\n",
      "Test set: Avg. loss: 0.0003, Accuracy: 9060/10000 (91%)\n",
      "\n",
      "Train Epoch: 2 [0/50000 (0%)]\tLoss: 0.073255\n",
      "Train Epoch: 2 [25600/50000 (51%)]\tLoss: 0.103718\n",
      "\n",
      "--------- Testing in global mode ---------\n",
      "size of test_loader dataset:  10000\n",
      "\n",
      "Test set: Avg. loss: 0.0003, Accuracy: 9067/10000 (91%)\n",
      "\n",
      "Train Epoch: 3 [0/50000 (0%)]\tLoss: 0.083000\n",
      "Train Epoch: 3 [25600/50000 (51%)]\tLoss: 0.083856\n",
      "\n",
      "--------- Testing in global mode ---------\n",
      "size of test_loader dataset:  10000\n",
      "\n",
      "Test set: Avg. loss: 0.0003, Accuracy: 9062/10000 (91%)\n",
      "\n",
      "Train Epoch: 4 [0/50000 (0%)]\tLoss: 0.098531\n",
      "Train Epoch: 4 [25600/50000 (51%)]\tLoss: 0.079837\n",
      "\n",
      "--------- Testing in global mode ---------\n",
      "size of test_loader dataset:  10000\n",
      "\n",
      "Test set: Avg. loss: 0.0003, Accuracy: 9067/10000 (91%)\n",
      "\n",
      "Train Epoch: 5 [0/50000 (0%)]\tLoss: 0.077823\n",
      "Train Epoch: 5 [25600/50000 (51%)]\tLoss: 0.107119\n",
      "\n",
      "--------- Testing in global mode ---------\n",
      "size of test_loader dataset:  10000\n",
      "\n",
      "Test set: Avg. loss: 0.0003, Accuracy: 9067/10000 (91%)\n",
      "\n",
      "Train Epoch: 6 [0/50000 (0%)]\tLoss: 0.076464\n",
      "Train Epoch: 6 [25600/50000 (51%)]\tLoss: 0.073253\n",
      "\n",
      "--------- Testing in global mode ---------\n",
      "size of test_loader dataset:  10000\n",
      "\n",
      "Test set: Avg. loss: 0.0003, Accuracy: 9071/10000 (91%)\n",
      "\n",
      "Train Epoch: 7 [0/50000 (0%)]\tLoss: 0.078044\n",
      "Train Epoch: 7 [25600/50000 (51%)]\tLoss: 0.065795\n",
      "\n",
      "--------- Testing in global mode ---------\n",
      "size of test_loader dataset:  10000\n",
      "\n",
      "Test set: Avg. loss: 0.0003, Accuracy: 9071/10000 (91%)\n",
      "\n",
      "Train Epoch: 8 [0/50000 (0%)]\tLoss: 0.065322\n",
      "Train Epoch: 8 [25600/50000 (51%)]\tLoss: 0.079977\n",
      "\n",
      "--------- Testing in global mode ---------\n",
      "size of test_loader dataset:  10000\n",
      "\n",
      "Test set: Avg. loss: 0.0003, Accuracy: 9072/10000 (91%)\n",
      "\n",
      "Train Epoch: 9 [0/50000 (0%)]\tLoss: 0.076664\n",
      "Train Epoch: 9 [25600/50000 (51%)]\tLoss: 0.075497\n",
      "\n",
      "--------- Testing in global mode ---------\n",
      "size of test_loader dataset:  10000\n",
      "\n",
      "Test set: Avg. loss: 0.0003, Accuracy: 9070/10000 (91%)\n",
      "\n",
      "Train Epoch: 10 [0/50000 (0%)]\tLoss: 0.073971\n",
      "Train Epoch: 10 [25600/50000 (51%)]\tLoss: 0.069002\n",
      "\n",
      "--------- Testing in global mode ---------\n",
      "size of test_loader dataset:  10000\n",
      "\n",
      "Test set: Avg. loss: 0.0003, Accuracy: 9068/10000 (91%)\n",
      "\n",
      "in times 0, now cumm_ratio 0.03333333333333333,accuracies:[90.51, 90.68],losses:[0.00030333673655986786, 0.00030192571878433227]\n",
      "Train Epoch: 1 [0/50000 (0%)]\tLoss: 0.052920\n",
      "Train Epoch: 1 [25600/50000 (51%)]\tLoss: 0.072113\n",
      "\n",
      "--------- Testing in global mode ---------\n",
      "size of test_loader dataset:  10000\n",
      "\n",
      "Test set: Avg. loss: 0.0003, Accuracy: 9054/10000 (91%)\n",
      "\n",
      "Train Epoch: 2 [0/50000 (0%)]\tLoss: 0.083419\n",
      "Train Epoch: 2 [25600/50000 (51%)]\tLoss: 0.063678\n",
      "\n",
      "--------- Testing in global mode ---------\n",
      "size of test_loader dataset:  10000\n",
      "\n",
      "Test set: Avg. loss: 0.0003, Accuracy: 9055/10000 (91%)\n",
      "\n",
      "Train Epoch: 3 [0/50000 (0%)]\tLoss: 0.064513\n",
      "Train Epoch: 3 [25600/50000 (51%)]\tLoss: 0.070707\n",
      "\n",
      "--------- Testing in global mode ---------\n",
      "size of test_loader dataset:  10000\n",
      "\n",
      "Test set: Avg. loss: 0.0003, Accuracy: 9047/10000 (90%)\n",
      "\n",
      "Train Epoch: 4 [0/50000 (0%)]\tLoss: 0.051898\n",
      "Train Epoch: 4 [25600/50000 (51%)]\tLoss: 0.064160\n",
      "\n",
      "--------- Testing in global mode ---------\n",
      "size of test_loader dataset:  10000\n",
      "\n",
      "Test set: Avg. loss: 0.0003, Accuracy: 9048/10000 (90%)\n",
      "\n",
      "Train Epoch: 5 [0/50000 (0%)]\tLoss: 0.052950\n",
      "Train Epoch: 5 [25600/50000 (51%)]\tLoss: 0.058834\n",
      "\n",
      "--------- Testing in global mode ---------\n",
      "size of test_loader dataset:  10000\n",
      "\n",
      "Test set: Avg. loss: 0.0003, Accuracy: 9047/10000 (90%)\n",
      "\n",
      "Train Epoch: 6 [0/50000 (0%)]\tLoss: 0.069279\n",
      "Train Epoch: 6 [25600/50000 (51%)]\tLoss: 0.054011\n",
      "\n",
      "--------- Testing in global mode ---------\n",
      "size of test_loader dataset:  10000\n",
      "\n",
      "Test set: Avg. loss: 0.0003, Accuracy: 9051/10000 (91%)\n",
      "\n",
      "Train Epoch: 7 [0/50000 (0%)]\tLoss: 0.048257\n",
      "Train Epoch: 7 [25600/50000 (51%)]\tLoss: 0.071134\n",
      "\n",
      "--------- Testing in global mode ---------\n",
      "size of test_loader dataset:  10000\n",
      "\n",
      "Test set: Avg. loss: 0.0003, Accuracy: 9045/10000 (90%)\n",
      "\n",
      "Train Epoch: 8 [0/50000 (0%)]\tLoss: 0.063225\n",
      "Train Epoch: 8 [25600/50000 (51%)]\tLoss: 0.056818\n",
      "\n",
      "--------- Testing in global mode ---------\n",
      "size of test_loader dataset:  10000\n",
      "\n",
      "Test set: Avg. loss: 0.0003, Accuracy: 9044/10000 (90%)\n",
      "\n",
      "Train Epoch: 9 [0/50000 (0%)]\tLoss: 0.064223\n",
      "Train Epoch: 9 [25600/50000 (51%)]\tLoss: 0.056348\n",
      "\n",
      "--------- Testing in global mode ---------\n",
      "size of test_loader dataset:  10000\n",
      "\n",
      "Test set: Avg. loss: 0.0003, Accuracy: 9045/10000 (90%)\n",
      "\n",
      "Train Epoch: 10 [0/50000 (0%)]\tLoss: 0.071974\n",
      "Train Epoch: 10 [25600/50000 (51%)]\tLoss: 0.055513\n",
      "\n",
      "--------- Testing in global mode ---------\n",
      "size of test_loader dataset:  10000\n",
      "\n",
      "Test set: Avg. loss: 0.0003, Accuracy: 9045/10000 (90%)\n",
      "\n",
      "Train Epoch: 1 [0/50000 (0%)]\tLoss: 0.064554\n",
      "Train Epoch: 1 [25600/50000 (51%)]\tLoss: 0.061221\n",
      "\n",
      "--------- Testing in global mode ---------\n",
      "size of test_loader dataset:  10000\n",
      "\n",
      "Test set: Avg. loss: 0.0003, Accuracy: 9068/10000 (91%)\n",
      "\n",
      "Train Epoch: 2 [0/50000 (0%)]\tLoss: 0.070662\n",
      "Train Epoch: 2 [25600/50000 (51%)]\tLoss: 0.062435\n",
      "\n",
      "--------- Testing in global mode ---------\n",
      "size of test_loader dataset:  10000\n",
      "\n",
      "Test set: Avg. loss: 0.0003, Accuracy: 9066/10000 (91%)\n",
      "\n",
      "Train Epoch: 3 [0/50000 (0%)]\tLoss: 0.070993\n",
      "Train Epoch: 3 [25600/50000 (51%)]\tLoss: 0.066137\n",
      "\n",
      "--------- Testing in global mode ---------\n",
      "size of test_loader dataset:  10000\n",
      "\n",
      "Test set: Avg. loss: 0.0003, Accuracy: 9063/10000 (91%)\n",
      "\n",
      "Train Epoch: 4 [0/50000 (0%)]\tLoss: 0.076849\n",
      "Train Epoch: 4 [25600/50000 (51%)]\tLoss: 0.059218\n",
      "\n",
      "--------- Testing in global mode ---------\n",
      "size of test_loader dataset:  10000\n",
      "\n",
      "Test set: Avg. loss: 0.0003, Accuracy: 9062/10000 (91%)\n",
      "\n",
      "Train Epoch: 5 [0/50000 (0%)]\tLoss: 0.060350\n",
      "Train Epoch: 5 [25600/50000 (51%)]\tLoss: 0.077067\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--------- Testing in global mode ---------\n",
      "size of test_loader dataset:  10000\n",
      "\n",
      "Test set: Avg. loss: 0.0003, Accuracy: 9061/10000 (91%)\n",
      "\n",
      "Train Epoch: 6 [0/50000 (0%)]\tLoss: 0.065222\n",
      "Train Epoch: 6 [25600/50000 (51%)]\tLoss: 0.054083\n",
      "\n",
      "--------- Testing in global mode ---------\n",
      "size of test_loader dataset:  10000\n",
      "\n",
      "Test set: Avg. loss: 0.0003, Accuracy: 9060/10000 (91%)\n",
      "\n",
      "Train Epoch: 7 [0/50000 (0%)]\tLoss: 0.045974\n",
      "Train Epoch: 7 [25600/50000 (51%)]\tLoss: 0.057825\n",
      "\n",
      "--------- Testing in global mode ---------\n",
      "size of test_loader dataset:  10000\n",
      "\n",
      "Test set: Avg. loss: 0.0003, Accuracy: 9058/10000 (91%)\n",
      "\n",
      "Train Epoch: 8 [0/50000 (0%)]\tLoss: 0.061643\n",
      "Train Epoch: 8 [25600/50000 (51%)]\tLoss: 0.065359\n",
      "\n",
      "--------- Testing in global mode ---------\n",
      "size of test_loader dataset:  10000\n",
      "\n",
      "Test set: Avg. loss: 0.0003, Accuracy: 9054/10000 (91%)\n",
      "\n",
      "Train Epoch: 9 [0/50000 (0%)]\tLoss: 0.074291\n",
      "Train Epoch: 9 [25600/50000 (51%)]\tLoss: 0.061033\n",
      "\n",
      "--------- Testing in global mode ---------\n",
      "size of test_loader dataset:  10000\n",
      "\n",
      "Test set: Avg. loss: 0.0003, Accuracy: 9053/10000 (91%)\n",
      "\n",
      "Train Epoch: 10 [0/50000 (0%)]\tLoss: 0.063593\n",
      "Train Epoch: 10 [25600/50000 (51%)]\tLoss: 0.070399\n",
      "\n",
      "--------- Testing in global mode ---------\n",
      "size of test_loader dataset:  10000\n",
      "\n",
      "Test set: Avg. loss: 0.0003, Accuracy: 9053/10000 (91%)\n",
      "\n",
      "in times 1, now cumm_ratio 0.06666666666666667,accuracies:[90.45, 90.53],losses:[0.00030001046359539034, 0.00029769405722618103]\n",
      "Train Epoch: 1 [0/50000 (0%)]\tLoss: 0.052005\n",
      "Train Epoch: 1 [25600/50000 (51%)]\tLoss: 0.063967\n",
      "\n",
      "--------- Testing in global mode ---------\n",
      "size of test_loader dataset:  10000\n",
      "\n",
      "Test set: Avg. loss: 0.0003, Accuracy: 9047/10000 (90%)\n",
      "\n",
      "Train Epoch: 2 [0/50000 (0%)]\tLoss: 0.054603\n",
      "Train Epoch: 2 [25600/50000 (51%)]\tLoss: 0.057529\n",
      "\n",
      "--------- Testing in global mode ---------\n",
      "size of test_loader dataset:  10000\n",
      "\n",
      "Test set: Avg. loss: 0.0003, Accuracy: 9049/10000 (90%)\n",
      "\n",
      "Train Epoch: 3 [0/50000 (0%)]\tLoss: 0.073644\n",
      "Train Epoch: 3 [25600/50000 (51%)]\tLoss: 0.057823\n",
      "\n",
      "--------- Testing in global mode ---------\n",
      "size of test_loader dataset:  10000\n",
      "\n",
      "Test set: Avg. loss: 0.0003, Accuracy: 9047/10000 (90%)\n",
      "\n",
      "Train Epoch: 4 [0/50000 (0%)]\tLoss: 0.063326\n",
      "Train Epoch: 4 [25600/50000 (51%)]\tLoss: 0.056720\n",
      "\n",
      "--------- Testing in global mode ---------\n",
      "size of test_loader dataset:  10000\n",
      "\n",
      "Test set: Avg. loss: 0.0003, Accuracy: 9047/10000 (90%)\n",
      "\n",
      "Train Epoch: 5 [0/50000 (0%)]\tLoss: 0.062407\n",
      "Train Epoch: 5 [25600/50000 (51%)]\tLoss: 0.054488\n",
      "\n",
      "--------- Testing in global mode ---------\n",
      "size of test_loader dataset:  10000\n",
      "\n",
      "Test set: Avg. loss: 0.0003, Accuracy: 9049/10000 (90%)\n",
      "\n",
      "Train Epoch: 6 [0/50000 (0%)]\tLoss: 0.061674\n",
      "Train Epoch: 6 [25600/50000 (51%)]\tLoss: 0.052896\n",
      "\n",
      "--------- Testing in global mode ---------\n",
      "size of test_loader dataset:  10000\n",
      "\n",
      "Test set: Avg. loss: 0.0003, Accuracy: 9050/10000 (90%)\n",
      "\n",
      "Train Epoch: 7 [0/50000 (0%)]\tLoss: 0.039441\n",
      "Train Epoch: 7 [25600/50000 (51%)]\tLoss: 0.055944\n",
      "\n",
      "--------- Testing in global mode ---------\n",
      "size of test_loader dataset:  10000\n",
      "\n",
      "Test set: Avg. loss: 0.0003, Accuracy: 9049/10000 (90%)\n",
      "\n",
      "Train Epoch: 8 [0/50000 (0%)]\tLoss: 0.061419\n",
      "Train Epoch: 8 [25600/50000 (51%)]\tLoss: 0.048129\n",
      "\n",
      "--------- Testing in global mode ---------\n",
      "size of test_loader dataset:  10000\n",
      "\n",
      "Test set: Avg. loss: 0.0003, Accuracy: 9046/10000 (90%)\n",
      "\n",
      "Train Epoch: 9 [0/50000 (0%)]\tLoss: 0.048719\n",
      "Train Epoch: 9 [25600/50000 (51%)]\tLoss: 0.050286\n",
      "\n",
      "--------- Testing in global mode ---------\n",
      "size of test_loader dataset:  10000\n",
      "\n",
      "Test set: Avg. loss: 0.0003, Accuracy: 9044/10000 (90%)\n",
      "\n",
      "Train Epoch: 10 [0/50000 (0%)]\tLoss: 0.050466\n",
      "Train Epoch: 10 [25600/50000 (51%)]\tLoss: 0.061096\n",
      "\n",
      "--------- Testing in global mode ---------\n",
      "size of test_loader dataset:  10000\n",
      "\n",
      "Test set: Avg. loss: 0.0003, Accuracy: 9045/10000 (90%)\n",
      "\n",
      "Train Epoch: 1 [0/50000 (0%)]\tLoss: 0.053399\n",
      "Train Epoch: 1 [25600/50000 (51%)]\tLoss: 0.057804\n",
      "\n",
      "--------- Testing in global mode ---------\n",
      "size of test_loader dataset:  10000\n",
      "\n",
      "Test set: Avg. loss: 0.0003, Accuracy: 9061/10000 (91%)\n",
      "\n",
      "Train Epoch: 2 [0/50000 (0%)]\tLoss: 0.047683\n",
      "Train Epoch: 2 [25600/50000 (51%)]\tLoss: 0.060200\n",
      "\n",
      "--------- Testing in global mode ---------\n",
      "size of test_loader dataset:  10000\n",
      "\n",
      "Test set: Avg. loss: 0.0003, Accuracy: 9060/10000 (91%)\n",
      "\n",
      "Train Epoch: 3 [0/50000 (0%)]\tLoss: 0.066995\n",
      "Train Epoch: 3 [25600/50000 (51%)]\tLoss: 0.054487\n",
      "\n",
      "--------- Testing in global mode ---------\n",
      "size of test_loader dataset:  10000\n",
      "\n",
      "Test set: Avg. loss: 0.0003, Accuracy: 9064/10000 (91%)\n",
      "\n",
      "Train Epoch: 4 [0/50000 (0%)]\tLoss: 0.046389\n",
      "Train Epoch: 4 [25600/50000 (51%)]\tLoss: 0.059033\n",
      "\n",
      "--------- Testing in global mode ---------\n",
      "size of test_loader dataset:  10000\n",
      "\n",
      "Test set: Avg. loss: 0.0003, Accuracy: 9063/10000 (91%)\n",
      "\n",
      "Train Epoch: 5 [0/50000 (0%)]\tLoss: 0.063298\n",
      "Train Epoch: 5 [25600/50000 (51%)]\tLoss: 0.051620\n",
      "\n",
      "--------- Testing in global mode ---------\n",
      "size of test_loader dataset:  10000\n",
      "\n",
      "Test set: Avg. loss: 0.0003, Accuracy: 9061/10000 (91%)\n",
      "\n",
      "Train Epoch: 6 [0/50000 (0%)]\tLoss: 0.050110\n",
      "Train Epoch: 6 [25600/50000 (51%)]\tLoss: 0.057379\n",
      "\n",
      "--------- Testing in global mode ---------\n",
      "size of test_loader dataset:  10000\n",
      "\n",
      "Test set: Avg. loss: 0.0003, Accuracy: 9063/10000 (91%)\n",
      "\n",
      "Train Epoch: 7 [0/50000 (0%)]\tLoss: 0.061627\n",
      "Train Epoch: 7 [25600/50000 (51%)]\tLoss: 0.066013\n",
      "\n",
      "--------- Testing in global mode ---------\n",
      "size of test_loader dataset:  10000\n",
      "\n",
      "Test set: Avg. loss: 0.0003, Accuracy: 9060/10000 (91%)\n",
      "\n",
      "Train Epoch: 8 [0/50000 (0%)]\tLoss: 0.056454\n",
      "Train Epoch: 8 [25600/50000 (51%)]\tLoss: 0.055239\n",
      "\n",
      "--------- Testing in global mode ---------\n",
      "size of test_loader dataset:  10000\n",
      "\n",
      "Test set: Avg. loss: 0.0003, Accuracy: 9060/10000 (91%)\n",
      "\n",
      "Train Epoch: 9 [0/50000 (0%)]\tLoss: 0.047419\n",
      "Train Epoch: 9 [25600/50000 (51%)]\tLoss: 0.063295\n",
      "\n",
      "--------- Testing in global mode ---------\n",
      "size of test_loader dataset:  10000\n",
      "\n",
      "Test set: Avg. loss: 0.0003, Accuracy: 9058/10000 (91%)\n",
      "\n",
      "Train Epoch: 10 [0/50000 (0%)]\tLoss: 0.049474\n",
      "Train Epoch: 10 [25600/50000 (51%)]\tLoss: 0.058109\n",
      "\n",
      "--------- Testing in global mode ---------\n",
      "size of test_loader dataset:  10000\n",
      "\n",
      "Test set: Avg. loss: 0.0003, Accuracy: 9057/10000 (91%)\n",
      "\n",
      "in times 2, now cumm_ratio 0.1,accuracies:[90.45, 90.57],losses:[0.00029936890304088594, 0.00029634605497121813]\n",
      "===============================================\n",
      "===============================================\n",
      "before Ensemble,acc: [90.45, 90.57] loss [0.00029936890304088594, 0.00029634605497121813]\n",
      "===============================================\n",
      "===============================================\n",
      "check zero rate for model 0\n",
      "odict_keys(['features.0.weight', 'features.3.weight', 'features.6.weight', 'features.8.weight', 'features.11.weight', 'features.13.weight', 'features.16.weight', 'features.18.weight', 'classifier.weight'])\n",
      "the ratio of zero for features.0.weight\n",
      "tensor(0.1001, device='cuda:0')\n",
      "the ratio of zero for features.3.weight\n",
      "tensor(0.1000, device='cuda:0')\n",
      "the ratio of zero for features.6.weight\n",
      "tensor(0.1000, device='cuda:0')\n",
      "the ratio of zero for features.8.weight\n",
      "tensor(0.1000, device='cuda:0')\n",
      "the ratio of zero for features.11.weight\n",
      "tensor(0.1000, device='cuda:0')\n",
      "the ratio of zero for features.13.weight\n",
      "tensor(0.1000, device='cuda:0')\n",
      "the ratio of zero for features.16.weight\n",
      "tensor(0.1000, device='cuda:0')\n",
      "the ratio of zero for features.18.weight\n",
      "tensor(0.1000, device='cuda:0')\n",
      "the ratio of zero for classifier.weight\n",
      "tensor(0.1000, device='cuda:0')\n",
      "check zero rate for model 1\n",
      "odict_keys(['features.0.weight', 'features.3.weight', 'features.6.weight', 'features.8.weight', 'features.11.weight', 'features.13.weight', 'features.16.weight', 'features.18.weight', 'classifier.weight'])\n",
      "the ratio of zero for features.0.weight\n",
      "tensor(0.1001, device='cuda:0')\n",
      "the ratio of zero for features.3.weight\n",
      "tensor(0.1000, device='cuda:0')\n",
      "the ratio of zero for features.6.weight\n",
      "tensor(0.1000, device='cuda:0')\n",
      "the ratio of zero for features.8.weight\n",
      "tensor(0.1000, device='cuda:0')\n",
      "the ratio of zero for features.11.weight\n",
      "tensor(0.1000, device='cuda:0')\n",
      "the ratio of zero for features.13.weight\n",
      "tensor(0.1000, device='cuda:0')\n",
      "the ratio of zero for features.16.weight\n",
      "tensor(0.1000, device='cuda:0')\n",
      "the ratio of zero for features.18.weight\n",
      "tensor(0.1000, device='cuda:0')\n",
      "the ratio of zero for classifier.weight\n",
      "tensor(0.1000, device='cuda:0')\n",
      "layer features.0.weight has #params  1728\n",
      "layer features.3.weight has #params  73728\n",
      "layer features.6.weight has #params  294912\n",
      "layer features.8.weight has #params  589824\n",
      "layer features.11.weight has #params  1179648\n",
      "layer features.13.weight has #params  2359296\n",
      "layer features.16.weight has #params  2359296\n",
      "layer features.18.weight has #params  2359296\n",
      "layer classifier.weight has #params  5120\n",
      "Activation Timer start\n",
      "Activation Timer ends\n",
      "------- Geometric Ensembling -------\n",
      "Timer start\n",
      "Previous layer shape is  None\n",
      "Processing the coordinates to form ground_metric\n",
      "dont leave off the squaring of the ground metric\n",
      "returns a uniform measure of cardinality:  64\n",
      "returns a uniform measure of cardinality:  64\n",
      "the transport map is  tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')\n",
      "Ratio of trace to the matrix sum:  tensor(0.0469, device='cuda:0')\n",
      "Here, trace is 2.9999806880950928 and matrix sum is 63.99958801269531 \n",
      "this is past correction for weight mode\n",
      "Shape of aligned wt is  torch.Size([64, 3, 9])\n",
      "Shape of fc_layer0_weight_data is  torch.Size([64, 3, 9])\n",
      "Previous layer shape is  torch.Size([64, 3, 3, 3])\n",
      "shape of layer: model 0 torch.Size([128, 64, 9])\n",
      "shape of layer: model 1 torch.Size([128, 64, 9])\n",
      "shape of previous transport map torch.Size([64, 64])\n",
      "Processing the coordinates to form ground_metric\n",
      "dont leave off the squaring of the ground metric\n",
      "returns a uniform measure of cardinality:  128\n",
      "returns a uniform measure of cardinality:  128\n",
      "the transport map is  tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        ...,\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0078],\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],\n",
      "       device='cuda:0')\n",
      "Ratio of trace to the matrix sum:  tensor(0.0078, device='cuda:0')\n",
      "Here, trace is 0.9999872446060181 and matrix sum is 127.99836730957031 \n",
      "this is past correction for weight mode\n",
      "Shape of aligned wt is  torch.Size([128, 64, 9])\n",
      "Shape of fc_layer0_weight_data is  torch.Size([128, 64, 9])\n",
      "Previous layer shape is  torch.Size([128, 64, 3, 3])\n",
      "shape of layer: model 0 torch.Size([256, 128, 9])\n",
      "shape of layer: model 1 torch.Size([256, 128, 9])\n",
      "shape of previous transport map torch.Size([128, 128])\n",
      "Processing the coordinates to form ground_metric\n",
      "dont leave off the squaring of the ground metric\n",
      "returns a uniform measure of cardinality:  256\n",
      "returns a uniform measure of cardinality:  256\n",
      "the transport map is  tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')\n",
      "Ratio of trace to the matrix sum:  tensor(0., device='cuda:0')\n",
      "Here, trace is 0.0 and matrix sum is 255.99343872070312 \n",
      "this is past correction for weight mode\n",
      "Shape of aligned wt is  torch.Size([256, 128, 9])\n",
      "Shape of fc_layer0_weight_data is  torch.Size([256, 128, 9])\n",
      "Previous layer shape is  torch.Size([256, 128, 3, 3])\n",
      "shape of layer: model 0 torch.Size([256, 256, 9])\n",
      "shape of layer: model 1 torch.Size([256, 256, 9])\n",
      "shape of previous transport map torch.Size([256, 256])\n",
      "Processing the coordinates to form ground_metric\n",
      "dont leave off the squaring of the ground metric\n",
      "returns a uniform measure of cardinality:  256\n",
      "returns a uniform measure of cardinality:  256\n",
      "the transport map is  tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        ...,\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.0039, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],\n",
      "       device='cuda:0')\n",
      "Ratio of trace to the matrix sum:  tensor(0.0039, device='cuda:0')\n",
      "Here, trace is 0.9999743700027466 and matrix sum is 255.99343872070312 \n",
      "this is past correction for weight mode\n",
      "Shape of aligned wt is  torch.Size([256, 256, 9])\n",
      "Shape of fc_layer0_weight_data is  torch.Size([256, 256, 9])\n",
      "Previous layer shape is  torch.Size([256, 256, 3, 3])\n",
      "shape of layer: model 0 torch.Size([512, 256, 9])\n",
      "shape of layer: model 1 torch.Size([512, 256, 9])\n",
      "shape of previous transport map torch.Size([256, 256])\n",
      "Processing the coordinates to form ground_metric\n",
      "dont leave off the squaring of the ground metric\n",
      "returns a uniform measure of cardinality:  512\n",
      "returns a uniform measure of cardinality:  512\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the transport map is  tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')\n",
      "Ratio of trace to the matrix sum:  tensor(0.0020, device='cuda:0')\n",
      "Here, trace is 0.9999488592147827 and matrix sum is 511.97381591796875 \n",
      "this is past correction for weight mode\n",
      "Shape of aligned wt is  torch.Size([512, 256, 9])\n",
      "Shape of fc_layer0_weight_data is  torch.Size([512, 256, 9])\n",
      "Previous layer shape is  torch.Size([512, 256, 3, 3])\n",
      "shape of layer: model 0 torch.Size([512, 512, 9])\n",
      "shape of layer: model 1 torch.Size([512, 512, 9])\n",
      "shape of previous transport map torch.Size([512, 512])\n",
      "Processing the coordinates to form ground_metric\n",
      "dont leave off the squaring of the ground metric\n",
      "returns a uniform measure of cardinality:  512\n",
      "returns a uniform measure of cardinality:  512\n",
      "the transport map is  tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')\n",
      "Ratio of trace to the matrix sum:  tensor(0.0020, device='cuda:0')\n",
      "Here, trace is 0.9999488592147827 and matrix sum is 511.97381591796875 \n",
      "this is past correction for weight mode\n",
      "Shape of aligned wt is  torch.Size([512, 512, 9])\n",
      "Shape of fc_layer0_weight_data is  torch.Size([512, 512, 9])\n",
      "Previous layer shape is  torch.Size([512, 512, 3, 3])\n",
      "shape of layer: model 0 torch.Size([512, 512, 9])\n",
      "shape of layer: model 1 torch.Size([512, 512, 9])\n",
      "shape of previous transport map torch.Size([512, 512])\n",
      "Processing the coordinates to form ground_metric\n",
      "dont leave off the squaring of the ground metric\n",
      "returns a uniform measure of cardinality:  512\n",
      "returns a uniform measure of cardinality:  512\n",
      "the transport map is  tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')\n",
      "Ratio of trace to the matrix sum:  tensor(0.0039, device='cuda:0')\n",
      "Here, trace is 1.9998977184295654 and matrix sum is 511.97381591796875 \n",
      "this is past correction for weight mode\n",
      "Shape of aligned wt is  torch.Size([512, 512, 9])\n",
      "Shape of fc_layer0_weight_data is  torch.Size([512, 512, 9])\n",
      "Previous layer shape is  torch.Size([512, 512, 3, 3])\n",
      "shape of layer: model 0 torch.Size([512, 512, 9])\n",
      "shape of layer: model 1 torch.Size([512, 512, 9])\n",
      "shape of previous transport map torch.Size([512, 512])\n",
      "Processing the coordinates to form ground_metric\n",
      "dont leave off the squaring of the ground metric\n",
      "returns a uniform measure of cardinality:  512\n",
      "returns a uniform measure of cardinality:  512\n",
      "the transport map is  tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        [0.0020, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        ...,\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],\n",
      "       device='cuda:0')\n",
      "Ratio of trace to the matrix sum:  tensor(0., device='cuda:0')\n",
      "Here, trace is 0.0 and matrix sum is 511.97381591796875 \n",
      "this is past correction for weight mode\n",
      "Shape of aligned wt is  torch.Size([512, 512, 9])\n",
      "Shape of fc_layer0_weight_data is  torch.Size([512, 512, 9])\n",
      "Previous layer shape is  torch.Size([512, 512, 3, 3])\n",
      "shape of layer: model 0 torch.Size([10, 512])\n",
      "shape of layer: model 1 torch.Size([10, 512])\n",
      "shape of previous transport map torch.Size([512, 512])\n",
      "Processing the coordinates to form ground_metric\n",
      "dont leave off the squaring of the ground metric\n",
      "ground metric is  tensor([[1.4360, 2.5227, 2.7785, 2.8816, 2.7027, 2.9026, 2.7151, 2.6399, 2.5706,\n",
      "         2.6037],\n",
      "        [2.4859, 1.3311, 2.6891, 2.5852, 2.7621, 2.5368, 2.4300, 2.5396, 2.2962,\n",
      "         2.4210],\n",
      "        [2.7835, 2.6372, 1.3620, 2.9579, 2.8787, 2.8925, 2.7585, 2.8473, 2.7703,\n",
      "         2.7849],\n",
      "        [2.7570, 2.6959, 2.9170, 1.5578, 2.8636, 2.9587, 2.7421, 2.8029, 2.8244,\n",
      "         2.6636],\n",
      "        [2.8134, 2.5870, 2.8786, 2.8611, 1.5254, 2.7415, 2.7693, 2.6601, 2.7000,\n",
      "         2.7210],\n",
      "        [2.9143, 2.5322, 2.9595, 2.8316, 2.7317, 1.5473, 2.7934, 2.7398, 2.7886,\n",
      "         2.7039],\n",
      "        [2.7171, 2.4579, 2.6961, 2.9090, 2.6499, 2.8525, 1.3848, 2.6972, 2.5115,\n",
      "         2.5465],\n",
      "        [2.7299, 2.5424, 2.7691, 2.8069, 2.7583, 2.6580, 2.7216, 1.3906, 2.5781,\n",
      "         2.4946],\n",
      "        [2.6068, 2.4047, 2.7697, 2.7057, 2.6973, 2.8396, 2.5357, 2.6265, 1.2836,\n",
      "         2.3851],\n",
      "        [2.5941, 2.2617, 2.8003, 2.7398, 2.7138, 2.6849, 2.5644, 2.5463, 2.5224,\n",
      "         1.2974]], device='cuda:0', grad_fn=<PowBackward0>)\n",
      "returns a uniform measure of cardinality:  10\n",
      "returns a uniform measure of cardinality:  10\n",
      "the transport map is  tensor([[0.1000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0000, 0.1000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0000, 0.0000, 0.1000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.1000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.1000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.1000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.1000, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.1000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.1000,\n",
      "         0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.1000]], device='cuda:0')\n",
      "Ratio of trace to the matrix sum:  tensor(1., device='cuda:0')\n",
      "Here, trace is 9.999990463256836 and matrix sum is 9.999990463256836 \n",
      "this is past correction for weight mode\n",
      "Shape of aligned wt is  torch.Size([10, 512])\n",
      "Shape of fc_layer0_weight_data is  torch.Size([10, 512])\n",
      "using independent method\n",
      "in _make_layers [Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), AvgPool2d(kernel_size=1, stride=1, padding=0)]\n",
      "Relu Inplace is  False\n",
      "model parameters are \n",
      " [torch.Size([64, 3, 3, 3]), torch.Size([128, 64, 3, 3]), torch.Size([256, 128, 3, 3]), torch.Size([256, 256, 3, 3]), torch.Size([512, 256, 3, 3]), torch.Size([512, 512, 3, 3]), torch.Size([512, 512, 3, 3]), torch.Size([512, 512, 3, 3]), torch.Size([10, 512])]\n",
      "\n",
      "--------- Testing in global mode ---------\n",
      "size of test_loader dataset:  10000\n",
      "\n",
      "Test set: Avg. loss: 0.0023, Accuracy: 1000/10000 (10%)\n",
      "\n",
      "len of model parameters and avg aligned layers is  9 9\n",
      "len of model_state_dict is  9\n",
      "len of param_list is  9\n",
      "\n",
      "--------- Testing in global mode ---------\n",
      "size of test_loader dataset:  10000\n",
      "\n",
      "Test set: Avg. loss: 0.0014, Accuracy: 8640/10000 (86%)\n",
      "\n",
      "===============================================\n",
      "===============================================\n",
      "After Ensemble,acc: 86.4 loss 0.001376275134086609\n",
      "===============================================\n",
      "===============================================\n",
      "Timer ends\n",
      "Time taken for geometric ensembling is 2.8493440658785403 seconds\n",
      "------- Prediction based ensembling -------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Avg. loss: 0.0003, Accuracy: 9136/10000 (91%)\n",
      "\n",
      "------- Naive ensembling of weights -------\n",
      "[torch.Size([64, 3, 3, 3]), torch.Size([64, 3, 3, 3])]\n",
      "torch.Size([64, 3, 3, 3])\n",
      "[torch.Size([128, 64, 3, 3]), torch.Size([128, 64, 3, 3])]\n",
      "torch.Size([128, 64, 3, 3])\n",
      "[torch.Size([256, 128, 3, 3]), torch.Size([256, 128, 3, 3])]\n",
      "torch.Size([256, 128, 3, 3])\n",
      "[torch.Size([256, 256, 3, 3]), torch.Size([256, 256, 3, 3])]\n",
      "torch.Size([256, 256, 3, 3])\n",
      "[torch.Size([512, 256, 3, 3]), torch.Size([512, 256, 3, 3])]\n",
      "torch.Size([512, 256, 3, 3])\n",
      "[torch.Size([512, 512, 3, 3]), torch.Size([512, 512, 3, 3])]\n",
      "torch.Size([512, 512, 3, 3])\n",
      "[torch.Size([512, 512, 3, 3]), torch.Size([512, 512, 3, 3])]\n",
      "torch.Size([512, 512, 3, 3])\n",
      "[torch.Size([512, 512, 3, 3]), torch.Size([512, 512, 3, 3])]\n",
      "torch.Size([512, 512, 3, 3])\n",
      "[torch.Size([10, 512]), torch.Size([10, 512])]\n",
      "torch.Size([10, 512])\n",
      "in _make_layers [Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), AvgPool2d(kernel_size=1, stride=1, padding=0)]\n",
      "Relu Inplace is  False\n",
      "model parameters are \n",
      " [torch.Size([64, 3, 3, 3]), torch.Size([128, 64, 3, 3]), torch.Size([256, 128, 3, 3]), torch.Size([256, 256, 3, 3]), torch.Size([512, 256, 3, 3]), torch.Size([512, 512, 3, 3]), torch.Size([512, 512, 3, 3]), torch.Size([512, 512, 3, 3]), torch.Size([10, 512])]\n",
      "\n",
      "--------- Testing in global mode ---------\n",
      "size of test_loader dataset:  10000\n",
      "\n",
      "Test set: Avg. loss: 0.0023, Accuracy: 904/10000 (9%)\n",
      "\n",
      "\n",
      "--------- Testing in global mode ---------\n",
      "size of test_loader dataset:  10000\n",
      "\n",
      "Test set: Avg. loss: 0.0023, Accuracy: 1598/10000 (16%)\n",
      "\n",
      "-------- Retraining the models ---------\n",
      "Retraining model :  model_0\n",
      "num_epochs---------------+++))))))+++ 300\n",
      "num_epochs---------------+++))))))+++ 300\n",
      "lr is  0.05\n",
      "number of epochs would be  30\n",
      "num_epochs-------------- 30\n",
      "Epoch 000\n",
      "/usr/local/pace-apps/manual/packages/pytorch/1.12.0/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:131: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\"Detected call of `lr_scheduler.step()` before `optimizer.step()`. \"\n",
      "/usr/local/pace-apps/manual/packages/pytorch/1.12.0/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:156: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "accuracy: {'epoch': 0, 'value': 0.9043200000190734} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 0, 'value': 0.28662791665077203} ({'split': 'train'})\n",
      "accuracy: {'epoch': 0, 'value': 0.8419000387191773} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 0, 'value': 0.4896463543176652} ({'split': 'test'})\n",
      "Epoch 001\n",
      "accuracy: {'epoch': 1, 'value': 0.9278200000381471} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 1, 'value': 0.21255183053016646} ({'split': 'train'})\n",
      "accuracy: {'epoch': 1, 'value': 0.847700035572052} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 1, 'value': 0.4863329768180847} ({'split': 'test'})\n",
      "Epoch 002\n",
      "accuracy: {'epoch': 2, 'value': 0.95012} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 2, 'value': 0.14960968061447147} ({'split': 'train'})\n",
      "accuracy: {'epoch': 2, 'value': 0.8555000424385071} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 2, 'value': 0.4887148201465606} ({'split': 'test'})\n",
      "Epoch 003\n",
      "accuracy: {'epoch': 3, 'value': 0.9618800000572206} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 3, 'value': 0.11403765600681305} ({'split': 'train'})\n",
      "accuracy: {'epoch': 3, 'value': 0.8464000403881073} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 3, 'value': 0.5597568482160568} ({'split': 'test'})\n",
      "Epoch 004\n",
      "accuracy: {'epoch': 4, 'value': 0.9714000000381469} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 4, 'value': 0.08638631324052808} ({'split': 'train'})\n",
      "accuracy: {'epoch': 4, 'value': 0.8515000343322754} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 4, 'value': 0.5326467603445053} ({'split': 'test'})\n",
      "Epoch 005\n",
      "accuracy: {'epoch': 5, 'value': 0.97306} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 5, 'value': 0.08278465083122256} ({'split': 'train'})\n",
      "accuracy: {'epoch': 5, 'value': 0.8534000396728516} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 5, 'value': 0.5965647757053375} ({'split': 'test'})\n",
      "Epoch 006\n",
      "accuracy: {'epoch': 6, 'value': 0.9760000000190736} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 6, 'value': 0.07447590138435362} ({'split': 'train'})\n",
      "accuracy: {'epoch': 6, 'value': 0.8607000410556793} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 6, 'value': 0.5632681041955948} ({'split': 'test'})\n",
      "Epoch 007\n",
      "accuracy: {'epoch': 7, 'value': 0.9810399999809266} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 7, 'value': 0.05903908743381498} ({'split': 'train'})\n",
      "accuracy: {'epoch': 7, 'value': 0.8535000324249268} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 7, 'value': 0.5974332928657532} ({'split': 'test'})\n",
      "Epoch 008\n",
      "accuracy: {'epoch': 8, 'value': 0.9809600000190737} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 8, 'value': 0.061726386228799786} ({'split': 'train'})\n",
      "accuracy: {'epoch': 8, 'value': 0.8640000462532044} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 8, 'value': 0.5758823186159134} ({'split': 'test'})\n",
      "Epoch 009\n",
      "accuracy: {'epoch': 9, 'value': 0.9832800000000004} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 9, 'value': 0.05049589157581327} ({'split': 'train'})\n",
      "accuracy: {'epoch': 9, 'value': 0.8639000475406646} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 9, 'value': 0.6092848598957062} ({'split': 'test'})\n",
      "Epoch 010\n",
      "accuracy: {'epoch': 10, 'value': 0.9843000000190729} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 10, 'value': 0.05097979647517204} ({'split': 'train'})\n",
      "accuracy: {'epoch': 10, 'value': 0.8658000469207764} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 10, 'value': 0.5462636768817901} ({'split': 'test'})\n",
      "Epoch 011\n",
      "accuracy: {'epoch': 11, 'value': 0.9888800000000006} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 11, 'value': 0.03604799832910298} ({'split': 'train'})\n",
      "accuracy: {'epoch': 11, 'value': 0.8586000442504882} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 11, 'value': 0.5872252762317658} ({'split': 'test'})\n",
      "Epoch 012\n",
      "accuracy: {'epoch': 12, 'value': 0.9890199999999996} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 12, 'value': 0.03516694603681563} ({'split': 'train'})\n",
      "accuracy: {'epoch': 12, 'value': 0.8557000398635863} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 12, 'value': 0.560184970498085} ({'split': 'test'})\n",
      "Epoch 013\n",
      "accuracy: {'epoch': 13, 'value': 0.99086} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 13, 'value': 0.028215861023068417} ({'split': 'train'})\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: {'epoch': 13, 'value': 0.8698000371456147} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 13, 'value': 0.5605000674724577} ({'split': 'test'})\n",
      "Epoch 014\n",
      "accuracy: {'epoch': 14, 'value': 0.9908200000000003} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 14, 'value': 0.029483051697015757} ({'split': 'train'})\n",
      "accuracy: {'epoch': 14, 'value': 0.8589000344276428} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 14, 'value': 0.5612063497304917} ({'split': 'test'})\n",
      "Epoch 015\n",
      "accuracy: {'epoch': 15, 'value': 0.99006} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 15, 'value': 0.032207976761460304} ({'split': 'train'})\n",
      "accuracy: {'epoch': 15, 'value': 0.8549000442028046} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 15, 'value': 0.5866919338703156} ({'split': 'test'})\n",
      "Epoch 016\n",
      "accuracy: {'epoch': 16, 'value': 0.9899600000381469} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 16, 'value': 0.033467233130931834} ({'split': 'train'})\n",
      "accuracy: {'epoch': 16, 'value': 0.8552000403404236} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 16, 'value': 0.6333597064018249} ({'split': 'test'})\n",
      "Epoch 017\n",
      "accuracy: {'epoch': 17, 'value': 0.9907200000000006} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 17, 'value': 0.03139478129386902} ({'split': 'train'})\n",
      "accuracy: {'epoch': 17, 'value': 0.8555000364780425} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 17, 'value': 0.6165819108486176} ({'split': 'test'})\n",
      "Epoch 018\n",
      "accuracy: {'epoch': 18, 'value': 0.988440000019074} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 18, 'value': 0.038694537721872306} ({'split': 'train'})\n",
      "accuracy: {'epoch': 18, 'value': 0.8557000279426574} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 18, 'value': 0.575390538573265} ({'split': 'test'})\n",
      "Epoch 019\n",
      "accuracy: {'epoch': 19, 'value': 0.9884400000572204} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 19, 'value': 0.03620006866693499} ({'split': 'train'})\n",
      "accuracy: {'epoch': 19, 'value': 0.8536000430583954} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 19, 'value': 0.611642736196518} ({'split': 'test'})\n",
      "Epoch 020\n",
      "accuracy: {'epoch': 20, 'value': 0.9874200000190736} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 20, 'value': 0.04088353992581367} ({'split': 'train'})\n",
      "accuracy: {'epoch': 20, 'value': 0.8594000458717346} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 20, 'value': 0.5808997690677643} ({'split': 'test'})\n",
      "Epoch 021\n",
      "accuracy: {'epoch': 21, 'value': 0.9903400000190733} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 21, 'value': 0.03208442268013954} ({'split': 'train'})\n",
      "accuracy: {'epoch': 21, 'value': 0.8569000422954559} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 21, 'value': 0.6059961080551147} ({'split': 'test'})\n",
      "Epoch 022\n",
      "accuracy: {'epoch': 22, 'value': 0.9871600000000001} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 22, 'value': 0.041559741885066025} ({'split': 'train'})\n",
      "accuracy: {'epoch': 22, 'value': 0.8583000421524049} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 22, 'value': 0.5651170313358307} ({'split': 'test'})\n",
      "Epoch 023\n",
      "accuracy: {'epoch': 23, 'value': 0.9864400000190731} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 23, 'value': 0.04354582623839377} ({'split': 'train'})\n",
      "accuracy: {'epoch': 23, 'value': 0.8517000496387482} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 23, 'value': 0.5606928348541259} ({'split': 'test'})\n",
      "Epoch 024\n",
      "accuracy: {'epoch': 24, 'value': 0.9844200000381467} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 24, 'value': 0.050046374787092215} ({'split': 'train'})\n",
      "accuracy: {'epoch': 24, 'value': 0.8425000369548797} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 24, 'value': 0.5822696089744568} ({'split': 'test'})\n",
      "Epoch 025\n",
      "accuracy: {'epoch': 25, 'value': 0.9883000000190735} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 25, 'value': 0.03705352786540983} ({'split': 'train'})\n",
      "accuracy: {'epoch': 25, 'value': 0.8475000381469726} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 25, 'value': 0.6749101758003235} ({'split': 'test'})\n",
      "Epoch 026\n",
      "accuracy: {'epoch': 26, 'value': 0.9875800000572199} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 26, 'value': 0.04093526043415068} ({'split': 'train'})\n",
      "accuracy: {'epoch': 26, 'value': 0.8435000479221344} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 26, 'value': 0.6084402441978454} ({'split': 'test'})\n",
      "Epoch 027\n",
      "accuracy: {'epoch': 27, 'value': 0.98704} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 27, 'value': 0.04281531173527238} ({'split': 'train'})\n",
      "accuracy: {'epoch': 27, 'value': 0.8523000478744507} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 27, 'value': 0.6286467909812927} ({'split': 'test'})\n",
      "Epoch 028\n",
      "accuracy: {'epoch': 28, 'value': 0.9903400000000002} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 28, 'value': 0.0329429486903548} ({'split': 'train'})\n",
      "accuracy: {'epoch': 28, 'value': 0.8474000394344331} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 28, 'value': 0.6304514467716217} ({'split': 'test'})\n",
      "Epoch 029\n",
      "accuracy: {'epoch': 29, 'value': 0.9902000000190738} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 29, 'value': 0.032513213729858416} ({'split': 'train'})\n",
      "accuracy: {'epoch': 29, 'value': 0.8503000438213348} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 29, 'value': 0.6098313570022583} ({'split': 'test'})\n",
      "mean_test_accuracy <cifar_utils.accumulators.Mean object at 0x2aabe1576e80>\n",
      "QWERTY: Enter Cifar 2, acc 0.8698000371456147\n",
      "Retraining model :  model_1\n",
      "num_epochs---------------+++))))))+++ 30\n",
      "num_epochs---------------+++))))))+++ 30\n",
      "lr is  0.05\n",
      "number of epochs would be  30\n",
      "num_epochs-------------- 30\n",
      "Epoch 000\n",
      "accuracy: {'epoch': 0, 'value': 0.9010800000190733} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 0, 'value': 0.29517701090812687} ({'split': 'train'})\n",
      "accuracy: {'epoch': 0, 'value': 0.8279000341892242} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 0, 'value': 0.539367163181305} ({'split': 'test'})\n",
      "Epoch 001\n",
      "accuracy: {'epoch': 1, 'value': 0.9316600000572205} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 1, 'value': 0.2063474263763428} ({'split': 'train'})\n",
      "accuracy: {'epoch': 1, 'value': 0.844400030374527} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 1, 'value': 0.5214258372783661} ({'split': 'test'})\n",
      "Epoch 002\n",
      "accuracy: {'epoch': 2, 'value': 0.9480199999809265} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 2, 'value': 0.15566014263153072} ({'split': 'train'})\n",
      "accuracy: {'epoch': 2, 'value': 0.8503000378608704} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 2, 'value': 0.522276911139488} ({'split': 'test'})\n",
      "Epoch 003\n",
      "accuracy: {'epoch': 3, 'value': 0.9599000000381472} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 3, 'value': 0.12164848279953004} ({'split': 'train'})\n",
      "accuracy: {'epoch': 3, 'value': 0.8630000412464142} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 3, 'value': 0.4835171967744827} ({'split': 'test'})\n",
      "Epoch 004\n"
     ]
    }
   ],
   "source": [
    "!python main.py --gpu-id 0 --model-name vgg11_nobias --n-epochs 10 --save-result-file sample.csv \\\n",
    "--sweep-name exp_sample --correction --ground-metric euclidean --weight-stats \\\n",
    "--geom-ensemble-type wts --ground-metric-normalize none --sweep-id 90 \\\n",
    "--ckpt-type best --dataset Cifar10 --ground-metric-eff --recheck-cifar --activation-seed {activation_seed} \\\n",
    "--prelu-acts --past-correction --not-squared --exact\\\n",
    "--learning-rate 0.00003\\\n",
    "--momentum 0.5\\\n",
    "--batch-size-train 256 --experiment-dir {experiment_dir}\\\n",
    "--prunint-rate {prunint_rate}\\\n",
    "--run-mode {run_mode}\\\n",
    "--to-download\\\n",
    "--load-model-dir {load_model_dir}\\\n",
    "--retrain 30\\\n",
    "--prune_times 3\n",
    "# 256 0.00003 86.67"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "29a14370-8231-4282-b050-cda48f71b3f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#剪枝实验1:prune without finetune #85.02\n",
    "activation_seed=21\n",
    "experiment_dir=\"experiment0.5\"\n",
    "prunint_rate=0.1\n",
    "run_mode='prune'\n",
    "load_model_dir='./cifar_models'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a2488ee6-9aab-4165-9663-cf713df42dee",
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------- Setting up parameters -------\n",
      "dumping parameters at  /root/autodl-tmp/OTfusion_pruning/otfusion/exp_sample/configurations\n",
      "The parameters are: \n",
      " Namespace(act_bug=False, act_num_samples=100, activation_histograms=False, activation_mode=None, activation_seed=21, alpha=0.7, baseroot='/root/autodl-tmp/OTfusion_pruning/otfusion', batch_size_test=1000, batch_size_train=64, center_acts=False, choice='0 2 4 6 8', cifar_style_data=False, ckpt_type='best', clip_gm=False, clip_max=5, clip_min=0, config_dir='/root/autodl-tmp/OTfusion_pruning/otfusion/exp_sample/configurations', config_file=None, correction=True, csv_dir='/root/autodl-tmp/OTfusion_pruning/otfusion/exp_sample/csv', dataset='Cifar10', debug=False, deprecated=None, deterministic=False, diff_init=False, disable_bias=True, dist_epochs=60, dist_normalize=False, dump_final_models=False, dump_model=False, enable_dropout=False, ensemble_step=0.5, eval_aligned=False, exact=True, exp_name='exp_2023-09-17_12-02-14_094163', experiment_dir='experiment0.5', geom_ensemble_type='wts', gpu_id=0, gromov=False, gromov_loss='square_loss', ground_metric='euclidean', ground_metric_eff=True, ground_metric_normalize='none', handle_skips=False, importance=None, learning_rate=0.08, load_model_dir='./cifar_models', load_models='', log_interval=100, model_name='vgg11_nobias', momentum=0.5, n_epochs=90, no_random_trainloaders=False, normalize_acts=False, normalize_wts=False, not_squared=True, num_hidden_nodes=400, num_hidden_nodes1=400, num_hidden_nodes2=200, num_hidden_nodes3=100, num_hidden_nodes4=50, num_models=2, options_type='generic', partial_reshape=False, partition_dataloader=-1, partition_type='labels', past_correction=True, personal_class_idx=9, personal_split_frac=0.1, pool_acts=False, pool_relu=False, prediction_wts=False, prelu_acts=True, print_distances=False, proper_marginals=False, prunint_rate=0.1, recheck_acc=False, recheck_cifar=True, reg=0.01, reg_m=0.001, reinit_trainloaders=False, result_dir='/root/autodl-tmp/OTfusion_pruning/otfusion/exp_sample/results', retrain=30, retrain_avg_only=False, retrain_geometric_only=False, retrain_lr_decay=-1, retrain_lr_decay_epochs=None, retrain_lr_decay_factor=None, retrain_seed=-1, rootdir='/root/autodl-tmp/OTfusion_pruning/otfusion/exp_sample', run_mode='prune', same_model=-1, save_result_file='sample.csv', second_model_name=None, sinkhorn_type='normal', skip_last_layer=False, skip_last_layer_type='average', skip_personal_idx=False, skip_retrain=-1, softmax_temperature=1, standardize_acts=False, sweep_id=90, sweep_name='exp_sample', temperature=20, tensorboard=False, tensorboard_root='./tensorboard', timestamp='2023-09-17_12-02-14_094163', tmap_stats=False, to_download=True, transform_acts=False, unbalanced=False, update_acts=False, weight_stats=True, width_ratio=1)\n",
      "refactored get_config\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "in _make_layers [Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), AvgPool2d(kernel_size=1, stride=1, padding=0)]\n",
      "Relu Inplace is  False\n",
      "model parameters are \n",
      " [torch.Size([64, 3, 3, 3]), torch.Size([128, 64, 3, 3]), torch.Size([256, 128, 3, 3]), torch.Size([256, 256, 3, 3]), torch.Size([512, 256, 3, 3]), torch.Size([512, 512, 3, 3]), torch.Size([512, 512, 3, 3]), torch.Size([512, 512, 3, 3]), torch.Size([10, 512])]\n",
      "in _make_layers [Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), AvgPool2d(kernel_size=1, stride=1, padding=0)]\n",
      "Relu Inplace is  False\n",
      "model parameters are \n",
      " [torch.Size([64, 3, 3, 3]), torch.Size([128, 64, 3, 3]), torch.Size([256, 128, 3, 3]), torch.Size([256, 256, 3, 3]), torch.Size([512, 256, 3, 3]), torch.Size([512, 512, 3, 3]), torch.Size([512, 512, 3, 3]), torch.Size([512, 512, 3, 3]), torch.Size([10, 512])]\n",
      "in _make_layers [Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), AvgPool2d(kernel_size=1, stride=1, padding=0)]\n",
      "Relu Inplace is  False\n",
      "model parameters are \n",
      " [torch.Size([64, 3, 3, 3]), torch.Size([128, 64, 3, 3]), torch.Size([256, 128, 3, 3]), torch.Size([256, 256, 3, 3]), torch.Size([512, 256, 3, 3]), torch.Size([512, 512, 3, 3]), torch.Size([512, 512, 3, 3]), torch.Size([512, 512, 3, 3]), torch.Size([10, 512])]\n",
      "in _make_layers [Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), AvgPool2d(kernel_size=1, stride=1, padding=0)]\n",
      "Relu Inplace is  False\n",
      "model parameters are \n",
      " [torch.Size([64, 3, 3, 3]), torch.Size([128, 64, 3, 3]), torch.Size([256, 128, 3, 3]), torch.Size([256, 256, 3, 3]), torch.Size([512, 256, 3, 3]), torch.Size([512, 512, 3, 3]), torch.Size([512, 512, 3, 3]), torch.Size([512, 512, 3, 3]), torch.Size([10, 512])]\n",
      "\n",
      "--------- Testing in global mode ---------\n",
      "size of test_loader dataset:  10000\n",
      "\n",
      "Test set: Avg. loss: 0.0003, Accuracy: 9021/10000 (90%)\n",
      "\n",
      "\n",
      "--------- Testing in global mode ---------\n",
      "size of test_loader dataset:  10000\n",
      "\n",
      "Test set: Avg. loss: 0.0003, Accuracy: 9029/10000 (90%)\n",
      "\n",
      "check zero rate for model 0\n",
      "odict_keys(['features.0.weight', 'features.3.weight', 'features.6.weight', 'features.8.weight', 'features.11.weight', 'features.13.weight', 'features.16.weight', 'features.18.weight', 'classifier.weight'])\n",
      "the ratio of zero for features.0.weight\n",
      "tensor(0.1001, device='cuda:0')\n",
      "the ratio of zero for features.3.weight\n",
      "tensor(0.1000, device='cuda:0')\n",
      "the ratio of zero for features.6.weight\n",
      "tensor(0.1000, device='cuda:0')\n",
      "the ratio of zero for features.8.weight\n",
      "tensor(0.1000, device='cuda:0')\n",
      "the ratio of zero for features.11.weight\n",
      "tensor(0.1000, device='cuda:0')\n",
      "the ratio of zero for features.13.weight\n",
      "tensor(0.1000, device='cuda:0')\n",
      "the ratio of zero for features.16.weight\n",
      "tensor(0.1000, device='cuda:0')\n",
      "the ratio of zero for features.18.weight\n",
      "tensor(0.1000, device='cuda:0')\n",
      "the ratio of zero for classifier.weight\n",
      "tensor(0.1000, device='cuda:0')\n",
      "check zero rate for model 1\n",
      "odict_keys(['features.0.weight', 'features.3.weight', 'features.6.weight', 'features.8.weight', 'features.11.weight', 'features.13.weight', 'features.16.weight', 'features.18.weight', 'classifier.weight'])\n",
      "the ratio of zero for features.0.weight\n",
      "tensor(0.1001, device='cuda:0')\n",
      "the ratio of zero for features.3.weight\n",
      "tensor(0.1000, device='cuda:0')\n",
      "the ratio of zero for features.6.weight\n",
      "tensor(0.1000, device='cuda:0')\n",
      "the ratio of zero for features.8.weight\n",
      "tensor(0.1000, device='cuda:0')\n",
      "the ratio of zero for features.11.weight\n",
      "tensor(0.1000, device='cuda:0')\n",
      "the ratio of zero for features.13.weight\n",
      "tensor(0.1000, device='cuda:0')\n",
      "the ratio of zero for features.16.weight\n",
      "tensor(0.1000, device='cuda:0')\n",
      "the ratio of zero for features.18.weight\n",
      "tensor(0.1000, device='cuda:0')\n",
      "the ratio of zero for classifier.weight\n",
      "tensor(0.1000, device='cuda:0')\n",
      "layer features.0.weight has #params  1728\n",
      "layer features.3.weight has #params  73728\n",
      "layer features.6.weight has #params  294912\n",
      "layer features.8.weight has #params  589824\n",
      "layer features.11.weight has #params  1179648\n",
      "layer features.13.weight has #params  2359296\n",
      "layer features.16.weight has #params  2359296\n",
      "layer features.18.weight has #params  2359296\n",
      "layer classifier.weight has #params  5120\n",
      "Activation Timer start\n",
      "Activation Timer ends\n",
      "------- Geometric Ensembling -------\n",
      "Timer start\n",
      "odict_keys(['features.0.weight', 'features.3.weight', 'features.6.weight', 'features.8.weight', 'features.11.weight', 'features.13.weight', 'features.16.weight', 'features.18.weight', 'classifier.weight'])\n",
      "the ratio of zero for features.0.weight\n",
      "tensor(0.1001, device='cuda:0')\n",
      "the ratio of zero for features.3.weight\n",
      "tensor(0.1000, device='cuda:0')\n",
      "the ratio of zero for features.6.weight\n",
      "tensor(0.1000, device='cuda:0')\n",
      "the ratio of zero for features.8.weight\n",
      "tensor(0.1000, device='cuda:0')\n",
      "the ratio of zero for features.11.weight\n",
      "tensor(0.1000, device='cuda:0')\n",
      "the ratio of zero for features.13.weight\n",
      "tensor(0.1000, device='cuda:0')\n",
      "the ratio of zero for features.16.weight\n",
      "tensor(0.1000, device='cuda:0')\n",
      "the ratio of zero for features.18.weight\n",
      "tensor(0.1000, device='cuda:0')\n",
      "the ratio of zero for classifier.weight\n",
      "tensor(0.1000, device='cuda:0')\n",
      "Previous layer shape is  None\n",
      "Processing the coordinates to form ground_metric\n",
      "dont leave off the squaring of the ground metric\n",
      "returns a uniform measure of cardinality:  64\n",
      "returns a uniform measure of cardinality:  64\n",
      "the transport map is  tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')\n",
      "Ratio of trace to the matrix sum:  tensor(0.0469, device='cuda:0')\n",
      "Here, trace is 3.0 and matrix sum is 64.0 \n",
      "this is past correction for weight mode\n",
      "Shape of aligned wt is  torch.Size([64, 3, 9])\n",
      "Shape of fc_layer0_weight_data is  torch.Size([64, 3, 9])\n",
      "Previous layer shape is  torch.Size([64, 3, 3, 3])\n",
      "shape of layer: model 0 torch.Size([128, 64, 9])\n",
      "shape of layer: model 1 torch.Size([128, 64, 9])\n",
      "shape of previous transport map torch.Size([64, 64])\n",
      "Processing the coordinates to form ground_metric\n",
      "dont leave off the squaring of the ground metric\n",
      "returns a uniform measure of cardinality:  128\n",
      "returns a uniform measure of cardinality:  128\n",
      "the transport map is  tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        ...,\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0078],\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],\n",
      "       device='cuda:0')\n",
      "Ratio of trace to the matrix sum:  tensor(0.0078, device='cuda:0')\n",
      "Here, trace is 1.0 and matrix sum is 128.0 \n",
      "this is past correction for weight mode\n",
      "Shape of aligned wt is  torch.Size([128, 64, 9])\n",
      "Shape of fc_layer0_weight_data is  torch.Size([128, 64, 9])\n",
      "Previous layer shape is  torch.Size([128, 64, 3, 3])\n",
      "shape of layer: model 0 torch.Size([256, 128, 9])\n",
      "shape of layer: model 1 torch.Size([256, 128, 9])\n",
      "shape of previous transport map torch.Size([128, 128])\n",
      "Processing the coordinates to form ground_metric\n",
      "dont leave off the squaring of the ground metric\n",
      "returns a uniform measure of cardinality:  256\n",
      "returns a uniform measure of cardinality:  256\n",
      "the transport map is  tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')\n",
      "Ratio of trace to the matrix sum:  tensor(0., device='cuda:0')\n",
      "Here, trace is 0.0 and matrix sum is 256.0 \n",
      "this is past correction for weight mode\n",
      "Shape of aligned wt is  torch.Size([256, 128, 9])\n",
      "Shape of fc_layer0_weight_data is  torch.Size([256, 128, 9])\n",
      "Previous layer shape is  torch.Size([256, 128, 3, 3])\n",
      "shape of layer: model 0 torch.Size([256, 256, 9])\n",
      "shape of layer: model 1 torch.Size([256, 256, 9])\n",
      "shape of previous transport map torch.Size([256, 256])\n",
      "Processing the coordinates to form ground_metric\n",
      "dont leave off the squaring of the ground metric\n",
      "returns a uniform measure of cardinality:  256\n",
      "returns a uniform measure of cardinality:  256\n",
      "the transport map is  tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        ...,\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.0039, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],\n",
      "       device='cuda:0')\n",
      "Ratio of trace to the matrix sum:  tensor(0.0039, device='cuda:0')\n",
      "Here, trace is 1.0 and matrix sum is 256.0 \n",
      "this is past correction for weight mode\n",
      "Shape of aligned wt is  torch.Size([256, 256, 9])\n",
      "Shape of fc_layer0_weight_data is  torch.Size([256, 256, 9])\n",
      "Previous layer shape is  torch.Size([256, 256, 3, 3])\n",
      "shape of layer: model 0 torch.Size([512, 256, 9])\n",
      "shape of layer: model 1 torch.Size([512, 256, 9])\n",
      "shape of previous transport map torch.Size([256, 256])\n",
      "Processing the coordinates to form ground_metric\n",
      "dont leave off the squaring of the ground metric\n",
      "returns a uniform measure of cardinality:  512\n",
      "returns a uniform measure of cardinality:  512\n",
      "the transport map is  tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')\n",
      "Ratio of trace to the matrix sum:  tensor(0.0020, device='cuda:0')\n",
      "Here, trace is 1.0 and matrix sum is 512.0 \n",
      "this is past correction for weight mode\n",
      "Shape of aligned wt is  torch.Size([512, 256, 9])\n",
      "Shape of fc_layer0_weight_data is  torch.Size([512, 256, 9])\n",
      "Previous layer shape is  torch.Size([512, 256, 3, 3])\n",
      "shape of layer: model 0 torch.Size([512, 512, 9])\n",
      "shape of layer: model 1 torch.Size([512, 512, 9])\n",
      "shape of previous transport map torch.Size([512, 512])\n",
      "Processing the coordinates to form ground_metric\n",
      "dont leave off the squaring of the ground metric\n",
      "returns a uniform measure of cardinality:  512\n",
      "returns a uniform measure of cardinality:  512\n",
      "the transport map is  tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')\n",
      "Ratio of trace to the matrix sum:  tensor(0.0020, device='cuda:0')\n",
      "Here, trace is 1.0 and matrix sum is 512.0 \n",
      "this is past correction for weight mode\n",
      "Shape of aligned wt is  torch.Size([512, 512, 9])\n",
      "Shape of fc_layer0_weight_data is  torch.Size([512, 512, 9])\n",
      "Previous layer shape is  torch.Size([512, 512, 3, 3])\n",
      "shape of layer: model 0 torch.Size([512, 512, 9])\n",
      "shape of layer: model 1 torch.Size([512, 512, 9])\n",
      "shape of previous transport map torch.Size([512, 512])\n",
      "Processing the coordinates to form ground_metric\n",
      "dont leave off the squaring of the ground metric\n",
      "returns a uniform measure of cardinality:  512\n",
      "returns a uniform measure of cardinality:  512\n",
      "the transport map is  tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')\n",
      "Ratio of trace to the matrix sum:  tensor(0.0039, device='cuda:0')\n",
      "Here, trace is 2.0 and matrix sum is 512.0 \n",
      "this is past correction for weight mode\n",
      "Shape of aligned wt is  torch.Size([512, 512, 9])\n",
      "Shape of fc_layer0_weight_data is  torch.Size([512, 512, 9])\n",
      "Previous layer shape is  torch.Size([512, 512, 3, 3])\n",
      "shape of layer: model 0 torch.Size([512, 512, 9])\n",
      "shape of layer: model 1 torch.Size([512, 512, 9])\n",
      "shape of previous transport map torch.Size([512, 512])\n",
      "Processing the coordinates to form ground_metric\n",
      "dont leave off the squaring of the ground metric\n",
      "returns a uniform measure of cardinality:  512\n",
      "returns a uniform measure of cardinality:  512\n",
      "the transport map is  tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        [0.0020, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        ...,\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],\n",
      "       device='cuda:0')\n",
      "Ratio of trace to the matrix sum:  tensor(0.0020, device='cuda:0')\n",
      "Here, trace is 1.0 and matrix sum is 512.0 \n",
      "this is past correction for weight mode\n",
      "Shape of aligned wt is  torch.Size([512, 512, 9])\n",
      "Shape of fc_layer0_weight_data is  torch.Size([512, 512, 9])\n",
      "Previous layer shape is  torch.Size([512, 512, 3, 3])\n",
      "shape of layer: model 0 torch.Size([10, 512])\n",
      "shape of layer: model 1 torch.Size([10, 512])\n",
      "shape of previous transport map torch.Size([512, 512])\n",
      "Processing the coordinates to form ground_metric\n",
      "dont leave off the squaring of the ground metric\n",
      "ground metric is  tensor([[1.4414, 2.5047, 2.7771, 2.8747, 2.7059, 2.8728, 2.7212, 2.6281, 2.5808,\n",
      "         2.6023],\n",
      "        [2.4672, 1.3304, 2.7034, 2.5699, 2.7501, 2.5312, 2.4384, 2.5478, 2.2904,\n",
      "         2.4116],\n",
      "        [2.7715, 2.6414, 1.3092, 2.9687, 2.9210, 2.8578, 2.7534, 2.8539, 2.7660,\n",
      "         2.7565],\n",
      "        [2.8012, 2.6701, 2.8992, 1.5522, 2.8164, 2.9813, 2.7401, 2.7988, 2.7893,\n",
      "         2.6842],\n",
      "        [2.7964, 2.5644, 2.8516, 2.9012, 1.5086, 2.7325, 2.7784, 2.6606, 2.6916,\n",
      "         2.7173],\n",
      "        [2.9252, 2.5716, 2.9624, 2.7741, 2.7506, 1.5403, 2.7653, 2.7105, 2.8149,\n",
      "         2.6827],\n",
      "        [2.6901, 2.4399, 2.6906, 2.9216, 2.6574, 2.8479, 1.3951, 2.7158, 2.4897,\n",
      "         2.5414],\n",
      "        [2.7541, 2.5566, 2.7862, 2.7695, 2.7254, 2.6908, 2.6792, 1.3514, 2.5742,\n",
      "         2.5084],\n",
      "        [2.5593, 2.4034, 2.7509, 2.7208, 2.7148, 2.8119, 2.5539, 2.6392, 1.2810,\n",
      "         2.3784],\n",
      "        [2.5847, 2.2538, 2.8169, 2.7323, 2.6821, 2.7020, 2.5650, 2.5255, 2.5218,\n",
      "         1.3093]], device='cuda:0', grad_fn=<PowBackward0>)\n",
      "returns a uniform measure of cardinality:  10\n",
      "returns a uniform measure of cardinality:  10\n",
      "the transport map is  tensor([[0.1000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0000, 0.1000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0000, 0.0000, 0.1000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.1000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.1000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.1000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.1000, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.1000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.1000,\n",
      "         0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.1000]], device='cuda:0')\n",
      "Ratio of trace to the matrix sum:  tensor(1., device='cuda:0')\n",
      "Here, trace is 9.99755859375 and matrix sum is 9.99755859375 \n",
      "this is past correction for weight mode\n",
      "Shape of aligned wt is  torch.Size([10, 512])\n",
      "Shape of fc_layer0_weight_data is  torch.Size([10, 512])\n",
      "using independent method\n",
      "in _make_layers [Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), AvgPool2d(kernel_size=1, stride=1, padding=0)]\n",
      "Relu Inplace is  False\n",
      "model parameters are \n",
      " [torch.Size([64, 3, 3, 3]), torch.Size([128, 64, 3, 3]), torch.Size([256, 128, 3, 3]), torch.Size([256, 256, 3, 3]), torch.Size([512, 256, 3, 3]), torch.Size([512, 512, 3, 3]), torch.Size([512, 512, 3, 3]), torch.Size([512, 512, 3, 3]), torch.Size([10, 512])]\n",
      "\n",
      "--------- Testing in global mode ---------\n",
      "size of test_loader dataset:  10000\n",
      "\n",
      "Test set: Avg. loss: 0.0023, Accuracy: 1019/10000 (10%)\n",
      "\n",
      "len of model parameters and avg aligned layers is  9 9\n",
      "len of model_state_dict is  9\n",
      "len of param_list is  9\n",
      "\n",
      "--------- Testing in global mode ---------\n",
      "size of test_loader dataset:  10000\n",
      "\n",
      "Test set: Avg. loss: 0.0015, Accuracy: 8502/10000 (85%)\n",
      "\n",
      "===============================================\n",
      "===============================================\n",
      "After Ensemble: 85.02\n",
      "===============================================\n",
      "===============================================\n",
      "Timer ends\n",
      "Time taken for geometric ensembling is 3.484161041676998 seconds\n",
      "------- Prediction based ensembling -------\n",
      "\n",
      "Test set: Avg. loss: 0.0003, Accuracy: 9124/10000 (91%)\n",
      "\n",
      "------- Naive ensembling of weights -------\n",
      "[torch.Size([64, 3, 3, 3]), torch.Size([64, 3, 3, 3])]\n",
      "torch.Size([64, 3, 3, 3])\n",
      "[torch.Size([128, 64, 3, 3]), torch.Size([128, 64, 3, 3])]\n",
      "torch.Size([128, 64, 3, 3])\n",
      "[torch.Size([256, 128, 3, 3]), torch.Size([256, 128, 3, 3])]\n",
      "torch.Size([256, 128, 3, 3])\n",
      "[torch.Size([256, 256, 3, 3]), torch.Size([256, 256, 3, 3])]\n",
      "torch.Size([256, 256, 3, 3])\n",
      "[torch.Size([512, 256, 3, 3]), torch.Size([512, 256, 3, 3])]\n",
      "torch.Size([512, 256, 3, 3])\n",
      "[torch.Size([512, 512, 3, 3]), torch.Size([512, 512, 3, 3])]\n",
      "torch.Size([512, 512, 3, 3])\n",
      "[torch.Size([512, 512, 3, 3]), torch.Size([512, 512, 3, 3])]\n",
      "torch.Size([512, 512, 3, 3])\n",
      "[torch.Size([512, 512, 3, 3]), torch.Size([512, 512, 3, 3])]\n",
      "torch.Size([512, 512, 3, 3])\n",
      "[torch.Size([10, 512]), torch.Size([10, 512])]\n",
      "torch.Size([10, 512])\n",
      "in _make_layers [Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), AvgPool2d(kernel_size=1, stride=1, padding=0)]\n",
      "Relu Inplace is  False\n",
      "model parameters are \n",
      " [torch.Size([64, 3, 3, 3]), torch.Size([128, 64, 3, 3]), torch.Size([256, 128, 3, 3]), torch.Size([256, 256, 3, 3]), torch.Size([512, 256, 3, 3]), torch.Size([512, 512, 3, 3]), torch.Size([512, 512, 3, 3]), torch.Size([512, 512, 3, 3]), torch.Size([10, 512])]\n",
      "\n",
      "--------- Testing in global mode ---------\n",
      "size of test_loader dataset:  10000\n",
      "\n",
      "Test set: Avg. loss: 0.0023, Accuracy: 1143/10000 (11%)\n",
      "\n",
      "\n",
      "--------- Testing in global mode ---------\n",
      "size of test_loader dataset:  10000\n",
      "\n",
      "Test set: Avg. loss: 0.0023, Accuracy: 1680/10000 (17%)\n",
      "\n",
      "-------- Retraining the models ---------\n",
      "Retraining model :  model_0\n",
      "num_epochs---------------+++))))))+++ 300\n",
      "num_epochs---------------+++))))))+++ 300\n",
      "lr is  0.05\n",
      "number of epochs would be  30\n",
      "num_epochs-------------- 30\n",
      "Epoch 000\n",
      "/root/miniconda3/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:129: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\"Detected call of `lr_scheduler.step()` before `optimizer.step()`. \"\n",
      "/root/miniconda3/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:154: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "accuracy: {'epoch': 0, 'value': 0.5604800000000004} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 0, 'value': 1.2513591571140266} ({'split': 'train'})\n",
      "accuracy: {'epoch': 0, 'value': 0.711400032043457} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 0, 'value': 0.8908344745635987} ({'split': 'test'})\n",
      "Epoch 001\n",
      "^C\n",
      "Traceback (most recent call last):\n",
      "  File \"main.py\", line 413, in <module>\n",
      "    _, best_retrain_acc = routines.retrain_models(args, [*original_models, geometric_model, naive_model], retrain_loader, test_loader, config, tensorboard_obj=tensorboard_obj, initial_acc=initial_acc, nicks=nicks)\n",
      "  File \"/root/autodl-tmp/OTfusion_pruning/otfusion/routines.py\", line 394, in retrain_models\n",
      "    retrained_network, acc = cifar_train.get_retrained_model(args, retrain_loader, test_loader, old_networks[i], config, output_root_dir, tensorboard_obj=tensorboard_obj, nick=nick, start_acc=initial_acc[i])\n",
      "  File \"/root/autodl-tmp/OTfusion_pruning/otfusion/./cifar/train.py\", line 333, in get_retrained_model\n",
      "    best_acc,model = main(config, output_dir, args.gpu_id, pretrained_model=old_network, pretrained_dataset=(train_loader, test_loader), tensorboard_obj=tensorboard_obj,return_model=True)\n",
      "  File \"/root/autodl-tmp/OTfusion_pruning/otfusion/./cifar/train.py\", line 67, in main\n",
      "    for batch_x, batch_y in training_loader:\n",
      "  File \"/root/miniconda3/lib/python3.8/site-packages/torch/utils/data/dataloader.py\", line 521, in __next__\n",
      "    data = self._next_data()\n",
      "  File \"/root/miniconda3/lib/python3.8/site-packages/torch/utils/data/dataloader.py\", line 561, in _next_data\n",
      "    data = self._dataset_fetcher.fetch(index)  # may raise StopIteration\n",
      "  File \"/root/miniconda3/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py\", line 49, in fetch\n",
      "    data = [self.dataset[idx] for idx in possibly_batched_index]\n",
      "  File \"/root/miniconda3/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py\", line 49, in <listcomp>\n",
      "    data = [self.dataset[idx] for idx in possibly_batched_index]\n",
      "  File \"/root/miniconda3/lib/python3.8/site-packages/torchvision/datasets/cifar.py\", line 121, in __getitem__\n",
      "    img = self.transform(img)\n",
      "  File \"/root/miniconda3/lib/python3.8/site-packages/torchvision/transforms/transforms.py\", line 61, in __call__\n",
      "    img = t(img)\n",
      "  File \"/root/miniconda3/lib/python3.8/site-packages/torchvision/transforms/transforms.py\", line 98, in __call__\n",
      "    return F.to_tensor(pic)\n",
      "  File \"/root/miniconda3/lib/python3.8/site-packages/torchvision/transforms/functional.py\", line 141, in to_tensor\n",
      "    np.array(pic, mode_to_nptype.get(pic.mode, np.uint8), copy=True)\n",
      "KeyboardInterrupt\n"
     ]
    }
   ],
   "source": [
    "!python main.py --gpu-id 0 --model-name vgg11_nobias --n-epochs 90 --save-result-file sample.csv \\\n",
    "--sweep-name exp_sample --correction --ground-metric euclidean --weight-stats \\\n",
    "--geom-ensemble-type wts --ground-metric-normalize none --sweep-id 90 \\\n",
    "--ckpt-type best --dataset Cifar10 --ground-metric-eff --recheck-cifar --activation-seed {activation_seed} \\\n",
    "--prelu-acts --past-correction --not-squared --exact\\\n",
    "--learning-rate 0.08\\\n",
    "--momentum 0.5\\\n",
    "--batch-size-train 64 --experiment-dir {experiment_dir}\\\n",
    "--prunint-rate {prunint_rate}\\\n",
    "--run-mode {run_mode}\\\n",
    "--to-download\\\n",
    "--load-model-dir {load_model_dir}\\\n",
    "--retrain 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "19ce2567-adad-4f09-95a3-4449ed54d54d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#剪枝实验2:fine prune \n",
    "activation_seed=21\n",
    "experiment_dir=\"experiment0.5\"\n",
    "prunint_rate=0.1\n",
    "run_mode='finetune'\n",
    "load_model_dir='./cifar_models'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c577ae36-7f34-416e-b893-957ade768633",
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------- Setting up parameters -------\n",
      "dumping parameters at  /storage/coda1/p-jli3175/0/mwu385/OTfusion_pruning_final/otfusion/exp_sample/configurations\n",
      "The parameters are: \n",
      " Namespace(experiment_dir='experiment0.5', prunint_rate=0.1, run_mode='finetune', load_model_dir='./cifar_models', prune_times=4, finetunetimes_benchmark=0, n_epochs=10, batch_size_train=256, batch_size_test=1000, learning_rate=3e-05, momentum=0.5, log_interval=100, to_download=True, disable_bias=True, dataset='Cifar10', num_models=2, model_name='vgg11_nobias', config_file=None, config_dir='/storage/coda1/p-jli3175/0/mwu385/OTfusion_pruning_final/otfusion/exp_sample/configurations', num_hidden_nodes=400, num_hidden_nodes1=400, num_hidden_nodes2=200, num_hidden_nodes3=100, num_hidden_nodes4=50, sweep_id=90, gpu_id=0, skip_last_layer=False, skip_last_layer_type='average', debug=False, cifar_style_data=False, activation_histograms=False, act_num_samples=100, softmax_temperature=1, activation_mode=None, options_type='generic', deprecated=None, save_result_file='sample.csv', sweep_name='exp_sample', reg=0.01, reg_m=0.001, ground_metric='euclidean', ground_metric_normalize='none', not_squared=True, clip_gm=False, clip_min=0, clip_max=5, tmap_stats=False, ensemble_step=0.5, ground_metric_eff=True, retrain=30, retrain_lr_decay=-1, retrain_lr_decay_factor=None, retrain_lr_decay_epochs=None, retrain_avg_only=False, retrain_geometric_only=False, load_models='', ckpt_type='best', recheck_cifar=True, recheck_acc=False, eval_aligned=False, enable_dropout=False, dump_model=False, dump_final_models=False, correction=True, activation_seed=21, weight_stats=True, sinkhorn_type='normal', geom_ensemble_type='wts', act_bug=False, standardize_acts=False, transform_acts=False, center_acts=False, prelu_acts=True, pool_acts=False, pool_relu=False, normalize_acts=False, normalize_wts=False, gromov=False, gromov_loss='square_loss', tensorboard_root='./tensorboard', tensorboard=False, same_model=-1, dist_normalize=False, update_acts=False, past_correction=True, partial_reshape=False, choice='0 2 4 6 8', diff_init=False, partition_type='labels', personal_class_idx=9, partition_dataloader=-1, personal_split_frac=0.1, exact=True, skip_personal_idx=False, prediction_wts=False, width_ratio=1, proper_marginals=False, retrain_seed=-1, no_random_trainloaders=False, reinit_trainloaders=False, second_model_name=None, print_distances=False, deterministic=False, skip_retrain=-1, importance=None, unbalanced=False, temperature=20, alpha=0.7, dist_epochs=60, handle_skips=False, timestamp='2023-09-19_00-54-41_220971', rootdir='/storage/coda1/p-jli3175/0/mwu385/OTfusion_pruning_final/otfusion/exp_sample', baseroot='/storage/coda1/p-jli3175/0/mwu385/OTfusion_pruning_final/otfusion', result_dir='/storage/coda1/p-jli3175/0/mwu385/OTfusion_pruning_final/otfusion/exp_sample/results', exp_name='exp_2023-09-19_00-54-41_220971', csv_dir='/storage/coda1/p-jli3175/0/mwu385/OTfusion_pruning_final/otfusion/exp_sample/csv')\n",
      "refactored get_config\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "in _make_layers [Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), AvgPool2d(kernel_size=1, stride=1, padding=0)]\n",
      "Relu Inplace is  False\n",
      "model parameters are \n",
      " [torch.Size([64, 3, 3, 3]), torch.Size([128, 64, 3, 3]), torch.Size([256, 128, 3, 3]), torch.Size([256, 256, 3, 3]), torch.Size([512, 256, 3, 3]), torch.Size([512, 512, 3, 3]), torch.Size([512, 512, 3, 3]), torch.Size([512, 512, 3, 3]), torch.Size([10, 512])]\n",
      "in _make_layers [Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), AvgPool2d(kernel_size=1, stride=1, padding=0)]\n",
      "Relu Inplace is  False\n",
      "model parameters are \n",
      " [torch.Size([64, 3, 3, 3]), torch.Size([128, 64, 3, 3]), torch.Size([256, 128, 3, 3]), torch.Size([256, 256, 3, 3]), torch.Size([512, 256, 3, 3]), torch.Size([512, 512, 3, 3]), torch.Size([512, 512, 3, 3]), torch.Size([512, 512, 3, 3]), torch.Size([10, 512])]\n",
      "in _make_layers [Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), AvgPool2d(kernel_size=1, stride=1, padding=0)]\n",
      "Relu Inplace is  False\n",
      "model parameters are \n",
      " [torch.Size([64, 3, 3, 3]), torch.Size([128, 64, 3, 3]), torch.Size([256, 128, 3, 3]), torch.Size([256, 256, 3, 3]), torch.Size([512, 256, 3, 3]), torch.Size([512, 512, 3, 3]), torch.Size([512, 512, 3, 3]), torch.Size([512, 512, 3, 3]), torch.Size([10, 512])]\n",
      "in _make_layers [Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), AvgPool2d(kernel_size=1, stride=1, padding=0)]\n",
      "Relu Inplace is  False\n",
      "model parameters are \n",
      " [torch.Size([64, 3, 3, 3]), torch.Size([128, 64, 3, 3]), torch.Size([256, 128, 3, 3]), torch.Size([256, 256, 3, 3]), torch.Size([512, 256, 3, 3]), torch.Size([512, 512, 3, 3]), torch.Size([512, 512, 3, 3]), torch.Size([512, 512, 3, 3]), torch.Size([10, 512])]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [0/50000 (0%)]\tLoss: 0.101038\n",
      "Train Epoch: 1 [25600/50000 (51%)]\tLoss: 0.097483\n",
      "\n",
      "--------- Testing in global mode ---------\n",
      "size of test_loader dataset:  10000\n",
      "\n",
      "Test set: Avg. loss: 0.0003, Accuracy: 9027/10000 (90%)\n",
      "\n",
      "Train Epoch: 2 [0/50000 (0%)]\tLoss: 0.076121\n",
      "Train Epoch: 2 [25600/50000 (51%)]\tLoss: 0.095955\n",
      "\n",
      "--------- Testing in global mode ---------\n",
      "size of test_loader dataset:  10000\n",
      "\n",
      "Test set: Avg. loss: 0.0003, Accuracy: 9021/10000 (90%)\n",
      "\n",
      "Train Epoch: 3 [0/50000 (0%)]\tLoss: 0.071711\n",
      "Train Epoch: 3 [25600/50000 (51%)]\tLoss: 0.067176\n",
      "\n",
      "--------- Testing in global mode ---------\n",
      "size of test_loader dataset:  10000\n",
      "\n",
      "Test set: Avg. loss: 0.0003, Accuracy: 9025/10000 (90%)\n",
      "\n",
      "Train Epoch: 4 [0/50000 (0%)]\tLoss: 0.078726\n",
      "Train Epoch: 4 [25600/50000 (51%)]\tLoss: 0.065343\n",
      "\n",
      "--------- Testing in global mode ---------\n",
      "size of test_loader dataset:  10000\n",
      "\n",
      "Test set: Avg. loss: 0.0003, Accuracy: 9033/10000 (90%)\n",
      "\n",
      "Train Epoch: 5 [0/50000 (0%)]\tLoss: 0.075568\n",
      "Train Epoch: 5 [25600/50000 (51%)]\tLoss: 0.082846\n",
      "\n",
      "--------- Testing in global mode ---------\n",
      "size of test_loader dataset:  10000\n",
      "\n",
      "Test set: Avg. loss: 0.0003, Accuracy: 9041/10000 (90%)\n",
      "\n",
      "Train Epoch: 6 [0/50000 (0%)]\tLoss: 0.072385\n",
      "Train Epoch: 6 [25600/50000 (51%)]\tLoss: 0.073196\n",
      "\n",
      "--------- Testing in global mode ---------\n",
      "size of test_loader dataset:  10000\n",
      "\n",
      "Test set: Avg. loss: 0.0003, Accuracy: 9044/10000 (90%)\n",
      "\n",
      "Train Epoch: 7 [0/50000 (0%)]\tLoss: 0.086264\n",
      "Train Epoch: 7 [25600/50000 (51%)]\tLoss: 0.087274\n",
      "\n",
      "--------- Testing in global mode ---------\n",
      "size of test_loader dataset:  10000\n",
      "\n",
      "Test set: Avg. loss: 0.0003, Accuracy: 9048/10000 (90%)\n",
      "\n",
      "Train Epoch: 8 [0/50000 (0%)]\tLoss: 0.068105\n",
      "Train Epoch: 8 [25600/50000 (51%)]\tLoss: 0.086269\n",
      "\n",
      "--------- Testing in global mode ---------\n",
      "size of test_loader dataset:  10000\n",
      "\n",
      "Test set: Avg. loss: 0.0003, Accuracy: 9046/10000 (90%)\n",
      "\n",
      "Train Epoch: 9 [0/50000 (0%)]\tLoss: 0.073709\n",
      "Train Epoch: 9 [25600/50000 (51%)]\tLoss: 0.078567\n",
      "\n",
      "--------- Testing in global mode ---------\n",
      "size of test_loader dataset:  10000\n",
      "\n",
      "Test set: Avg. loss: 0.0003, Accuracy: 9050/10000 (90%)\n",
      "\n",
      "Train Epoch: 10 [0/50000 (0%)]\tLoss: 0.076801\n",
      "Train Epoch: 10 [25600/50000 (51%)]\tLoss: 0.077062\n",
      "\n",
      "--------- Testing in global mode ---------\n",
      "size of test_loader dataset:  10000\n",
      "\n",
      "Test set: Avg. loss: 0.0003, Accuracy: 9053/10000 (91%)\n",
      "\n",
      "Train Epoch: 1 [0/50000 (0%)]\tLoss: 0.099159\n",
      "Train Epoch: 1 [25600/50000 (51%)]\tLoss: 0.081805\n",
      "\n",
      "--------- Testing in global mode ---------\n",
      "size of test_loader dataset:  10000\n",
      "\n",
      "Test set: Avg. loss: 0.0003, Accuracy: 9043/10000 (90%)\n",
      "\n",
      "Train Epoch: 2 [0/50000 (0%)]\tLoss: 0.074062\n",
      "Train Epoch: 2 [25600/50000 (51%)]\tLoss: 0.104073\n",
      "\n",
      "--------- Testing in global mode ---------\n",
      "size of test_loader dataset:  10000\n",
      "\n",
      "Test set: Avg. loss: 0.0003, Accuracy: 9054/10000 (91%)\n",
      "\n",
      "Train Epoch: 3 [0/50000 (0%)]\tLoss: 0.083706\n",
      "Train Epoch: 3 [25600/50000 (51%)]\tLoss: 0.085157\n",
      "\n",
      "--------- Testing in global mode ---------\n",
      "size of test_loader dataset:  10000\n",
      "\n",
      "Test set: Avg. loss: 0.0003, Accuracy: 9057/10000 (91%)\n",
      "\n",
      "Train Epoch: 4 [0/50000 (0%)]\tLoss: 0.099782\n",
      "Train Epoch: 4 [25600/50000 (51%)]\tLoss: 0.080162\n",
      "\n",
      "--------- Testing in global mode ---------\n",
      "size of test_loader dataset:  10000\n",
      "\n",
      "Test set: Avg. loss: 0.0003, Accuracy: 9071/10000 (91%)\n",
      "\n",
      "Train Epoch: 5 [0/50000 (0%)]\tLoss: 0.078944\n",
      "Train Epoch: 5 [25600/50000 (51%)]\tLoss: 0.109845\n",
      "\n",
      "--------- Testing in global mode ---------\n",
      "size of test_loader dataset:  10000\n",
      "\n",
      "Test set: Avg. loss: 0.0003, Accuracy: 9065/10000 (91%)\n",
      "\n",
      "Train Epoch: 6 [0/50000 (0%)]\tLoss: 0.076989\n",
      "Train Epoch: 6 [25600/50000 (51%)]\tLoss: 0.074020\n",
      "\n",
      "--------- Testing in global mode ---------\n",
      "size of test_loader dataset:  10000\n",
      "\n",
      "Test set: Avg. loss: 0.0003, Accuracy: 9063/10000 (91%)\n",
      "\n",
      "Train Epoch: 7 [0/50000 (0%)]\tLoss: 0.079069\n",
      "Train Epoch: 7 [25600/50000 (51%)]\tLoss: 0.066402\n",
      "\n",
      "--------- Testing in global mode ---------\n",
      "size of test_loader dataset:  10000\n",
      "\n",
      "Test set: Avg. loss: 0.0003, Accuracy: 9059/10000 (91%)\n",
      "\n",
      "Train Epoch: 8 [0/50000 (0%)]\tLoss: 0.065957\n",
      "Train Epoch: 8 [25600/50000 (51%)]\tLoss: 0.080865\n",
      "\n",
      "--------- Testing in global mode ---------\n",
      "size of test_loader dataset:  10000\n",
      "\n",
      "Test set: Avg. loss: 0.0003, Accuracy: 9068/10000 (91%)\n",
      "\n",
      "Train Epoch: 9 [0/50000 (0%)]\tLoss: 0.077443\n",
      "Train Epoch: 9 [25600/50000 (51%)]\tLoss: 0.077068\n",
      "\n",
      "--------- Testing in global mode ---------\n",
      "size of test_loader dataset:  10000\n",
      "\n",
      "Test set: Avg. loss: 0.0003, Accuracy: 9060/10000 (91%)\n",
      "\n",
      "Train Epoch: 10 [0/50000 (0%)]\tLoss: 0.075029\n",
      "Train Epoch: 10 [25600/50000 (51%)]\tLoss: 0.069719\n",
      "\n",
      "--------- Testing in global mode ---------\n",
      "size of test_loader dataset:  10000\n",
      "\n",
      "Test set: Avg. loss: 0.0003, Accuracy: 9061/10000 (91%)\n",
      "\n",
      "===============================================\n",
      "===============================================\n",
      "before Ensemble,acc: [90.53, 90.61] loss [0.0003037546396255493, 0.00030294047892093657]\n",
      "===============================================\n",
      "===============================================\n",
      "check zero rate for model 0\n",
      "odict_keys(['features.0.weight', 'features.3.weight', 'features.6.weight', 'features.8.weight', 'features.11.weight', 'features.13.weight', 'features.16.weight', 'features.18.weight', 'classifier.weight'])\n",
      "the ratio of zero for features.0.weight\n",
      "tensor(0.1001, device='cuda:0')\n",
      "the ratio of zero for features.3.weight\n",
      "tensor(0.1000, device='cuda:0')\n",
      "the ratio of zero for features.6.weight\n",
      "tensor(0.1000, device='cuda:0')\n",
      "the ratio of zero for features.8.weight\n",
      "tensor(0.1000, device='cuda:0')\n",
      "the ratio of zero for features.11.weight\n",
      "tensor(0.1000, device='cuda:0')\n",
      "the ratio of zero for features.13.weight\n",
      "tensor(0.1000, device='cuda:0')\n",
      "the ratio of zero for features.16.weight\n",
      "tensor(0.1000, device='cuda:0')\n",
      "the ratio of zero for features.18.weight\n",
      "tensor(0.1000, device='cuda:0')\n",
      "the ratio of zero for classifier.weight\n",
      "tensor(0.1000, device='cuda:0')\n",
      "check zero rate for model 1\n",
      "odict_keys(['features.0.weight', 'features.3.weight', 'features.6.weight', 'features.8.weight', 'features.11.weight', 'features.13.weight', 'features.16.weight', 'features.18.weight', 'classifier.weight'])\n",
      "the ratio of zero for features.0.weight\n",
      "tensor(0.1001, device='cuda:0')\n",
      "the ratio of zero for features.3.weight\n",
      "tensor(0.1000, device='cuda:0')\n",
      "the ratio of zero for features.6.weight\n",
      "tensor(0.1000, device='cuda:0')\n",
      "the ratio of zero for features.8.weight\n",
      "tensor(0.1000, device='cuda:0')\n",
      "the ratio of zero for features.11.weight\n",
      "tensor(0.1000, device='cuda:0')\n",
      "the ratio of zero for features.13.weight\n",
      "tensor(0.1000, device='cuda:0')\n",
      "the ratio of zero for features.16.weight\n",
      "tensor(0.1000, device='cuda:0')\n",
      "the ratio of zero for features.18.weight\n",
      "tensor(0.1000, device='cuda:0')\n",
      "the ratio of zero for classifier.weight\n",
      "tensor(0.1000, device='cuda:0')\n",
      "layer features.0.weight has #params  1728\n",
      "layer features.3.weight has #params  73728\n",
      "layer features.6.weight has #params  294912\n",
      "layer features.8.weight has #params  589824\n",
      "layer features.11.weight has #params  1179648\n",
      "layer features.13.weight has #params  2359296\n",
      "layer features.16.weight has #params  2359296\n",
      "layer features.18.weight has #params  2359296\n",
      "layer classifier.weight has #params  5120\n",
      "Activation Timer start\n",
      "Activation Timer ends\n",
      "------- Geometric Ensembling -------\n",
      "Timer start\n",
      "Previous layer shape is  None\n",
      "Processing the coordinates to form ground_metric\n",
      "dont leave off the squaring of the ground metric\n",
      "returns a uniform measure of cardinality:  64\n",
      "returns a uniform measure of cardinality:  64\n",
      "the transport map is  tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')\n",
      "Ratio of trace to the matrix sum:  tensor(0.0469, device='cuda:0')\n",
      "Here, trace is 2.9999806880950928 and matrix sum is 63.99958801269531 \n",
      "this is past correction for weight mode\n",
      "Shape of aligned wt is  torch.Size([64, 3, 9])\n",
      "Shape of fc_layer0_weight_data is  torch.Size([64, 3, 9])\n",
      "Previous layer shape is  torch.Size([64, 3, 3, 3])\n",
      "shape of layer: model 0 torch.Size([128, 64, 9])\n",
      "shape of layer: model 1 torch.Size([128, 64, 9])\n",
      "shape of previous transport map torch.Size([64, 64])\n",
      "Processing the coordinates to form ground_metric\n",
      "dont leave off the squaring of the ground metric\n",
      "returns a uniform measure of cardinality:  128\n",
      "returns a uniform measure of cardinality:  128\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the transport map is  tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        ...,\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0078],\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],\n",
      "       device='cuda:0')\n",
      "Ratio of trace to the matrix sum:  tensor(0.0078, device='cuda:0')\n",
      "Here, trace is 0.9999872446060181 and matrix sum is 127.99836730957031 \n",
      "this is past correction for weight mode\n",
      "Shape of aligned wt is  torch.Size([128, 64, 9])\n",
      "Shape of fc_layer0_weight_data is  torch.Size([128, 64, 9])\n",
      "Previous layer shape is  torch.Size([128, 64, 3, 3])\n",
      "shape of layer: model 0 torch.Size([256, 128, 9])\n",
      "shape of layer: model 1 torch.Size([256, 128, 9])\n",
      "shape of previous transport map torch.Size([128, 128])\n",
      "Processing the coordinates to form ground_metric\n",
      "dont leave off the squaring of the ground metric\n",
      "returns a uniform measure of cardinality:  256\n",
      "returns a uniform measure of cardinality:  256\n",
      "the transport map is  tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')\n",
      "Ratio of trace to the matrix sum:  tensor(0., device='cuda:0')\n",
      "Here, trace is 0.0 and matrix sum is 255.99343872070312 \n",
      "this is past correction for weight mode\n",
      "Shape of aligned wt is  torch.Size([256, 128, 9])\n",
      "Shape of fc_layer0_weight_data is  torch.Size([256, 128, 9])\n",
      "Previous layer shape is  torch.Size([256, 128, 3, 3])\n",
      "shape of layer: model 0 torch.Size([256, 256, 9])\n",
      "shape of layer: model 1 torch.Size([256, 256, 9])\n",
      "shape of previous transport map torch.Size([256, 256])\n",
      "Processing the coordinates to form ground_metric\n",
      "dont leave off the squaring of the ground metric\n",
      "returns a uniform measure of cardinality:  256\n",
      "returns a uniform measure of cardinality:  256\n",
      "the transport map is  tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        ...,\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.0039, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],\n",
      "       device='cuda:0')\n",
      "Ratio of trace to the matrix sum:  tensor(0.0039, device='cuda:0')\n",
      "Here, trace is 0.9999743700027466 and matrix sum is 255.99343872070312 \n",
      "this is past correction for weight mode\n",
      "Shape of aligned wt is  torch.Size([256, 256, 9])\n",
      "Shape of fc_layer0_weight_data is  torch.Size([256, 256, 9])\n",
      "Previous layer shape is  torch.Size([256, 256, 3, 3])\n",
      "shape of layer: model 0 torch.Size([512, 256, 9])\n",
      "shape of layer: model 1 torch.Size([512, 256, 9])\n",
      "shape of previous transport map torch.Size([256, 256])\n",
      "Processing the coordinates to form ground_metric\n",
      "dont leave off the squaring of the ground metric\n",
      "returns a uniform measure of cardinality:  512\n",
      "returns a uniform measure of cardinality:  512\n",
      "the transport map is  tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')\n",
      "Ratio of trace to the matrix sum:  tensor(0.0020, device='cuda:0')\n",
      "Here, trace is 0.9999488592147827 and matrix sum is 511.97381591796875 \n",
      "this is past correction for weight mode\n",
      "Shape of aligned wt is  torch.Size([512, 256, 9])\n",
      "Shape of fc_layer0_weight_data is  torch.Size([512, 256, 9])\n",
      "Previous layer shape is  torch.Size([512, 256, 3, 3])\n",
      "shape of layer: model 0 torch.Size([512, 512, 9])\n",
      "shape of layer: model 1 torch.Size([512, 512, 9])\n",
      "shape of previous transport map torch.Size([512, 512])\n",
      "Processing the coordinates to form ground_metric\n",
      "dont leave off the squaring of the ground metric\n",
      "returns a uniform measure of cardinality:  512\n",
      "returns a uniform measure of cardinality:  512\n",
      "the transport map is  tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')\n",
      "Ratio of trace to the matrix sum:  tensor(0.0020, device='cuda:0')\n",
      "Here, trace is 0.9999488592147827 and matrix sum is 511.97381591796875 \n",
      "this is past correction for weight mode\n",
      "Shape of aligned wt is  torch.Size([512, 512, 9])\n",
      "Shape of fc_layer0_weight_data is  torch.Size([512, 512, 9])\n",
      "Previous layer shape is  torch.Size([512, 512, 3, 3])\n",
      "shape of layer: model 0 torch.Size([512, 512, 9])\n",
      "shape of layer: model 1 torch.Size([512, 512, 9])\n",
      "shape of previous transport map torch.Size([512, 512])\n",
      "Processing the coordinates to form ground_metric\n",
      "dont leave off the squaring of the ground metric\n",
      "returns a uniform measure of cardinality:  512\n",
      "returns a uniform measure of cardinality:  512\n",
      "the transport map is  tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')\n",
      "Ratio of trace to the matrix sum:  tensor(0.0039, device='cuda:0')\n",
      "Here, trace is 1.9998977184295654 and matrix sum is 511.97381591796875 \n",
      "this is past correction for weight mode\n",
      "Shape of aligned wt is  torch.Size([512, 512, 9])\n",
      "Shape of fc_layer0_weight_data is  torch.Size([512, 512, 9])\n",
      "Previous layer shape is  torch.Size([512, 512, 3, 3])\n",
      "shape of layer: model 0 torch.Size([512, 512, 9])\n",
      "shape of layer: model 1 torch.Size([512, 512, 9])\n",
      "shape of previous transport map torch.Size([512, 512])\n",
      "Processing the coordinates to form ground_metric\n",
      "dont leave off the squaring of the ground metric\n",
      "returns a uniform measure of cardinality:  512\n",
      "returns a uniform measure of cardinality:  512\n",
      "the transport map is  tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        [0.0020, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        ...,\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],\n",
      "       device='cuda:0')\n",
      "Ratio of trace to the matrix sum:  tensor(0.0020, device='cuda:0')\n",
      "Here, trace is 0.9999488592147827 and matrix sum is 511.97381591796875 \n",
      "this is past correction for weight mode\n",
      "Shape of aligned wt is  torch.Size([512, 512, 9])\n",
      "Shape of fc_layer0_weight_data is  torch.Size([512, 512, 9])\n",
      "Previous layer shape is  torch.Size([512, 512, 3, 3])\n",
      "shape of layer: model 0 torch.Size([10, 512])\n",
      "shape of layer: model 1 torch.Size([10, 512])\n",
      "shape of previous transport map torch.Size([512, 512])\n",
      "Processing the coordinates to form ground_metric\n",
      "dont leave off the squaring of the ground metric\n",
      "ground metric is  tensor([[1.4429, 2.4955, 2.7822, 2.8812, 2.7239, 2.8645, 2.7176, 2.6403, 2.5751,\n",
      "         2.6045],\n",
      "        [2.4798, 1.3380, 2.6901, 2.5796, 2.7545, 2.5437, 2.4366, 2.5316, 2.2987,\n",
      "         2.4039],\n",
      "        [2.7587, 2.6408, 1.3234, 2.9733, 2.9137, 2.8577, 2.7593, 2.8618, 2.7620,\n",
      "         2.7724],\n",
      "        [2.8281, 2.6528, 2.9076, 1.5498, 2.8338, 2.9670, 2.7409, 2.8089, 2.7932,\n",
      "         2.6665],\n",
      "        [2.7558, 2.5804, 2.8655, 2.8922, 1.5214, 2.7314, 2.7793, 2.6768, 2.6909,\n",
      "         2.7344],\n",
      "        [2.9395, 2.5642, 2.9658, 2.7769, 2.7452, 1.5456, 2.7678, 2.7048, 2.8176,\n",
      "         2.6880],\n",
      "        [2.6789, 2.4492, 2.7010, 2.9177, 2.6649, 2.8455, 1.3931, 2.7181, 2.4912,\n",
      "         2.5414],\n",
      "        [2.7532, 2.5600, 2.7779, 2.7856, 2.7070, 2.7067, 2.6852, 1.3420, 2.5758,\n",
      "         2.5113],\n",
      "        [2.5860, 2.4077, 2.7473, 2.7097, 2.7217, 2.8204, 2.5457, 2.6378, 1.2869,\n",
      "         2.3694],\n",
      "        [2.5877, 2.2658, 2.8099, 2.7356, 2.6685, 2.7090, 2.5731, 2.5164, 2.5311,\n",
      "         1.3139]], device='cuda:0', grad_fn=<PowBackward0>)\n",
      "returns a uniform measure of cardinality:  10\n",
      "returns a uniform measure of cardinality:  10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the transport map is  tensor([[0.1000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0000, 0.1000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0000, 0.0000, 0.1000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.1000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.1000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.1000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.1000, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.1000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.1000,\n",
      "         0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.1000]], device='cuda:0')\n",
      "Ratio of trace to the matrix sum:  tensor(1., device='cuda:0')\n",
      "Here, trace is 9.999990463256836 and matrix sum is 9.999990463256836 \n",
      "this is past correction for weight mode\n",
      "Shape of aligned wt is  torch.Size([10, 512])\n",
      "Shape of fc_layer0_weight_data is  torch.Size([10, 512])\n",
      "using independent method\n",
      "in _make_layers [Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), AvgPool2d(kernel_size=1, stride=1, padding=0)]\n",
      "Relu Inplace is  False\n",
      "model parameters are \n",
      " [torch.Size([64, 3, 3, 3]), torch.Size([128, 64, 3, 3]), torch.Size([256, 128, 3, 3]), torch.Size([256, 256, 3, 3]), torch.Size([512, 256, 3, 3]), torch.Size([512, 512, 3, 3]), torch.Size([512, 512, 3, 3]), torch.Size([512, 512, 3, 3]), torch.Size([10, 512])]\n",
      "\n",
      "--------- Testing in global mode ---------\n",
      "size of test_loader dataset:  10000\n",
      "\n",
      "Test set: Avg. loss: 0.0023, Accuracy: 777/10000 (8%)\n",
      "\n",
      "len of model parameters and avg aligned layers is  9 9\n",
      "len of model_state_dict is  9\n",
      "len of param_list is  9\n",
      "\n",
      "--------- Testing in global mode ---------\n",
      "size of test_loader dataset:  10000\n",
      "\n",
      "Test set: Avg. loss: 0.0014, Accuracy: 8625/10000 (86%)\n",
      "\n",
      "===============================================\n",
      "===============================================\n",
      "After Ensemble,acc: 86.25 loss 0.0014441177487373351\n",
      "===============================================\n",
      "===============================================\n",
      "Timer ends\n",
      "Time taken for geometric ensembling is 2.817615323001519 seconds\n",
      "------- Prediction based ensembling -------\n",
      "\n",
      "Test set: Avg. loss: 0.0003, Accuracy: 9151/10000 (92%)\n",
      "\n",
      "------- Naive ensembling of weights -------\n",
      "[torch.Size([64, 3, 3, 3]), torch.Size([64, 3, 3, 3])]\n",
      "torch.Size([64, 3, 3, 3])\n",
      "[torch.Size([128, 64, 3, 3]), torch.Size([128, 64, 3, 3])]\n",
      "torch.Size([128, 64, 3, 3])\n",
      "[torch.Size([256, 128, 3, 3]), torch.Size([256, 128, 3, 3])]\n",
      "torch.Size([256, 128, 3, 3])\n",
      "[torch.Size([256, 256, 3, 3]), torch.Size([256, 256, 3, 3])]\n",
      "torch.Size([256, 256, 3, 3])\n",
      "[torch.Size([512, 256, 3, 3]), torch.Size([512, 256, 3, 3])]\n",
      "torch.Size([512, 256, 3, 3])\n",
      "[torch.Size([512, 512, 3, 3]), torch.Size([512, 512, 3, 3])]\n",
      "torch.Size([512, 512, 3, 3])\n",
      "[torch.Size([512, 512, 3, 3]), torch.Size([512, 512, 3, 3])]\n",
      "torch.Size([512, 512, 3, 3])\n",
      "[torch.Size([512, 512, 3, 3]), torch.Size([512, 512, 3, 3])]\n",
      "torch.Size([512, 512, 3, 3])\n",
      "[torch.Size([10, 512]), torch.Size([10, 512])]\n",
      "torch.Size([10, 512])\n",
      "in _make_layers [Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), AvgPool2d(kernel_size=1, stride=1, padding=0)]\n",
      "Relu Inplace is  False\n",
      "model parameters are \n",
      " [torch.Size([64, 3, 3, 3]), torch.Size([128, 64, 3, 3]), torch.Size([256, 128, 3, 3]), torch.Size([256, 256, 3, 3]), torch.Size([512, 256, 3, 3]), torch.Size([512, 512, 3, 3]), torch.Size([512, 512, 3, 3]), torch.Size([512, 512, 3, 3]), torch.Size([10, 512])]\n",
      "\n",
      "--------- Testing in global mode ---------\n",
      "size of test_loader dataset:  10000\n",
      "\n",
      "Test set: Avg. loss: 0.0023, Accuracy: 1066/10000 (11%)\n",
      "\n",
      "\n",
      "--------- Testing in global mode ---------\n",
      "size of test_loader dataset:  10000\n",
      "\n",
      "Test set: Avg. loss: 0.0023, Accuracy: 1676/10000 (17%)\n",
      "\n",
      "-------- Retraining the models ---------\n",
      "Retraining model :  model_0\n",
      "num_epochs---------------+++))))))+++ 300\n",
      "num_epochs---------------+++))))))+++ 300\n",
      "lr is  0.05\n",
      "number of epochs would be  30\n",
      "num_epochs-------------- 30\n",
      "Epoch 000\n",
      "/usr/local/pace-apps/manual/packages/pytorch/1.12.0/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:131: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\"Detected call of `lr_scheduler.step()` before `optimizer.step()`. \"\n",
      "/usr/local/pace-apps/manual/packages/pytorch/1.12.0/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:156: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "accuracy: {'epoch': 0, 'value': 0.907380000019074} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 0, 'value': 0.2782932788562776} ({'split': 'train'})\n",
      "accuracy: {'epoch': 0, 'value': 0.8390000462532043} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 0, 'value': 0.5183517932891846} ({'split': 'test'})\n",
      "Epoch 001\n",
      "accuracy: {'epoch': 1, 'value': 0.9305800000381469} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 1, 'value': 0.20438237972497927} ({'split': 'train'})\n",
      "accuracy: {'epoch': 1, 'value': 0.8497000277042388} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 1, 'value': 0.5040244460105896} ({'split': 'test'})\n",
      "Epoch 002\n",
      "accuracy: {'epoch': 2, 'value': 0.9538799999809261} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 2, 'value': 0.13877111948966972} ({'split': 'train'})\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: {'epoch': 2, 'value': 0.8407000422477722} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 2, 'value': 0.6033352792263031} ({'split': 'test'})\n",
      "Epoch 003\n",
      "accuracy: {'epoch': 3, 'value': 0.9580200000572207} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 3, 'value': 0.12697804683923716} ({'split': 'train'})\n",
      "accuracy: {'epoch': 3, 'value': 0.8679000437259673} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 3, 'value': 0.44617763757705686} ({'split': 'test'})\n",
      "Epoch 004\n",
      "accuracy: {'epoch': 4, 'value': 0.9691999999999998} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 4, 'value': 0.09264685559272769} ({'split': 'train'})\n",
      "accuracy: {'epoch': 4, 'value': 0.8598000347614289} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 4, 'value': 0.5093065172433853} ({'split': 'test'})\n",
      "Epoch 005\n",
      "accuracy: {'epoch': 5, 'value': 0.9763400000000004} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 5, 'value': 0.07036837092399598} ({'split': 'train'})\n",
      "accuracy: {'epoch': 5, 'value': 0.8539000451564789} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 5, 'value': 0.5676460266113281} ({'split': 'test'})\n",
      "Epoch 006\n",
      "accuracy: {'epoch': 6, 'value': 0.9788800000381468} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 6, 'value': 0.06597471817255025} ({'split': 'train'})\n",
      "accuracy: {'epoch': 6, 'value': 0.8525000393390656} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 6, 'value': 0.5730371832847594} ({'split': 'test'})\n",
      "Epoch 007\n",
      "accuracy: {'epoch': 7, 'value': 0.9836800000190739} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 7, 'value': 0.05023608040690426} ({'split': 'train'})\n",
      "accuracy: {'epoch': 7, 'value': 0.8563000440597535} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 7, 'value': 0.5950831830501555} ({'split': 'test'})\n",
      "Epoch 008\n",
      "accuracy: {'epoch': 8, 'value': 0.9843799999999998} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 8, 'value': 0.04738656396314503} ({'split': 'train'})\n",
      "accuracy: {'epoch': 8, 'value': 0.8622000336647033} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 8, 'value': 0.5646180778741836} ({'split': 'test'})\n",
      "Epoch 009\n",
      "accuracy: {'epoch': 9, 'value': 0.98594} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 9, 'value': 0.044100000257492056} ({'split': 'train'})\n",
      "accuracy: {'epoch': 9, 'value': 0.8611000359058381} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 9, 'value': 0.5736344903707504} ({'split': 'test'})\n",
      "Epoch 010\n",
      "accuracy: {'epoch': 10, 'value': 0.9867799999999995} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 10, 'value': 0.04168730880618098} ({'split': 'train'})\n",
      "accuracy: {'epoch': 10, 'value': 0.8631000459194185} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 10, 'value': 0.5929542124271393} ({'split': 'test'})\n",
      "Epoch 011\n",
      "accuracy: {'epoch': 11, 'value': 0.9878600000381469} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 11, 'value': 0.03948501270055773} ({'split': 'train'})\n",
      "accuracy: {'epoch': 11, 'value': 0.8515000343322754} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 11, 'value': 0.6316570401191711} ({'split': 'test'})\n",
      "Epoch 012\n",
      "accuracy: {'epoch': 12, 'value': 0.9850800000190737} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 12, 'value': 0.04639937217712401} ({'split': 'train'})\n",
      "accuracy: {'epoch': 12, 'value': 0.8520000338554382} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 12, 'value': 0.6148259520530701} ({'split': 'test'})\n",
      "Epoch 013\n",
      "accuracy: {'epoch': 13, 'value': 0.984140000019074} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 13, 'value': 0.05104724986553193} ({'split': 'train'})\n",
      "accuracy: {'epoch': 13, 'value': 0.857500034570694} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 13, 'value': 0.5984022200107575} ({'split': 'test'})\n",
      "Epoch 014\n",
      "^C\n",
      "Traceback (most recent call last):\n",
      "  File \"/storage/coda1/p-jli3175/0/mwu385/OTfusion_pruning_final/otfusion/main.py\", line 460, in <module>\n",
      "    _, best_retrain_acc = routines.retrain_models(args, [*original_models, geometric_model, naive_model], retrain_loader, test_loader, config, tensorboard_obj=tensorboard_obj, initial_acc=initial_acc, nicks=nicks)\n",
      "  File \"/storage/coda1/p-jli3175/0/mwu385/OTfusion_pruning_final/otfusion/routines.py\", line 394, in retrain_models\n",
      "    retrained_network, acc = cifar_train.get_retrained_model(args, retrain_loader, test_loader, old_networks[i], config, output_root_dir, tensorboard_obj=tensorboard_obj, nick=nick, start_acc=initial_acc[i])\n",
      "  File \"/storage/coda1/p-jli3175/0/mwu385/OTfusion_pruning_final/otfusion/./cifar/train.py\", line 333, in get_retrained_model\n",
      "    best_acc,model = main(config, output_dir, args.gpu_id, pretrained_model=old_network, pretrained_dataset=(train_loader, test_loader), tensorboard_obj=tensorboard_obj,return_model=True)\n",
      "  File \"/storage/coda1/p-jli3175/0/mwu385/OTfusion_pruning_final/otfusion/./cifar/train.py\", line 67, in main\n",
      "    for batch_x, batch_y in training_loader:\n",
      "  File \"/usr/local/pace-apps/manual/packages/pytorch/1.12.0/lib/python3.9/site-packages/torch/utils/data/dataloader.py\", line 652, in __next__\n",
      "    data = self._next_data()\n",
      "  File \"/usr/local/pace-apps/manual/packages/pytorch/1.12.0/lib/python3.9/site-packages/torch/utils/data/dataloader.py\", line 692, in _next_data\n",
      "    data = self._dataset_fetcher.fetch(index)  # may raise StopIteration\n",
      "  File \"/usr/local/pace-apps/manual/packages/pytorch/1.12.0/lib/python3.9/site-packages/torch/utils/data/_utils/fetch.py\", line 49, in fetch\n",
      "    data = [self.dataset[idx] for idx in possibly_batched_index]\n",
      "  File \"/usr/local/pace-apps/manual/packages/pytorch/1.12.0/lib/python3.9/site-packages/torch/utils/data/_utils/fetch.py\", line 49, in <listcomp>\n",
      "    data = [self.dataset[idx] for idx in possibly_batched_index]\n",
      "  File \"/usr/local/pace-apps/manual/packages/pytorch/1.12.0/lib/python3.9/site-packages/torchvision/datasets/cifar.py\", line 118, in __getitem__\n",
      "    img = self.transform(img)\n",
      "  File \"/usr/local/pace-apps/manual/packages/pytorch/1.12.0/lib/python3.9/site-packages/torchvision/transforms/transforms.py\", line 94, in __call__\n",
      "    img = t(img)\n",
      "  File \"/usr/local/pace-apps/manual/packages/pytorch/1.12.0/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 1130, in _call_impl\n",
      "    return forward_call(*input, **kwargs)\n",
      "  File \"/usr/local/pace-apps/manual/packages/pytorch/1.12.0/lib/python3.9/site-packages/torchvision/transforms/transforms.py\", line 269, in forward\n",
      "    return F.normalize(tensor, self.mean, self.std, self.inplace)\n",
      "  File \"/usr/local/pace-apps/manual/packages/pytorch/1.12.0/lib/python3.9/site-packages/torchvision/transforms/functional.py\", line 360, in normalize\n",
      "    return F_t.normalize(tensor, mean=mean, std=std, inplace=inplace)\n",
      "  File \"/usr/local/pace-apps/manual/packages/pytorch/1.12.0/lib/python3.9/site-packages/torchvision/transforms/functional_tensor.py\", line 953, in normalize\n",
      "    if (std == 0).any():\n",
      "KeyboardInterrupt\n"
     ]
    }
   ],
   "source": [
    "!python main.py --gpu-id 0 --model-name vgg11_nobias --n-epochs 10 --save-result-file sample.csv \\\n",
    "--sweep-name exp_sample --correction --ground-metric euclidean --weight-stats \\\n",
    "--geom-ensemble-type wts --ground-metric-normalize none --sweep-id 90 \\\n",
    "--ckpt-type best --dataset Cifar10 --ground-metric-eff --recheck-cifar --activation-seed {activation_seed} \\\n",
    "--prelu-acts --past-correction --not-squared --exact\\\n",
    "--learning-rate 0.00003\\\n",
    "--momentum 0.5\\\n",
    "--batch-size-train 256 --experiment-dir {experiment_dir}\\\n",
    "--prunint-rate {prunint_rate}\\\n",
    "--run-mode {run_mode}\\\n",
    "--to-download\\\n",
    "--load-model-dir {load_model_dir}\\\n",
    "--retrain 30\n",
    "# After Ensemble: 86.33"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5846a14",
   "metadata": {},
   "outputs": [],
   "source": [
    "#剪枝实验2:fine prune without finetune\n",
    "activation_seed=21\n",
    "experiment_dir=\"experiment0.5\"\n",
    "prunint_rate=0.1\n",
    "run_mode='finetune'\n",
    "load_model_dir='./cifar_models'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85412569-b402-468e-9014-991148ecb970",
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------- Setting up parameters -------\n",
      "dumping parameters at  /root/autodl-tmp/OTfusion_pruning/otfusion/exp_sample/configurations\n",
      "The parameters are: \n",
      " Namespace(act_bug=False, act_num_samples=100, activation_histograms=False, activation_mode=None, activation_seed=21, alpha=0.7, baseroot='/root/autodl-tmp/OTfusion_pruning/otfusion', batch_size_test=1000, batch_size_train=256, center_acts=False, choice='0 2 4 6 8', cifar_style_data=False, ckpt_type='best', clip_gm=False, clip_max=5, clip_min=0, config_dir='/root/autodl-tmp/OTfusion_pruning/otfusion/exp_sample/configurations', config_file=None, correction=True, csv_dir='/root/autodl-tmp/OTfusion_pruning/otfusion/exp_sample/csv', dataset='Cifar10', debug=False, deprecated=None, deterministic=False, diff_init=False, disable_bias=True, dist_epochs=60, dist_normalize=False, dump_final_models=False, dump_model=False, enable_dropout=False, ensemble_step=0.5, eval_aligned=False, exact=True, exp_name='exp_2023-09-16_17-42-51_756852', experiment_dir='experiment2', geom_ensemble_type='wts', gpu_id=0, gromov=False, gromov_loss='square_loss', ground_metric='euclidean', ground_metric_eff=True, ground_metric_normalize='none', handle_skips=False, importance=None, learning_rate=3e-05, load_model_dir='./cifar_models', load_models='', log_interval=100, model_name='vgg11_nobias', momentum=0.5, n_epochs=30, no_random_trainloaders=False, normalize_acts=False, normalize_wts=False, not_squared=True, num_hidden_nodes=400, num_hidden_nodes1=400, num_hidden_nodes2=200, num_hidden_nodes3=100, num_hidden_nodes4=50, num_models=2, options_type='generic', partial_reshape=False, partition_dataloader=-1, partition_type='labels', past_correction=True, personal_class_idx=9, personal_split_frac=0.1, pool_acts=False, pool_relu=False, prediction_wts=False, prelu_acts=True, print_distances=False, proper_marginals=False, prunint_rate=0.2, recheck_acc=False, recheck_cifar=True, reg=0.01, reg_m=0.001, reinit_trainloaders=False, result_dir='/root/autodl-tmp/OTfusion_pruning/otfusion/exp_sample/results', retrain=30, retrain_avg_only=False, retrain_geometric_only=False, retrain_lr_decay=-1, retrain_lr_decay_epochs=None, retrain_lr_decay_factor=None, retrain_seed=-1, rootdir='/root/autodl-tmp/OTfusion_pruning/otfusion/exp_sample', run_mode='finetune', same_model=-1, save_result_file='sample.csv', second_model_name=None, sinkhorn_type='normal', skip_last_layer=False, skip_last_layer_type='average', skip_personal_idx=False, skip_retrain=-1, softmax_temperature=1, standardize_acts=False, sweep_id=90, sweep_name='exp_sample', temperature=20, tensorboard=False, tensorboard_root='./tensorboard', timestamp='2023-09-16_17-42-51_756852', tmap_stats=False, to_download=True, transform_acts=False, unbalanced=False, update_acts=False, weight_stats=True, width_ratio=1)\n",
      "refactored get_config\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "in _make_layers [Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), AvgPool2d(kernel_size=1, stride=1, padding=0)]\n",
      "Relu Inplace is  False\n",
      "model parameters are \n",
      " [torch.Size([64, 3, 3, 3]), torch.Size([128, 64, 3, 3]), torch.Size([256, 128, 3, 3]), torch.Size([256, 256, 3, 3]), torch.Size([512, 256, 3, 3]), torch.Size([512, 512, 3, 3]), torch.Size([512, 512, 3, 3]), torch.Size([512, 512, 3, 3]), torch.Size([10, 512])]\n",
      "in _make_layers [Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), AvgPool2d(kernel_size=1, stride=1, padding=0)]\n",
      "Relu Inplace is  False\n",
      "model parameters are \n",
      " [torch.Size([64, 3, 3, 3]), torch.Size([128, 64, 3, 3]), torch.Size([256, 128, 3, 3]), torch.Size([256, 256, 3, 3]), torch.Size([512, 256, 3, 3]), torch.Size([512, 512, 3, 3]), torch.Size([512, 512, 3, 3]), torch.Size([512, 512, 3, 3]), torch.Size([10, 512])]\n",
      "in _make_layers [Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), AvgPool2d(kernel_size=1, stride=1, padding=0)]\n",
      "Relu Inplace is  False\n",
      "model parameters are \n",
      " [torch.Size([64, 3, 3, 3]), torch.Size([128, 64, 3, 3]), torch.Size([256, 128, 3, 3]), torch.Size([256, 256, 3, 3]), torch.Size([512, 256, 3, 3]), torch.Size([512, 512, 3, 3]), torch.Size([512, 512, 3, 3]), torch.Size([512, 512, 3, 3]), torch.Size([10, 512])]\n",
      "in _make_layers [Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), AvgPool2d(kernel_size=1, stride=1, padding=0)]\n",
      "Relu Inplace is  False\n",
      "model parameters are \n",
      " [torch.Size([64, 3, 3, 3]), torch.Size([128, 64, 3, 3]), torch.Size([256, 128, 3, 3]), torch.Size([256, 256, 3, 3]), torch.Size([512, 256, 3, 3]), torch.Size([512, 512, 3, 3]), torch.Size([512, 512, 3, 3]), torch.Size([512, 512, 3, 3]), torch.Size([10, 512])]\n",
      "Train Epoch: 1 [0/50000 (0%)]\tLoss: 0.104261\n",
      "Train Epoch: 1 [25600/50000 (51%)]\tLoss: 0.099476\n",
      "\n",
      "--------- Testing in global mode ---------\n",
      "size of test_loader dataset:  10000\n",
      "\n",
      "Test set: Avg. loss: 0.0003, Accuracy: 9025/10000 (90%)\n",
      "\n",
      "Train Epoch: 2 [0/50000 (0%)]\tLoss: 0.077032\n",
      "Train Epoch: 2 [25600/50000 (51%)]\tLoss: 0.096654\n",
      "\n",
      "--------- Testing in global mode ---------\n",
      "size of test_loader dataset:  10000\n",
      "\n",
      "Test set: Avg. loss: 0.0003, Accuracy: 9030/10000 (90%)\n",
      "\n",
      "Train Epoch: 3 [0/50000 (0%)]\tLoss: 0.071083\n",
      "Train Epoch: 3 [25600/50000 (51%)]\tLoss: 0.067032\n",
      "\n",
      "--------- Testing in global mode ---------\n",
      "size of test_loader dataset:  10000\n",
      "\n",
      "Test set: Avg. loss: 0.0003, Accuracy: 9037/10000 (90%)\n",
      "\n",
      "Train Epoch: 4 [0/50000 (0%)]\tLoss: 0.079727\n",
      "Train Epoch: 4 [25600/50000 (51%)]\tLoss: 0.065154\n",
      "\n",
      "--------- Testing in global mode ---------\n",
      "size of test_loader dataset:  10000\n",
      "\n",
      "Test set: Avg. loss: 0.0003, Accuracy: 9038/10000 (90%)\n",
      "\n",
      "Train Epoch: 5 [0/50000 (0%)]\tLoss: 0.077752\n",
      "Train Epoch: 5 [25600/50000 (51%)]\tLoss: 0.081237\n",
      "\n",
      "--------- Testing in global mode ---------\n",
      "size of test_loader dataset:  10000\n",
      "\n",
      "Test set: Avg. loss: 0.0003, Accuracy: 9040/10000 (90%)\n",
      "\n",
      "Train Epoch: 6 [0/50000 (0%)]\tLoss: 0.074283\n",
      "Train Epoch: 6 [25600/50000 (51%)]\tLoss: 0.075979\n",
      "\n",
      "--------- Testing in global mode ---------\n",
      "size of test_loader dataset:  10000\n",
      "\n",
      "Test set: Avg. loss: 0.0003, Accuracy: 9041/10000 (90%)\n",
      "\n",
      "Train Epoch: 7 [0/50000 (0%)]\tLoss: 0.085717\n",
      "Train Epoch: 7 [25600/50000 (51%)]\tLoss: 0.086501\n",
      "\n",
      "--------- Testing in global mode ---------\n",
      "size of test_loader dataset:  10000\n",
      "\n",
      "Test set: Avg. loss: 0.0003, Accuracy: 9045/10000 (90%)\n",
      "\n",
      "Train Epoch: 8 [0/50000 (0%)]\tLoss: 0.069545\n",
      "Train Epoch: 8 [25600/50000 (51%)]\tLoss: 0.087994\n",
      "\n",
      "--------- Testing in global mode ---------\n",
      "size of test_loader dataset:  10000\n",
      "\n",
      "Test set: Avg. loss: 0.0003, Accuracy: 9049/10000 (90%)\n",
      "\n",
      "Train Epoch: 9 [0/50000 (0%)]\tLoss: 0.075486\n",
      "Train Epoch: 9 [25600/50000 (51%)]\tLoss: 0.080120\n",
      "\n",
      "--------- Testing in global mode ---------\n",
      "size of test_loader dataset:  10000\n",
      "\n",
      "Test set: Avg. loss: 0.0003, Accuracy: 9048/10000 (90%)\n",
      "\n",
      "Train Epoch: 10 [0/50000 (0%)]\tLoss: 0.078696\n",
      "Train Epoch: 10 [25600/50000 (51%)]\tLoss: 0.079160\n",
      "\n",
      "--------- Testing in global mode ---------\n",
      "size of test_loader dataset:  10000\n",
      "\n",
      "Test set: Avg. loss: 0.0003, Accuracy: 9049/10000 (90%)\n",
      "\n",
      "Train Epoch: 11 [0/50000 (0%)]\tLoss: 0.068520\n",
      "Train Epoch: 11 [25600/50000 (51%)]\tLoss: 0.058188\n",
      "\n",
      "--------- Testing in global mode ---------\n",
      "size of test_loader dataset:  10000\n",
      "\n",
      "Test set: Avg. loss: 0.0003, Accuracy: 9051/10000 (91%)\n",
      "\n",
      "Train Epoch: 12 [0/50000 (0%)]\tLoss: 0.053162\n",
      "Train Epoch: 12 [25600/50000 (51%)]\tLoss: 0.081524\n",
      "\n",
      "--------- Testing in global mode ---------\n",
      "size of test_loader dataset:  10000\n",
      "\n",
      "Test set: Avg. loss: 0.0003, Accuracy: 9049/10000 (90%)\n",
      "\n",
      "Train Epoch: 13 [0/50000 (0%)]\tLoss: 0.064279\n",
      "Train Epoch: 13 [25600/50000 (51%)]\tLoss: 0.064323\n",
      "\n",
      "--------- Testing in global mode ---------\n",
      "size of test_loader dataset:  10000\n",
      "\n",
      "Test set: Avg. loss: 0.0003, Accuracy: 9048/10000 (90%)\n",
      "\n",
      "Train Epoch: 14 [0/50000 (0%)]\tLoss: 0.074403\n",
      "Train Epoch: 14 [25600/50000 (51%)]\tLoss: 0.065026\n",
      "\n",
      "--------- Testing in global mode ---------\n",
      "size of test_loader dataset:  10000\n",
      "\n",
      "Test set: Avg. loss: 0.0003, Accuracy: 9046/10000 (90%)\n",
      "\n",
      "Train Epoch: 15 [0/50000 (0%)]\tLoss: 0.068268\n",
      "Train Epoch: 15 [25600/50000 (51%)]\tLoss: 0.070094\n",
      "\n",
      "--------- Testing in global mode ---------\n",
      "size of test_loader dataset:  10000\n",
      "\n",
      "Test set: Avg. loss: 0.0003, Accuracy: 9043/10000 (90%)\n",
      "\n",
      "Train Epoch: 16 [0/50000 (0%)]\tLoss: 0.065257\n",
      "Train Epoch: 16 [25600/50000 (51%)]\tLoss: 0.060155\n",
      "\n",
      "--------- Testing in global mode ---------\n",
      "size of test_loader dataset:  10000\n",
      "\n",
      "Test set: Avg. loss: 0.0003, Accuracy: 9041/10000 (90%)\n",
      "\n",
      "Train Epoch: 17 [0/50000 (0%)]\tLoss: 0.065473\n",
      "Train Epoch: 17 [25600/50000 (51%)]\tLoss: 0.052789\n",
      "\n",
      "--------- Testing in global mode ---------\n",
      "size of test_loader dataset:  10000\n",
      "\n",
      "Test set: Avg. loss: 0.0003, Accuracy: 9040/10000 (90%)\n",
      "\n",
      "Train Epoch: 18 [0/50000 (0%)]\tLoss: 0.054574\n",
      "Train Epoch: 18 [25600/50000 (51%)]\tLoss: 0.059772\n",
      "\n",
      "--------- Testing in global mode ---------\n",
      "size of test_loader dataset:  10000\n",
      "\n",
      "Test set: Avg. loss: 0.0003, Accuracy: 9040/10000 (90%)\n",
      "\n",
      "Train Epoch: 19 [0/50000 (0%)]\tLoss: 0.057253\n",
      "Train Epoch: 19 [25600/50000 (51%)]\tLoss: 0.060042\n",
      "\n",
      "--------- Testing in global mode ---------\n",
      "size of test_loader dataset:  10000\n",
      "\n",
      "Test set: Avg. loss: 0.0003, Accuracy: 9040/10000 (90%)\n",
      "\n",
      "Train Epoch: 20 [0/50000 (0%)]\tLoss: 0.069399\n",
      "Train Epoch: 20 [25600/50000 (51%)]\tLoss: 0.057831\n",
      "\n",
      "--------- Testing in global mode ---------\n",
      "size of test_loader dataset:  10000\n",
      "\n",
      "Test set: Avg. loss: 0.0003, Accuracy: 9039/10000 (90%)\n",
      "\n",
      "Train Epoch: 21 [0/50000 (0%)]\tLoss: 0.047542\n",
      "Train Epoch: 21 [25600/50000 (51%)]\tLoss: 0.067094\n",
      "\n",
      "--------- Testing in global mode ---------\n",
      "size of test_loader dataset:  10000\n",
      "\n",
      "Test set: Avg. loss: 0.0003, Accuracy: 9040/10000 (90%)\n",
      "\n",
      "Train Epoch: 22 [0/50000 (0%)]\tLoss: 0.078798\n",
      "Train Epoch: 22 [25600/50000 (51%)]\tLoss: 0.058467\n",
      "\n",
      "--------- Testing in global mode ---------\n",
      "size of test_loader dataset:  10000\n",
      "\n",
      "Test set: Avg. loss: 0.0003, Accuracy: 9042/10000 (90%)\n",
      "\n",
      "Train Epoch: 23 [0/50000 (0%)]\tLoss: 0.058753\n",
      "Train Epoch: 23 [25600/50000 (51%)]\tLoss: 0.062569\n",
      "\n",
      "--------- Testing in global mode ---------\n",
      "size of test_loader dataset:  10000\n",
      "\n",
      "Test set: Avg. loss: 0.0003, Accuracy: 9044/10000 (90%)\n",
      "\n",
      "Train Epoch: 24 [0/50000 (0%)]\tLoss: 0.048322\n",
      "Train Epoch: 24 [25600/50000 (51%)]\tLoss: 0.060467\n",
      "\n",
      "--------- Testing in global mode ---------\n",
      "size of test_loader dataset:  10000\n",
      "\n",
      "Test set: Avg. loss: 0.0003, Accuracy: 9044/10000 (90%)\n",
      "\n",
      "Train Epoch: 25 [0/50000 (0%)]\tLoss: 0.050152\n",
      "Train Epoch: 25 [25600/50000 (51%)]\tLoss: 0.054410\n",
      "\n",
      "--------- Testing in global mode ---------\n",
      "size of test_loader dataset:  10000\n",
      "\n",
      "Test set: Avg. loss: 0.0003, Accuracy: 9045/10000 (90%)\n",
      "\n",
      "Train Epoch: 26 [0/50000 (0%)]\tLoss: 0.063586\n",
      "Train Epoch: 26 [25600/50000 (51%)]\tLoss: 0.049462\n",
      "\n",
      "--------- Testing in global mode ---------\n",
      "size of test_loader dataset:  10000\n",
      "\n",
      "Test set: Avg. loss: 0.0003, Accuracy: 9045/10000 (90%)\n",
      "\n",
      "Train Epoch: 27 [0/50000 (0%)]\tLoss: 0.043810\n",
      "Train Epoch: 27 [25600/50000 (51%)]\tLoss: 0.066313\n",
      "\n",
      "--------- Testing in global mode ---------\n",
      "size of test_loader dataset:  10000\n",
      "\n",
      "Test set: Avg. loss: 0.0003, Accuracy: 9047/10000 (90%)\n",
      "\n",
      "Train Epoch: 28 [0/50000 (0%)]\tLoss: 0.058286\n",
      "Train Epoch: 28 [25600/50000 (51%)]\tLoss: 0.053142\n",
      "\n",
      "--------- Testing in global mode ---------\n",
      "size of test_loader dataset:  10000\n",
      "\n",
      "Test set: Avg. loss: 0.0003, Accuracy: 9046/10000 (90%)\n",
      "\n",
      "Train Epoch: 29 [0/50000 (0%)]\tLoss: 0.059199\n",
      "Train Epoch: 29 [25600/50000 (51%)]\tLoss: 0.053426\n",
      "\n",
      "--------- Testing in global mode ---------\n",
      "size of test_loader dataset:  10000\n",
      "\n",
      "Test set: Avg. loss: 0.0003, Accuracy: 9048/10000 (90%)\n",
      "\n",
      "Train Epoch: 30 [0/50000 (0%)]\tLoss: 0.067050\n",
      "Train Epoch: 30 [25600/50000 (51%)]\tLoss: 0.053070\n",
      "\n",
      "--------- Testing in global mode ---------\n",
      "size of test_loader dataset:  10000\n",
      "\n",
      "Test set: Avg. loss: 0.0003, Accuracy: 9047/10000 (90%)\n",
      "\n",
      "Train Epoch: 1 [0/50000 (0%)]\tLoss: 0.088863\n",
      "Train Epoch: 1 [25600/50000 (51%)]\tLoss: 0.080079\n",
      "\n",
      "--------- Testing in global mode ---------\n",
      "size of test_loader dataset:  10000\n",
      "\n",
      "Test set: Avg. loss: 0.0003, Accuracy: 9051/10000 (91%)\n",
      "\n",
      "Train Epoch: 2 [0/50000 (0%)]\tLoss: 0.093360\n",
      "Train Epoch: 2 [25600/50000 (51%)]\tLoss: 0.085492\n",
      "\n",
      "--------- Testing in global mode ---------\n",
      "size of test_loader dataset:  10000\n",
      "\n",
      "Test set: Avg. loss: 0.0003, Accuracy: 9054/10000 (91%)\n",
      "\n",
      "Train Epoch: 3 [0/50000 (0%)]\tLoss: 0.089713\n",
      "Train Epoch: 3 [25600/50000 (51%)]\tLoss: 0.085880\n",
      "\n",
      "--------- Testing in global mode ---------\n",
      "size of test_loader dataset:  10000\n",
      "\n",
      "Test set: Avg. loss: 0.0003, Accuracy: 9056/10000 (91%)\n",
      "\n",
      "Train Epoch: 4 [0/50000 (0%)]\tLoss: 0.097871\n",
      "Train Epoch: 4 [25600/50000 (51%)]\tLoss: 0.075384\n",
      "\n",
      "--------- Testing in global mode ---------\n",
      "size of test_loader dataset:  10000\n",
      "\n",
      "Test set: Avg. loss: 0.0003, Accuracy: 9061/10000 (91%)\n",
      "\n",
      "Train Epoch: 5 [0/50000 (0%)]\tLoss: 0.075559\n",
      "Train Epoch: 5 [25600/50000 (51%)]\tLoss: 0.095856\n",
      "\n",
      "--------- Testing in global mode ---------\n",
      "size of test_loader dataset:  10000\n",
      "\n",
      "Test set: Avg. loss: 0.0003, Accuracy: 9060/10000 (91%)\n",
      "\n",
      "Train Epoch: 6 [0/50000 (0%)]\tLoss: 0.076149\n",
      "Train Epoch: 6 [25600/50000 (51%)]\tLoss: 0.068957\n",
      "\n",
      "--------- Testing in global mode ---------\n",
      "size of test_loader dataset:  10000\n",
      "\n",
      "Test set: Avg. loss: 0.0003, Accuracy: 9062/10000 (91%)\n",
      "\n",
      "Train Epoch: 7 [0/50000 (0%)]\tLoss: 0.059246\n",
      "Train Epoch: 7 [25600/50000 (51%)]\tLoss: 0.071553\n",
      "\n",
      "--------- Testing in global mode ---------\n",
      "size of test_loader dataset:  10000\n",
      "\n",
      "Test set: Avg. loss: 0.0003, Accuracy: 9068/10000 (91%)\n",
      "\n",
      "Train Epoch: 8 [0/50000 (0%)]\tLoss: 0.075608\n",
      "Train Epoch: 8 [25600/50000 (51%)]\tLoss: 0.078435\n",
      "\n",
      "--------- Testing in global mode ---------\n",
      "size of test_loader dataset:  10000\n",
      "\n",
      "Test set: Avg. loss: 0.0003, Accuracy: 9073/10000 (91%)\n",
      "\n",
      "Train Epoch: 9 [0/50000 (0%)]\tLoss: 0.092692\n",
      "Train Epoch: 9 [25600/50000 (51%)]\tLoss: 0.075124\n",
      "\n",
      "--------- Testing in global mode ---------\n",
      "size of test_loader dataset:  10000\n",
      "\n",
      "Test set: Avg. loss: 0.0003, Accuracy: 9073/10000 (91%)\n",
      "\n",
      "Train Epoch: 10 [0/50000 (0%)]\tLoss: 0.078839\n",
      "Train Epoch: 10 [25600/50000 (51%)]\tLoss: 0.081890\n",
      "\n",
      "--------- Testing in global mode ---------\n",
      "size of test_loader dataset:  10000\n",
      "\n",
      "Test set: Avg. loss: 0.0003, Accuracy: 9070/10000 (91%)\n",
      "\n",
      "Train Epoch: 11 [0/50000 (0%)]\tLoss: 0.068546\n",
      "Train Epoch: 11 [25600/50000 (51%)]\tLoss: 0.069791\n",
      "\n",
      "--------- Testing in global mode ---------\n",
      "size of test_loader dataset:  10000\n",
      "\n",
      "Test set: Avg. loss: 0.0003, Accuracy: 9066/10000 (91%)\n",
      "\n",
      "Train Epoch: 12 [0/50000 (0%)]\tLoss: 0.077572\n",
      "Train Epoch: 12 [25600/50000 (51%)]\tLoss: 0.062933\n",
      "\n",
      "--------- Testing in global mode ---------\n",
      "size of test_loader dataset:  10000\n",
      "\n",
      "Test set: Avg. loss: 0.0003, Accuracy: 9070/10000 (91%)\n",
      "\n",
      "Train Epoch: 13 [0/50000 (0%)]\tLoss: 0.089173\n",
      "Train Epoch: 13 [25600/50000 (51%)]\tLoss: 0.068156\n",
      "\n",
      "--------- Testing in global mode ---------\n",
      "size of test_loader dataset:  10000\n",
      "\n",
      "Test set: Avg. loss: 0.0003, Accuracy: 9068/10000 (91%)\n",
      "\n",
      "Train Epoch: 14 [0/50000 (0%)]\tLoss: 0.070633\n",
      "Train Epoch: 14 [25600/50000 (51%)]\tLoss: 0.062428\n",
      "\n",
      "--------- Testing in global mode ---------\n",
      "size of test_loader dataset:  10000\n",
      "\n",
      "Test set: Avg. loss: 0.0003, Accuracy: 9066/10000 (91%)\n",
      "\n",
      "Train Epoch: 15 [0/50000 (0%)]\tLoss: 0.069825\n",
      "Train Epoch: 15 [25600/50000 (51%)]\tLoss: 0.071413\n",
      "\n",
      "--------- Testing in global mode ---------\n",
      "size of test_loader dataset:  10000\n",
      "\n",
      "Test set: Avg. loss: 0.0003, Accuracy: 9065/10000 (91%)\n",
      "\n",
      "Train Epoch: 16 [0/50000 (0%)]\tLoss: 0.076022\n",
      "Train Epoch: 16 [25600/50000 (51%)]\tLoss: 0.050538\n",
      "\n",
      "--------- Testing in global mode ---------\n",
      "size of test_loader dataset:  10000\n",
      "\n",
      "Test set: Avg. loss: 0.0003, Accuracy: 9067/10000 (91%)\n",
      "\n",
      "Train Epoch: 17 [0/50000 (0%)]\tLoss: 0.061727\n",
      "Train Epoch: 17 [25600/50000 (51%)]\tLoss: 0.065578\n",
      "\n",
      "--------- Testing in global mode ---------\n",
      "size of test_loader dataset:  10000\n",
      "\n",
      "Test set: Avg. loss: 0.0003, Accuracy: 9066/10000 (91%)\n",
      "\n",
      "Train Epoch: 18 [0/50000 (0%)]\tLoss: 0.076982\n",
      "Train Epoch: 18 [25600/50000 (51%)]\tLoss: 0.064926\n",
      "\n",
      "--------- Testing in global mode ---------\n",
      "size of test_loader dataset:  10000\n",
      "\n",
      "Test set: Avg. loss: 0.0003, Accuracy: 9069/10000 (91%)\n",
      "\n",
      "Train Epoch: 19 [0/50000 (0%)]\tLoss: 0.059013\n",
      "Train Epoch: 19 [25600/50000 (51%)]\tLoss: 0.059925\n",
      "\n",
      "--------- Testing in global mode ---------\n",
      "size of test_loader dataset:  10000\n",
      "\n",
      "Test set: Avg. loss: 0.0003, Accuracy: 9069/10000 (91%)\n",
      "\n",
      "Train Epoch: 20 [0/50000 (0%)]\tLoss: 0.074368\n",
      "Train Epoch: 20 [25600/50000 (51%)]\tLoss: 0.069893\n",
      "\n",
      "--------- Testing in global mode ---------\n",
      "size of test_loader dataset:  10000\n",
      "\n",
      "Test set: Avg. loss: 0.0003, Accuracy: 9068/10000 (91%)\n",
      "\n",
      "Train Epoch: 21 [0/50000 (0%)]\tLoss: 0.056733\n",
      "Train Epoch: 21 [25600/50000 (51%)]\tLoss: 0.061063\n",
      "\n",
      "--------- Testing in global mode ---------\n",
      "size of test_loader dataset:  10000\n",
      "\n",
      "Test set: Avg. loss: 0.0003, Accuracy: 9065/10000 (91%)\n",
      "\n",
      "Train Epoch: 22 [0/50000 (0%)]\tLoss: 0.049868\n",
      "Train Epoch: 22 [25600/50000 (51%)]\tLoss: 0.063242\n",
      "\n",
      "--------- Testing in global mode ---------\n",
      "size of test_loader dataset:  10000\n",
      "\n",
      "Test set: Avg. loss: 0.0003, Accuracy: 9067/10000 (91%)\n",
      "\n",
      "Train Epoch: 23 [0/50000 (0%)]\tLoss: 0.069445\n",
      "Train Epoch: 23 [25600/50000 (51%)]\tLoss: 0.057784\n",
      "\n",
      "--------- Testing in global mode ---------\n",
      "size of test_loader dataset:  10000\n",
      "\n",
      "Test set: Avg. loss: 0.0003, Accuracy: 9065/10000 (91%)\n",
      "\n",
      "Train Epoch: 24 [0/50000 (0%)]\tLoss: 0.049788\n",
      "Train Epoch: 24 [25600/50000 (51%)]\tLoss: 0.061648\n",
      "\n",
      "--------- Testing in global mode ---------\n",
      "size of test_loader dataset:  10000\n",
      "\n",
      "Test set: Avg. loss: 0.0003, Accuracy: 9064/10000 (91%)\n",
      "\n",
      "Train Epoch: 25 [0/50000 (0%)]\tLoss: 0.067175\n",
      "Train Epoch: 25 [25600/50000 (51%)]\tLoss: 0.053345\n",
      "\n",
      "--------- Testing in global mode ---------\n",
      "size of test_loader dataset:  10000\n",
      "\n",
      "Test set: Avg. loss: 0.0003, Accuracy: 9066/10000 (91%)\n",
      "\n",
      "Train Epoch: 26 [0/50000 (0%)]\tLoss: 0.053120\n",
      "Train Epoch: 26 [25600/50000 (51%)]\tLoss: 0.060254\n",
      "\n",
      "--------- Testing in global mode ---------\n",
      "size of test_loader dataset:  10000\n",
      "\n",
      "Test set: Avg. loss: 0.0003, Accuracy: 9067/10000 (91%)\n",
      "\n",
      "Train Epoch: 27 [0/50000 (0%)]\tLoss: 0.065097\n",
      "Train Epoch: 27 [25600/50000 (51%)]\tLoss: 0.068547\n",
      "\n",
      "--------- Testing in global mode ---------\n",
      "size of test_loader dataset:  10000\n",
      "\n",
      "Test set: Avg. loss: 0.0003, Accuracy: 9067/10000 (91%)\n",
      "\n",
      "Train Epoch: 28 [0/50000 (0%)]\tLoss: 0.059631\n",
      "Train Epoch: 28 [25600/50000 (51%)]\tLoss: 0.057361\n",
      "\n",
      "--------- Testing in global mode ---------\n",
      "size of test_loader dataset:  10000\n",
      "\n",
      "Test set: Avg. loss: 0.0003, Accuracy: 9068/10000 (91%)\n",
      "\n",
      "Train Epoch: 29 [0/50000 (0%)]\tLoss: 0.050734\n",
      "Train Epoch: 29 [25600/50000 (51%)]\tLoss: 0.067965\n",
      "\n",
      "--------- Testing in global mode ---------\n",
      "size of test_loader dataset:  10000\n",
      "\n",
      "Test set: Avg. loss: 0.0003, Accuracy: 9069/10000 (91%)\n",
      "\n",
      "Train Epoch: 30 [0/50000 (0%)]\tLoss: 0.051376\n",
      "Train Epoch: 30 [25600/50000 (51%)]\tLoss: 0.061955\n",
      "\n",
      "--------- Testing in global mode ---------\n",
      "size of test_loader dataset:  10000\n",
      "\n",
      "Test set: Avg. loss: 0.0003, Accuracy: 9067/10000 (91%)\n",
      "\n",
      "========================================================\n",
      "accuracies [90.47, 90.67]\n",
      "=========================================================\n",
      "layer features.0.weight_orig has #params  1728\n",
      "layer features.3.weight_orig has #params  73728\n",
      "layer features.6.weight_orig has #params  294912\n",
      "layer features.8.weight_orig has #params  589824\n",
      "layer features.11.weight_orig has #params  1179648\n",
      "layer features.13.weight_orig has #params  2359296\n",
      "layer features.16.weight_orig has #params  2359296\n",
      "layer features.18.weight_orig has #params  2359296\n",
      "layer classifier.weight has #params  5120\n",
      "Activation Timer start\n",
      "Activation Timer ends\n",
      "------- Geometric Ensembling -------\n",
      "Timer start\n",
      "Previous layer shape is  None\n",
      "Processing the coordinates to form ground_metric\n",
      "dont leave off the squaring of the ground metric\n",
      "returns a uniform measure of cardinality:  64\n",
      "returns a uniform measure of cardinality:  64\n",
      "the transport map is  tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')\n",
      "Ratio of trace to the matrix sum:  tensor(0.0469, device='cuda:0')\n",
      "Here, trace is 3.0 and matrix sum is 64.0 \n",
      "this is past correction for weight mode\n",
      "Shape of aligned wt is  torch.Size([64, 3, 9])\n",
      "Shape of fc_layer0_weight_data is  torch.Size([64, 3, 9])\n",
      "Previous layer shape is  torch.Size([64, 3, 3, 3])\n",
      "shape of layer: model 0 torch.Size([128, 64, 9])\n",
      "shape of layer: model 1 torch.Size([128, 64, 9])\n",
      "shape of previous transport map torch.Size([64, 64])\n",
      "Processing the coordinates to form ground_metric\n",
      "dont leave off the squaring of the ground metric\n",
      "returns a uniform measure of cardinality:  128\n",
      "returns a uniform measure of cardinality:  128\n",
      "the transport map is  tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        ...,\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0078],\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],\n",
      "       device='cuda:0')\n",
      "Ratio of trace to the matrix sum:  tensor(0.0078, device='cuda:0')\n",
      "Here, trace is 1.0 and matrix sum is 128.0 \n",
      "this is past correction for weight mode\n",
      "Shape of aligned wt is  torch.Size([128, 64, 9])\n",
      "Shape of fc_layer0_weight_data is  torch.Size([128, 64, 9])\n",
      "Previous layer shape is  torch.Size([128, 64, 3, 3])\n",
      "shape of layer: model 0 torch.Size([256, 128, 9])\n",
      "shape of layer: model 1 torch.Size([256, 128, 9])\n",
      "shape of previous transport map torch.Size([128, 128])\n",
      "Processing the coordinates to form ground_metric\n",
      "dont leave off the squaring of the ground metric\n",
      "returns a uniform measure of cardinality:  256\n",
      "returns a uniform measure of cardinality:  256\n",
      "the transport map is  tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')\n",
      "Ratio of trace to the matrix sum:  tensor(0., device='cuda:0')\n",
      "Here, trace is 0.0 and matrix sum is 256.0 \n",
      "this is past correction for weight mode\n",
      "Shape of aligned wt is  torch.Size([256, 128, 9])\n",
      "Shape of fc_layer0_weight_data is  torch.Size([256, 128, 9])\n",
      "Previous layer shape is  torch.Size([256, 128, 3, 3])\n",
      "shape of layer: model 0 torch.Size([256, 256, 9])\n",
      "shape of layer: model 1 torch.Size([256, 256, 9])\n",
      "shape of previous transport map torch.Size([256, 256])\n",
      "Processing the coordinates to form ground_metric\n",
      "dont leave off the squaring of the ground metric\n",
      "returns a uniform measure of cardinality:  256\n",
      "returns a uniform measure of cardinality:  256\n",
      "the transport map is  tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        ...,\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.0039, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],\n",
      "       device='cuda:0')\n",
      "Ratio of trace to the matrix sum:  tensor(0.0078, device='cuda:0')\n",
      "Here, trace is 2.0 and matrix sum is 256.0 \n",
      "this is past correction for weight mode\n",
      "Shape of aligned wt is  torch.Size([256, 256, 9])\n",
      "Shape of fc_layer0_weight_data is  torch.Size([256, 256, 9])\n",
      "Previous layer shape is  torch.Size([256, 256, 3, 3])\n",
      "shape of layer: model 0 torch.Size([512, 256, 9])\n",
      "shape of layer: model 1 torch.Size([512, 256, 9])\n",
      "shape of previous transport map torch.Size([256, 256])\n",
      "Processing the coordinates to form ground_metric\n",
      "dont leave off the squaring of the ground metric\n",
      "returns a uniform measure of cardinality:  512\n",
      "returns a uniform measure of cardinality:  512\n",
      "the transport map is  tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')\n",
      "Ratio of trace to the matrix sum:  tensor(0.0039, device='cuda:0')\n",
      "Here, trace is 2.0 and matrix sum is 512.0 \n",
      "this is past correction for weight mode\n",
      "Shape of aligned wt is  torch.Size([512, 256, 9])\n",
      "Shape of fc_layer0_weight_data is  torch.Size([512, 256, 9])\n",
      "Previous layer shape is  torch.Size([512, 256, 3, 3])\n",
      "shape of layer: model 0 torch.Size([512, 512, 9])\n",
      "shape of layer: model 1 torch.Size([512, 512, 9])\n",
      "shape of previous transport map torch.Size([512, 512])\n",
      "Processing the coordinates to form ground_metric\n",
      "dont leave off the squaring of the ground metric\n",
      "returns a uniform measure of cardinality:  512\n",
      "returns a uniform measure of cardinality:  512\n",
      "the transport map is  tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')\n",
      "Ratio of trace to the matrix sum:  tensor(0.0020, device='cuda:0')\n",
      "Here, trace is 1.0 and matrix sum is 512.0 \n",
      "this is past correction for weight mode\n",
      "Shape of aligned wt is  torch.Size([512, 512, 9])\n",
      "Shape of fc_layer0_weight_data is  torch.Size([512, 512, 9])\n",
      "Previous layer shape is  torch.Size([512, 512, 3, 3])\n",
      "shape of layer: model 0 torch.Size([512, 512, 9])\n",
      "shape of layer: model 1 torch.Size([512, 512, 9])\n",
      "shape of previous transport map torch.Size([512, 512])\n",
      "Processing the coordinates to form ground_metric\n",
      "dont leave off the squaring of the ground metric\n",
      "returns a uniform measure of cardinality:  512\n",
      "returns a uniform measure of cardinality:  512\n",
      "the transport map is  tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')\n",
      "Ratio of trace to the matrix sum:  tensor(0.0020, device='cuda:0')\n",
      "Here, trace is 1.0 and matrix sum is 512.0 \n",
      "this is past correction for weight mode\n",
      "Shape of aligned wt is  torch.Size([512, 512, 9])\n",
      "Shape of fc_layer0_weight_data is  torch.Size([512, 512, 9])\n",
      "Previous layer shape is  torch.Size([512, 512, 3, 3])\n",
      "shape of layer: model 0 torch.Size([512, 512, 9])\n",
      "shape of layer: model 1 torch.Size([512, 512, 9])\n",
      "shape of previous transport map torch.Size([512, 512])\n",
      "Processing the coordinates to form ground_metric\n",
      "dont leave off the squaring of the ground metric\n",
      "returns a uniform measure of cardinality:  512\n",
      "returns a uniform measure of cardinality:  512\n",
      "the transport map is  tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        [0.0020, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        ...,\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],\n",
      "       device='cuda:0')\n",
      "Ratio of trace to the matrix sum:  tensor(0., device='cuda:0')\n",
      "Here, trace is 0.0 and matrix sum is 512.0 \n",
      "this is past correction for weight mode\n",
      "Shape of aligned wt is  torch.Size([512, 512, 9])\n",
      "Shape of fc_layer0_weight_data is  torch.Size([512, 512, 9])\n",
      "Previous layer shape is  torch.Size([512, 512, 3, 3])\n",
      "shape of layer: model 0 torch.Size([10, 512])\n",
      "shape of layer: model 1 torch.Size([10, 512])\n",
      "shape of previous transport map torch.Size([512, 512])\n",
      "Processing the coordinates to form ground_metric\n",
      "dont leave off the squaring of the ground metric\n",
      "ground metric is  tensor([[1.4271, 2.4922, 2.7862, 2.8781, 2.7431, 2.8863, 2.7321, 2.6394, 2.5695,\n",
      "         2.5972],\n",
      "        [2.4934, 1.3270, 2.6780, 2.5959, 2.7628, 2.5542, 2.4201, 2.5251, 2.3091,\n",
      "         2.4135],\n",
      "        [2.7651, 2.6757, 1.3856, 2.9520, 2.9119, 2.8756, 2.7317, 2.8432, 2.7539,\n",
      "         2.7912],\n",
      "        [2.8407, 2.6840, 2.8979, 1.5460, 2.8659, 2.9219, 2.7620, 2.8031, 2.8222,\n",
      "         2.6401],\n",
      "        [2.7604, 2.6006, 2.8687, 2.8505, 1.5070, 2.7499, 2.7895, 2.6856, 2.7087,\n",
      "         2.7341],\n",
      "        [2.9245, 2.5438, 2.9523, 2.8471, 2.7322, 1.5530, 2.7822, 2.7347, 2.8033,\n",
      "         2.6742],\n",
      "        [2.6821, 2.4485, 2.7233, 2.9156, 2.6525, 2.8568, 1.3945, 2.6925, 2.5020,\n",
      "         2.5591],\n",
      "        [2.7440, 2.5532, 2.7847, 2.7952, 2.7197, 2.6634, 2.7222, 1.3944, 2.5630,\n",
      "         2.5156],\n",
      "        [2.6027, 2.3700, 2.7749, 2.6922, 2.7075, 2.8337, 2.5358, 2.6410, 1.3028,\n",
      "         2.4081],\n",
      "        [2.5963, 2.2761, 2.7862, 2.7621, 2.6737, 2.7317, 2.5508, 2.5323, 2.5252,\n",
      "         1.2894]], device='cuda:0', grad_fn=<PowBackward0>)\n",
      "returns a uniform measure of cardinality:  10\n",
      "returns a uniform measure of cardinality:  10\n",
      "the transport map is  tensor([[0.1000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0000, 0.1000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0000, 0.0000, 0.1000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.1000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.1000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.1000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.1000, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.1000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.1000,\n",
      "         0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.1000]], device='cuda:0')\n",
      "Ratio of trace to the matrix sum:  tensor(1., device='cuda:0')\n",
      "Here, trace is 9.99755859375 and matrix sum is 9.99755859375 \n",
      "this is past correction for weight mode\n",
      "Shape of aligned wt is  torch.Size([10, 512])\n",
      "Shape of fc_layer0_weight_data is  torch.Size([10, 512])\n",
      "using independent method\n",
      "in _make_layers [Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), AvgPool2d(kernel_size=1, stride=1, padding=0)]\n",
      "Relu Inplace is  False\n",
      "model parameters are \n",
      " [torch.Size([64, 3, 3, 3]), torch.Size([128, 64, 3, 3]), torch.Size([256, 128, 3, 3]), torch.Size([256, 256, 3, 3]), torch.Size([512, 256, 3, 3]), torch.Size([512, 512, 3, 3]), torch.Size([512, 512, 3, 3]), torch.Size([512, 512, 3, 3]), torch.Size([10, 512])]\n",
      "\n",
      "--------- Testing in global mode ---------\n",
      "size of test_loader dataset:  10000\n",
      "\n",
      "Test set: Avg. loss: 0.0023, Accuracy: 1000/10000 (10%)\n",
      "\n",
      "len of model parameters and avg aligned layers is  9 9\n",
      "len of model_state_dict is  9\n",
      "len of param_list is  9\n",
      "\n",
      "--------- Testing in global mode ---------\n",
      "size of test_loader dataset:  10000\n",
      "\n",
      "Test set: Avg. loss: 0.0014, Accuracy: 8692/10000 (87%)\n",
      "\n",
      "===============================================\n",
      "===============================================\n",
      "After Ensemble: 86.92\n",
      "===============================================\n",
      "===============================================\n",
      "Timer ends\n",
      "Time taken for geometric ensembling is 3.268563900142908 seconds\n",
      "------- Prediction based ensembling -------\n",
      "\n",
      "Test set: Avg. loss: 0.0003, Accuracy: 9126/10000 (91%)\n",
      "\n",
      "------- Naive ensembling of weights -------\n",
      "[torch.Size([64, 3, 3, 3]), torch.Size([64, 3, 3, 3])]\n",
      "torch.Size([64, 3, 3, 3])\n",
      "[torch.Size([128, 64, 3, 3]), torch.Size([128, 64, 3, 3])]\n",
      "torch.Size([128, 64, 3, 3])\n",
      "[torch.Size([256, 128, 3, 3]), torch.Size([256, 128, 3, 3])]\n",
      "torch.Size([256, 128, 3, 3])\n",
      "[torch.Size([256, 256, 3, 3]), torch.Size([256, 256, 3, 3])]\n",
      "torch.Size([256, 256, 3, 3])\n",
      "[torch.Size([512, 256, 3, 3]), torch.Size([512, 256, 3, 3])]\n",
      "torch.Size([512, 256, 3, 3])\n",
      "[torch.Size([512, 512, 3, 3]), torch.Size([512, 512, 3, 3])]\n",
      "torch.Size([512, 512, 3, 3])\n",
      "[torch.Size([512, 512, 3, 3]), torch.Size([512, 512, 3, 3])]\n",
      "torch.Size([512, 512, 3, 3])\n",
      "[torch.Size([512, 512, 3, 3]), torch.Size([512, 512, 3, 3])]\n",
      "torch.Size([512, 512, 3, 3])\n",
      "[torch.Size([10, 512]), torch.Size([10, 512])]\n",
      "torch.Size([10, 512])\n",
      "in _make_layers [Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), AvgPool2d(kernel_size=1, stride=1, padding=0)]\n",
      "Relu Inplace is  False\n",
      "model parameters are \n",
      " [torch.Size([64, 3, 3, 3]), torch.Size([128, 64, 3, 3]), torch.Size([256, 128, 3, 3]), torch.Size([256, 256, 3, 3]), torch.Size([512, 256, 3, 3]), torch.Size([512, 512, 3, 3]), torch.Size([512, 512, 3, 3]), torch.Size([512, 512, 3, 3]), torch.Size([10, 512])]\n",
      "\n",
      "--------- Testing in global mode ---------\n",
      "size of test_loader dataset:  10000\n",
      "\n",
      "Test set: Avg. loss: 0.0023, Accuracy: 904/10000 (9%)\n",
      "\n",
      "\n",
      "--------- Testing in global mode ---------\n",
      "size of test_loader dataset:  10000\n",
      "\n",
      "Test set: Avg. loss: 0.0023, Accuracy: 1610/10000 (16%)\n",
      "\n",
      "-------- Retraining the models ---------\n",
      "Retraining model :  model_0\n",
      "num_epochs---------------+++))))))+++ 300\n",
      "num_epochs---------------+++))))))+++ 300\n",
      "lr is  0.05\n",
      "number of epochs would be  30\n",
      "num_epochs-------------- 30\n",
      "Epoch 000\n",
      "/root/miniconda3/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:129: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\"Detected call of `lr_scheduler.step()` before `optimizer.step()`. \"\n",
      "/root/miniconda3/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:154: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "accuracy: {'epoch': 0, 'value': 0.9152000000572206} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 0, 'value': 0.2573326487636566} ({'split': 'train'})\n",
      "accuracy: {'epoch': 0, 'value': 0.845900046825409} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 0, 'value': 0.48958107829093933} ({'split': 'test'})\n",
      "Epoch 001\n",
      "accuracy: {'epoch': 1, 'value': 0.9382599999809267} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 1, 'value': 0.18455429563522344} ({'split': 'train'})\n",
      "accuracy: {'epoch': 1, 'value': 0.8386000454425812} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 1, 'value': 0.5406209081411362} ({'split': 'test'})\n",
      "Epoch 002\n",
      "accuracy: {'epoch': 2, 'value': 0.95368} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 2, 'value': 0.13821284977912898} ({'split': 'train'})\n",
      "accuracy: {'epoch': 2, 'value': 0.8605000317096709} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 2, 'value': 0.4791329145431519} ({'split': 'test'})\n",
      "Epoch 003\n",
      "accuracy: {'epoch': 3, 'value': 0.9675400000381476} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 3, 'value': 0.09821827132344246} ({'split': 'train'})\n",
      "accuracy: {'epoch': 3, 'value': 0.847100031375885} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 3, 'value': 0.6057005524635315} ({'split': 'test'})\n",
      "Epoch 004\n",
      "accuracy: {'epoch': 4, 'value': 0.9749200000572206} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 4, 'value': 0.07695993330001831} ({'split': 'train'})\n",
      "accuracy: {'epoch': 4, 'value': 0.8703000485897064} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 4, 'value': 0.4759733289480209} ({'split': 'test'})\n",
      "Epoch 005\n",
      "accuracy: {'epoch': 5, 'value': 0.9786000000572208} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 5, 'value': 0.06710543148517611} ({'split': 'train'})\n",
      "accuracy: {'epoch': 5, 'value': 0.8683000445365906} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 5, 'value': 0.5575224369764328} ({'split': 'test'})\n",
      "Epoch 006\n",
      "accuracy: {'epoch': 6, 'value': 0.9791400000000002} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 6, 'value': 0.06468789183616641} ({'split': 'train'})\n",
      "accuracy: {'epoch': 6, 'value': 0.854300045967102} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 6, 'value': 0.6146420121192933} ({'split': 'test'})\n",
      "Epoch 007\n",
      "accuracy: {'epoch': 7, 'value': 0.9817600000381469} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 7, 'value': 0.057966810228824606} ({'split': 'train'})\n",
      "accuracy: {'epoch': 7, 'value': 0.8583000361919404} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 7, 'value': 0.5900658071041106} ({'split': 'test'})\n",
      "Epoch 008\n",
      "accuracy: {'epoch': 8, 'value': 0.9825800000190735} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 8, 'value': 0.053353335274457944} ({'split': 'train'})\n",
      "accuracy: {'epoch': 8, 'value': 0.8596000432968139} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 8, 'value': 0.5483353108167649} ({'split': 'test'})\n",
      "Epoch 009\n",
      "accuracy: {'epoch': 9, 'value': 0.9898600000190735} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 9, 'value': 0.033669225981235514} ({'split': 'train'})\n",
      "accuracy: {'epoch': 9, 'value': 0.8620000422000885} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 9, 'value': 0.53200224339962} ({'split': 'test'})\n",
      "Epoch 010\n",
      "accuracy: {'epoch': 10, 'value': 0.9891599999999999} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 10, 'value': 0.03427870583117008} ({'split': 'train'})\n",
      "accuracy: {'epoch': 10, 'value': 0.8778000414371491} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 10, 'value': 0.5089761853218078} ({'split': 'test'})\n",
      "Epoch 011\n",
      "accuracy: {'epoch': 11, 'value': 0.9908800000000001} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 11, 'value': 0.03086342088997363} ({'split': 'train'})\n",
      "accuracy: {'epoch': 11, 'value': 0.8595000267028808} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 11, 'value': 0.5946291238069534} ({'split': 'test'})\n",
      "Epoch 012\n",
      "accuracy: {'epoch': 12, 'value': 0.989460000038147} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 12, 'value': 0.036124360507726674} ({'split': 'train'})\n",
      "accuracy: {'epoch': 12, 'value': 0.8610000371932984} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 12, 'value': 0.5177317142486573} ({'split': 'test'})\n",
      "Epoch 013\n",
      "accuracy: {'epoch': 13, 'value': 0.9903000000000003} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 13, 'value': 0.03169752392351628} ({'split': 'train'})\n",
      "accuracy: {'epoch': 13, 'value': 0.8625000417232513} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 13, 'value': 0.546364089846611} ({'split': 'test'})\n",
      "Epoch 014\n",
      "accuracy: {'epoch': 14, 'value': 0.9877599999999999} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 14, 'value': 0.04067933338701725} ({'split': 'train'})\n",
      "accuracy: {'epoch': 14, 'value': 0.8622000396251679} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 14, 'value': 0.5471359699964524} ({'split': 'test'})\n",
      "Epoch 015\n",
      "accuracy: {'epoch': 15, 'value': 0.9901000000190735} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 15, 'value': 0.031597539637088785} ({'split': 'train'})\n",
      "accuracy: {'epoch': 15, 'value': 0.8654000461101532} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 15, 'value': 0.534986475110054} ({'split': 'test'})\n",
      "Epoch 016\n",
      "accuracy: {'epoch': 16, 'value': 0.9909400000381472} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 16, 'value': 0.030707070574760433} ({'split': 'train'})\n",
      "accuracy: {'epoch': 16, 'value': 0.8600000381469727} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 16, 'value': 0.5582717955112457} ({'split': 'test'})\n",
      "Epoch 017\n",
      "accuracy: {'epoch': 17, 'value': 0.9877199999809266} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 17, 'value': 0.04048573212623596} ({'split': 'train'})\n",
      "accuracy: {'epoch': 17, 'value': 0.8595000386238099} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 17, 'value': 0.6174550503492355} ({'split': 'test'})\n",
      "Epoch 018\n",
      "accuracy: {'epoch': 18, 'value': 0.9858799999999998} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 18, 'value': 0.04454881580471993} ({'split': 'train'})\n",
      "accuracy: {'epoch': 18, 'value': 0.8615000486373903} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 18, 'value': 0.5270050674676896} ({'split': 'test'})\n",
      "Epoch 019\n",
      "accuracy: {'epoch': 19, 'value': 0.98844000005722} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 19, 'value': 0.03719635285377503} ({'split': 'train'})\n",
      "accuracy: {'epoch': 19, 'value': 0.8633000373840332} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 19, 'value': 0.5528099119663238} ({'split': 'test'})\n",
      "Epoch 020\n",
      "accuracy: {'epoch': 20, 'value': 0.98368} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 20, 'value': 0.05335847288668157} ({'split': 'train'})\n",
      "accuracy: {'epoch': 20, 'value': 0.8493000388145446} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 20, 'value': 0.5953926086425781} ({'split': 'test'})\n",
      "Epoch 021\n",
      "accuracy: {'epoch': 21, 'value': 0.9876200000190731} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 21, 'value': 0.03861548553526401} ({'split': 'train'})\n",
      "accuracy: {'epoch': 21, 'value': 0.8639000415802002} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 21, 'value': 0.5931665837764739} ({'split': 'test'})\n",
      "Epoch 022\n",
      "accuracy: {'epoch': 22, 'value': 0.9881799999999998} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 22, 'value': 0.03795458248198036} ({'split': 'train'})\n",
      "accuracy: {'epoch': 22, 'value': 0.8585000455379487} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 22, 'value': 0.6157381594181062} ({'split': 'test'})\n",
      "Epoch 023\n",
      "accuracy: {'epoch': 23, 'value': 0.9874800000190737} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 23, 'value': 0.040454833548069} ({'split': 'train'})\n",
      "accuracy: {'epoch': 23, 'value': 0.8657000422477722} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 23, 'value': 0.5103138118982316} ({'split': 'test'})\n",
      "Epoch 024\n",
      "accuracy: {'epoch': 24, 'value': 0.9894800000000002} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 24, 'value': 0.034635194418281307} ({'split': 'train'})\n",
      "accuracy: {'epoch': 24, 'value': 0.8574000358581543} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 24, 'value': 0.6252937495708467} ({'split': 'test'})\n",
      "Epoch 025\n",
      "accuracy: {'epoch': 25, 'value': 0.9925599999999999} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 25, 'value': 0.025624177465736857} ({'split': 'train'})\n",
      "accuracy: {'epoch': 25, 'value': 0.8613000392913819} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 25, 'value': 0.6069830507040024} ({'split': 'test'})\n",
      "Epoch 026\n",
      "accuracy: {'epoch': 26, 'value': 0.9889800000190734} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 26, 'value': 0.03657554663658142} ({'split': 'train'})\n",
      "accuracy: {'epoch': 26, 'value': 0.8533000409603119} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 26, 'value': 0.5854728460311889} ({'split': 'test'})\n",
      "Epoch 027\n",
      "accuracy: {'epoch': 27, 'value': 0.9917400000190733} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 27, 'value': 0.02638020851135253} ({'split': 'train'})\n",
      "accuracy: {'epoch': 27, 'value': 0.8586000442504882} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 27, 'value': 0.5831950426101685} ({'split': 'test'})\n",
      "Epoch 028\n",
      "accuracy: {'epoch': 28, 'value': 0.9920600000000003} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 28, 'value': 0.02743642715990544} ({'split': 'train'})\n",
      "accuracy: {'epoch': 28, 'value': 0.8461000382900239} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 28, 'value': 0.615286123752594} ({'split': 'test'})\n",
      "Epoch 029\n",
      "accuracy: {'epoch': 29, 'value': 0.9903400000572203} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 29, 'value': 0.030538869609832764} ({'split': 'train'})\n",
      "accuracy: {'epoch': 29, 'value': 0.8526000440120697} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 29, 'value': 0.6188903331756591} ({'split': 'test'})\n",
      "mean_test_accuracy <cifar_utils.accumulators.Mean object at 0x7f500e286520>\n",
      "QWERTY: Enter Cifar 2, acc 0.8778000414371491\n",
      "Retraining model :  model_1\n",
      "num_epochs---------------+++))))))+++ 30\n",
      "num_epochs---------------+++))))))+++ 30\n",
      "lr is  0.05\n",
      "number of epochs would be  30\n",
      "num_epochs-------------- 30\n",
      "Epoch 000\n",
      "accuracy: {'epoch': 0, 'value': 0.9147000000572203} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 0, 'value': 0.260452842645645} ({'split': 'train'})\n",
      "accuracy: {'epoch': 0, 'value': 0.8491000413894654} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 0, 'value': 0.4883124232292176} ({'split': 'test'})\n",
      "Epoch 001\n",
      "accuracy: {'epoch': 1, 'value': 0.9420000000190729} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 1, 'value': 0.17121312939643862} ({'split': 'train'})\n",
      "accuracy: {'epoch': 1, 'value': 0.8481000363826751} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 1, 'value': 0.5072705805301666} ({'split': 'test'})\n",
      "Epoch 002\n",
      "accuracy: {'epoch': 2, 'value': 0.9565000000000002} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 2, 'value': 0.13053088589906703} ({'split': 'train'})\n",
      "accuracy: {'epoch': 2, 'value': 0.8478000402450562} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 2, 'value': 0.565441381931305} ({'split': 'test'})\n",
      "Epoch 003\n",
      "accuracy: {'epoch': 3, 'value': 0.9659000000381468} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 3, 'value': 0.10287666073322292} ({'split': 'train'})\n",
      "accuracy: {'epoch': 3, 'value': 0.8527000308036804} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 3, 'value': 0.5231015652418137} ({'split': 'test'})\n",
      "Epoch 004\n",
      "accuracy: {'epoch': 4, 'value': 0.9748999999809266} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 4, 'value': 0.07679137050151828} ({'split': 'train'})\n",
      "accuracy: {'epoch': 4, 'value': 0.8512000501155854} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 4, 'value': 0.5938333451747895} ({'split': 'test'})\n",
      "Epoch 005\n",
      "accuracy: {'epoch': 5, 'value': 0.97592} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 5, 'value': 0.07297497313499446} ({'split': 'train'})\n",
      "accuracy: {'epoch': 5, 'value': 0.861100047826767} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 5, 'value': 0.5337436527013778} ({'split': 'test'})\n",
      "Epoch 006\n",
      "accuracy: {'epoch': 6, 'value': 0.9805600000381468} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 6, 'value': 0.060080642378330225} ({'split': 'train'})\n",
      "accuracy: {'epoch': 6, 'value': 0.8658000409603118} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 6, 'value': 0.5252657413482666} ({'split': 'test'})\n",
      "Epoch 007\n",
      "accuracy: {'epoch': 7, 'value': 0.9866199999999996} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 7, 'value': 0.041937977937459965} ({'split': 'train'})\n",
      "accuracy: {'epoch': 7, 'value': 0.8633000373840333} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 7, 'value': 0.5633464932441713} ({'split': 'test'})\n",
      "Epoch 008\n",
      "accuracy: {'epoch': 8, 'value': 0.9872399999999997} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 8, 'value': 0.040232415927052505} ({'split': 'train'})\n",
      "accuracy: {'epoch': 8, 'value': 0.8692000508308411} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 8, 'value': 0.5343274444341659} ({'split': 'test'})\n",
      "Epoch 009\n",
      "accuracy: {'epoch': 9, 'value': 0.9875000000190733} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 9, 'value': 0.03961496969223023} ({'split': 'train'})\n",
      "accuracy: {'epoch': 9, 'value': 0.8769000351428984} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 9, 'value': 0.48420770764350896} ({'split': 'test'})\n",
      "Epoch 010\n",
      "accuracy: {'epoch': 10, 'value': 0.9887000000000002} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 10, 'value': 0.037295006837546836} ({'split': 'train'})\n",
      "accuracy: {'epoch': 10, 'value': 0.8684000432491302} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 10, 'value': 0.5331309109926224} ({'split': 'test'})\n",
      "Epoch 011\n",
      "accuracy: {'epoch': 11, 'value': 0.9895999999999997} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 11, 'value': 0.03426178315222263} ({'split': 'train'})\n",
      "accuracy: {'epoch': 11, 'value': 0.8629000365734101} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 11, 'value': 0.5369384407997132} ({'split': 'test'})\n",
      "Epoch 012\n",
      "accuracy: {'epoch': 12, 'value': 0.9916400000190733} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 12, 'value': 0.0293814580720663} ({'split': 'train'})\n",
      "accuracy: {'epoch': 12, 'value': 0.8669000387191772} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 12, 'value': 0.52722969353199} ({'split': 'test'})\n",
      "Epoch 013\n",
      "accuracy: {'epoch': 13, 'value': 0.9889599999809262} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 13, 'value': 0.035142258057594286} ({'split': 'train'})\n",
      "accuracy: {'epoch': 13, 'value': 0.855500042438507} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 13, 'value': 0.6242181479930877} ({'split': 'test'})\n",
      "Epoch 014\n",
      "accuracy: {'epoch': 14, 'value': 0.9881200000190735} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 14, 'value': 0.038431129592657064} ({'split': 'train'})\n",
      "accuracy: {'epoch': 14, 'value': 0.8573000431060791} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 14, 'value': 0.6184102654457093} ({'split': 'test'})\n",
      "Epoch 015\n",
      "accuracy: {'epoch': 15, 'value': 0.9903800000190738} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 15, 'value': 0.03163159935951232} ({'split': 'train'})\n",
      "accuracy: {'epoch': 15, 'value': 0.8628000438213348} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 15, 'value': 0.5641337037086487} ({'split': 'test'})\n",
      "Epoch 016\n",
      "accuracy: {'epoch': 16, 'value': 0.9865800000190734} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 16, 'value': 0.04232359866619112} ({'split': 'train'})\n",
      "accuracy: {'epoch': 16, 'value': 0.8512000441551208} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 16, 'value': 0.6579134702682495} ({'split': 'test'})\n",
      "Epoch 017\n",
      "accuracy: {'epoch': 17, 'value': 0.9901000000190733} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 17, 'value': 0.03285635039687157} ({'split': 'train'})\n",
      "accuracy: {'epoch': 17, 'value': 0.871500039100647} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 17, 'value': 0.5071184754371644} ({'split': 'test'})\n",
      "Epoch 018\n",
      "accuracy: {'epoch': 18, 'value': 0.9909800000190733} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 18, 'value': 0.03148969022452831} ({'split': 'train'})\n",
      "accuracy: {'epoch': 18, 'value': 0.8588000416755676} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 18, 'value': 0.5696226537227631} ({'split': 'test'})\n",
      "Epoch 019\n",
      "accuracy: {'epoch': 19, 'value': 0.9903400000190736} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 19, 'value': 0.03300848416686057} ({'split': 'train'})\n",
      "accuracy: {'epoch': 19, 'value': 0.8708000361919404} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 19, 'value': 0.5191742807626724} ({'split': 'test'})\n",
      "Epoch 020\n",
      "accuracy: {'epoch': 20, 'value': 0.9911000000190737} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 20, 'value': 0.029930600541830058} ({'split': 'train'})\n",
      "accuracy: {'epoch': 20, 'value': 0.8609000384807588} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 20, 'value': 0.574293303489685} ({'split': 'test'})\n",
      "Epoch 021\n",
      "accuracy: {'epoch': 21, 'value': 0.9919400000190735} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 21, 'value': 0.02680735088706016} ({'split': 'train'})\n",
      "accuracy: {'epoch': 21, 'value': 0.8684000372886658} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 21, 'value': 0.5294922918081284} ({'split': 'test'})\n",
      "Epoch 022\n",
      "accuracy: {'epoch': 22, 'value': 0.9908800000000003} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 22, 'value': 0.0298619554179907} ({'split': 'train'})\n",
      "accuracy: {'epoch': 22, 'value': 0.8572000443935395} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 22, 'value': 0.6321509420871735} ({'split': 'test'})\n",
      "Epoch 023\n",
      "accuracy: {'epoch': 23, 'value': 0.9879800000381469} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 23, 'value': 0.039903861670494076} ({'split': 'train'})\n",
      "accuracy: {'epoch': 23, 'value': 0.8627000391483307} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 23, 'value': 0.5108248859643937} ({'split': 'test'})\n",
      "Epoch 024\n",
      "accuracy: {'epoch': 24, 'value': 0.9842800000381471} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 24, 'value': 0.049522915165424355} ({'split': 'train'})\n",
      "accuracy: {'epoch': 24, 'value': 0.852200037240982} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 24, 'value': 0.6426631152629851} ({'split': 'test'})\n",
      "Epoch 025\n",
      "accuracy: {'epoch': 25, 'value': 0.9872400000381474} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 25, 'value': 0.041020989079475365} ({'split': 'train'})\n",
      "accuracy: {'epoch': 25, 'value': 0.8601000368595123} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 25, 'value': 0.597728431224823} ({'split': 'test'})\n",
      "Epoch 026\n",
      "accuracy: {'epoch': 26, 'value': 0.9852400000000003} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 26, 'value': 0.04650070916652678} ({'split': 'train'})\n",
      "accuracy: {'epoch': 26, 'value': 0.8629000425338744} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 26, 'value': 0.5812793880701065} ({'split': 'test'})\n",
      "Epoch 027\n",
      "accuracy: {'epoch': 27, 'value': 0.9882600000000002} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 27, 'value': 0.03787535741865635} ({'split': 'train'})\n",
      "accuracy: {'epoch': 27, 'value': 0.8568000435829163} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 27, 'value': 0.6554000079631807} ({'split': 'test'})\n",
      "Epoch 028\n",
      "accuracy: {'epoch': 28, 'value': 0.9923200000381465} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 28, 'value': 0.026261712193489065} ({'split': 'train'})\n",
      "accuracy: {'epoch': 28, 'value': 0.8490000486373901} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 28, 'value': 0.635520899295807} ({'split': 'test'})\n",
      "Epoch 029\n",
      "accuracy: {'epoch': 29, 'value': 0.9891400000381466} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 29, 'value': 0.03485698883771895} ({'split': 'train'})\n",
      "accuracy: {'epoch': 29, 'value': 0.8564000427722931} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 29, 'value': 0.6065239787101746} ({'split': 'test'})\n",
      "mean_test_accuracy <cifar_utils.accumulators.Mean object at 0x7f5012279e80>\n",
      "QWERTY: Enter Cifar 2, acc 0.8769000351428984\n",
      "Retraining model :  geometric\n",
      "num_epochs---------------+++))))))+++ 30\n",
      "num_epochs---------------+++))))))+++ 30\n",
      "lr is  0.05\n",
      "number of epochs would be  30\n",
      "num_epochs-------------- 30\n",
      "Epoch 000\n",
      "accuracy: {'epoch': 0, 'value': 0.14946000000953685} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 0, 'value': 2.402203867721559} ({'split': 'train'})\n",
      "accuracy: {'epoch': 0, 'value': 0.14160000532865524} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 0, 'value': 2.2895836353302004} ({'split': 'test'})\n",
      "Epoch 001\n",
      "accuracy: {'epoch': 1, 'value': 0.13188000000000008} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 1, 'value': 2.21680511428833} ({'split': 'train'})\n",
      "accuracy: {'epoch': 1, 'value': 0.11410000473260878} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 1, 'value': 2.2076149225234984} ({'split': 'test'})\n",
      "Epoch 002\n",
      "accuracy: {'epoch': 2, 'value': 0.22274000002861022} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 2, 'value': 2.0024989862442015} ({'split': 'train'})\n",
      "accuracy: {'epoch': 2, 'value': 0.28070001602172856} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 2, 'value': 1.8732013702392578} ({'split': 'test'})\n",
      "Epoch 003\n",
      "accuracy: {'epoch': 3, 'value': 0.31368000002861024} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 3, 'value': 1.8004127941131594} ({'split': 'train'})\n",
      "accuracy: {'epoch': 3, 'value': 0.37000001668930055} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 3, 'value': 1.6761927127838137} ({'split': 'test'})\n",
      "Epoch 004\n",
      "accuracy: {'epoch': 4, 'value': 0.39120000001907346} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 4, 'value': 1.6128876578140257} ({'split': 'train'})\n",
      "accuracy: {'epoch': 4, 'value': 0.41570002138614653} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 4, 'value': 1.5744562625885008} ({'split': 'test'})\n",
      "Epoch 005\n",
      "accuracy: {'epoch': 5, 'value': 0.4624800000286102} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 5, 'value': 1.4608652771759032} ({'split': 'train'})\n",
      "accuracy: {'epoch': 5, 'value': 0.4782000213861466} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 5, 'value': 1.4313624739646913} ({'split': 'test'})\n",
      "Epoch 006\n",
      "accuracy: {'epoch': 6, 'value': 0.5115600000000002} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 6, 'value': 1.3477421820449826} ({'split': 'train'})\n",
      "accuracy: {'epoch': 6, 'value': 0.49260002076625825} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 6, 'value': 1.5001825928688048} ({'split': 'test'})\n",
      "Epoch 007\n",
      "accuracy: {'epoch': 7, 'value': 0.5543200000000003} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 7, 'value': 1.2517400452423086} ({'split': 'train'})\n",
      "accuracy: {'epoch': 7, 'value': 0.5410000205039978} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 7, 'value': 1.3023215055465696} ({'split': 'test'})\n",
      "Epoch 008\n",
      "accuracy: {'epoch': 8, 'value': 0.6037800000572204} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 8, 'value': 1.118882740154267} ({'split': 'train'})\n",
      "accuracy: {'epoch': 8, 'value': 0.5642000257968902} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 8, 'value': 1.2607917904853818} ({'split': 'test'})\n",
      "Epoch 009\n",
      "accuracy: {'epoch': 9, 'value': 0.635040000038147} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 9, 'value': 1.03784497642517} ({'split': 'train'})\n",
      "accuracy: {'epoch': 9, 'value': 0.5779000282287597} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 9, 'value': 1.220839250087738} ({'split': 'test'})\n",
      "Epoch 010\n",
      "accuracy: {'epoch': 10, 'value': 0.6669000000000002} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 10, 'value': 0.9540842436218261} ({'split': 'train'})\n",
      "accuracy: {'epoch': 10, 'value': 0.6186000347137451} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 10, 'value': 1.105293095111847} ({'split': 'test'})\n",
      "Epoch 011\n",
      "accuracy: {'epoch': 11, 'value': 0.6985800000190733} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 11, 'value': 0.8587545590591431} ({'split': 'train'})\n",
      "accuracy: {'epoch': 11, 'value': 0.6504000246524811} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 11, 'value': 1.0509942173957825} ({'split': 'test'})\n",
      "Epoch 012\n",
      "accuracy: {'epoch': 12, 'value': 0.7274800000190733} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 12, 'value': 0.7801955489921566} ({'split': 'train'})\n",
      "accuracy: {'epoch': 12, 'value': 0.6716000318527221} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 12, 'value': 0.9748183429241182} ({'split': 'test'})\n",
      "Epoch 013\n",
      "accuracy: {'epoch': 13, 'value': 0.7520799999809265} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 13, 'value': 0.7130848260879515} ({'split': 'train'})\n",
      "accuracy: {'epoch': 13, 'value': 0.6735000371932983} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 13, 'value': 0.9858799874782562} ({'split': 'test'})\n",
      "Epoch 014\n",
      "accuracy: {'epoch': 14, 'value': 0.7737200000572209} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 14, 'value': 0.6489729078578944} ({'split': 'train'})\n",
      "accuracy: {'epoch': 14, 'value': 0.6759000301361083} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 14, 'value': 1.0039806723594664} ({'split': 'test'})\n",
      "Epoch 015\n",
      "accuracy: {'epoch': 15, 'value': 0.7932000000190736} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 15, 'value': 0.59233730670929} ({'split': 'train'})\n",
      "accuracy: {'epoch': 15, 'value': 0.6828000366687774} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 15, 'value': 0.9824602425098419} ({'split': 'test'})\n",
      "Epoch 016\n",
      "accuracy: {'epoch': 16, 'value': 0.8162599999999997} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 16, 'value': 0.5288584117126466} ({'split': 'train'})\n",
      "accuracy: {'epoch': 16, 'value': 0.6465000331401823} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 16, 'value': 1.162063717842102} ({'split': 'test'})\n",
      "Epoch 017\n",
      "accuracy: {'epoch': 17, 'value': 0.8310400000381463} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 17, 'value': 0.48297335572242717} ({'split': 'train'})\n",
      "accuracy: {'epoch': 17, 'value': 0.6310000360012056} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 17, 'value': 1.3088776230812074} ({'split': 'test'})\n",
      "Epoch 018\n",
      "accuracy: {'epoch': 18, 'value': 0.8429000000190733} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 18, 'value': 0.45160695908546455} ({'split': 'train'})\n",
      "accuracy: {'epoch': 18, 'value': 0.671500027179718} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 18, 'value': 1.1093480467796328} ({'split': 'test'})\n",
      "Epoch 019\n",
      "accuracy: {'epoch': 19, 'value': 0.8642000000000001} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 19, 'value': 0.39410776943206793} ({'split': 'train'})\n",
      "accuracy: {'epoch': 19, 'value': 0.6760000288486481} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 19, 'value': 1.1803273200988769} ({'split': 'test'})\n",
      "Epoch 020\n",
      "accuracy: {'epoch': 20, 'value': 0.8729599999809261} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 20, 'value': 0.36627115371704083} ({'split': 'train'})\n",
      "accuracy: {'epoch': 20, 'value': 0.6764000296592713} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 20, 'value': 1.2001104831695557} ({'split': 'test'})\n",
      "Epoch 021\n",
      "accuracy: {'epoch': 21, 'value': 0.8848600000572207} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 21, 'value': 0.3330754294490815} ({'split': 'train'})\n",
      "accuracy: {'epoch': 21, 'value': 0.6586000323295593} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 21, 'value': 1.3612267494201662} ({'split': 'test'})\n",
      "Epoch 022\n",
      "accuracy: {'epoch': 22, 'value': 0.8893000000190739} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 22, 'value': 0.32212121148109446} ({'split': 'train'})\n",
      "accuracy: {'epoch': 22, 'value': 0.682500046491623} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 22, 'value': 1.248389256000519} ({'split': 'test'})\n",
      "Epoch 023\n",
      "accuracy: {'epoch': 23, 'value': 0.9050799999809265} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 23, 'value': 0.2800083024215699} ({'split': 'train'})\n",
      "accuracy: {'epoch': 23, 'value': 0.6865000367164612} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 23, 'value': 1.2551151275634767} ({'split': 'test'})\n",
      "Epoch 024\n",
      "accuracy: {'epoch': 24, 'value': 0.9128000000572204} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 24, 'value': 0.25373248421669015} ({'split': 'train'})\n",
      "accuracy: {'epoch': 24, 'value': 0.6848000228404999} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 24, 'value': 1.3577993750572204} ({'split': 'test'})\n",
      "Epoch 025\n",
      "accuracy: {'epoch': 25, 'value': 0.9211} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 25, 'value': 0.2323446141004563} ({'split': 'train'})\n",
      "accuracy: {'epoch': 25, 'value': 0.6540000200271607} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 25, 'value': 1.5130586624145508} ({'split': 'test'})\n",
      "Epoch 026\n",
      "accuracy: {'epoch': 26, 'value': 0.9155600000381472} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 26, 'value': 0.2507215150260925} ({'split': 'train'})\n",
      "accuracy: {'epoch': 26, 'value': 0.6750000298023223} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 26, 'value': 1.2971309900283812} ({'split': 'test'})\n",
      "Epoch 027\n",
      "accuracy: {'epoch': 27, 'value': 0.9289600000381474} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 27, 'value': 0.21319688903331757} ({'split': 'train'})\n",
      "accuracy: {'epoch': 27, 'value': 0.6848000347614288} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 27, 'value': 1.322657871246338} ({'split': 'test'})\n",
      "Epoch 028\n",
      "accuracy: {'epoch': 28, 'value': 0.9326599999809264} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 28, 'value': 0.1988973051643371} ({'split': 'train'})\n",
      "accuracy: {'epoch': 28, 'value': 0.6618000268936157} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 28, 'value': 1.5572775602340698} ({'split': 'test'})\n",
      "Epoch 029\n",
      "accuracy: {'epoch': 29, 'value': 0.9331600000572204} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 29, 'value': 0.20300840456962582} ({'split': 'train'})\n",
      "accuracy: {'epoch': 29, 'value': 0.6828000366687774} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 29, 'value': 1.3678458333015442} ({'split': 'test'})\n",
      "mean_test_accuracy <cifar_utils.accumulators.Mean object at 0x7f5012259880>\n",
      "QWERTY: Enter Cifar 2, acc 0.6865000367164612\n",
      "Retraining model :  naive_averaging\n",
      "num_epochs---------------+++))))))+++ 30\n",
      "num_epochs---------------+++))))))+++ 30\n",
      "lr is  0.05\n",
      "number of epochs would be  30\n",
      "num_epochs-------------- 30\n",
      "Epoch 000\n",
      "accuracy: {'epoch': 0, 'value': 0.38526000005722044} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 0, 'value': 1.6991427669525145} ({'split': 'train'})\n",
      "accuracy: {'epoch': 0, 'value': 0.5624000251293182} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 0, 'value': 1.2830089688301085} ({'split': 'test'})\n",
      "Epoch 001\n",
      "accuracy: {'epoch': 1, 'value': 0.7115200000381467} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 1, 'value': 0.8496749090003963} ({'split': 'train'})\n",
      "accuracy: {'epoch': 1, 'value': 0.7668000280857085} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 1, 'value': 0.7167498826980591} ({'split': 'test'})\n",
      "Epoch 002\n",
      "accuracy: {'epoch': 2, 'value': 0.8151600000000001} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 2, 'value': 0.5451683435249326} ({'split': 'train'})\n",
      "accuracy: {'epoch': 2, 'value': 0.8122000396251678} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 2, 'value': 0.5620096176862717} ({'split': 'test'})\n",
      "Epoch 003\n",
      "accuracy: {'epoch': 3, 'value': 0.8590000000190733} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 3, 'value': 0.41685803015708905} ({'split': 'train'})\n",
      "accuracy: {'epoch': 3, 'value': 0.8218000292778014} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 3, 'value': 0.5612656474113464} ({'split': 'test'})\n",
      "Epoch 004\n",
      "accuracy: {'epoch': 4, 'value': 0.8943000000572208} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 4, 'value': 0.3088664338493347} ({'split': 'train'})\n",
      "accuracy: {'epoch': 4, 'value': 0.81640003323555} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 4, 'value': 0.5741850316524506} ({'split': 'test'})\n",
      "Epoch 005\n",
      "accuracy: {'epoch': 5, 'value': 0.9227200000000001} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 5, 'value': 0.22540445897102362} ({'split': 'train'})\n",
      "accuracy: {'epoch': 5, 'value': 0.8235000491142273} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 5, 'value': 0.5996475994586944} ({'split': 'test'})\n",
      "Epoch 006\n",
      "accuracy: {'epoch': 6, 'value': 0.9419800000381466} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 6, 'value': 0.1716551009917259} ({'split': 'train'})\n",
      "accuracy: {'epoch': 6, 'value': 0.8455000281333923} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 6, 'value': 0.5556852847337722} ({'split': 'test'})\n",
      "Epoch 007\n",
      "accuracy: {'epoch': 7, 'value': 0.95204} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 7, 'value': 0.14259729103088384} ({'split': 'train'})\n",
      "accuracy: {'epoch': 7, 'value': 0.8311000406742096} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 7, 'value': 0.5892740845680237} ({'split': 'test'})\n",
      "Epoch 008\n",
      "accuracy: {'epoch': 8, 'value': 0.956940000038147} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 8, 'value': 0.12547837949991228} ({'split': 'train'})\n",
      "accuracy: {'epoch': 8, 'value': 0.8370000422000885} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 8, 'value': 0.6135058403015136} ({'split': 'test'})\n",
      "Epoch 009\n",
      "accuracy: {'epoch': 9, 'value': 0.9669599999809266} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 9, 'value': 0.09963233341217045} ({'split': 'train'})\n",
      "accuracy: {'epoch': 9, 'value': 0.8351000368595124} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 9, 'value': 0.6097926259040833} ({'split': 'test'})\n",
      "Epoch 010\n",
      "accuracy: {'epoch': 10, 'value': 0.9674200000572207} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 10, 'value': 0.09710951803207397} ({'split': 'train'})\n",
      "accuracy: {'epoch': 10, 'value': 0.8384000480175018} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 10, 'value': 0.6548097372055053} ({'split': 'test'})\n",
      "Epoch 011\n",
      "accuracy: {'epoch': 11, 'value': 0.9720800000381468} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 11, 'value': 0.08116553901672366} ({'split': 'train'})\n",
      "accuracy: {'epoch': 11, 'value': 0.8303000390529632} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 11, 'value': 0.7637226581573486} ({'split': 'test'})\n",
      "Epoch 012\n",
      "accuracy: {'epoch': 12, 'value': 0.9711400000190734} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 12, 'value': 0.08323176696538931} ({'split': 'train'})\n",
      "accuracy: {'epoch': 12, 'value': 0.8482000470161438} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 12, 'value': 0.6027654051780701} ({'split': 'test'})\n",
      "Epoch 013\n"
     ]
    }
   ],
   "source": [
    "!python main.py --gpu-id 0 --model-name vgg11_nobias --n-epochs 10 --save-result-file sample.csv \\\n",
    "--sweep-name exp_sample --correction --ground-metric euclidean --weight-stats \\\n",
    "--geom-ensemble-type wts --ground-metric-normalize none --sweep-id 90 \\\n",
    "--ckpt-type best --dataset Cifar10 --ground-metric-eff --recheck-cifar --activation-seed {activation_seed} \\\n",
    "--prelu-acts --past-correction --not-squared --exact\\\n",
    "--learning-rate 0.00003\\\n",
    "--momentum 0.5\\\n",
    "--batch-size-train 256 --experiment-dir {experiment_dir}\\\n",
    "--prunint-rate {prunint_rate}\\\n",
    "--run-mode {run_mode}\\\n",
    "--to-download\\\n",
    "--load-model-dir {load_model_dir}\\\n",
    "--retrain 30"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
