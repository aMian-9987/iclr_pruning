{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9548f487-ef4e-48bc-b034-06d3a5938736",
   "metadata": {},
   "outputs": [],
   "source": [
    "activation_seed=21\n",
    "experiment_dir=\"experiment3\"\n",
    "prunint_rate=0.4\n",
    "run_mode='benchmark'\n",
    "load_model_dir='./cifar_models'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f6e80ead-659e-47bc-bf87-d87c13b16229",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------- Setting up parameters -------\n",
      "dumping parameters at  /root/autodl-tmp/OTfusion_pruning/otfusion/exp_sample/configurations\n",
      "The parameters are: \n",
      " Namespace(act_bug=False, act_num_samples=100, activation_histograms=False, activation_mode=None, activation_seed=21, alpha=0.7, baseroot='/root/autodl-tmp/OTfusion_pruning/otfusion', batch_size_test=1000, batch_size_train=64, center_acts=False, choice='0 2 4 6 8', cifar_style_data=False, ckpt_type='best', clip_gm=False, clip_max=5, clip_min=0, config_dir='/root/autodl-tmp/OTfusion_pruning/otfusion/exp_sample/configurations', config_file=None, correction=True, csv_dir='/root/autodl-tmp/OTfusion_pruning/otfusion/exp_sample/csv', dataset='Cifar10', debug=False, deprecated=None, deterministic=False, diff_init=False, disable_bias=True, dist_epochs=60, dist_normalize=False, dump_final_models=False, dump_model=False, enable_dropout=False, ensemble_step=0.5, eval_aligned=True, exact=True, exp_name='exp_2023-09-16_17-40-49_960421', experiment_dir='experiment3', geom_ensemble_type='wts', gpu_id=0, gromov=False, gromov_loss='square_loss', ground_metric='euclidean', ground_metric_eff=True, ground_metric_normalize='none', handle_skips=False, importance=None, learning_rate=0.004, load_model_dir='./cifar_models', load_models='', log_interval=100, model_name='vgg11_nobias', momentum=0.5, n_epochs=0, no_random_trainloaders=False, normalize_acts=False, normalize_wts=False, not_squared=True, num_hidden_nodes=400, num_hidden_nodes1=400, num_hidden_nodes2=200, num_hidden_nodes3=100, num_hidden_nodes4=50, num_models=2, options_type='generic', partial_reshape=False, partition_dataloader=-1, partition_type='labels', past_correction=True, personal_class_idx=9, personal_split_frac=0.1, pool_acts=False, pool_relu=False, prediction_wts=False, prelu_acts=True, print_distances=False, proper_marginals=False, prunint_rate=0.4, recheck_acc=False, recheck_cifar=True, reg=0.01, reg_m=0.001, reinit_trainloaders=False, result_dir='/root/autodl-tmp/OTfusion_pruning/otfusion/exp_sample/results', retrain=30, retrain_avg_only=False, retrain_geometric_only=False, retrain_lr_decay=-1, retrain_lr_decay_epochs=None, retrain_lr_decay_factor=None, retrain_seed=-1, rootdir='/root/autodl-tmp/OTfusion_pruning/otfusion/exp_sample', run_mode='benchmark', same_model=-1, save_result_file='sample.csv', second_model_name=None, sinkhorn_type='normal', skip_last_layer=False, skip_last_layer_type='average', skip_personal_idx=False, skip_retrain=-1, softmax_temperature=1, standardize_acts=False, sweep_id=90, sweep_name='exp_sample', temperature=20, tensorboard=False, tensorboard_root='./tensorboard', timestamp='2023-09-16_17-40-49_960421', tmap_stats=False, to_download=True, transform_acts=False, unbalanced=False, update_acts=False, weight_stats=True, width_ratio=1)\n",
      "refactored get_config\n",
      "------- Obtain dataloaders -------\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "------- Training independent models -------\n",
      "QWERTY: Enter Cifar 1\n",
      "number of epochs would b>>>>>>>e  300\n",
      "in _make_layers [Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), AvgPool2d(kernel_size=1, stride=1, padding=0)]\n",
      "Relu Inplace is  False\n",
      "model parameters are \n",
      " [torch.Size([64, 3, 3, 3]), torch.Size([128, 64, 3, 3]), torch.Size([256, 128, 3, 3]), torch.Size([256, 256, 3, 3]), torch.Size([512, 256, 3, 3]), torch.Size([512, 512, 3, 3]), torch.Size([512, 512, 3, 3]), torch.Size([512, 512, 3, 3]), torch.Size([10, 512])]\n",
      "Loading model at path ./cifar_models/model_0/best.checkpoint which had accuracy 0.9030999821424489 and at epoch 127\n",
      "QWERTY: Enter Cifar 1\n",
      "number of epochs would b>>>>>>>e  300\n",
      "in _make_layers [Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), AvgPool2d(kernel_size=1, stride=1, padding=0)]\n",
      "Relu Inplace is  False\n",
      "model parameters are \n",
      " [torch.Size([64, 3, 3, 3]), torch.Size([128, 64, 3, 3]), torch.Size([256, 128, 3, 3]), torch.Size([256, 256, 3, 3]), torch.Size([512, 256, 3, 3]), torch.Size([512, 512, 3, 3]), torch.Size([512, 512, 3, 3]), torch.Size([512, 512, 3, 3]), torch.Size([10, 512])]\n",
      "Loading model at path ./cifar_models/model_1/best.checkpoint which had accuracy 0.9049999803304669 and at epoch 134\n",
      "num_epochs---------------++++++ 300\n",
      "========================================\n",
      "========================================\n",
      "TEST:args.learning_rate,args.momentum 0.004 0.5\n",
      "========================================\n",
      "========================================\n",
      "========================================\n",
      "\n",
      "--------- Testing in global mode ---------\n",
      "size of test_loader dataset:  10000\n",
      "\n",
      "Test set: Avg. loss: 0.0003, Accuracy: 9019/10000 (90%)\n",
      "\n",
      "========================================\n",
      "========================================\n",
      "TEST:args.learning_rate,args.momentum 0.004 0.5\n",
      "========================================\n",
      "========================================\n",
      "========================================\n",
      "\n",
      "--------- Testing in global mode ---------\n",
      "size of test_loader dataset:  10000\n",
      "\n",
      "Test set: Avg. loss: 0.0003, Accuracy: 9046/10000 (90%)\n",
      "\n",
      "models [VGG(\n",
      "  (features): Sequential(\n",
      "    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    (1): ReLU()\n",
      "    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (3): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    (4): ReLU()\n",
      "    (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (6): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    (7): ReLU()\n",
      "    (8): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    (9): ReLU()\n",
      "    (10): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (11): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    (12): ReLU()\n",
      "    (13): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    (14): ReLU()\n",
      "    (15): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (16): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    (17): ReLU()\n",
      "    (18): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    (19): ReLU()\n",
      "    (20): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (21): AvgPool2d(kernel_size=1, stride=1, padding=0)\n",
      "  )\n",
      "  (classifier): Linear(in_features=512, out_features=10, bias=False)\n",
      "), VGG(\n",
      "  (features): Sequential(\n",
      "    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    (1): ReLU()\n",
      "    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (3): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    (4): ReLU()\n",
      "    (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (6): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    (7): ReLU()\n",
      "    (8): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    (9): ReLU()\n",
      "    (10): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (11): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    (12): ReLU()\n",
      "    (13): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    (14): ReLU()\n",
      "    (15): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (16): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    (17): ReLU()\n",
      "    (18): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    (19): ReLU()\n",
      "    (20): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (21): AvgPool2d(kernel_size=1, stride=1, padding=0)\n",
      "  )\n",
      "  (classifier): Linear(in_features=512, out_features=10, bias=False)\n",
      ")]\n",
      "layer features.0.weight has #params  1728\n",
      "layer features.3.weight has #params  73728\n",
      "layer features.6.weight has #params  294912\n",
      "layer features.8.weight has #params  589824\n",
      "layer features.11.weight has #params  1179648\n",
      "layer features.13.weight has #params  2359296\n",
      "layer features.16.weight has #params  2359296\n",
      "layer features.18.weight has #params  2359296\n",
      "layer classifier.weight has #params  5120\n",
      "Activation Timer start\n",
      "Activation Timer ends\n",
      "------- Geometric Ensembling -------\n",
      "Timer start\n",
      "Previous layer shape is  None\n",
      "Processing the coordinates to form ground_metric\n",
      "dont leave off the squaring of the ground metric\n",
      "returns a uniform measure of cardinality:  64\n",
      "returns a uniform measure of cardinality:  64\n",
      "the transport map is  tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')\n",
      "Ratio of trace to the matrix sum:  tensor(0.0469, device='cuda:0')\n",
      "Here, trace is 3.0 and matrix sum is 64.0 \n",
      "this is past correction for weight mode\n",
      "Shape of aligned wt is  torch.Size([64, 3, 9])\n",
      "Shape of fc_layer0_weight_data is  torch.Size([64, 3, 9])\n",
      "in _make_layers [Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), AvgPool2d(kernel_size=1, stride=1, padding=0)]\n",
      "Relu Inplace is  False\n",
      "model parameters are \n",
      " [torch.Size([64, 3, 3, 3]), torch.Size([128, 64, 3, 3]), torch.Size([256, 128, 3, 3]), torch.Size([256, 256, 3, 3]), torch.Size([512, 256, 3, 3]), torch.Size([512, 512, 3, 3]), torch.Size([512, 512, 3, 3]), torch.Size([512, 512, 3, 3]), torch.Size([10, 512])]\n",
      "len of model_state_dict is  9\n",
      "len of new_params is  1\n",
      "updated parameters for layer  features.0.weight\n",
      "\n",
      "--------- Testing in global mode ---------\n",
      "size of test_loader dataset:  10000\n",
      "\n",
      "Test set: Avg. loss: 0.0027, Accuracy: 1249/10000 (12%)\n",
      "\n",
      "accuracy after update is  12.49\n",
      "For layer idx 0, accuracy of the updated model is 12.49\n",
      "Previous layer shape is  torch.Size([64, 3, 3, 3])\n",
      "shape of layer: model 0 torch.Size([128, 64, 9])\n",
      "shape of layer: model 1 torch.Size([128, 64, 9])\n",
      "shape of previous transport map torch.Size([64, 64])\n",
      "Processing the coordinates to form ground_metric\n",
      "dont leave off the squaring of the ground metric\n",
      "returns a uniform measure of cardinality:  128\n",
      "returns a uniform measure of cardinality:  128\n",
      "the transport map is  tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        ...,\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0078],\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],\n",
      "       device='cuda:0')\n",
      "Ratio of trace to the matrix sum:  tensor(0.0078, device='cuda:0')\n",
      "Here, trace is 1.0 and matrix sum is 128.0 \n",
      "this is past correction for weight mode\n",
      "Shape of aligned wt is  torch.Size([128, 64, 9])\n",
      "Shape of fc_layer0_weight_data is  torch.Size([128, 64, 9])\n",
      "in _make_layers [Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), AvgPool2d(kernel_size=1, stride=1, padding=0)]\n",
      "Relu Inplace is  False\n",
      "model parameters are \n",
      " [torch.Size([64, 3, 3, 3]), torch.Size([128, 64, 3, 3]), torch.Size([256, 128, 3, 3]), torch.Size([256, 256, 3, 3]), torch.Size([512, 256, 3, 3]), torch.Size([512, 512, 3, 3]), torch.Size([512, 512, 3, 3]), torch.Size([512, 512, 3, 3]), torch.Size([10, 512])]\n",
      "len of model_state_dict is  9\n",
      "len of new_params is  2\n",
      "updated parameters for layer  features.0.weight\n",
      "updated parameters for layer  features.3.weight\n",
      "\n",
      "--------- Testing in global mode ---------\n",
      "size of test_loader dataset:  10000\n",
      "\n",
      "Test set: Avg. loss: 0.0024, Accuracy: 1036/10000 (10%)\n",
      "\n",
      "accuracy after update is  10.36\n",
      "For layer idx 1, accuracy of the updated model is 10.36\n",
      "Previous layer shape is  torch.Size([128, 64, 3, 3])\n",
      "shape of layer: model 0 torch.Size([256, 128, 9])\n",
      "shape of layer: model 1 torch.Size([256, 128, 9])\n",
      "shape of previous transport map torch.Size([128, 128])\n",
      "Processing the coordinates to form ground_metric\n",
      "dont leave off the squaring of the ground metric\n",
      "returns a uniform measure of cardinality:  256\n",
      "returns a uniform measure of cardinality:  256\n",
      "the transport map is  tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')\n",
      "Ratio of trace to the matrix sum:  tensor(0., device='cuda:0')\n",
      "Here, trace is 0.0 and matrix sum is 256.0 \n",
      "this is past correction for weight mode\n",
      "Shape of aligned wt is  torch.Size([256, 128, 9])\n",
      "Shape of fc_layer0_weight_data is  torch.Size([256, 128, 9])\n",
      "in _make_layers [Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), AvgPool2d(kernel_size=1, stride=1, padding=0)]\n",
      "Relu Inplace is  False\n",
      "model parameters are \n",
      " [torch.Size([64, 3, 3, 3]), torch.Size([128, 64, 3, 3]), torch.Size([256, 128, 3, 3]), torch.Size([256, 256, 3, 3]), torch.Size([512, 256, 3, 3]), torch.Size([512, 512, 3, 3]), torch.Size([512, 512, 3, 3]), torch.Size([512, 512, 3, 3]), torch.Size([10, 512])]\n",
      "len of model_state_dict is  9\n",
      "len of new_params is  3\n",
      "updated parameters for layer  features.0.weight\n",
      "updated parameters for layer  features.3.weight\n",
      "updated parameters for layer  features.6.weight\n",
      "\n",
      "--------- Testing in global mode ---------\n",
      "size of test_loader dataset:  10000\n",
      "\n",
      "Test set: Avg. loss: 0.0023, Accuracy: 1146/10000 (11%)\n",
      "\n",
      "accuracy after update is  11.46\n",
      "For layer idx 2, accuracy of the updated model is 11.46\n",
      "Previous layer shape is  torch.Size([256, 128, 3, 3])\n",
      "shape of layer: model 0 torch.Size([256, 256, 9])\n",
      "shape of layer: model 1 torch.Size([256, 256, 9])\n",
      "shape of previous transport map torch.Size([256, 256])\n",
      "Processing the coordinates to form ground_metric\n",
      "dont leave off the squaring of the ground metric\n",
      "returns a uniform measure of cardinality:  256\n",
      "returns a uniform measure of cardinality:  256\n",
      "the transport map is  tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        ...,\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.0039, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],\n",
      "       device='cuda:0')\n",
      "Ratio of trace to the matrix sum:  tensor(0.0078, device='cuda:0')\n",
      "Here, trace is 2.0 and matrix sum is 256.0 \n",
      "this is past correction for weight mode\n",
      "Shape of aligned wt is  torch.Size([256, 256, 9])\n",
      "Shape of fc_layer0_weight_data is  torch.Size([256, 256, 9])\n",
      "in _make_layers [Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), AvgPool2d(kernel_size=1, stride=1, padding=0)]\n",
      "Relu Inplace is  False\n",
      "model parameters are \n",
      " [torch.Size([64, 3, 3, 3]), torch.Size([128, 64, 3, 3]), torch.Size([256, 128, 3, 3]), torch.Size([256, 256, 3, 3]), torch.Size([512, 256, 3, 3]), torch.Size([512, 512, 3, 3]), torch.Size([512, 512, 3, 3]), torch.Size([512, 512, 3, 3]), torch.Size([10, 512])]\n",
      "len of model_state_dict is  9\n",
      "len of new_params is  4\n",
      "updated parameters for layer  features.0.weight\n",
      "updated parameters for layer  features.3.weight\n",
      "updated parameters for layer  features.6.weight\n",
      "updated parameters for layer  features.8.weight\n",
      "\n",
      "--------- Testing in global mode ---------\n",
      "size of test_loader dataset:  10000\n",
      "\n",
      "Test set: Avg. loss: 0.0024, Accuracy: 1249/10000 (12%)\n",
      "\n",
      "accuracy after update is  12.49\n",
      "For layer idx 3, accuracy of the updated model is 12.49\n",
      "Previous layer shape is  torch.Size([256, 256, 3, 3])\n",
      "shape of layer: model 0 torch.Size([512, 256, 9])\n",
      "shape of layer: model 1 torch.Size([512, 256, 9])\n",
      "shape of previous transport map torch.Size([256, 256])\n",
      "Processing the coordinates to form ground_metric\n",
      "dont leave off the squaring of the ground metric\n",
      "returns a uniform measure of cardinality:  512\n",
      "returns a uniform measure of cardinality:  512\n",
      "the transport map is  tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')\n",
      "Ratio of trace to the matrix sum:  tensor(0.0039, device='cuda:0')\n",
      "Here, trace is 2.0 and matrix sum is 512.0 \n",
      "this is past correction for weight mode\n",
      "Shape of aligned wt is  torch.Size([512, 256, 9])\n",
      "Shape of fc_layer0_weight_data is  torch.Size([512, 256, 9])\n",
      "in _make_layers [Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), AvgPool2d(kernel_size=1, stride=1, padding=0)]\n",
      "Relu Inplace is  False\n",
      "model parameters are \n",
      " [torch.Size([64, 3, 3, 3]), torch.Size([128, 64, 3, 3]), torch.Size([256, 128, 3, 3]), torch.Size([256, 256, 3, 3]), torch.Size([512, 256, 3, 3]), torch.Size([512, 512, 3, 3]), torch.Size([512, 512, 3, 3]), torch.Size([512, 512, 3, 3]), torch.Size([10, 512])]\n",
      "len of model_state_dict is  9\n",
      "len of new_params is  5\n",
      "updated parameters for layer  features.0.weight\n",
      "updated parameters for layer  features.3.weight\n",
      "updated parameters for layer  features.6.weight\n",
      "updated parameters for layer  features.8.weight\n",
      "updated parameters for layer  features.11.weight\n",
      "\n",
      "--------- Testing in global mode ---------\n",
      "size of test_loader dataset:  10000\n",
      "\n",
      "Test set: Avg. loss: 0.0027, Accuracy: 903/10000 (9%)\n",
      "\n",
      "accuracy after update is  9.03\n",
      "For layer idx 4, accuracy of the updated model is 9.03\n",
      "Previous layer shape is  torch.Size([512, 256, 3, 3])\n",
      "shape of layer: model 0 torch.Size([512, 512, 9])\n",
      "shape of layer: model 1 torch.Size([512, 512, 9])\n",
      "shape of previous transport map torch.Size([512, 512])\n",
      "Processing the coordinates to form ground_metric\n",
      "dont leave off the squaring of the ground metric\n",
      "returns a uniform measure of cardinality:  512\n",
      "returns a uniform measure of cardinality:  512\n",
      "the transport map is  tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')\n",
      "Ratio of trace to the matrix sum:  tensor(0.0020, device='cuda:0')\n",
      "Here, trace is 1.0 and matrix sum is 512.0 \n",
      "this is past correction for weight mode\n",
      "Shape of aligned wt is  torch.Size([512, 512, 9])\n",
      "Shape of fc_layer0_weight_data is  torch.Size([512, 512, 9])\n",
      "in _make_layers [Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), AvgPool2d(kernel_size=1, stride=1, padding=0)]\n",
      "Relu Inplace is  False\n",
      "model parameters are \n",
      " [torch.Size([64, 3, 3, 3]), torch.Size([128, 64, 3, 3]), torch.Size([256, 128, 3, 3]), torch.Size([256, 256, 3, 3]), torch.Size([512, 256, 3, 3]), torch.Size([512, 512, 3, 3]), torch.Size([512, 512, 3, 3]), torch.Size([512, 512, 3, 3]), torch.Size([10, 512])]\n",
      "len of model_state_dict is  9\n",
      "len of new_params is  6\n",
      "updated parameters for layer  features.0.weight\n",
      "updated parameters for layer  features.3.weight\n",
      "updated parameters for layer  features.6.weight\n",
      "updated parameters for layer  features.8.weight\n",
      "updated parameters for layer  features.11.weight\n",
      "updated parameters for layer  features.13.weight\n",
      "\n",
      "--------- Testing in global mode ---------\n",
      "size of test_loader dataset:  10000\n",
      "\n",
      "Test set: Avg. loss: 0.0024, Accuracy: 1410/10000 (14%)\n",
      "\n",
      "accuracy after update is  14.1\n",
      "For layer idx 5, accuracy of the updated model is 14.1\n",
      "Previous layer shape is  torch.Size([512, 512, 3, 3])\n",
      "shape of layer: model 0 torch.Size([512, 512, 9])\n",
      "shape of layer: model 1 torch.Size([512, 512, 9])\n",
      "shape of previous transport map torch.Size([512, 512])\n",
      "Processing the coordinates to form ground_metric\n",
      "dont leave off the squaring of the ground metric\n",
      "returns a uniform measure of cardinality:  512\n",
      "returns a uniform measure of cardinality:  512\n",
      "the transport map is  tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')\n",
      "Ratio of trace to the matrix sum:  tensor(0.0020, device='cuda:0')\n",
      "Here, trace is 1.0 and matrix sum is 512.0 \n",
      "this is past correction for weight mode\n",
      "Shape of aligned wt is  torch.Size([512, 512, 9])\n",
      "Shape of fc_layer0_weight_data is  torch.Size([512, 512, 9])\n",
      "in _make_layers [Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), AvgPool2d(kernel_size=1, stride=1, padding=0)]\n",
      "Relu Inplace is  False\n",
      "model parameters are \n",
      " [torch.Size([64, 3, 3, 3]), torch.Size([128, 64, 3, 3]), torch.Size([256, 128, 3, 3]), torch.Size([256, 256, 3, 3]), torch.Size([512, 256, 3, 3]), torch.Size([512, 512, 3, 3]), torch.Size([512, 512, 3, 3]), torch.Size([512, 512, 3, 3]), torch.Size([10, 512])]\n",
      "len of model_state_dict is  9\n",
      "len of new_params is  7\n",
      "updated parameters for layer  features.0.weight\n",
      "updated parameters for layer  features.3.weight\n",
      "updated parameters for layer  features.6.weight\n",
      "updated parameters for layer  features.8.weight\n",
      "updated parameters for layer  features.11.weight\n",
      "updated parameters for layer  features.13.weight\n",
      "updated parameters for layer  features.16.weight\n",
      "\n",
      "--------- Testing in global mode ---------\n",
      "size of test_loader dataset:  10000\n",
      "\n",
      "Test set: Avg. loss: 0.0022, Accuracy: 2217/10000 (22%)\n",
      "\n",
      "accuracy after update is  22.17\n",
      "For layer idx 6, accuracy of the updated model is 22.17\n",
      "Previous layer shape is  torch.Size([512, 512, 3, 3])\n",
      "shape of layer: model 0 torch.Size([512, 512, 9])\n",
      "shape of layer: model 1 torch.Size([512, 512, 9])\n",
      "shape of previous transport map torch.Size([512, 512])\n",
      "Processing the coordinates to form ground_metric\n",
      "dont leave off the squaring of the ground metric\n",
      "returns a uniform measure of cardinality:  512\n",
      "returns a uniform measure of cardinality:  512\n",
      "the transport map is  tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        [0.0020, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        ...,\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],\n",
      "       device='cuda:0')\n",
      "Ratio of trace to the matrix sum:  tensor(0., device='cuda:0')\n",
      "Here, trace is 0.0 and matrix sum is 512.0 \n",
      "this is past correction for weight mode\n",
      "Shape of aligned wt is  torch.Size([512, 512, 9])\n",
      "Shape of fc_layer0_weight_data is  torch.Size([512, 512, 9])\n",
      "in _make_layers [Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), AvgPool2d(kernel_size=1, stride=1, padding=0)]\n",
      "Relu Inplace is  False\n",
      "model parameters are \n",
      " [torch.Size([64, 3, 3, 3]), torch.Size([128, 64, 3, 3]), torch.Size([256, 128, 3, 3]), torch.Size([256, 256, 3, 3]), torch.Size([512, 256, 3, 3]), torch.Size([512, 512, 3, 3]), torch.Size([512, 512, 3, 3]), torch.Size([512, 512, 3, 3]), torch.Size([10, 512])]\n",
      "len of model_state_dict is  9\n",
      "len of new_params is  8\n",
      "updated parameters for layer  features.0.weight\n",
      "updated parameters for layer  features.3.weight\n",
      "updated parameters for layer  features.6.weight\n",
      "updated parameters for layer  features.8.weight\n",
      "updated parameters for layer  features.11.weight\n",
      "updated parameters for layer  features.13.weight\n",
      "updated parameters for layer  features.16.weight\n",
      "updated parameters for layer  features.18.weight\n",
      "\n",
      "--------- Testing in global mode ---------\n",
      "size of test_loader dataset:  10000\n",
      "\n",
      "Test set: Avg. loss: 0.0024, Accuracy: 582/10000 (6%)\n",
      "\n",
      "accuracy after update is  5.82\n",
      "For layer idx 7, accuracy of the updated model is 5.82\n",
      "Previous layer shape is  torch.Size([512, 512, 3, 3])\n",
      "shape of layer: model 0 torch.Size([10, 512])\n",
      "shape of layer: model 1 torch.Size([10, 512])\n",
      "shape of previous transport map torch.Size([512, 512])\n",
      "Processing the coordinates to form ground_metric\n",
      "dont leave off the squaring of the ground metric\n",
      "ground metric is  tensor([[1.4241, 2.4872, 2.7804, 2.8719, 2.7374, 2.8799, 2.7276, 2.6342, 2.5641,\n",
      "         2.5922],\n",
      "        [2.4889, 1.3253, 2.6732, 2.5914, 2.7584, 2.5501, 2.4174, 2.5215, 2.3048,\n",
      "         2.4101],\n",
      "        [2.7600, 2.6711, 1.3832, 2.9465, 2.9065, 2.8701, 2.7276, 2.8385, 2.7485,\n",
      "         2.7866],\n",
      "        [2.8352, 2.6791, 2.8919, 1.5430, 2.8605, 2.9159, 2.7574, 2.7980, 2.8164,\n",
      "         2.6354],\n",
      "        [2.7551, 2.5960, 2.8635, 2.8450, 1.5042, 2.7447, 2.7853, 2.6811, 2.7031,\n",
      "         2.7294],\n",
      "        [2.9190, 2.5396, 2.9469, 2.8416, 2.7272, 1.5502, 2.7781, 2.7303, 2.7977,\n",
      "         2.6697],\n",
      "        [2.6778, 2.4457, 2.7190, 2.9111, 2.6488, 2.8526, 1.3931, 2.6890, 2.4981,\n",
      "         2.5560],\n",
      "        [2.7394, 2.5496, 2.7799, 2.7903, 2.7151, 2.6590, 2.7190, 1.3924, 2.5583,\n",
      "         2.5121],\n",
      "        [2.5972, 2.3660, 2.7693, 2.6872, 2.7026, 2.8280, 2.5325, 2.6366, 1.2999,\n",
      "         2.4041],\n",
      "        [2.5918, 2.2730, 2.7816, 2.7576, 2.6697, 2.7273, 2.5482, 2.5290, 2.5207,\n",
      "         1.2878]], device='cuda:0', grad_fn=<PowBackward0>)\n",
      "returns a uniform measure of cardinality:  10\n",
      "returns a uniform measure of cardinality:  10\n",
      "the transport map is  tensor([[0.1000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0000, 0.1000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0000, 0.0000, 0.1000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.1000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.1000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.1000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.1000, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.1000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.1000,\n",
      "         0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.1000]], device='cuda:0')\n",
      "Ratio of trace to the matrix sum:  tensor(1., device='cuda:0')\n",
      "Here, trace is 9.99755859375 and matrix sum is 9.99755859375 \n",
      "this is past correction for weight mode\n",
      "Shape of aligned wt is  torch.Size([10, 512])\n",
      "Shape of fc_layer0_weight_data is  torch.Size([10, 512])\n",
      "in _make_layers [Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), AvgPool2d(kernel_size=1, stride=1, padding=0)]\n",
      "Relu Inplace is  False\n",
      "model parameters are \n",
      " [torch.Size([64, 3, 3, 3]), torch.Size([128, 64, 3, 3]), torch.Size([256, 128, 3, 3]), torch.Size([256, 256, 3, 3]), torch.Size([512, 256, 3, 3]), torch.Size([512, 512, 3, 3]), torch.Size([512, 512, 3, 3]), torch.Size([512, 512, 3, 3]), torch.Size([10, 512])]\n",
      "len of model_state_dict is  9\n",
      "len of new_params is  9\n",
      "updated parameters for layer  features.0.weight\n",
      "updated parameters for layer  features.3.weight\n",
      "updated parameters for layer  features.6.weight\n",
      "updated parameters for layer  features.8.weight\n",
      "updated parameters for layer  features.11.weight\n",
      "updated parameters for layer  features.13.weight\n",
      "updated parameters for layer  features.16.weight\n",
      "updated parameters for layer  features.18.weight\n",
      "updated parameters for layer  classifier.weight\n",
      "\n",
      "--------- Testing in global mode ---------\n",
      "size of test_loader dataset:  10000\n",
      "\n",
      "Test set: Avg. loss: 0.0003, Accuracy: 9019/10000 (90%)\n",
      "\n",
      "accuracy after update is  90.19\n",
      "For layer idx 8, accuracy of the updated model is 90.19\n",
      "using independent method\n",
      "in _make_layers [Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), AvgPool2d(kernel_size=1, stride=1, padding=0)]\n",
      "Relu Inplace is  False\n",
      "model parameters are \n",
      " [torch.Size([64, 3, 3, 3]), torch.Size([128, 64, 3, 3]), torch.Size([256, 128, 3, 3]), torch.Size([256, 256, 3, 3]), torch.Size([512, 256, 3, 3]), torch.Size([512, 512, 3, 3]), torch.Size([512, 512, 3, 3]), torch.Size([512, 512, 3, 3]), torch.Size([10, 512])]\n",
      "\n",
      "--------- Testing in global mode ---------\n",
      "size of test_loader dataset:  10000\n",
      "\n",
      "Test set: Avg. loss: 0.0023, Accuracy: 1091/10000 (11%)\n",
      "\n",
      "len of model parameters and avg aligned layers is  9 9\n",
      "len of model_state_dict is  9\n",
      "len of param_list is  9\n",
      "\n",
      "--------- Testing in global mode ---------\n",
      "size of test_loader dataset:  10000\n",
      "\n",
      "Test set: Avg. loss: 0.0015, Accuracy: 8544/10000 (85%)\n",
      "\n",
      "===============================================\n",
      "===============================================\n",
      "After Ensemble: 85.44\n",
      "===============================================\n",
      "===============================================\n",
      "Timer ends\n",
      "Time taken for geometric ensembling is 51.965344570577145 seconds\n",
      "------- Prediction based ensembling -------\n",
      "\n",
      "Test set: Avg. loss: 0.0003, Accuracy: 9133/10000 (91%)\n",
      "\n",
      "------- Naive ensembling of weights -------\n",
      "[torch.Size([64, 3, 3, 3]), torch.Size([64, 3, 3, 3])]\n",
      "torch.Size([64, 3, 3, 3])\n",
      "[torch.Size([128, 64, 3, 3]), torch.Size([128, 64, 3, 3])]\n",
      "torch.Size([128, 64, 3, 3])\n",
      "[torch.Size([256, 128, 3, 3]), torch.Size([256, 128, 3, 3])]\n",
      "torch.Size([256, 128, 3, 3])\n",
      "[torch.Size([256, 256, 3, 3]), torch.Size([256, 256, 3, 3])]\n",
      "torch.Size([256, 256, 3, 3])\n",
      "[torch.Size([512, 256, 3, 3]), torch.Size([512, 256, 3, 3])]\n",
      "torch.Size([512, 256, 3, 3])\n",
      "[torch.Size([512, 512, 3, 3]), torch.Size([512, 512, 3, 3])]\n",
      "torch.Size([512, 512, 3, 3])\n",
      "[torch.Size([512, 512, 3, 3]), torch.Size([512, 512, 3, 3])]\n",
      "torch.Size([512, 512, 3, 3])\n",
      "[torch.Size([512, 512, 3, 3]), torch.Size([512, 512, 3, 3])]\n",
      "torch.Size([512, 512, 3, 3])\n",
      "[torch.Size([10, 512]), torch.Size([10, 512])]\n",
      "torch.Size([10, 512])\n",
      "in _make_layers [Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), AvgPool2d(kernel_size=1, stride=1, padding=0)]\n",
      "Relu Inplace is  False\n",
      "model parameters are \n",
      " [torch.Size([64, 3, 3, 3]), torch.Size([128, 64, 3, 3]), torch.Size([256, 128, 3, 3]), torch.Size([256, 256, 3, 3]), torch.Size([512, 256, 3, 3]), torch.Size([512, 512, 3, 3]), torch.Size([512, 512, 3, 3]), torch.Size([512, 512, 3, 3]), torch.Size([10, 512])]\n",
      "\n",
      "--------- Testing in global mode ---------\n",
      "size of test_loader dataset:  10000\n",
      "\n",
      "Test set: Avg. loss: 0.0023, Accuracy: 1003/10000 (10%)\n",
      "\n",
      "\n",
      "--------- Testing in global mode ---------\n",
      "size of test_loader dataset:  10000\n",
      "\n",
      "Test set: Avg. loss: 0.0023, Accuracy: 1689/10000 (17%)\n",
      "\n",
      "-------- Retraining the models ---------\n",
      "Retraining model :  model_0\n",
      "num_epochs---------------+++))))))+++ 300\n",
      "num_epochs---------------+++))))))+++ 300\n",
      "lr is  0.05\n",
      "number of epochs would be  30\n",
      "num_epochs-------------- 30\n",
      "Epoch 000\n",
      "/root/miniconda3/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:129: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\"Detected call of `lr_scheduler.step()` before `optimizer.step()`. \"\n",
      "/root/miniconda3/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:154: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "^C\n",
      "Traceback (most recent call last):\n",
      "  File \"main.py\", line 351, in <module>\n",
      "    _, best_retrain_acc = routines.retrain_models(args, [*original_models, geometric_model, naive_model], retrain_loader, test_loader, config, tensorboard_obj=tensorboard_obj, initial_acc=initial_acc, nicks=nicks)\n",
      "  File \"/root/autodl-tmp/OTfusion_pruning/otfusion/routines.py\", line 394, in retrain_models\n",
      "    retrained_network, acc = cifar_train.get_retrained_model(args, retrain_loader, test_loader, old_networks[i], config, output_root_dir, tensorboard_obj=tensorboard_obj, nick=nick, start_acc=initial_acc[i])\n",
      "  File \"/root/autodl-tmp/OTfusion_pruning/otfusion/./cifar/train.py\", line 333, in get_retrained_model\n",
      "    best_acc,model = main(config, output_dir, args.gpu_id, pretrained_model=old_network, pretrained_dataset=(train_loader, test_loader), tensorboard_obj=tensorboard_obj,return_model=True)\n",
      "  File \"/root/autodl-tmp/OTfusion_pruning/otfusion/./cifar/train.py\", line 72, in main\n",
      "    prediction = model(batch_x)\n",
      "  File \"/root/miniconda3/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1102, in _call_impl\n",
      "    return forward_call(*input, **kwargs)\n",
      "  File \"/root/autodl-tmp/OTfusion_pruning/otfusion/./cifar/models/vgg.py\", line 28, in forward\n",
      "    out = self.features(x)\n",
      "  File \"/root/miniconda3/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1102, in _call_impl\n",
      "    return forward_call(*input, **kwargs)\n",
      "  File \"/root/miniconda3/lib/python3.8/site-packages/torch/nn/modules/container.py\", line 141, in forward\n",
      "    input = module(input)\n",
      "  File \"/root/miniconda3/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1102, in _call_impl\n",
      "    return forward_call(*input, **kwargs)\n",
      "  File \"/root/miniconda3/lib/python3.8/site-packages/torch/nn/modules/conv.py\", line 446, in forward\n",
      "    return self._conv_forward(input, self.weight, self.bias)\n",
      "  File \"/root/miniconda3/lib/python3.8/site-packages/torch/nn/modules/conv.py\", line 442, in _conv_forward\n",
      "    return F.conv2d(input, weight, bias, self.stride,\n",
      "KeyboardInterrupt\n"
     ]
    }
   ],
   "source": [
    "!python main.py --gpu-id 0 --model-name vgg11_nobias --n-epochs 0 --save-result-file sample.csv \\\n",
    "--sweep-name exp_sample --correction --ground-metric euclidean --weight-stats \\\n",
    "--geom-ensemble-type wts --ground-metric-normalize none --sweep-id 90 \\\n",
    "--ckpt-type best --dataset Cifar10 --ground-metric-eff --recheck-cifar --activation-seed {activation_seed} \\\n",
    "--prelu-acts --past-correction --not-squared --exact\\\n",
    "--learning-rate 0.004\\\n",
    "--momentum 0.5\\\n",
    "--batch-size-train 64 --experiment-dir {experiment_dir}\\\n",
    "--prunint-rate {prunint_rate}\\\n",
    "--run-mode {run_mode}\\\n",
    "--to-download\\\n",
    "--load-model-dir {load_model_dir}\\\n",
    "--retrain 30\\\n",
    "--eval-aligned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "688c56ca-bfeb-4f7a-a9e7-35463c71b6b0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "29a14370-8231-4282-b050-cda48f71b3f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#1:prune without finetune\n",
    "activation_seed=21\n",
    "experiment_dir=\"experiment3\"\n",
    "prunint_rate=0.4\n",
    "run_mode='prune'\n",
    "load_model_dir='./cifar_models'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a2488ee6-9aab-4165-9663-cf713df42dee",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------- Setting up parameters -------\n",
      "dumping parameters at  /root/autodl-tmp/OTfusion_pruning/otfusion/exp_sample/configurations\n",
      "The parameters are: \n",
      " Namespace(act_bug=False, act_num_samples=100, activation_histograms=False, activation_mode=None, activation_seed=21, alpha=0.7, baseroot='/root/autodl-tmp/OTfusion_pruning/otfusion', batch_size_test=1000, batch_size_train=64, center_acts=False, choice='0 2 4 6 8', cifar_style_data=False, ckpt_type='best', clip_gm=False, clip_max=5, clip_min=0, config_dir='/root/autodl-tmp/OTfusion_pruning/otfusion/exp_sample/configurations', config_file=None, correction=True, csv_dir='/root/autodl-tmp/OTfusion_pruning/otfusion/exp_sample/csv', dataset='Cifar10', debug=False, deprecated=None, deterministic=False, diff_init=False, disable_bias=True, dist_epochs=60, dist_normalize=False, dump_final_models=False, dump_model=False, enable_dropout=False, ensemble_step=0.5, eval_aligned=False, exact=True, exp_name='exp_2023-09-17_11-33-53_576853', experiment_dir='experiment3', geom_ensemble_type='wts', gpu_id=0, gromov=False, gromov_loss='square_loss', ground_metric='euclidean', ground_metric_eff=True, ground_metric_normalize='none', handle_skips=False, importance=None, learning_rate=0.08, load_model_dir='./cifar_models', load_models='', log_interval=100, model_name='vgg11_nobias', momentum=0.5, n_epochs=90, no_random_trainloaders=False, normalize_acts=False, normalize_wts=False, not_squared=True, num_hidden_nodes=400, num_hidden_nodes1=400, num_hidden_nodes2=200, num_hidden_nodes3=100, num_hidden_nodes4=50, num_models=2, options_type='generic', partial_reshape=False, partition_dataloader=-1, partition_type='labels', past_correction=True, personal_class_idx=9, personal_split_frac=0.1, pool_acts=False, pool_relu=False, prediction_wts=False, prelu_acts=True, print_distances=False, proper_marginals=False, prunint_rate=0.4, recheck_acc=False, recheck_cifar=True, reg=0.01, reg_m=0.001, reinit_trainloaders=False, result_dir='/root/autodl-tmp/OTfusion_pruning/otfusion/exp_sample/results', retrain=30, retrain_avg_only=False, retrain_geometric_only=False, retrain_lr_decay=-1, retrain_lr_decay_epochs=None, retrain_lr_decay_factor=None, retrain_seed=-1, rootdir='/root/autodl-tmp/OTfusion_pruning/otfusion/exp_sample', run_mode='prune', same_model=-1, save_result_file='sample.csv', second_model_name=None, sinkhorn_type='normal', skip_last_layer=False, skip_last_layer_type='average', skip_personal_idx=False, skip_retrain=-1, softmax_temperature=1, standardize_acts=False, sweep_id=90, sweep_name='exp_sample', temperature=20, tensorboard=False, tensorboard_root='./tensorboard', timestamp='2023-09-17_11-33-53_576853', tmap_stats=False, to_download=True, transform_acts=False, unbalanced=False, update_acts=False, weight_stats=True, width_ratio=1)\n",
      "refactored get_config\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "in _make_layers [Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), AvgPool2d(kernel_size=1, stride=1, padding=0)]\n",
      "Relu Inplace is  False\n",
      "model parameters are \n",
      " [torch.Size([64, 3, 3, 3]), torch.Size([128, 64, 3, 3]), torch.Size([256, 128, 3, 3]), torch.Size([256, 256, 3, 3]), torch.Size([512, 256, 3, 3]), torch.Size([512, 512, 3, 3]), torch.Size([512, 512, 3, 3]), torch.Size([512, 512, 3, 3]), torch.Size([10, 512])]\n",
      "in _make_layers [Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), AvgPool2d(kernel_size=1, stride=1, padding=0)]\n",
      "Relu Inplace is  False\n",
      "model parameters are \n",
      " [torch.Size([64, 3, 3, 3]), torch.Size([128, 64, 3, 3]), torch.Size([256, 128, 3, 3]), torch.Size([256, 256, 3, 3]), torch.Size([512, 256, 3, 3]), torch.Size([512, 512, 3, 3]), torch.Size([512, 512, 3, 3]), torch.Size([512, 512, 3, 3]), torch.Size([10, 512])]\n",
      "in _make_layers [Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), AvgPool2d(kernel_size=1, stride=1, padding=0)]\n",
      "Relu Inplace is  False\n",
      "model parameters are \n",
      " [torch.Size([64, 3, 3, 3]), torch.Size([128, 64, 3, 3]), torch.Size([256, 128, 3, 3]), torch.Size([256, 256, 3, 3]), torch.Size([512, 256, 3, 3]), torch.Size([512, 512, 3, 3]), torch.Size([512, 512, 3, 3]), torch.Size([512, 512, 3, 3]), torch.Size([10, 512])]\n",
      "in _make_layers [Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), AvgPool2d(kernel_size=1, stride=1, padding=0)]\n",
      "Relu Inplace is  False\n",
      "model parameters are \n",
      " [torch.Size([64, 3, 3, 3]), torch.Size([128, 64, 3, 3]), torch.Size([256, 128, 3, 3]), torch.Size([256, 256, 3, 3]), torch.Size([512, 256, 3, 3]), torch.Size([512, 512, 3, 3]), torch.Size([512, 512, 3, 3]), torch.Size([512, 512, 3, 3]), torch.Size([10, 512])]\n",
      "\n",
      "--------- Testing in global mode ---------\n",
      "size of test_loader dataset:  10000\n",
      "\n",
      "Test set: Avg. loss: 0.0004, Accuracy: 8808/10000 (88%)\n",
      "\n",
      "\n",
      "--------- Testing in global mode ---------\n",
      "size of test_loader dataset:  10000\n",
      "\n",
      "Test set: Avg. loss: 0.0004, Accuracy: 8944/10000 (89%)\n",
      "\n",
      "check zero rate for model 0\n",
      "odict_keys(['features.0.weight', 'features.3.weight', 'features.6.weight', 'features.8.weight', 'features.11.weight', 'features.13.weight', 'features.16.weight', 'features.18.weight', 'classifier.weight'])\n",
      "the ratio of zero for features.0.weight\n",
      "tensor(0.3999, device='cuda:0')\n",
      "the ratio of zero for features.3.weight\n",
      "tensor(0.4000, device='cuda:0')\n",
      "the ratio of zero for features.6.weight\n",
      "tensor(0.4000, device='cuda:0')\n",
      "the ratio of zero for features.8.weight\n",
      "tensor(0.4000, device='cuda:0')\n",
      "the ratio of zero for features.11.weight\n",
      "tensor(0.4000, device='cuda:0')\n",
      "the ratio of zero for features.13.weight\n",
      "tensor(0.4000, device='cuda:0')\n",
      "the ratio of zero for features.16.weight\n",
      "tensor(0.4000, device='cuda:0')\n",
      "the ratio of zero for features.18.weight\n",
      "tensor(0.4000, device='cuda:0')\n",
      "the ratio of zero for classifier.weight\n",
      "tensor(0.4000, device='cuda:0')\n",
      "check zero rate for model 1\n",
      "odict_keys(['features.0.weight', 'features.3.weight', 'features.6.weight', 'features.8.weight', 'features.11.weight', 'features.13.weight', 'features.16.weight', 'features.18.weight', 'classifier.weight'])\n",
      "the ratio of zero for features.0.weight\n",
      "tensor(0.3999, device='cuda:0')\n",
      "the ratio of zero for features.3.weight\n",
      "tensor(0.4000, device='cuda:0')\n",
      "the ratio of zero for features.6.weight\n",
      "tensor(0.4000, device='cuda:0')\n",
      "the ratio of zero for features.8.weight\n",
      "tensor(0.4000, device='cuda:0')\n",
      "the ratio of zero for features.11.weight\n",
      "tensor(0.4000, device='cuda:0')\n",
      "the ratio of zero for features.13.weight\n",
      "tensor(0.4000, device='cuda:0')\n",
      "the ratio of zero for features.16.weight\n",
      "tensor(0.4000, device='cuda:0')\n",
      "the ratio of zero for features.18.weight\n",
      "tensor(0.4000, device='cuda:0')\n",
      "the ratio of zero for classifier.weight\n",
      "tensor(0.4000, device='cuda:0')\n",
      "layer features.0.weight has #params  1728\n",
      "layer features.3.weight has #params  73728\n",
      "layer features.6.weight has #params  294912\n",
      "layer features.8.weight has #params  589824\n",
      "layer features.11.weight has #params  1179648\n",
      "layer features.13.weight has #params  2359296\n",
      "layer features.16.weight has #params  2359296\n",
      "layer features.18.weight has #params  2359296\n",
      "layer classifier.weight has #params  5120\n",
      "Activation Timer start\n",
      "Activation Timer ends\n",
      "------- Geometric Ensembling -------\n",
      "Timer start\n",
      "odict_keys(['features.0.weight', 'features.3.weight', 'features.6.weight', 'features.8.weight', 'features.11.weight', 'features.13.weight', 'features.16.weight', 'features.18.weight', 'classifier.weight'])\n",
      "the ratio of zero for features.0.weight\n",
      "tensor(0.3999, device='cuda:0')\n",
      "the ratio of zero for features.3.weight\n",
      "tensor(0.4000, device='cuda:0')\n",
      "the ratio of zero for features.6.weight\n",
      "tensor(0.4000, device='cuda:0')\n",
      "the ratio of zero for features.8.weight\n",
      "tensor(0.4000, device='cuda:0')\n",
      "the ratio of zero for features.11.weight\n",
      "tensor(0.4000, device='cuda:0')\n",
      "the ratio of zero for features.13.weight\n",
      "tensor(0.4000, device='cuda:0')\n",
      "the ratio of zero for features.16.weight\n",
      "tensor(0.4000, device='cuda:0')\n",
      "the ratio of zero for features.18.weight\n",
      "tensor(0.4000, device='cuda:0')\n",
      "the ratio of zero for classifier.weight\n",
      "tensor(0.4000, device='cuda:0')\n",
      "Previous layer shape is  None\n",
      "Processing the coordinates to form ground_metric\n",
      "dont leave off the squaring of the ground metric\n",
      "returns a uniform measure of cardinality:  64\n",
      "returns a uniform measure of cardinality:  64\n",
      "the transport map is  tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')\n",
      "Ratio of trace to the matrix sum:  tensor(0.0469, device='cuda:0')\n",
      "Here, trace is 3.0 and matrix sum is 64.0 \n",
      "this is past correction for weight mode\n",
      "Shape of aligned wt is  torch.Size([64, 3, 9])\n",
      "Shape of fc_layer0_weight_data is  torch.Size([64, 3, 9])\n",
      "Previous layer shape is  torch.Size([64, 3, 3, 3])\n",
      "shape of layer: model 0 torch.Size([128, 64, 9])\n",
      "shape of layer: model 1 torch.Size([128, 64, 9])\n",
      "shape of previous transport map torch.Size([64, 64])\n",
      "Processing the coordinates to form ground_metric\n",
      "dont leave off the squaring of the ground metric\n",
      "returns a uniform measure of cardinality:  128\n",
      "returns a uniform measure of cardinality:  128\n",
      "the transport map is  tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        ...,\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0078],\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],\n",
      "       device='cuda:0')\n",
      "Ratio of trace to the matrix sum:  tensor(0., device='cuda:0')\n",
      "Here, trace is 0.0 and matrix sum is 128.0 \n",
      "this is past correction for weight mode\n",
      "Shape of aligned wt is  torch.Size([128, 64, 9])\n",
      "Shape of fc_layer0_weight_data is  torch.Size([128, 64, 9])\n",
      "Previous layer shape is  torch.Size([128, 64, 3, 3])\n",
      "shape of layer: model 0 torch.Size([256, 128, 9])\n",
      "shape of layer: model 1 torch.Size([256, 128, 9])\n",
      "shape of previous transport map torch.Size([128, 128])\n",
      "Processing the coordinates to form ground_metric\n",
      "dont leave off the squaring of the ground metric\n",
      "returns a uniform measure of cardinality:  256\n",
      "returns a uniform measure of cardinality:  256\n",
      "the transport map is  tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')\n",
      "Ratio of trace to the matrix sum:  tensor(0., device='cuda:0')\n",
      "Here, trace is 0.0 and matrix sum is 256.0 \n",
      "this is past correction for weight mode\n",
      "Shape of aligned wt is  torch.Size([256, 128, 9])\n",
      "Shape of fc_layer0_weight_data is  torch.Size([256, 128, 9])\n",
      "Previous layer shape is  torch.Size([256, 128, 3, 3])\n",
      "shape of layer: model 0 torch.Size([256, 256, 9])\n",
      "shape of layer: model 1 torch.Size([256, 256, 9])\n",
      "shape of previous transport map torch.Size([256, 256])\n",
      "Processing the coordinates to form ground_metric\n",
      "dont leave off the squaring of the ground metric\n",
      "returns a uniform measure of cardinality:  256\n",
      "returns a uniform measure of cardinality:  256\n",
      "the transport map is  tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        ...,\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.0039, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],\n",
      "       device='cuda:0')\n",
      "Ratio of trace to the matrix sum:  tensor(0.0039, device='cuda:0')\n",
      "Here, trace is 1.0 and matrix sum is 256.0 \n",
      "this is past correction for weight mode\n",
      "Shape of aligned wt is  torch.Size([256, 256, 9])\n",
      "Shape of fc_layer0_weight_data is  torch.Size([256, 256, 9])\n",
      "Previous layer shape is  torch.Size([256, 256, 3, 3])\n",
      "shape of layer: model 0 torch.Size([512, 256, 9])\n",
      "shape of layer: model 1 torch.Size([512, 256, 9])\n",
      "shape of previous transport map torch.Size([256, 256])\n",
      "Processing the coordinates to form ground_metric\n",
      "dont leave off the squaring of the ground metric\n",
      "returns a uniform measure of cardinality:  512\n",
      "returns a uniform measure of cardinality:  512\n",
      "the transport map is  tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')\n",
      "Ratio of trace to the matrix sum:  tensor(0.0020, device='cuda:0')\n",
      "Here, trace is 1.0 and matrix sum is 512.0 \n",
      "this is past correction for weight mode\n",
      "Shape of aligned wt is  torch.Size([512, 256, 9])\n",
      "Shape of fc_layer0_weight_data is  torch.Size([512, 256, 9])\n",
      "Previous layer shape is  torch.Size([512, 256, 3, 3])\n",
      "shape of layer: model 0 torch.Size([512, 512, 9])\n",
      "shape of layer: model 1 torch.Size([512, 512, 9])\n",
      "shape of previous transport map torch.Size([512, 512])\n",
      "Processing the coordinates to form ground_metric\n",
      "dont leave off the squaring of the ground metric\n",
      "returns a uniform measure of cardinality:  512\n",
      "returns a uniform measure of cardinality:  512\n",
      "the transport map is  tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')\n",
      "Ratio of trace to the matrix sum:  tensor(0.0020, device='cuda:0')\n",
      "Here, trace is 1.0 and matrix sum is 512.0 \n",
      "this is past correction for weight mode\n",
      "Shape of aligned wt is  torch.Size([512, 512, 9])\n",
      "Shape of fc_layer0_weight_data is  torch.Size([512, 512, 9])\n",
      "Previous layer shape is  torch.Size([512, 512, 3, 3])\n",
      "shape of layer: model 0 torch.Size([512, 512, 9])\n",
      "shape of layer: model 1 torch.Size([512, 512, 9])\n",
      "shape of previous transport map torch.Size([512, 512])\n",
      "Processing the coordinates to form ground_metric\n",
      "dont leave off the squaring of the ground metric\n",
      "returns a uniform measure of cardinality:  512\n",
      "returns a uniform measure of cardinality:  512\n",
      "the transport map is  tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')\n",
      "Ratio of trace to the matrix sum:  tensor(0.0020, device='cuda:0')\n",
      "Here, trace is 1.0 and matrix sum is 512.0 \n",
      "this is past correction for weight mode\n",
      "Shape of aligned wt is  torch.Size([512, 512, 9])\n",
      "Shape of fc_layer0_weight_data is  torch.Size([512, 512, 9])\n",
      "Previous layer shape is  torch.Size([512, 512, 3, 3])\n",
      "shape of layer: model 0 torch.Size([512, 512, 9])\n",
      "shape of layer: model 1 torch.Size([512, 512, 9])\n",
      "shape of previous transport map torch.Size([512, 512])\n",
      "Processing the coordinates to form ground_metric\n",
      "dont leave off the squaring of the ground metric\n",
      "returns a uniform measure of cardinality:  512\n",
      "returns a uniform measure of cardinality:  512\n",
      "the transport map is  tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        [0.0020, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        ...,\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],\n",
      "       device='cuda:0')\n",
      "Ratio of trace to the matrix sum:  tensor(0., device='cuda:0')\n",
      "Here, trace is 0.0 and matrix sum is 512.0 \n",
      "this is past correction for weight mode\n",
      "Shape of aligned wt is  torch.Size([512, 512, 9])\n",
      "Shape of fc_layer0_weight_data is  torch.Size([512, 512, 9])\n",
      "Previous layer shape is  torch.Size([512, 512, 3, 3])\n",
      "shape of layer: model 0 torch.Size([10, 512])\n",
      "shape of layer: model 1 torch.Size([10, 512])\n",
      "shape of previous transport map torch.Size([512, 512])\n",
      "Processing the coordinates to form ground_metric\n",
      "dont leave off the squaring of the ground metric\n",
      "ground metric is  tensor([[1.4451, 2.4596, 2.7757, 2.8507, 2.7238, 2.8365, 2.6898, 2.6386, 2.5573,\n",
      "         2.5488],\n",
      "        [2.4684, 1.3465, 2.6634, 2.5541, 2.7038, 2.5145, 2.3754, 2.5224, 2.2895,\n",
      "         2.4020],\n",
      "        [2.7492, 2.6013, 1.3671, 2.9478, 2.8916, 2.8390, 2.7007, 2.8328, 2.7186,\n",
      "         2.7493],\n",
      "        [2.7873, 2.6409, 2.8869, 1.5660, 2.8133, 2.9364, 2.6969, 2.8066, 2.7956,\n",
      "         2.6433],\n",
      "        [2.7631, 2.5184, 2.8516, 2.8748, 1.5164, 2.7244, 2.7632, 2.6470, 2.6614,\n",
      "         2.6927],\n",
      "        [2.8911, 2.5556, 2.9351, 2.7766, 2.7216, 1.5563, 2.7679, 2.6745, 2.7893,\n",
      "         2.6553],\n",
      "        [2.6410, 2.3964, 2.6794, 2.8841, 2.6470, 2.8319, 1.4250, 2.6919, 2.4649,\n",
      "         2.5078],\n",
      "        [2.7156, 2.5411, 2.7180, 2.7575, 2.6967, 2.6837, 2.6739, 1.3587, 2.5345,\n",
      "         2.4949],\n",
      "        [2.5511, 2.3950, 2.7451, 2.6871, 2.6836, 2.7819, 2.4976, 2.6037, 1.2900,\n",
      "         2.3777],\n",
      "        [2.5687, 2.2608, 2.7517, 2.7280, 2.6355, 2.6877, 2.5257, 2.4817, 2.5039,\n",
      "         1.3080]], device='cuda:0', grad_fn=<PowBackward0>)\n",
      "returns a uniform measure of cardinality:  10\n",
      "returns a uniform measure of cardinality:  10\n",
      "the transport map is  tensor([[0.1000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0000, 0.1000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0000, 0.0000, 0.1000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.1000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.1000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.1000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.1000, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.1000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.1000,\n",
      "         0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.1000]], device='cuda:0')\n",
      "Ratio of trace to the matrix sum:  tensor(1., device='cuda:0')\n",
      "Here, trace is 9.99755859375 and matrix sum is 9.99755859375 \n",
      "this is past correction for weight mode\n",
      "Shape of aligned wt is  torch.Size([10, 512])\n",
      "Shape of fc_layer0_weight_data is  torch.Size([10, 512])\n",
      "using independent method\n",
      "in _make_layers [Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), AvgPool2d(kernel_size=1, stride=1, padding=0)]\n",
      "Relu Inplace is  False\n",
      "model parameters are \n",
      " [torch.Size([64, 3, 3, 3]), torch.Size([128, 64, 3, 3]), torch.Size([256, 128, 3, 3]), torch.Size([256, 256, 3, 3]), torch.Size([512, 256, 3, 3]), torch.Size([512, 512, 3, 3]), torch.Size([512, 512, 3, 3]), torch.Size([512, 512, 3, 3]), torch.Size([10, 512])]\n",
      "\n",
      "--------- Testing in global mode ---------\n",
      "size of test_loader dataset:  10000\n",
      "\n",
      "Test set: Avg. loss: 0.0023, Accuracy: 1019/10000 (10%)\n",
      "\n",
      "len of model parameters and avg aligned layers is  9 9\n",
      "len of model_state_dict is  9\n",
      "len of param_list is  9\n",
      "\n",
      "--------- Testing in global mode ---------\n",
      "size of test_loader dataset:  10000\n",
      "\n",
      "Test set: Avg. loss: 0.0016, Accuracy: 8391/10000 (84%)\n",
      "\n",
      "===============================================\n",
      "===============================================\n",
      "After Ensemble: 83.91\n",
      "===============================================\n",
      "===============================================\n",
      "Timer ends\n",
      "Time taken for geometric ensembling is 3.2709886245429516 seconds\n",
      "------- Prediction based ensembling -------\n",
      "\n",
      "Test set: Avg. loss: 0.0003, Accuracy: 9029/10000 (90%)\n",
      "\n",
      "------- Naive ensembling of weights -------\n",
      "[torch.Size([64, 3, 3, 3]), torch.Size([64, 3, 3, 3])]\n",
      "torch.Size([64, 3, 3, 3])\n",
      "[torch.Size([128, 64, 3, 3]), torch.Size([128, 64, 3, 3])]\n",
      "torch.Size([128, 64, 3, 3])\n",
      "[torch.Size([256, 128, 3, 3]), torch.Size([256, 128, 3, 3])]\n",
      "torch.Size([256, 128, 3, 3])\n",
      "[torch.Size([256, 256, 3, 3]), torch.Size([256, 256, 3, 3])]\n",
      "torch.Size([256, 256, 3, 3])\n",
      "[torch.Size([512, 256, 3, 3]), torch.Size([512, 256, 3, 3])]\n",
      "torch.Size([512, 256, 3, 3])\n",
      "[torch.Size([512, 512, 3, 3]), torch.Size([512, 512, 3, 3])]\n",
      "torch.Size([512, 512, 3, 3])\n",
      "[torch.Size([512, 512, 3, 3]), torch.Size([512, 512, 3, 3])]\n",
      "torch.Size([512, 512, 3, 3])\n",
      "[torch.Size([512, 512, 3, 3]), torch.Size([512, 512, 3, 3])]\n",
      "torch.Size([512, 512, 3, 3])\n",
      "[torch.Size([10, 512]), torch.Size([10, 512])]\n",
      "torch.Size([10, 512])\n",
      "in _make_layers [Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), AvgPool2d(kernel_size=1, stride=1, padding=0)]\n",
      "Relu Inplace is  False\n",
      "model parameters are \n",
      " [torch.Size([64, 3, 3, 3]), torch.Size([128, 64, 3, 3]), torch.Size([256, 128, 3, 3]), torch.Size([256, 256, 3, 3]), torch.Size([512, 256, 3, 3]), torch.Size([512, 512, 3, 3]), torch.Size([512, 512, 3, 3]), torch.Size([512, 512, 3, 3]), torch.Size([10, 512])]\n",
      "\n",
      "--------- Testing in global mode ---------\n",
      "size of test_loader dataset:  10000\n",
      "\n",
      "Test set: Avg. loss: 0.0023, Accuracy: 1143/10000 (11%)\n",
      "\n",
      "\n",
      "--------- Testing in global mode ---------\n",
      "size of test_loader dataset:  10000\n",
      "\n",
      "Test set: Avg. loss: 0.0023, Accuracy: 1629/10000 (16%)\n",
      "\n",
      "-------- Retraining the models ---------\n",
      "Retraining model :  model_0\n",
      "num_epochs---------------+++))))))+++ 300\n",
      "num_epochs---------------+++))))))+++ 300\n",
      "lr is  0.05\n",
      "number of epochs would be  30\n",
      "num_epochs-------------- 30\n",
      "Epoch 000\n",
      "/root/miniconda3/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:129: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\"Detected call of `lr_scheduler.step()` before `optimizer.step()`. \"\n",
      "/root/miniconda3/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:154: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "accuracy: {'epoch': 0, 'value': 0.1213199999999999} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 0, 'value': 2.335312788600924} ({'split': 'train'})\n",
      "accuracy: {'epoch': 0, 'value': 0.11970000490546227} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 0, 'value': 2.302535700798035} ({'split': 'test'})\n",
      "Epoch 001\n",
      "accuracy: {'epoch': 1, 'value': 0.10489999999999995} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 1, 'value': 2.2922797400665313} ({'split': 'train'})\n",
      "accuracy: {'epoch': 1, 'value': 0.08630000427365303} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 1, 'value': 2.30199294090271} ({'split': 'test'})\n",
      "Epoch 002\n",
      "accuracy: {'epoch': 2, 'value': 0.10721999999999994} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 2, 'value': 2.273133944854737} ({'split': 'train'})\n",
      "accuracy: {'epoch': 2, 'value': 0.1514000087976456} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 2, 'value': 2.1621949434280396} ({'split': 'test'})\n",
      "Epoch 003\n",
      "accuracy: {'epoch': 3, 'value': 0.18295999999999998} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 3, 'value': 2.142271877212523} ({'split': 'train'})\n",
      "accuracy: {'epoch': 3, 'value': 0.22600001096725464} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 3, 'value': 2.3797920703887936} ({'split': 'test'})\n",
      "Epoch 004\n",
      "accuracy: {'epoch': 4, 'value': 0.2563199999999998} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 4, 'value': 2.008309655189513} ({'split': 'train'})\n",
      "accuracy: {'epoch': 4, 'value': 0.285700011253357} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 4, 'value': 1.906404483318329} ({'split': 'test'})\n",
      "Epoch 005\n",
      "accuracy: {'epoch': 5, 'value': 0.32659999999999967} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 5, 'value': 1.8409339381027203} ({'split': 'train'})\n",
      "accuracy: {'epoch': 5, 'value': 0.34710001945495605} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 5, 'value': 1.82076872587204} ({'split': 'test'})\n",
      "Epoch 006\n",
      "^C\n",
      "Traceback (most recent call last):\n",
      "  File \"main.py\", line 412, in <module>\n",
      "    _, best_retrain_acc = routines.retrain_models(args, [*original_models, geometric_model, naive_model], retrain_loader, test_loader, config, tensorboard_obj=tensorboard_obj, initial_acc=initial_acc, nicks=nicks)\n",
      "  File \"/root/autodl-tmp/OTfusion_pruning/otfusion/routines.py\", line 394, in retrain_models\n",
      "    retrained_network, acc = cifar_train.get_retrained_model(args, retrain_loader, test_loader, old_networks[i], config, output_root_dir, tensorboard_obj=tensorboard_obj, nick=nick, start_acc=initial_acc[i])\n",
      "  File \"/root/autodl-tmp/OTfusion_pruning/otfusion/./cifar/train.py\", line 333, in get_retrained_model\n",
      "    best_acc,model = main(config, output_dir, args.gpu_id, pretrained_model=old_network, pretrained_dataset=(train_loader, test_loader), tensorboard_obj=tensorboard_obj,return_model=True)\n",
      "  File \"/root/autodl-tmp/OTfusion_pruning/otfusion/./cifar/train.py\", line 75, in main\n",
      "    loss.backward()\n",
      "  File \"/root/miniconda3/lib/python3.8/site-packages/torch/_tensor.py\", line 307, in backward\n",
      "    torch.autograd.backward(self, gradient, retain_graph, create_graph, inputs=inputs)\n",
      "  File \"/root/miniconda3/lib/python3.8/site-packages/torch/autograd/__init__.py\", line 154, in backward\n",
      "    Variable._execution_engine.run_backward(\n",
      "KeyboardInterrupt\n"
     ]
    }
   ],
   "source": [
    "!python main.py --gpu-id 0 --model-name vgg11_nobias --n-epochs 90 --save-result-file sample.csv \\\n",
    "--sweep-name exp_sample --correction --ground-metric euclidean --weight-stats \\\n",
    "--geom-ensemble-type wts --ground-metric-normalize none --sweep-id 90 \\\n",
    "--ckpt-type best --dataset Cifar10 --ground-metric-eff --recheck-cifar --activation-seed {activation_seed} \\\n",
    "--prelu-acts --past-correction --not-squared --exact\\\n",
    "--learning-rate 0.08\\\n",
    "--momentum 0.5\\\n",
    "--batch-size-train 64 --experiment-dir {experiment_dir}\\\n",
    "--prunint-rate {prunint_rate}\\\n",
    "--run-mode {run_mode}\\\n",
    "--to-download\\\n",
    "--load-model-dir {load_model_dir}\\\n",
    "--retrain 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "19ce2567-adad-4f09-95a3-4449ed54d54d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#2:fine prune without finetune\n",
    "activation_seed=21\n",
    "experiment_dir=\"experiment3\"\n",
    "prunint_rate=0.4\n",
    "run_mode='finetune'\n",
    "load_model_dir='./cifar_models'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "85412569-b402-468e-9014-991148ecb970",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------- Setting up parameters -------\n",
      "dumping parameters at  /root/autodl-tmp/OTfusion_pruning/otfusion/exp_sample/configurations\n",
      "The parameters are: \n",
      " Namespace(act_bug=False, act_num_samples=100, activation_histograms=False, activation_mode=None, activation_seed=21, alpha=0.7, baseroot='/root/autodl-tmp/OTfusion_pruning/otfusion', batch_size_test=1000, batch_size_train=256, center_acts=False, choice='0 2 4 6 8', cifar_style_data=False, ckpt_type='best', clip_gm=False, clip_max=5, clip_min=0, config_dir='/root/autodl-tmp/OTfusion_pruning/otfusion/exp_sample/configurations', config_file=None, correction=True, csv_dir='/root/autodl-tmp/OTfusion_pruning/otfusion/exp_sample/csv', dataset='Cifar10', debug=False, deprecated=None, deterministic=False, diff_init=False, disable_bias=True, dist_epochs=60, dist_normalize=False, dump_final_models=False, dump_model=False, enable_dropout=False, ensemble_step=0.5, eval_aligned=False, exact=True, exp_name='exp_2023-09-17_11-50-46_510369', experiment_dir='experiment3', geom_ensemble_type='wts', gpu_id=0, gromov=False, gromov_loss='square_loss', ground_metric='euclidean', ground_metric_eff=True, ground_metric_normalize='none', handle_skips=False, importance=None, learning_rate=0.0001, load_model_dir='./cifar_models', load_models='', log_interval=100, model_name='vgg11_nobias', momentum=0.5, n_epochs=30, no_random_trainloaders=False, normalize_acts=False, normalize_wts=False, not_squared=True, num_hidden_nodes=400, num_hidden_nodes1=400, num_hidden_nodes2=200, num_hidden_nodes3=100, num_hidden_nodes4=50, num_models=2, options_type='generic', partial_reshape=False, partition_dataloader=-1, partition_type='labels', past_correction=True, personal_class_idx=9, personal_split_frac=0.1, pool_acts=False, pool_relu=False, prediction_wts=False, prelu_acts=True, print_distances=False, proper_marginals=False, prunint_rate=0.4, recheck_acc=False, recheck_cifar=True, reg=0.01, reg_m=0.001, reinit_trainloaders=False, result_dir='/root/autodl-tmp/OTfusion_pruning/otfusion/exp_sample/results', retrain=30, retrain_avg_only=False, retrain_geometric_only=False, retrain_lr_decay=-1, retrain_lr_decay_epochs=None, retrain_lr_decay_factor=None, retrain_seed=-1, rootdir='/root/autodl-tmp/OTfusion_pruning/otfusion/exp_sample', run_mode='finetune', same_model=-1, save_result_file='sample.csv', second_model_name=None, sinkhorn_type='normal', skip_last_layer=False, skip_last_layer_type='average', skip_personal_idx=False, skip_retrain=-1, softmax_temperature=1, standardize_acts=False, sweep_id=90, sweep_name='exp_sample', temperature=20, tensorboard=False, tensorboard_root='./tensorboard', timestamp='2023-09-17_11-50-46_510369', tmap_stats=False, to_download=True, transform_acts=False, unbalanced=False, update_acts=False, weight_stats=True, width_ratio=1)\n",
      "refactored get_config\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "in _make_layers [Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), AvgPool2d(kernel_size=1, stride=1, padding=0)]\n",
      "Relu Inplace is  False\n",
      "model parameters are \n",
      " [torch.Size([64, 3, 3, 3]), torch.Size([128, 64, 3, 3]), torch.Size([256, 128, 3, 3]), torch.Size([256, 256, 3, 3]), torch.Size([512, 256, 3, 3]), torch.Size([512, 512, 3, 3]), torch.Size([512, 512, 3, 3]), torch.Size([512, 512, 3, 3]), torch.Size([10, 512])]\n",
      "in _make_layers [Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), AvgPool2d(kernel_size=1, stride=1, padding=0)]\n",
      "Relu Inplace is  False\n",
      "model parameters are \n",
      " [torch.Size([64, 3, 3, 3]), torch.Size([128, 64, 3, 3]), torch.Size([256, 128, 3, 3]), torch.Size([256, 256, 3, 3]), torch.Size([512, 256, 3, 3]), torch.Size([512, 512, 3, 3]), torch.Size([512, 512, 3, 3]), torch.Size([512, 512, 3, 3]), torch.Size([10, 512])]\n",
      "in _make_layers [Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), AvgPool2d(kernel_size=1, stride=1, padding=0)]\n",
      "Relu Inplace is  False\n",
      "model parameters are \n",
      " [torch.Size([64, 3, 3, 3]), torch.Size([128, 64, 3, 3]), torch.Size([256, 128, 3, 3]), torch.Size([256, 256, 3, 3]), torch.Size([512, 256, 3, 3]), torch.Size([512, 512, 3, 3]), torch.Size([512, 512, 3, 3]), torch.Size([512, 512, 3, 3]), torch.Size([10, 512])]\n",
      "in _make_layers [Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), AvgPool2d(kernel_size=1, stride=1, padding=0)]\n",
      "Relu Inplace is  False\n",
      "model parameters are \n",
      " [torch.Size([64, 3, 3, 3]), torch.Size([128, 64, 3, 3]), torch.Size([256, 128, 3, 3]), torch.Size([256, 256, 3, 3]), torch.Size([512, 256, 3, 3]), torch.Size([512, 512, 3, 3]), torch.Size([512, 512, 3, 3]), torch.Size([512, 512, 3, 3]), torch.Size([10, 512])]\n",
      "Train Epoch: 1 [0/50000 (0%)]\tLoss: 0.178969\n",
      "Train Epoch: 1 [25600/50000 (51%)]\tLoss: 0.180128\n",
      "\n",
      "--------- Testing in global mode ---------\n",
      "size of test_loader dataset:  10000\n",
      "\n",
      "Test set: Avg. loss: 0.0004, Accuracy: 8892/10000 (89%)\n",
      "\n",
      "Train Epoch: 2 [0/50000 (0%)]\tLoss: 0.140478\n",
      "Train Epoch: 2 [25600/50000 (51%)]\tLoss: 0.145628\n",
      "\n",
      "--------- Testing in global mode ---------\n",
      "size of test_loader dataset:  10000\n",
      "\n",
      "Test set: Avg. loss: 0.0003, Accuracy: 8926/10000 (89%)\n",
      "\n",
      "Train Epoch: 3 [0/50000 (0%)]\tLoss: 0.093215\n",
      "Train Epoch: 3 [25600/50000 (51%)]\tLoss: 0.089919\n",
      "\n",
      "--------- Testing in global mode ---------\n",
      "size of test_loader dataset:  10000\n",
      "\n",
      "Test set: Avg. loss: 0.0003, Accuracy: 8951/10000 (90%)\n",
      "\n",
      "Train Epoch: 4 [0/50000 (0%)]\tLoss: 0.114992\n",
      "Train Epoch: 4 [25600/50000 (51%)]\tLoss: 0.094960\n",
      "\n",
      "--------- Testing in global mode ---------\n",
      "size of test_loader dataset:  10000\n",
      "\n",
      "Test set: Avg. loss: 0.0003, Accuracy: 8975/10000 (90%)\n",
      "\n",
      "Train Epoch: 5 [0/50000 (0%)]\tLoss: 0.105624\n",
      "Train Epoch: 5 [25600/50000 (51%)]\tLoss: 0.094985\n",
      "\n",
      "--------- Testing in global mode ---------\n",
      "size of test_loader dataset:  10000\n",
      "\n",
      "Test set: Avg. loss: 0.0003, Accuracy: 8977/10000 (90%)\n",
      "\n",
      "Train Epoch: 6 [0/50000 (0%)]\tLoss: 0.102806\n",
      "Train Epoch: 6 [25600/50000 (51%)]\tLoss: 0.095137\n",
      "\n",
      "--------- Testing in global mode ---------\n",
      "size of test_loader dataset:  10000\n",
      "\n",
      "Test set: Avg. loss: 0.0003, Accuracy: 8979/10000 (90%)\n",
      "\n",
      "Train Epoch: 7 [0/50000 (0%)]\tLoss: 0.097771\n",
      "Train Epoch: 7 [25600/50000 (51%)]\tLoss: 0.116046\n",
      "\n",
      "--------- Testing in global mode ---------\n",
      "size of test_loader dataset:  10000\n",
      "\n",
      "Test set: Avg. loss: 0.0003, Accuracy: 8988/10000 (90%)\n",
      "\n",
      "Train Epoch: 8 [0/50000 (0%)]\tLoss: 0.081791\n",
      "Train Epoch: 8 [25600/50000 (51%)]\tLoss: 0.106019\n",
      "\n",
      "--------- Testing in global mode ---------\n",
      "size of test_loader dataset:  10000\n",
      "\n",
      "Test set: Avg. loss: 0.0003, Accuracy: 8998/10000 (90%)\n",
      "\n",
      "Train Epoch: 9 [0/50000 (0%)]\tLoss: 0.083203\n",
      "Train Epoch: 9 [25600/50000 (51%)]\tLoss: 0.098845\n",
      "\n",
      "--------- Testing in global mode ---------\n",
      "size of test_loader dataset:  10000\n",
      "\n",
      "Test set: Avg. loss: 0.0003, Accuracy: 9002/10000 (90%)\n",
      "\n",
      "Train Epoch: 10 [0/50000 (0%)]\tLoss: 0.083263\n",
      "Train Epoch: 10 [25600/50000 (51%)]\tLoss: 0.085924\n",
      "\n",
      "--------- Testing in global mode ---------\n",
      "size of test_loader dataset:  10000\n",
      "\n",
      "Test set: Avg. loss: 0.0003, Accuracy: 9002/10000 (90%)\n",
      "\n",
      "Train Epoch: 11 [0/50000 (0%)]\tLoss: 0.078714\n",
      "Train Epoch: 11 [25600/50000 (51%)]\tLoss: 0.064297\n",
      "\n",
      "--------- Testing in global mode ---------\n",
      "size of test_loader dataset:  10000\n",
      "\n",
      "Test set: Avg. loss: 0.0003, Accuracy: 9006/10000 (90%)\n",
      "\n",
      "Train Epoch: 12 [0/50000 (0%)]\tLoss: 0.063442\n",
      "Train Epoch: 12 [25600/50000 (51%)]\tLoss: 0.086731\n",
      "\n",
      "--------- Testing in global mode ---------\n",
      "size of test_loader dataset:  10000\n",
      "\n",
      "Test set: Avg. loss: 0.0003, Accuracy: 9004/10000 (90%)\n",
      "\n",
      "Train Epoch: 13 [0/50000 (0%)]\tLoss: 0.063324\n",
      "Train Epoch: 13 [25600/50000 (51%)]\tLoss: 0.062709\n",
      "\n",
      "--------- Testing in global mode ---------\n",
      "size of test_loader dataset:  10000\n",
      "\n",
      "Test set: Avg. loss: 0.0003, Accuracy: 9005/10000 (90%)\n",
      "\n",
      "Train Epoch: 14 [0/50000 (0%)]\tLoss: 0.085402\n",
      "Train Epoch: 14 [25600/50000 (51%)]\tLoss: 0.075090\n",
      "\n",
      "--------- Testing in global mode ---------\n",
      "size of test_loader dataset:  10000\n",
      "\n",
      "Test set: Avg. loss: 0.0003, Accuracy: 9008/10000 (90%)\n",
      "\n",
      "Train Epoch: 15 [0/50000 (0%)]\tLoss: 0.072183\n",
      "Train Epoch: 15 [25600/50000 (51%)]\tLoss: 0.082611\n",
      "\n",
      "--------- Testing in global mode ---------\n",
      "size of test_loader dataset:  10000\n",
      "\n",
      "Test set: Avg. loss: 0.0003, Accuracy: 9006/10000 (90%)\n",
      "\n",
      "Train Epoch: 16 [0/50000 (0%)]\tLoss: 0.071740\n",
      "Train Epoch: 16 [25600/50000 (51%)]\tLoss: 0.062745\n",
      "\n",
      "--------- Testing in global mode ---------\n",
      "size of test_loader dataset:  10000\n",
      "\n",
      "Test set: Avg. loss: 0.0003, Accuracy: 9005/10000 (90%)\n",
      "\n",
      "Train Epoch: 17 [0/50000 (0%)]\tLoss: 0.074094\n",
      "Train Epoch: 17 [25600/50000 (51%)]\tLoss: 0.058120\n",
      "\n",
      "--------- Testing in global mode ---------\n",
      "size of test_loader dataset:  10000\n",
      "\n",
      "Test set: Avg. loss: 0.0003, Accuracy: 9004/10000 (90%)\n",
      "\n",
      "Train Epoch: 18 [0/50000 (0%)]\tLoss: 0.058920\n",
      "Train Epoch: 18 [25600/50000 (51%)]\tLoss: 0.064377\n",
      "\n",
      "--------- Testing in global mode ---------\n",
      "size of test_loader dataset:  10000\n",
      "\n",
      "Test set: Avg. loss: 0.0003, Accuracy: 9003/10000 (90%)\n",
      "\n",
      "Train Epoch: 19 [0/50000 (0%)]\tLoss: 0.057359\n",
      "Train Epoch: 19 [25600/50000 (51%)]\tLoss: 0.057406\n",
      "\n",
      "--------- Testing in global mode ---------\n",
      "size of test_loader dataset:  10000\n",
      "\n",
      "Test set: Avg. loss: 0.0003, Accuracy: 9004/10000 (90%)\n",
      "\n",
      "Train Epoch: 20 [0/50000 (0%)]\tLoss: 0.076530\n",
      "Train Epoch: 20 [25600/50000 (51%)]\tLoss: 0.055137\n",
      "\n",
      "--------- Testing in global mode ---------\n",
      "size of test_loader dataset:  10000\n",
      "\n",
      "Test set: Avg. loss: 0.0003, Accuracy: 9004/10000 (90%)\n",
      "\n",
      "Train Epoch: 21 [0/50000 (0%)]\tLoss: 0.054973\n",
      "Train Epoch: 21 [25600/50000 (51%)]\tLoss: 0.078310\n",
      "\n",
      "--------- Testing in global mode ---------\n",
      "size of test_loader dataset:  10000\n",
      "\n",
      "Test set: Avg. loss: 0.0003, Accuracy: 9007/10000 (90%)\n",
      "\n",
      "Train Epoch: 22 [0/50000 (0%)]\tLoss: 0.082074\n",
      "Train Epoch: 22 [25600/50000 (51%)]\tLoss: 0.059663\n",
      "\n",
      "--------- Testing in global mode ---------\n",
      "size of test_loader dataset:  10000\n",
      "\n",
      "Test set: Avg. loss: 0.0003, Accuracy: 9005/10000 (90%)\n",
      "\n",
      "Train Epoch: 23 [0/50000 (0%)]\tLoss: 0.055686\n",
      "Train Epoch: 23 [25600/50000 (51%)]\tLoss: 0.064356\n",
      "\n",
      "--------- Testing in global mode ---------\n",
      "size of test_loader dataset:  10000\n",
      "\n",
      "Test set: Avg. loss: 0.0003, Accuracy: 9007/10000 (90%)\n",
      "\n",
      "Train Epoch: 24 [0/50000 (0%)]\tLoss: 0.051338\n",
      "Train Epoch: 24 [25600/50000 (51%)]\tLoss: 0.064764\n",
      "\n",
      "--------- Testing in global mode ---------\n",
      "size of test_loader dataset:  10000\n",
      "\n",
      "Test set: Avg. loss: 0.0003, Accuracy: 9009/10000 (90%)\n",
      "\n",
      "Train Epoch: 25 [0/50000 (0%)]\tLoss: 0.050922\n",
      "Train Epoch: 25 [25600/50000 (51%)]\tLoss: 0.053179\n",
      "\n",
      "--------- Testing in global mode ---------\n",
      "size of test_loader dataset:  10000\n",
      "\n",
      "Test set: Avg. loss: 0.0003, Accuracy: 9009/10000 (90%)\n",
      "\n",
      "Train Epoch: 26 [0/50000 (0%)]\tLoss: 0.065174\n",
      "Train Epoch: 26 [25600/50000 (51%)]\tLoss: 0.047972\n",
      "\n",
      "--------- Testing in global mode ---------\n",
      "size of test_loader dataset:  10000\n",
      "\n",
      "Test set: Avg. loss: 0.0003, Accuracy: 9010/10000 (90%)\n",
      "\n",
      "Train Epoch: 27 [0/50000 (0%)]\tLoss: 0.042085\n",
      "Train Epoch: 27 [25600/50000 (51%)]\tLoss: 0.070208\n",
      "\n",
      "--------- Testing in global mode ---------\n",
      "size of test_loader dataset:  10000\n",
      "\n",
      "Test set: Avg. loss: 0.0003, Accuracy: 9010/10000 (90%)\n",
      "\n",
      "Train Epoch: 28 [0/50000 (0%)]\tLoss: 0.058303\n",
      "Train Epoch: 28 [25600/50000 (51%)]\tLoss: 0.053525\n",
      "\n",
      "--------- Testing in global mode ---------\n",
      "size of test_loader dataset:  10000\n",
      "\n",
      "Test set: Avg. loss: 0.0003, Accuracy: 9012/10000 (90%)\n",
      "\n",
      "Train Epoch: 29 [0/50000 (0%)]\tLoss: 0.054408\n",
      "Train Epoch: 29 [25600/50000 (51%)]\tLoss: 0.058064\n",
      "\n",
      "--------- Testing in global mode ---------\n",
      "size of test_loader dataset:  10000\n",
      "\n",
      "Test set: Avg. loss: 0.0003, Accuracy: 9013/10000 (90%)\n",
      "\n",
      "Train Epoch: 30 [0/50000 (0%)]\tLoss: 0.067753\n",
      "Train Epoch: 30 [25600/50000 (51%)]\tLoss: 0.054484\n",
      "\n",
      "--------- Testing in global mode ---------\n",
      "size of test_loader dataset:  10000\n",
      "\n",
      "Test set: Avg. loss: 0.0003, Accuracy: 9016/10000 (90%)\n",
      "\n",
      "Train Epoch: 1 [0/50000 (0%)]\tLoss: 0.144045\n",
      "Train Epoch: 1 [25600/50000 (51%)]\tLoss: 0.120290\n",
      "\n",
      "--------- Testing in global mode ---------\n",
      "size of test_loader dataset:  10000\n",
      "\n",
      "Test set: Avg. loss: 0.0003, Accuracy: 8983/10000 (90%)\n",
      "\n",
      "Train Epoch: 2 [0/50000 (0%)]\tLoss: 0.120943\n",
      "Train Epoch: 2 [25600/50000 (51%)]\tLoss: 0.117957\n",
      "\n",
      "--------- Testing in global mode ---------\n",
      "size of test_loader dataset:  10000\n",
      "\n",
      "Test set: Avg. loss: 0.0003, Accuracy: 8988/10000 (90%)\n",
      "\n",
      "Train Epoch: 3 [0/50000 (0%)]\tLoss: 0.117400\n",
      "Train Epoch: 3 [25600/50000 (51%)]\tLoss: 0.109174\n",
      "\n",
      "--------- Testing in global mode ---------\n",
      "size of test_loader dataset:  10000\n",
      "\n",
      "Test set: Avg. loss: 0.0003, Accuracy: 9010/10000 (90%)\n",
      "\n",
      "Train Epoch: 4 [0/50000 (0%)]\tLoss: 0.115520\n",
      "Train Epoch: 4 [25600/50000 (51%)]\tLoss: 0.099790\n",
      "\n",
      "--------- Testing in global mode ---------\n",
      "size of test_loader dataset:  10000\n",
      "\n",
      "Test set: Avg. loss: 0.0003, Accuracy: 9009/10000 (90%)\n",
      "\n",
      "Train Epoch: 5 [0/50000 (0%)]\tLoss: 0.090496\n",
      "Train Epoch: 5 [25600/50000 (51%)]\tLoss: 0.104007\n",
      "\n",
      "--------- Testing in global mode ---------\n",
      "size of test_loader dataset:  10000\n",
      "\n",
      "Test set: Avg. loss: 0.0003, Accuracy: 9010/10000 (90%)\n",
      "\n",
      "Train Epoch: 6 [0/50000 (0%)]\tLoss: 0.088050\n",
      "Train Epoch: 6 [25600/50000 (51%)]\tLoss: 0.091592\n",
      "\n",
      "--------- Testing in global mode ---------\n",
      "size of test_loader dataset:  10000\n",
      "\n",
      "Test set: Avg. loss: 0.0003, Accuracy: 9006/10000 (90%)\n",
      "\n",
      "Train Epoch: 7 [0/50000 (0%)]\tLoss: 0.075472\n",
      "Train Epoch: 7 [25600/50000 (51%)]\tLoss: 0.077820\n",
      "\n",
      "--------- Testing in global mode ---------\n",
      "size of test_loader dataset:  10000\n",
      "\n",
      "Test set: Avg. loss: 0.0003, Accuracy: 9012/10000 (90%)\n",
      "\n",
      "Train Epoch: 8 [0/50000 (0%)]\tLoss: 0.082348\n",
      "Train Epoch: 8 [25600/50000 (51%)]\tLoss: 0.089109\n",
      "\n",
      "--------- Testing in global mode ---------\n",
      "size of test_loader dataset:  10000\n",
      "\n",
      "Test set: Avg. loss: 0.0003, Accuracy: 9011/10000 (90%)\n",
      "\n",
      "Train Epoch: 9 [0/50000 (0%)]\tLoss: 0.101829\n",
      "Train Epoch: 9 [25600/50000 (51%)]\tLoss: 0.080027\n",
      "\n",
      "--------- Testing in global mode ---------\n",
      "size of test_loader dataset:  10000\n",
      "\n",
      "Test set: Avg. loss: 0.0003, Accuracy: 9013/10000 (90%)\n",
      "\n",
      "Train Epoch: 10 [0/50000 (0%)]\tLoss: 0.080267\n",
      "Train Epoch: 10 [25600/50000 (51%)]\tLoss: 0.092365\n",
      "\n",
      "--------- Testing in global mode ---------\n",
      "size of test_loader dataset:  10000\n",
      "\n",
      "Test set: Avg. loss: 0.0003, Accuracy: 9015/10000 (90%)\n",
      "\n",
      "Train Epoch: 11 [0/50000 (0%)]\tLoss: 0.072173\n",
      "Train Epoch: 11 [25600/50000 (51%)]\tLoss: 0.073340\n",
      "\n",
      "--------- Testing in global mode ---------\n",
      "size of test_loader dataset:  10000\n",
      "\n",
      "Test set: Avg. loss: 0.0003, Accuracy: 9015/10000 (90%)\n",
      "\n",
      "Train Epoch: 12 [0/50000 (0%)]\tLoss: 0.082251\n",
      "Train Epoch: 12 [25600/50000 (51%)]\tLoss: 0.062662\n",
      "\n",
      "--------- Testing in global mode ---------\n",
      "size of test_loader dataset:  10000\n",
      "\n",
      "Test set: Avg. loss: 0.0003, Accuracy: 9017/10000 (90%)\n",
      "\n",
      "Train Epoch: 13 [0/50000 (0%)]\tLoss: 0.090417\n",
      "Train Epoch: 13 [25600/50000 (51%)]\tLoss: 0.066166\n",
      "\n",
      "--------- Testing in global mode ---------\n",
      "size of test_loader dataset:  10000\n",
      "\n",
      "Test set: Avg. loss: 0.0003, Accuracy: 9016/10000 (90%)\n",
      "\n",
      "Train Epoch: 14 [0/50000 (0%)]\tLoss: 0.073244\n",
      "Train Epoch: 14 [25600/50000 (51%)]\tLoss: 0.068941\n",
      "\n",
      "--------- Testing in global mode ---------\n",
      "size of test_loader dataset:  10000\n",
      "\n",
      "Test set: Avg. loss: 0.0003, Accuracy: 9011/10000 (90%)\n",
      "\n",
      "Train Epoch: 15 [0/50000 (0%)]\tLoss: 0.068140\n",
      "Train Epoch: 15 [25600/50000 (51%)]\tLoss: 0.073182\n",
      "\n",
      "--------- Testing in global mode ---------\n",
      "size of test_loader dataset:  10000\n",
      "\n",
      "Test set: Avg. loss: 0.0003, Accuracy: 9010/10000 (90%)\n",
      "\n",
      "Train Epoch: 16 [0/50000 (0%)]\tLoss: 0.085002\n",
      "Train Epoch: 16 [25600/50000 (51%)]\tLoss: 0.056779\n",
      "\n",
      "--------- Testing in global mode ---------\n",
      "size of test_loader dataset:  10000\n",
      "\n",
      "Test set: Avg. loss: 0.0003, Accuracy: 9012/10000 (90%)\n",
      "\n",
      "Train Epoch: 17 [0/50000 (0%)]\tLoss: 0.060542\n",
      "Train Epoch: 17 [25600/50000 (51%)]\tLoss: 0.064546\n",
      "\n",
      "--------- Testing in global mode ---------\n",
      "size of test_loader dataset:  10000\n",
      "\n",
      "Test set: Avg. loss: 0.0003, Accuracy: 9015/10000 (90%)\n",
      "\n",
      "Train Epoch: 18 [0/50000 (0%)]\tLoss: 0.082317\n",
      "Train Epoch: 18 [25600/50000 (51%)]\tLoss: 0.071617\n",
      "\n",
      "--------- Testing in global mode ---------\n",
      "size of test_loader dataset:  10000\n",
      "\n",
      "Test set: Avg. loss: 0.0003, Accuracy: 9016/10000 (90%)\n",
      "\n",
      "Train Epoch: 19 [0/50000 (0%)]\tLoss: 0.055011\n",
      "Train Epoch: 19 [25600/50000 (51%)]\tLoss: 0.061906\n",
      "\n",
      "--------- Testing in global mode ---------\n",
      "size of test_loader dataset:  10000\n",
      "\n",
      "Test set: Avg. loss: 0.0003, Accuracy: 9016/10000 (90%)\n",
      "\n",
      "Train Epoch: 20 [0/50000 (0%)]\tLoss: 0.069494\n",
      "Train Epoch: 20 [25600/50000 (51%)]\tLoss: 0.068329\n",
      "\n",
      "--------- Testing in global mode ---------\n",
      "size of test_loader dataset:  10000\n",
      "\n",
      "Test set: Avg. loss: 0.0003, Accuracy: 9015/10000 (90%)\n",
      "\n",
      "Train Epoch: 21 [0/50000 (0%)]\tLoss: 0.062743\n",
      "Train Epoch: 21 [25600/50000 (51%)]\tLoss: 0.065254\n",
      "\n",
      "--------- Testing in global mode ---------\n",
      "size of test_loader dataset:  10000\n",
      "\n",
      "Test set: Avg. loss: 0.0003, Accuracy: 9014/10000 (90%)\n",
      "\n",
      "Train Epoch: 22 [0/50000 (0%)]\tLoss: 0.047447\n",
      "Train Epoch: 22 [25600/50000 (51%)]\tLoss: 0.064054\n",
      "\n",
      "--------- Testing in global mode ---------\n",
      "size of test_loader dataset:  10000\n",
      "\n",
      "Test set: Avg. loss: 0.0003, Accuracy: 9013/10000 (90%)\n",
      "\n",
      "Train Epoch: 23 [0/50000 (0%)]\tLoss: 0.076183\n",
      "Train Epoch: 23 [25600/50000 (51%)]\tLoss: 0.052984\n",
      "\n",
      "--------- Testing in global mode ---------\n",
      "size of test_loader dataset:  10000\n",
      "\n",
      "Test set: Avg. loss: 0.0003, Accuracy: 9013/10000 (90%)\n",
      "\n",
      "Train Epoch: 24 [0/50000 (0%)]\tLoss: 0.047630\n",
      "Train Epoch: 24 [25600/50000 (51%)]\tLoss: 0.065088\n",
      "\n",
      "--------- Testing in global mode ---------\n",
      "size of test_loader dataset:  10000\n",
      "\n",
      "Test set: Avg. loss: 0.0003, Accuracy: 9015/10000 (90%)\n",
      "\n",
      "Train Epoch: 25 [0/50000 (0%)]\tLoss: 0.063131\n",
      "Train Epoch: 25 [25600/50000 (51%)]\tLoss: 0.054986\n",
      "\n",
      "--------- Testing in global mode ---------\n",
      "size of test_loader dataset:  10000\n",
      "\n",
      "Test set: Avg. loss: 0.0003, Accuracy: 9017/10000 (90%)\n",
      "\n",
      "Train Epoch: 26 [0/50000 (0%)]\tLoss: 0.049239\n",
      "Train Epoch: 26 [25600/50000 (51%)]\tLoss: 0.051804\n",
      "\n",
      "--------- Testing in global mode ---------\n",
      "size of test_loader dataset:  10000\n",
      "\n",
      "Test set: Avg. loss: 0.0003, Accuracy: 9021/10000 (90%)\n",
      "\n",
      "Train Epoch: 27 [0/50000 (0%)]\tLoss: 0.062709\n",
      "Train Epoch: 27 [25600/50000 (51%)]\tLoss: 0.060857\n",
      "\n",
      "--------- Testing in global mode ---------\n",
      "size of test_loader dataset:  10000\n",
      "\n",
      "Test set: Avg. loss: 0.0003, Accuracy: 9024/10000 (90%)\n",
      "\n",
      "Train Epoch: 28 [0/50000 (0%)]\tLoss: 0.062738\n",
      "Train Epoch: 28 [25600/50000 (51%)]\tLoss: 0.053865\n",
      "\n",
      "--------- Testing in global mode ---------\n",
      "size of test_loader dataset:  10000\n",
      "\n",
      "Test set: Avg. loss: 0.0003, Accuracy: 9026/10000 (90%)\n",
      "\n",
      "Train Epoch: 29 [0/50000 (0%)]\tLoss: 0.050647\n",
      "Train Epoch: 29 [25600/50000 (51%)]\tLoss: 0.064241\n",
      "\n",
      "--------- Testing in global mode ---------\n",
      "size of test_loader dataset:  10000\n",
      "\n",
      "Test set: Avg. loss: 0.0003, Accuracy: 9027/10000 (90%)\n",
      "\n",
      "Train Epoch: 30 [0/50000 (0%)]\tLoss: 0.046116\n",
      "Train Epoch: 30 [25600/50000 (51%)]\tLoss: 0.061338\n",
      "\n",
      "--------- Testing in global mode ---------\n",
      "size of test_loader dataset:  10000\n",
      "\n",
      "Test set: Avg. loss: 0.0003, Accuracy: 9023/10000 (90%)\n",
      "\n",
      "check zero rate for model 0\n",
      "odict_keys(['features.0.weight', 'features.3.weight', 'features.6.weight', 'features.8.weight', 'features.11.weight', 'features.13.weight', 'features.16.weight', 'features.18.weight', 'classifier.weight'])\n",
      "the ratio of zero for features.0.weight\n",
      "tensor(0.3999, device='cuda:0')\n",
      "the ratio of zero for features.3.weight\n",
      "tensor(0.4000, device='cuda:0')\n",
      "the ratio of zero for features.6.weight\n",
      "tensor(0.4000, device='cuda:0')\n",
      "the ratio of zero for features.8.weight\n",
      "tensor(0.4000, device='cuda:0')\n",
      "the ratio of zero for features.11.weight\n",
      "tensor(0.4000, device='cuda:0')\n",
      "the ratio of zero for features.13.weight\n",
      "tensor(0.4000, device='cuda:0')\n",
      "the ratio of zero for features.16.weight\n",
      "tensor(0.4000, device='cuda:0')\n",
      "the ratio of zero for features.18.weight\n",
      "tensor(0.4000, device='cuda:0')\n",
      "the ratio of zero for classifier.weight\n",
      "tensor(0.4000, device='cuda:0')\n",
      "check zero rate for model 1\n",
      "odict_keys(['features.0.weight', 'features.3.weight', 'features.6.weight', 'features.8.weight', 'features.11.weight', 'features.13.weight', 'features.16.weight', 'features.18.weight', 'classifier.weight'])\n",
      "the ratio of zero for features.0.weight\n",
      "tensor(0.3999, device='cuda:0')\n",
      "the ratio of zero for features.3.weight\n",
      "tensor(0.4000, device='cuda:0')\n",
      "the ratio of zero for features.6.weight\n",
      "tensor(0.4000, device='cuda:0')\n",
      "the ratio of zero for features.8.weight\n",
      "tensor(0.4000, device='cuda:0')\n",
      "the ratio of zero for features.11.weight\n",
      "tensor(0.4000, device='cuda:0')\n",
      "the ratio of zero for features.13.weight\n",
      "tensor(0.4000, device='cuda:0')\n",
      "the ratio of zero for features.16.weight\n",
      "tensor(0.4000, device='cuda:0')\n",
      "the ratio of zero for features.18.weight\n",
      "tensor(0.4000, device='cuda:0')\n",
      "the ratio of zero for classifier.weight\n",
      "tensor(0.4000, device='cuda:0')\n",
      "layer features.0.weight has #params  1728\n",
      "layer features.3.weight has #params  73728\n",
      "layer features.6.weight has #params  294912\n",
      "layer features.8.weight has #params  589824\n",
      "layer features.11.weight has #params  1179648\n",
      "layer features.13.weight has #params  2359296\n",
      "layer features.16.weight has #params  2359296\n",
      "layer features.18.weight has #params  2359296\n",
      "layer classifier.weight has #params  5120\n",
      "Activation Timer start\n",
      "Activation Timer ends\n",
      "------- Geometric Ensembling -------\n",
      "Timer start\n",
      "odict_keys(['features.0.weight', 'features.3.weight', 'features.6.weight', 'features.8.weight', 'features.11.weight', 'features.13.weight', 'features.16.weight', 'features.18.weight', 'classifier.weight'])\n",
      "the ratio of zero for features.0.weight\n",
      "tensor(0.3999, device='cuda:0')\n",
      "the ratio of zero for features.3.weight\n",
      "tensor(0.4000, device='cuda:0')\n",
      "the ratio of zero for features.6.weight\n",
      "tensor(0.4000, device='cuda:0')\n",
      "the ratio of zero for features.8.weight\n",
      "tensor(0.4000, device='cuda:0')\n",
      "the ratio of zero for features.11.weight\n",
      "tensor(0.4000, device='cuda:0')\n",
      "the ratio of zero for features.13.weight\n",
      "tensor(0.4000, device='cuda:0')\n",
      "the ratio of zero for features.16.weight\n",
      "tensor(0.4000, device='cuda:0')\n",
      "the ratio of zero for features.18.weight\n",
      "tensor(0.4000, device='cuda:0')\n",
      "the ratio of zero for classifier.weight\n",
      "tensor(0.4000, device='cuda:0')\n",
      "Previous layer shape is  None\n",
      "Processing the coordinates to form ground_metric\n",
      "dont leave off the squaring of the ground metric\n",
      "returns a uniform measure of cardinality:  64\n",
      "returns a uniform measure of cardinality:  64\n",
      "the transport map is  tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')\n",
      "Ratio of trace to the matrix sum:  tensor(0.0469, device='cuda:0')\n",
      "Here, trace is 3.0 and matrix sum is 64.0 \n",
      "this is past correction for weight mode\n",
      "Shape of aligned wt is  torch.Size([64, 3, 9])\n",
      "Shape of fc_layer0_weight_data is  torch.Size([64, 3, 9])\n",
      "Previous layer shape is  torch.Size([64, 3, 3, 3])\n",
      "shape of layer: model 0 torch.Size([128, 64, 9])\n",
      "shape of layer: model 1 torch.Size([128, 64, 9])\n",
      "shape of previous transport map torch.Size([64, 64])\n",
      "Processing the coordinates to form ground_metric\n",
      "dont leave off the squaring of the ground metric\n",
      "returns a uniform measure of cardinality:  128\n",
      "returns a uniform measure of cardinality:  128\n",
      "the transport map is  tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        ...,\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0078],\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],\n",
      "       device='cuda:0')\n",
      "Ratio of trace to the matrix sum:  tensor(0., device='cuda:0')\n",
      "Here, trace is 0.0 and matrix sum is 128.0 \n",
      "this is past correction for weight mode\n",
      "Shape of aligned wt is  torch.Size([128, 64, 9])\n",
      "Shape of fc_layer0_weight_data is  torch.Size([128, 64, 9])\n",
      "Previous layer shape is  torch.Size([128, 64, 3, 3])\n",
      "shape of layer: model 0 torch.Size([256, 128, 9])\n",
      "shape of layer: model 1 torch.Size([256, 128, 9])\n",
      "shape of previous transport map torch.Size([128, 128])\n",
      "Processing the coordinates to form ground_metric\n",
      "dont leave off the squaring of the ground metric\n",
      "returns a uniform measure of cardinality:  256\n",
      "returns a uniform measure of cardinality:  256\n",
      "the transport map is  tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')\n",
      "Ratio of trace to the matrix sum:  tensor(0., device='cuda:0')\n",
      "Here, trace is 0.0 and matrix sum is 256.0 \n",
      "this is past correction for weight mode\n",
      "Shape of aligned wt is  torch.Size([256, 128, 9])\n",
      "Shape of fc_layer0_weight_data is  torch.Size([256, 128, 9])\n",
      "Previous layer shape is  torch.Size([256, 128, 3, 3])\n",
      "shape of layer: model 0 torch.Size([256, 256, 9])\n",
      "shape of layer: model 1 torch.Size([256, 256, 9])\n",
      "shape of previous transport map torch.Size([256, 256])\n",
      "Processing the coordinates to form ground_metric\n",
      "dont leave off the squaring of the ground metric\n",
      "returns a uniform measure of cardinality:  256\n",
      "returns a uniform measure of cardinality:  256\n",
      "the transport map is  tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        ...,\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.0039, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],\n",
      "       device='cuda:0')\n",
      "Ratio of trace to the matrix sum:  tensor(0.0078, device='cuda:0')\n",
      "Here, trace is 2.0 and matrix sum is 256.0 \n",
      "this is past correction for weight mode\n",
      "Shape of aligned wt is  torch.Size([256, 256, 9])\n",
      "Shape of fc_layer0_weight_data is  torch.Size([256, 256, 9])\n",
      "Previous layer shape is  torch.Size([256, 256, 3, 3])\n",
      "shape of layer: model 0 torch.Size([512, 256, 9])\n",
      "shape of layer: model 1 torch.Size([512, 256, 9])\n",
      "shape of previous transport map torch.Size([256, 256])\n",
      "Processing the coordinates to form ground_metric\n",
      "dont leave off the squaring of the ground metric\n",
      "returns a uniform measure of cardinality:  512\n",
      "returns a uniform measure of cardinality:  512\n",
      "the transport map is  tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')\n",
      "Ratio of trace to the matrix sum:  tensor(0.0020, device='cuda:0')\n",
      "Here, trace is 1.0 and matrix sum is 512.0 \n",
      "this is past correction for weight mode\n",
      "Shape of aligned wt is  torch.Size([512, 256, 9])\n",
      "Shape of fc_layer0_weight_data is  torch.Size([512, 256, 9])\n",
      "Previous layer shape is  torch.Size([512, 256, 3, 3])\n",
      "shape of layer: model 0 torch.Size([512, 512, 9])\n",
      "shape of layer: model 1 torch.Size([512, 512, 9])\n",
      "shape of previous transport map torch.Size([512, 512])\n",
      "Processing the coordinates to form ground_metric\n",
      "dont leave off the squaring of the ground metric\n",
      "returns a uniform measure of cardinality:  512\n",
      "returns a uniform measure of cardinality:  512\n",
      "the transport map is  tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')\n",
      "Ratio of trace to the matrix sum:  tensor(0.0039, device='cuda:0')\n",
      "Here, trace is 2.0 and matrix sum is 512.0 \n",
      "this is past correction for weight mode\n",
      "Shape of aligned wt is  torch.Size([512, 512, 9])\n",
      "Shape of fc_layer0_weight_data is  torch.Size([512, 512, 9])\n",
      "Previous layer shape is  torch.Size([512, 512, 3, 3])\n",
      "shape of layer: model 0 torch.Size([512, 512, 9])\n",
      "shape of layer: model 1 torch.Size([512, 512, 9])\n",
      "shape of previous transport map torch.Size([512, 512])\n",
      "Processing the coordinates to form ground_metric\n",
      "dont leave off the squaring of the ground metric\n",
      "returns a uniform measure of cardinality:  512\n",
      "returns a uniform measure of cardinality:  512\n",
      "the transport map is  tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')\n",
      "Ratio of trace to the matrix sum:  tensor(0.0059, device='cuda:0')\n",
      "Here, trace is 3.0 and matrix sum is 512.0 \n",
      "this is past correction for weight mode\n",
      "Shape of aligned wt is  torch.Size([512, 512, 9])\n",
      "Shape of fc_layer0_weight_data is  torch.Size([512, 512, 9])\n",
      "Previous layer shape is  torch.Size([512, 512, 3, 3])\n",
      "shape of layer: model 0 torch.Size([512, 512, 9])\n",
      "shape of layer: model 1 torch.Size([512, 512, 9])\n",
      "shape of previous transport map torch.Size([512, 512])\n",
      "Processing the coordinates to form ground_metric\n",
      "dont leave off the squaring of the ground metric\n",
      "returns a uniform measure of cardinality:  512\n",
      "returns a uniform measure of cardinality:  512\n",
      "the transport map is  tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        [0.0020, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        ...,\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],\n",
      "       device='cuda:0')\n",
      "Ratio of trace to the matrix sum:  tensor(0.0020, device='cuda:0')\n",
      "Here, trace is 1.0 and matrix sum is 512.0 \n",
      "this is past correction for weight mode\n",
      "Shape of aligned wt is  torch.Size([512, 512, 9])\n",
      "Shape of fc_layer0_weight_data is  torch.Size([512, 512, 9])\n",
      "Previous layer shape is  torch.Size([512, 512, 3, 3])\n",
      "shape of layer: model 0 torch.Size([10, 512])\n",
      "shape of layer: model 1 torch.Size([10, 512])\n",
      "shape of previous transport map torch.Size([512, 512])\n",
      "Processing the coordinates to form ground_metric\n",
      "dont leave off the squaring of the ground metric\n",
      "ground metric is  tensor([[1.4530, 2.4939, 2.7946, 2.8701, 2.7299, 2.8462, 2.6971, 2.6400, 2.5851,\n",
      "         2.5707],\n",
      "        [2.4691, 1.3595, 2.6772, 2.5766, 2.7120, 2.5214, 2.4133, 2.5323, 2.3108,\n",
      "         2.4075],\n",
      "        [2.7542, 2.6134, 1.3705, 2.9654, 2.9083, 2.8651, 2.7205, 2.8463, 2.7388,\n",
      "         2.7608],\n",
      "        [2.8071, 2.6474, 2.9107, 1.5655, 2.8312, 2.9527, 2.7161, 2.8298, 2.8060,\n",
      "         2.6631],\n",
      "        [2.7657, 2.5262, 2.8824, 2.8830, 1.5312, 2.7297, 2.7806, 2.6674, 2.6744,\n",
      "         2.7053],\n",
      "        [2.9289, 2.5734, 2.9462, 2.7840, 2.7414, 1.5696, 2.7679, 2.7013, 2.7973,\n",
      "         2.6699],\n",
      "        [2.6595, 2.4131, 2.6836, 2.9015, 2.6589, 2.8515, 1.4214, 2.7098, 2.4767,\n",
      "         2.5147],\n",
      "        [2.7411, 2.5442, 2.7324, 2.7728, 2.6912, 2.7012, 2.6761, 1.3752, 2.5525,\n",
      "         2.5243],\n",
      "        [2.5727, 2.4023, 2.7683, 2.7034, 2.7124, 2.8023, 2.5153, 2.6181, 1.2683,\n",
      "         2.3773],\n",
      "        [2.5811, 2.2724, 2.7755, 2.7515, 2.6490, 2.6988, 2.5416, 2.4765, 2.5226,\n",
      "         1.3177]], device='cuda:0', grad_fn=<PowBackward0>)\n",
      "returns a uniform measure of cardinality:  10\n",
      "returns a uniform measure of cardinality:  10\n",
      "the transport map is  tensor([[0.1000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0000, 0.1000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0000, 0.0000, 0.1000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.1000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.1000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.1000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.1000, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.1000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.1000,\n",
      "         0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.1000]], device='cuda:0')\n",
      "Ratio of trace to the matrix sum:  tensor(1., device='cuda:0')\n",
      "Here, trace is 9.99755859375 and matrix sum is 9.99755859375 \n",
      "this is past correction for weight mode\n",
      "Shape of aligned wt is  torch.Size([10, 512])\n",
      "Shape of fc_layer0_weight_data is  torch.Size([10, 512])\n",
      "using independent method\n",
      "in _make_layers [Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), AvgPool2d(kernel_size=1, stride=1, padding=0)]\n",
      "Relu Inplace is  False\n",
      "model parameters are \n",
      " [torch.Size([64, 3, 3, 3]), torch.Size([128, 64, 3, 3]), torch.Size([256, 128, 3, 3]), torch.Size([256, 256, 3, 3]), torch.Size([512, 256, 3, 3]), torch.Size([512, 512, 3, 3]), torch.Size([512, 512, 3, 3]), torch.Size([512, 512, 3, 3]), torch.Size([10, 512])]\n",
      "\n",
      "--------- Testing in global mode ---------\n",
      "size of test_loader dataset:  10000\n",
      "\n",
      "Test set: Avg. loss: 0.0023, Accuracy: 1000/10000 (10%)\n",
      "\n",
      "len of model parameters and avg aligned layers is  9 9\n",
      "len of model_state_dict is  9\n",
      "len of param_list is  9\n",
      "\n",
      "--------- Testing in global mode ---------\n",
      "size of test_loader dataset:  10000\n",
      "\n",
      "Test set: Avg. loss: 0.0013, Accuracy: 8495/10000 (85%)\n",
      "\n",
      "===============================================\n",
      "===============================================\n",
      "After Ensemble: 84.95\n",
      "===============================================\n",
      "===============================================\n",
      "Timer ends\n",
      "Time taken for geometric ensembling is 3.450525116175413 seconds\n",
      "------- Prediction based ensembling -------\n",
      "\n",
      "Test set: Avg. loss: 0.0003, Accuracy: 9114/10000 (91%)\n",
      "\n",
      "------- Naive ensembling of weights -------\n",
      "[torch.Size([64, 3, 3, 3]), torch.Size([64, 3, 3, 3])]\n",
      "torch.Size([64, 3, 3, 3])\n",
      "[torch.Size([128, 64, 3, 3]), torch.Size([128, 64, 3, 3])]\n",
      "torch.Size([128, 64, 3, 3])\n",
      "[torch.Size([256, 128, 3, 3]), torch.Size([256, 128, 3, 3])]\n",
      "torch.Size([256, 128, 3, 3])\n",
      "[torch.Size([256, 256, 3, 3]), torch.Size([256, 256, 3, 3])]\n",
      "torch.Size([256, 256, 3, 3])\n",
      "[torch.Size([512, 256, 3, 3]), torch.Size([512, 256, 3, 3])]\n",
      "torch.Size([512, 256, 3, 3])\n",
      "[torch.Size([512, 512, 3, 3]), torch.Size([512, 512, 3, 3])]\n",
      "torch.Size([512, 512, 3, 3])\n",
      "[torch.Size([512, 512, 3, 3]), torch.Size([512, 512, 3, 3])]\n",
      "torch.Size([512, 512, 3, 3])\n",
      "[torch.Size([512, 512, 3, 3]), torch.Size([512, 512, 3, 3])]\n",
      "torch.Size([512, 512, 3, 3])\n",
      "[torch.Size([10, 512]), torch.Size([10, 512])]\n",
      "torch.Size([10, 512])\n",
      "in _make_layers [Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), AvgPool2d(kernel_size=1, stride=1, padding=0)]\n",
      "Relu Inplace is  False\n",
      "model parameters are \n",
      " [torch.Size([64, 3, 3, 3]), torch.Size([128, 64, 3, 3]), torch.Size([256, 128, 3, 3]), torch.Size([256, 256, 3, 3]), torch.Size([512, 256, 3, 3]), torch.Size([512, 512, 3, 3]), torch.Size([512, 512, 3, 3]), torch.Size([512, 512, 3, 3]), torch.Size([10, 512])]\n",
      "\n",
      "--------- Testing in global mode ---------\n",
      "size of test_loader dataset:  10000\n",
      "\n",
      "Test set: Avg. loss: 0.0023, Accuracy: 904/10000 (9%)\n",
      "\n",
      "\n",
      "--------- Testing in global mode ---------\n",
      "size of test_loader dataset:  10000\n",
      "\n",
      "Test set: Avg. loss: 0.0023, Accuracy: 1495/10000 (15%)\n",
      "\n",
      "-------- Retraining the models ---------\n",
      "Retraining model :  model_0\n",
      "num_epochs---------------+++))))))+++ 300\n",
      "num_epochs---------------+++))))))+++ 300\n",
      "lr is  0.05\n",
      "number of epochs would be  30\n",
      "num_epochs-------------- 30\n",
      "Epoch 000\n",
      "/root/miniconda3/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:129: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\"Detected call of `lr_scheduler.step()` before `optimizer.step()`. \"\n",
      "/root/miniconda3/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:154: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "accuracy: {'epoch': 0, 'value': 0.9014999999809264} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 0, 'value': 0.2962959847784042} ({'split': 'train'})\n",
      "accuracy: {'epoch': 0, 'value': 0.8306000411510467} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 0, 'value': 0.5266319811344147} ({'split': 'test'})\n",
      "Epoch 001\n",
      "accuracy: {'epoch': 1, 'value': 0.9320200000381469} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 1, 'value': 0.20236863104820244} ({'split': 'train'})\n",
      "accuracy: {'epoch': 1, 'value': 0.815300041437149} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 1, 'value': 0.6728778302669525} ({'split': 'test'})\n",
      "Epoch 002\n",
      "accuracy: {'epoch': 2, 'value': 0.9464799999809269} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 2, 'value': 0.1595773231315614} ({'split': 'train'})\n",
      "accuracy: {'epoch': 2, 'value': 0.8356000423431396} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 2, 'value': 0.5778758674860001} ({'split': 'test'})\n",
      "Epoch 003\n",
      "accuracy: {'epoch': 3, 'value': 0.9613800000572208} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 3, 'value': 0.11594657857418061} ({'split': 'train'})\n",
      "accuracy: {'epoch': 3, 'value': 0.8575000345706939} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 3, 'value': 0.5107872903347015} ({'split': 'test'})\n",
      "Epoch 004\n",
      "accuracy: {'epoch': 4, 'value': 0.9713000000190735} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 4, 'value': 0.08613526419639593} ({'split': 'train'})\n",
      "accuracy: {'epoch': 4, 'value': 0.8491000354290008} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 4, 'value': 0.640915310382843} ({'split': 'test'})\n",
      "Epoch 005\n",
      "accuracy: {'epoch': 5, 'value': 0.9749200000190731} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 5, 'value': 0.07675527560710914} ({'split': 'train'})\n",
      "accuracy: {'epoch': 5, 'value': 0.8377000451087951} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 5, 'value': 0.6314063727855683} ({'split': 'test'})\n",
      "Epoch 006\n",
      "accuracy: {'epoch': 6, 'value': 0.9717400000572205} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 6, 'value': 0.08521322052240372} ({'split': 'train'})\n",
      "accuracy: {'epoch': 6, 'value': 0.8627000391483307} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 6, 'value': 0.5502460449934006} ({'split': 'test'})\n",
      "Epoch 007\n",
      "accuracy: {'epoch': 7, 'value': 0.9795600000381466} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 7, 'value': 0.06186159688711166} ({'split': 'train'})\n",
      "accuracy: {'epoch': 7, 'value': 0.8464000463485718} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 7, 'value': 0.5840305864810944} ({'split': 'test'})\n",
      "Epoch 008\n",
      "accuracy: {'epoch': 8, 'value': 0.9823400000190728} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 8, 'value': 0.054217335458993904} ({'split': 'train'})\n",
      "accuracy: {'epoch': 8, 'value': 0.8684000492095947} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 8, 'value': 0.5184542179107665} ({'split': 'test'})\n",
      "Epoch 009\n",
      "accuracy: {'epoch': 9, 'value': 0.9846800000190731} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 9, 'value': 0.0477678112757206} ({'split': 'train'})\n",
      "accuracy: {'epoch': 9, 'value': 0.8618000447750092} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 9, 'value': 0.5799048066139221} ({'split': 'test'})\n",
      "Epoch 010\n",
      "accuracy: {'epoch': 10, 'value': 0.9863000000381467} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 10, 'value': 0.04351996134281157} ({'split': 'train'})\n",
      "accuracy: {'epoch': 10, 'value': 0.8551000416278839} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 10, 'value': 0.6293429553508759} ({'split': 'test'})\n",
      "Epoch 011\n",
      "accuracy: {'epoch': 11, 'value': 0.9890800000381473} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 11, 'value': 0.035741786301136017} ({'split': 'train'})\n",
      "accuracy: {'epoch': 11, 'value': 0.8561000406742096} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 11, 'value': 0.6657455265522003} ({'split': 'test'})\n",
      "Epoch 012\n",
      "accuracy: {'epoch': 12, 'value': 0.9864600000572205} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 12, 'value': 0.04216200528621674} ({'split': 'train'})\n",
      "accuracy: {'epoch': 12, 'value': 0.847400039434433} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 12, 'value': 0.6428834557533264} ({'split': 'test'})\n",
      "Epoch 013\n",
      "accuracy: {'epoch': 13, 'value': 0.9866599999809268} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 13, 'value': 0.04159324784755709} ({'split': 'train'})\n",
      "accuracy: {'epoch': 13, 'value': 0.8522000432014466} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 13, 'value': 0.655725246667862} ({'split': 'test'})\n",
      "Epoch 014\n",
      "accuracy: {'epoch': 14, 'value': 0.9842000000190732} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 14, 'value': 0.0509503021526337} ({'split': 'train'})\n",
      "accuracy: {'epoch': 14, 'value': 0.8612000405788423} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 14, 'value': 0.5659356206655503} ({'split': 'test'})\n",
      "Epoch 015\n",
      "accuracy: {'epoch': 15, 'value': 0.9865600000572207} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 15, 'value': 0.043087698926925634} ({'split': 'train'})\n",
      "accuracy: {'epoch': 15, 'value': 0.859000039100647} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 15, 'value': 0.5797133564949036} ({'split': 'test'})\n",
      "Epoch 016\n",
      "accuracy: {'epoch': 16, 'value': 0.9886000000572206} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 16, 'value': 0.03629865944147111} ({'split': 'train'})\n",
      "accuracy: {'epoch': 16, 'value': 0.8592000484466554} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 16, 'value': 0.6384920537471771} ({'split': 'test'})\n",
      "Epoch 017\n",
      "accuracy: {'epoch': 17, 'value': 0.9878400000381469} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 17, 'value': 0.04092597299337386} ({'split': 'train'})\n",
      "accuracy: {'epoch': 17, 'value': 0.8512000381946564} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 17, 'value': 0.6155357599258422} ({'split': 'test'})\n",
      "Epoch 018\n",
      "accuracy: {'epoch': 18, 'value': 0.9885200000381466} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 18, 'value': 0.03712275217115878} ({'split': 'train'})\n",
      "accuracy: {'epoch': 18, 'value': 0.8549000442028046} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 18, 'value': 0.6248877644538879} ({'split': 'test'})\n",
      "Epoch 019\n",
      "accuracy: {'epoch': 19, 'value': 0.9919200000190739} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 19, 'value': 0.026637642495632167} ({'split': 'train'})\n",
      "accuracy: {'epoch': 19, 'value': 0.8609000325202942} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 19, 'value': 0.6002756774425507} ({'split': 'test'})\n",
      "Epoch 020\n",
      "accuracy: {'epoch': 20, 'value': 0.9916800000190731} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 20, 'value': 0.02784833639025689} ({'split': 'train'})\n",
      "accuracy: {'epoch': 20, 'value': 0.8632000505924226} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 20, 'value': 0.5817237943410873} ({'split': 'test'})\n",
      "Epoch 021\n",
      "accuracy: {'epoch': 21, 'value': 0.9875800000190732} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 21, 'value': 0.040422199537754076} ({'split': 'train'})\n",
      "accuracy: {'epoch': 21, 'value': 0.8613000452518462} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 21, 'value': 0.5874298959970474} ({'split': 'test'})\n",
      "Epoch 022\n",
      "accuracy: {'epoch': 22, 'value': 0.9896600000000002} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 22, 'value': 0.031787263309955634} ({'split': 'train'})\n",
      "accuracy: {'epoch': 22, 'value': 0.8544000327587127} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 22, 'value': 0.7193684875965118} ({'split': 'test'})\n",
      "Epoch 023\n",
      "accuracy: {'epoch': 23, 'value': 0.9871200000572207} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 23, 'value': 0.0400903310918808} ({'split': 'train'})\n",
      "accuracy: {'epoch': 23, 'value': 0.8512000381946564} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 23, 'value': 0.5949470311403275} ({'split': 'test'})\n",
      "Epoch 024\n",
      "accuracy: {'epoch': 24, 'value': 0.9877200000000002} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 24, 'value': 0.03939472717404366} ({'split': 'train'})\n",
      "accuracy: {'epoch': 24, 'value': 0.8479000329971313} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 24, 'value': 0.6352494180202484} ({'split': 'test'})\n",
      "Epoch 025\n",
      "accuracy: {'epoch': 25, 'value': 0.9858800000572205} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 25, 'value': 0.04434790576696396} ({'split': 'train'})\n",
      "accuracy: {'epoch': 25, 'value': 0.8511000394821167} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 25, 'value': 0.6151949703693389} ({'split': 'test'})\n",
      "Epoch 026\n",
      "accuracy: {'epoch': 26, 'value': 0.9841000000000001} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 26, 'value': 0.05061496993422512} ({'split': 'train'})\n",
      "accuracy: {'epoch': 26, 'value': 0.8536000430583954} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 26, 'value': 0.6423863768577576} ({'split': 'test'})\n",
      "Epoch 027\n",
      "accuracy: {'epoch': 27, 'value': 0.987620000038147} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 27, 'value': 0.04036093963146208} ({'split': 'train'})\n",
      "accuracy: {'epoch': 27, 'value': 0.853100037574768} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 27, 'value': 0.6351555168628692} ({'split': 'test'})\n",
      "Epoch 028\n",
      "accuracy: {'epoch': 28, 'value': 0.9882200000190734} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 28, 'value': 0.03831658083975315} ({'split': 'train'})\n",
      "accuracy: {'epoch': 28, 'value': 0.8481000363826752} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 28, 'value': 0.6575958490371704} ({'split': 'test'})\n",
      "Epoch 029\n",
      "accuracy: {'epoch': 29, 'value': 0.9900800000000001} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 29, 'value': 0.03240697373449804} ({'split': 'train'})\n",
      "accuracy: {'epoch': 29, 'value': 0.8514000356197358} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 29, 'value': 0.640863925218582} ({'split': 'test'})\n",
      "mean_test_accuracy <cifar_utils.accumulators.Mean object at 0x7f318c52dee0>\n",
      "QWERTY: Enter Cifar 2, acc 0.8684000492095947\n",
      "Retraining model :  model_1\n",
      "num_epochs---------------+++))))))+++ 30\n",
      "num_epochs---------------+++))))))+++ 30\n",
      "lr is  0.05\n",
      "number of epochs would be  30\n",
      "num_epochs-------------- 30\n",
      "Epoch 000\n",
      "accuracy: {'epoch': 0, 'value': 0.898520000038147} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 0, 'value': 0.30881472612857835} ({'split': 'train'})\n",
      "accuracy: {'epoch': 0, 'value': 0.8339000403881073} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 0, 'value': 0.5221650242805481} ({'split': 'test'})\n",
      "Epoch 001\n",
      "accuracy: {'epoch': 1, 'value': 0.92274000005722} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 1, 'value': 0.2300655124759675} ({'split': 'train'})\n",
      "accuracy: {'epoch': 1, 'value': 0.8517000317573548} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 1, 'value': 0.47220828831195827} ({'split': 'test'})\n",
      "Epoch 002\n",
      "accuracy: {'epoch': 2, 'value': 0.9504200000381469} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 2, 'value': 0.14991302669048306} ({'split': 'train'})\n",
      "accuracy: {'epoch': 2, 'value': 0.8507000386714935} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 2, 'value': 0.5565614730119705} ({'split': 'test'})\n",
      "Epoch 003\n",
      "accuracy: {'epoch': 3, 'value': 0.9639600000381469} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 3, 'value': 0.10862082661151887} ({'split': 'train'})\n",
      "accuracy: {'epoch': 3, 'value': 0.8580000400543213} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 3, 'value': 0.4917864739894867} ({'split': 'test'})\n",
      "Epoch 004\n",
      "accuracy: {'epoch': 4, 'value': 0.9720600000381472} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 4, 'value': 0.0844876015651226} ({'split': 'train'})\n",
      "accuracy: {'epoch': 4, 'value': 0.8725000500679017} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 4, 'value': 0.4941527724266052} ({'split': 'test'})\n",
      "Epoch 005\n",
      "accuracy: {'epoch': 5, 'value': 0.9705799999999999} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 5, 'value': 0.08924246401309965} ({'split': 'train'})\n",
      "accuracy: {'epoch': 5, 'value': 0.8521000385284423} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 5, 'value': 0.5496746987104415} ({'split': 'test'})\n",
      "Epoch 006\n",
      "accuracy: {'epoch': 6, 'value': 0.9778400000190731} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 6, 'value': 0.06731838547945022} ({'split': 'train'})\n",
      "accuracy: {'epoch': 6, 'value': 0.859900027513504} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 6, 'value': 0.5974322497844696} ({'split': 'test'})\n",
      "Epoch 007\n",
      "accuracy: {'epoch': 7, 'value': 0.9835800000572203} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 7, 'value': 0.0509939843416214} ({'split': 'train'})\n",
      "accuracy: {'epoch': 7, 'value': 0.8672000408172607} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 7, 'value': 0.5506765365600585} ({'split': 'test'})\n",
      "Epoch 008\n",
      "accuracy: {'epoch': 8, 'value': 0.9816799999809265} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 8, 'value': 0.057465847191810616} ({'split': 'train'})\n",
      "accuracy: {'epoch': 8, 'value': 0.8554000437259675} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 8, 'value': 0.5553898483514785} ({'split': 'test'})\n",
      "Epoch 009\n",
      "accuracy: {'epoch': 9, 'value': 0.9830800000190733} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 9, 'value': 0.05250220761716366} ({'split': 'train'})\n",
      "accuracy: {'epoch': 9, 'value': 0.8657000482082366} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 9, 'value': 0.5635408908128737} ({'split': 'test'})\n",
      "Epoch 010\n",
      "accuracy: {'epoch': 10, 'value': 0.9866600000190737} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 10, 'value': 0.041995035510063145} ({'split': 'train'})\n",
      "accuracy: {'epoch': 10, 'value': 0.8610000431537629} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 10, 'value': 0.5697057247161865} ({'split': 'test'})\n",
      "Epoch 011\n",
      "accuracy: {'epoch': 11, 'value': 0.9877799999999999} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 11, 'value': 0.03922296879589555} ({'split': 'train'})\n",
      "accuracy: {'epoch': 11, 'value': 0.8706000328063965} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 11, 'value': 0.5408853083848953} ({'split': 'test'})\n",
      "Epoch 012\n",
      "accuracy: {'epoch': 12, 'value': 0.9876400000190737} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 12, 'value': 0.038173657894134544} ({'split': 'train'})\n",
      "accuracy: {'epoch': 12, 'value': 0.8683000445365906} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 12, 'value': 0.5105891197919845} ({'split': 'test'})\n",
      "Epoch 013\n",
      "accuracy: {'epoch': 13, 'value': 0.98968000005722} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 13, 'value': 0.033102252886295314} ({'split': 'train'})\n",
      "accuracy: {'epoch': 13, 'value': 0.8573000311851502} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 13, 'value': 0.578517484664917} ({'split': 'test'})\n",
      "Epoch 014\n",
      "accuracy: {'epoch': 14, 'value': 0.9906000000000004} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 14, 'value': 0.03017893251657487} ({'split': 'train'})\n",
      "accuracy: {'epoch': 14, 'value': 0.8592000424861908} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 14, 'value': 0.6032907724380494} ({'split': 'test'})\n",
      "Epoch 015\n",
      "accuracy: {'epoch': 15, 'value': 0.9885000000000003} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 15, 'value': 0.03668780657410623} ({'split': 'train'})\n",
      "accuracy: {'epoch': 15, 'value': 0.8611000478267671} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 15, 'value': 0.5684309899806975} ({'split': 'test'})\n",
      "Epoch 016\n",
      "accuracy: {'epoch': 16, 'value': 0.9884400000190737} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 16, 'value': 0.03702196282029152} ({'split': 'train'})\n",
      "accuracy: {'epoch': 16, 'value': 0.8571000456809998} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 16, 'value': 0.5391833335161209} ({'split': 'test'})\n",
      "Epoch 017\n",
      "accuracy: {'epoch': 17, 'value': 0.9870600000190737} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 17, 'value': 0.04159250710606575} ({'split': 'train'})\n",
      "accuracy: {'epoch': 17, 'value': 0.8525000393390656} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 17, 'value': 0.6022155106067658} ({'split': 'test'})\n",
      "Epoch 018\n",
      "accuracy: {'epoch': 18, 'value': 0.9859599999999998} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 18, 'value': 0.045604344685077666} ({'split': 'train'})\n",
      "accuracy: {'epoch': 18, 'value': 0.8512000322341919} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 18, 'value': 0.6226116240024566} ({'split': 'test'})\n",
      "Epoch 019\n",
      "accuracy: {'epoch': 19, 'value': 0.9892200000190734} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 19, 'value': 0.034945271892547584} ({'split': 'train'})\n",
      "accuracy: {'epoch': 19, 'value': 0.8599000334739686} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 19, 'value': 0.5969808101654053} ({'split': 'test'})\n",
      "Epoch 020\n",
      "accuracy: {'epoch': 20, 'value': 0.9889399999999998} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 20, 'value': 0.03450204900294544} ({'split': 'train'})\n",
      "accuracy: {'epoch': 20, 'value': 0.8649000406265258} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 20, 'value': 0.5550479114055634} ({'split': 'test'})\n",
      "Epoch 021\n",
      "accuracy: {'epoch': 21, 'value': 0.9902400000190734} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 21, 'value': 0.03135012079358101} ({'split': 'train'})\n",
      "accuracy: {'epoch': 21, 'value': 0.8625000357627869} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 21, 'value': 0.5571037054061889} ({'split': 'test'})\n",
      "Epoch 022\n",
      "accuracy: {'epoch': 22, 'value': 0.9898600000190734} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 22, 'value': 0.032789035397768014} ({'split': 'train'})\n",
      "accuracy: {'epoch': 22, 'value': 0.8590000450611115} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 22, 'value': 0.6222849488258362} ({'split': 'test'})\n",
      "Epoch 023\n",
      "accuracy: {'epoch': 23, 'value': 0.9853800000190736} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 23, 'value': 0.045651444928646104} ({'split': 'train'})\n",
      "accuracy: {'epoch': 23, 'value': 0.8497000515460967} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 23, 'value': 0.7034199655056} ({'split': 'test'})\n",
      "Epoch 024\n",
      "accuracy: {'epoch': 24, 'value': 0.9871399999809263} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 24, 'value': 0.04088932981967931} ({'split': 'train'})\n",
      "accuracy: {'epoch': 24, 'value': 0.8520000338554383} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 24, 'value': 0.6002112627029419} ({'split': 'test'})\n",
      "Epoch 025\n",
      "accuracy: {'epoch': 25, 'value': 0.986720000019073} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 25, 'value': 0.04306471189379691} ({'split': 'train'})\n",
      "accuracy: {'epoch': 25, 'value': 0.851300036907196} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 25, 'value': 0.6768350720405579} ({'split': 'test'})\n",
      "Epoch 026\n",
      "accuracy: {'epoch': 26, 'value': 0.9902200000381471} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 26, 'value': 0.03259185280323027} ({'split': 'train'})\n",
      "accuracy: {'epoch': 26, 'value': 0.8577000319957733} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 26, 'value': 0.6037034690380096} ({'split': 'test'})\n",
      "Epoch 027\n",
      "accuracy: {'epoch': 27, 'value': 0.9860200000190734} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 27, 'value': 0.0435213215303421} ({'split': 'train'})\n",
      "accuracy: {'epoch': 27, 'value': 0.8519000470638276} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 27, 'value': 0.6155068755149842} ({'split': 'test'})\n",
      "Epoch 028\n",
      "accuracy: {'epoch': 28, 'value': 0.9853400000190733} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 28, 'value': 0.04706068258345127} ({'split': 'train'})\n",
      "accuracy: {'epoch': 28, 'value': 0.864100044965744} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 28, 'value': 0.5958672612905502} ({'split': 'test'})\n",
      "Epoch 029\n",
      "accuracy: {'epoch': 29, 'value': 0.98818} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 29, 'value': 0.0379996884703636} ({'split': 'train'})\n",
      "accuracy: {'epoch': 29, 'value': 0.8508000373840332} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 29, 'value': 0.5850529283285141} ({'split': 'test'})\n",
      "mean_test_accuracy <cifar_utils.accumulators.Mean object at 0x7f318df351c0>\n",
      "QWERTY: Enter Cifar 2, acc 0.8725000500679017\n",
      "Retraining model :  geometric\n",
      "num_epochs---------------+++))))))+++ 30\n",
      "num_epochs---------------+++))))))+++ 30\n",
      "lr is  0.05\n",
      "number of epochs would be  30\n",
      "num_epochs-------------- 30\n",
      "Epoch 000\n",
      "accuracy: {'epoch': 0, 'value': 0.12118000000476835} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 0, 'value': 18.697431476287843} ({'split': 'train'})\n",
      "accuracy: {'epoch': 0, 'value': 0.10000000447034836} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 0, 'value': 2.3033386707305907} ({'split': 'test'})\n",
      "Epoch 001\n",
      "accuracy: {'epoch': 1, 'value': 0.10000000000953674} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 1, 'value': 2.3026378993225096} ({'split': 'train'})\n",
      "accuracy: {'epoch': 1, 'value': 0.10000000521540642} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 1, 'value': 2.3025848865509033} ({'split': 'test'})\n",
      "Epoch 002\n",
      "accuracy: {'epoch': 2, 'value': 0.09999999999999999} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 2, 'value': 2.3025848854064943} ({'split': 'train'})\n",
      "accuracy: {'epoch': 2, 'value': 0.10000000447034835} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 2, 'value': 2.3025848865509033} ({'split': 'test'})\n",
      "Epoch 003\n",
      "accuracy: {'epoch': 3, 'value': 0.1000000000023842} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 3, 'value': 2.3025848854064943} ({'split': 'train'})\n",
      "accuracy: {'epoch': 3, 'value': 0.1000000037252903} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 3, 'value': 2.3025848865509033} ({'split': 'test'})\n",
      "Epoch 004\n",
      "accuracy: {'epoch': 4, 'value': 0.10000000000238415} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 4, 'value': 2.3025848854064943} ({'split': 'train'})\n",
      "accuracy: {'epoch': 4, 'value': 0.1000000037252903} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 4, 'value': 2.3025848865509033} ({'split': 'test'})\n",
      "Epoch 005\n",
      "accuracy: {'epoch': 5, 'value': 0.10000000000476833} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 5, 'value': 2.3025848854064943} ({'split': 'train'})\n",
      "accuracy: {'epoch': 5, 'value': 0.10000000447034836} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 5, 'value': 2.3025848865509033} ({'split': 'test'})\n",
      "Epoch 006\n",
      "accuracy: {'epoch': 6, 'value': 0.09999999999761584} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 6, 'value': 2.3025848854064943} ({'split': 'train'})\n",
      "accuracy: {'epoch': 6, 'value': 0.10000000447034835} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 6, 'value': 2.3025848865509033} ({'split': 'test'})\n",
      "Epoch 007\n",
      "accuracy: {'epoch': 7, 'value': 0.10000000001430515} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 7, 'value': 2.3025848854064943} ({'split': 'train'})\n",
      "accuracy: {'epoch': 7, 'value': 0.1000000037252903} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 7, 'value': 2.3025848865509033} ({'split': 'test'})\n",
      "Epoch 008\n",
      "accuracy: {'epoch': 8, 'value': 0.09999999999761582} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 8, 'value': 2.3025848854064943} ({'split': 'train'})\n",
      "accuracy: {'epoch': 8, 'value': 0.1000000037252903} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 8, 'value': 2.3025848865509033} ({'split': 'test'})\n",
      "Epoch 009\n",
      "accuracy: {'epoch': 9, 'value': 0.10000000000238418} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 9, 'value': 2.3025848854064943} ({'split': 'train'})\n",
      "accuracy: {'epoch': 9, 'value': 0.10000000298023225} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 9, 'value': 2.3025848865509033} ({'split': 'test'})\n",
      "Epoch 010\n",
      "accuracy: {'epoch': 10, 'value': 0.09999999999999998} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 10, 'value': 2.3025848854064943} ({'split': 'train'})\n",
      "accuracy: {'epoch': 10, 'value': 0.10000000447034836} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 10, 'value': 2.3025848865509033} ({'split': 'test'})\n",
      "Epoch 011\n",
      "accuracy: {'epoch': 11, 'value': 0.10000000000238417} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 11, 'value': 2.3025848854064943} ({'split': 'train'})\n",
      "accuracy: {'epoch': 11, 'value': 0.10000000596046449} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 11, 'value': 2.3025848865509033} ({'split': 'test'})\n",
      "Epoch 012\n",
      "accuracy: {'epoch': 12, 'value': 0.10000000000476836} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 12, 'value': 2.3025848854064943} ({'split': 'train'})\n",
      "accuracy: {'epoch': 12, 'value': 0.10000000521540643} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 12, 'value': 2.3025848865509033} ({'split': 'test'})\n",
      "Epoch 013\n",
      "accuracy: {'epoch': 13, 'value': 0.09999999999761583} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 13, 'value': 2.3025848854064943} ({'split': 'train'})\n",
      "accuracy: {'epoch': 13, 'value': 0.10000000447034837} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 13, 'value': 2.3025848865509033} ({'split': 'test'})\n",
      "Epoch 014\n",
      "accuracy: {'epoch': 14, 'value': 0.09999999999761579} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 14, 'value': 2.3025848854064943} ({'split': 'train'})\n",
      "accuracy: {'epoch': 14, 'value': 0.10000000447034836} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 14, 'value': 2.3025848865509033} ({'split': 'test'})\n",
      "Epoch 015\n",
      "accuracy: {'epoch': 15, 'value': 0.10000000000476834} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 15, 'value': 2.3025848854064943} ({'split': 'train'})\n",
      "accuracy: {'epoch': 15, 'value': 0.10000000670552253} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 15, 'value': 2.3025848865509033} ({'split': 'test'})\n",
      "Epoch 016\n",
      "accuracy: {'epoch': 16, 'value': 0.10000000000476834} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 16, 'value': 2.3025848854064943} ({'split': 'train'})\n",
      "accuracy: {'epoch': 16, 'value': 0.10000000447034836} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 16, 'value': 2.3025848865509033} ({'split': 'test'})\n",
      "Epoch 017\n",
      "accuracy: {'epoch': 17, 'value': 0.09999999999999996} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 17, 'value': 2.3025848854064943} ({'split': 'train'})\n",
      "accuracy: {'epoch': 17, 'value': 0.10000000521540642} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 17, 'value': 2.3025848865509033} ({'split': 'test'})\n",
      "Epoch 018\n",
      "accuracy: {'epoch': 18, 'value': 0.1} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 18, 'value': 2.3025848854064943} ({'split': 'train'})\n",
      "accuracy: {'epoch': 18, 'value': 0.10000000521540642} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 18, 'value': 2.3025848865509033} ({'split': 'test'})\n",
      "Epoch 019\n",
      "accuracy: {'epoch': 19, 'value': 0.10000000000476839} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 19, 'value': 2.3025848854064943} ({'split': 'train'})\n",
      "accuracy: {'epoch': 19, 'value': 0.10000000447034836} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 19, 'value': 2.3025848865509033} ({'split': 'test'})\n",
      "Epoch 020\n",
      "accuracy: {'epoch': 20, 'value': 0.10000000000238418} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 20, 'value': 2.3025848854064943} ({'split': 'train'})\n",
      "accuracy: {'epoch': 20, 'value': 0.10000000447034836} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 20, 'value': 2.3025848865509033} ({'split': 'test'})\n",
      "Epoch 021\n",
      "accuracy: {'epoch': 21, 'value': 0.10000000000476837} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 21, 'value': 2.3025848854064943} ({'split': 'train'})\n",
      "accuracy: {'epoch': 21, 'value': 0.10000000596046447} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 21, 'value': 2.3025848865509033} ({'split': 'test'})\n",
      "Epoch 022\n",
      "accuracy: {'epoch': 22, 'value': 0.09999999999761579} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 22, 'value': 2.3025848854064943} ({'split': 'train'})\n",
      "accuracy: {'epoch': 22, 'value': 0.10000000447034836} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 22, 'value': 2.3025848865509033} ({'split': 'test'})\n",
      "Epoch 023\n",
      "accuracy: {'epoch': 23, 'value': 0.10000000000238418} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 23, 'value': 2.3025848854064943} ({'split': 'train'})\n",
      "accuracy: {'epoch': 23, 'value': 0.10000000521540642} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 23, 'value': 2.3025848865509033} ({'split': 'test'})\n",
      "Epoch 024\n",
      "accuracy: {'epoch': 24, 'value': 0.10000000000953675} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 24, 'value': 2.3025848854064943} ({'split': 'train'})\n",
      "accuracy: {'epoch': 24, 'value': 0.10000000521540642} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 24, 'value': 2.3025848865509033} ({'split': 'test'})\n",
      "Epoch 025\n",
      "accuracy: {'epoch': 25, 'value': 0.09999999999761582} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 25, 'value': 2.3025848854064943} ({'split': 'train'})\n",
      "accuracy: {'epoch': 25, 'value': 0.10000000521540642} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 25, 'value': 2.3025848865509033} ({'split': 'test'})\n",
      "Epoch 026\n",
      "accuracy: {'epoch': 26, 'value': 0.09999999999761586} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 26, 'value': 2.3025848854064943} ({'split': 'train'})\n",
      "accuracy: {'epoch': 26, 'value': 0.1000000037252903} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 26, 'value': 2.3025848865509033} ({'split': 'test'})\n",
      "Epoch 027\n",
      "accuracy: {'epoch': 27, 'value': 0.09999999999761582} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 27, 'value': 2.3025848854064943} ({'split': 'train'})\n",
      "accuracy: {'epoch': 27, 'value': 0.10000000447034836} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 27, 'value': 2.3025848865509033} ({'split': 'test'})\n",
      "Epoch 028\n",
      "accuracy: {'epoch': 28, 'value': 0.10000000000238417} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 28, 'value': 2.3025848854064943} ({'split': 'train'})\n",
      "accuracy: {'epoch': 28, 'value': 0.10000000447034836} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 28, 'value': 2.3025848865509033} ({'split': 'test'})\n",
      "Epoch 029\n",
      "accuracy: {'epoch': 29, 'value': 0.1} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 29, 'value': 2.3025848854064943} ({'split': 'train'})\n",
      "accuracy: {'epoch': 29, 'value': 0.10000000596046449} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 29, 'value': 2.3025848865509033} ({'split': 'test'})\n",
      "mean_test_accuracy <cifar_utils.accumulators.Mean object at 0x7f318c52d8b0>\n",
      "QWERTY: Enter Cifar 2, acc 0.10000000670552253\n",
      "Retraining model :  naive_averaging\n",
      "num_epochs---------------+++))))))+++ 30\n",
      "num_epochs---------------+++))))))+++ 30\n",
      "lr is  0.05\n",
      "number of epochs would be  30\n",
      "num_epochs-------------- 30\n",
      "Epoch 000\n",
      "accuracy: {'epoch': 0, 'value': 0.21660000000476845} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 0, 'value': 2.106229507598877} ({'split': 'train'})\n",
      "accuracy: {'epoch': 0, 'value': 0.10000000521540642} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 0, 'value': 2.28979914188385} ({'split': 'test'})\n",
      "Epoch 001\n",
      "accuracy: {'epoch': 1, 'value': 0.26628000001907354} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 1, 'value': 1.8741308319473264} ({'split': 'train'})\n",
      "accuracy: {'epoch': 1, 'value': 0.38880001604557035} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 1, 'value': 1.605519187450409} ({'split': 'test'})\n",
      "Epoch 002\n",
      "accuracy: {'epoch': 2, 'value': 0.5148200000572204} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 2, 'value': 1.315496015892029} ({'split': 'train'})\n",
      "accuracy: {'epoch': 2, 'value': 0.5893000304698944} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 2, 'value': 1.1398680686950686} ({'split': 'test'})\n",
      "Epoch 003\n",
      "accuracy: {'epoch': 3, 'value': 0.6524600000190734} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 3, 'value': 0.9879910815811158} ({'split': 'train'})\n",
      "accuracy: {'epoch': 3, 'value': 0.7053000330924988} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 3, 'value': 0.873788571357727} ({'split': 'test'})\n",
      "Epoch 004\n",
      "accuracy: {'epoch': 4, 'value': 0.7405800000572205} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 4, 'value': 0.7606747987747192} ({'split': 'train'})\n",
      "accuracy: {'epoch': 4, 'value': 0.7329000353813171} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 4, 'value': 0.7883234202861785} ({'split': 'test'})\n",
      "Epoch 005\n",
      "accuracy: {'epoch': 5, 'value': 0.7905200000381474} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 5, 'value': 0.6147508033370974} ({'split': 'train'})\n",
      "accuracy: {'epoch': 5, 'value': 0.7315000355243684} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 5, 'value': 0.8353032171726227} ({'split': 'test'})\n",
      "Epoch 006\n",
      "accuracy: {'epoch': 6, 'value': 0.8184600000572203} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 6, 'value': 0.5347655467987064} ({'split': 'train'})\n",
      "accuracy: {'epoch': 6, 'value': 0.7594000339508057} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 6, 'value': 0.7315961062908173} ({'split': 'test'})\n",
      "Epoch 007\n",
      "accuracy: {'epoch': 7, 'value': 0.8473000000190735} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 7, 'value': 0.44577186234474164} ({'split': 'train'})\n",
      "accuracy: {'epoch': 7, 'value': 0.8052000463008882} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 7, 'value': 0.5969999194145202} ({'split': 'test'})\n",
      "Epoch 008\n",
      "accuracy: {'epoch': 8, 'value': 0.8748200000190735} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 8, 'value': 0.36249371545791614} ({'split': 'train'})\n",
      "accuracy: {'epoch': 8, 'value': 0.8097000360488891} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 8, 'value': 0.6133900582790375} ({'split': 'test'})\n",
      "Epoch 009\n",
      "accuracy: {'epoch': 9, 'value': 0.8907400000381478} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 9, 'value': 0.3152217446899414} ({'split': 'train'})\n",
      "accuracy: {'epoch': 9, 'value': 0.8188000321388245} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 9, 'value': 0.6317461788654327} ({'split': 'test'})\n",
      "Epoch 010\n",
      "accuracy: {'epoch': 10, 'value': 0.9070399999809269} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 10, 'value': 0.2698350255680081} ({'split': 'train'})\n",
      "accuracy: {'epoch': 10, 'value': 0.8082000434398651} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 10, 'value': 0.643220043182373} ({'split': 'test'})\n",
      "Epoch 011\n",
      "accuracy: {'epoch': 11, 'value': 0.9287800000572204} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 11, 'value': 0.2114925147247314} ({'split': 'train'})\n",
      "accuracy: {'epoch': 11, 'value': 0.822600030899048} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 11, 'value': 0.6015815913677215} ({'split': 'test'})\n",
      "Epoch 012\n",
      "accuracy: {'epoch': 12, 'value': 0.9399000000572204} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 12, 'value': 0.1770171772980691} ({'split': 'train'})\n",
      "accuracy: {'epoch': 12, 'value': 0.7972000360488891} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 12, 'value': 0.754025560617447} ({'split': 'test'})\n",
      "Epoch 013\n",
      "accuracy: {'epoch': 13, 'value': 0.9402800000381465} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 13, 'value': 0.17840564094066627} ({'split': 'train'})\n",
      "accuracy: {'epoch': 13, 'value': 0.8101000368595124} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 13, 'value': 0.6938497364521027} ({'split': 'test'})\n",
      "Epoch 014\n",
      "accuracy: {'epoch': 14, 'value': 0.9503000000190734} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 14, 'value': 0.14704731078624722} ({'split': 'train'})\n",
      "accuracy: {'epoch': 14, 'value': 0.809900039434433} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 14, 'value': 0.7234997034072876} ({'split': 'test'})\n",
      "Epoch 015\n",
      "accuracy: {'epoch': 15, 'value': 0.9588800000381472} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 15, 'value': 0.12239242738008498} ({'split': 'train'})\n",
      "accuracy: {'epoch': 15, 'value': 0.7994000375270843} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 15, 'value': 0.8013254642486571} ({'split': 'test'})\n",
      "Epoch 016\n",
      "accuracy: {'epoch': 16, 'value': 0.9589800000190736} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 16, 'value': 0.12358278052330025} ({'split': 'train'})\n",
      "accuracy: {'epoch': 16, 'value': 0.8095000445842743} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 16, 'value': 0.773352712392807} ({'split': 'test'})\n",
      "Epoch 017\n",
      "accuracy: {'epoch': 17, 'value': 0.9595199999999997} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 17, 'value': 0.12053725092887876} ({'split': 'train'})\n",
      "accuracy: {'epoch': 17, 'value': 0.8017000377178194} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 17, 'value': 0.8605865955352783} ({'split': 'test'})\n",
      "Epoch 018\n",
      "accuracy: {'epoch': 18, 'value': 0.9606000000572206} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 18, 'value': 0.11922389477252959} ({'split': 'train'})\n",
      "accuracy: {'epoch': 18, 'value': 0.8078000366687775} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 18, 'value': 0.7960408806800843} ({'split': 'test'})\n",
      "Epoch 019\n",
      "accuracy: {'epoch': 19, 'value': 0.96688} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 19, 'value': 0.10208627264976501} ({'split': 'train'})\n",
      "accuracy: {'epoch': 19, 'value': 0.7975000321865082} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 19, 'value': 0.8472240149974822} ({'split': 'test'})\n",
      "Epoch 020\n",
      "accuracy: {'epoch': 20, 'value': 0.967320000038147} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 20, 'value': 0.10132905570268631} ({'split': 'train'})\n",
      "accuracy: {'epoch': 20, 'value': 0.8077000379562379} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 20, 'value': 0.8509120404720306} ({'split': 'test'})\n",
      "Epoch 021\n",
      "accuracy: {'epoch': 21, 'value': 0.9721999999999996} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 21, 'value': 0.08461326812744138} ({'split': 'train'})\n",
      "accuracy: {'epoch': 21, 'value': 0.8113000214099885} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 21, 'value': 0.8135017633438111} ({'split': 'test'})\n",
      "Epoch 022\n",
      "accuracy: {'epoch': 22, 'value': 0.9753400000190736} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 22, 'value': 0.075298409101963} ({'split': 'train'})\n",
      "accuracy: {'epoch': 22, 'value': 0.8018000364303589} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 22, 'value': 0.8695825278759003} ({'split': 'test'})\n",
      "Epoch 023\n",
      "accuracy: {'epoch': 23, 'value': 0.9747600000572207} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 23, 'value': 0.07676712194442747} ({'split': 'train'})\n",
      "accuracy: {'epoch': 23, 'value': 0.8092000424861908} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 23, 'value': 0.8259101748466492} ({'split': 'test'})\n",
      "Epoch 024\n",
      "accuracy: {'epoch': 24, 'value': 0.9732600000190736} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 24, 'value': 0.07999971344709395} ({'split': 'train'})\n",
      "accuracy: {'epoch': 24, 'value': 0.8128000319004058} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 24, 'value': 0.8514819979667664} ({'split': 'test'})\n",
      "Epoch 025\n",
      "accuracy: {'epoch': 25, 'value': 0.9779200000190739} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 25, 'value': 0.0676166216516495} ({'split': 'train'})\n",
      "accuracy: {'epoch': 25, 'value': 0.8107000350952148} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 25, 'value': 0.8322859704494476} ({'split': 'test'})\n",
      "Epoch 026\n",
      "accuracy: {'epoch': 26, 'value': 0.9830000000381469} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 26, 'value': 0.052672672820091214} ({'split': 'train'})\n",
      "accuracy: {'epoch': 26, 'value': 0.8228000402450562} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 26, 'value': 0.8272729516029358} ({'split': 'test'})\n",
      "Epoch 027\n",
      "accuracy: {'epoch': 27, 'value': 0.9778400000381468} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 27, 'value': 0.06830655173063277} ({'split': 'train'})\n",
      "accuracy: {'epoch': 27, 'value': 0.8083000242710113} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 27, 'value': 0.9475314497947692} ({'split': 'test'})\n",
      "Epoch 028\n",
      "accuracy: {'epoch': 28, 'value': 0.979100000019073} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 28, 'value': 0.06179112392425538} ({'split': 'train'})\n",
      "accuracy: {'epoch': 28, 'value': 0.8130000472068786} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 28, 'value': 0.8752629458904266} ({'split': 'test'})\n",
      "Epoch 029\n",
      "accuracy: {'epoch': 29, 'value': 0.9791399999999999} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 29, 'value': 0.06372243378400802} ({'split': 'train'})\n",
      "accuracy: {'epoch': 29, 'value': 0.8090000331401824} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 29, 'value': 0.8541375935077666} ({'split': 'test'})\n",
      "mean_test_accuracy <cifar_utils.accumulators.Mean object at 0x7f318c52db80>\n",
      "QWERTY: Enter Cifar 2, acc 0.8228000402450562\n",
      "----- Saved results at sample.csv ------\n",
      "{'exp_name': 'exp_2023-09-17_11-50-46_510369', 'model0_acc': 90.16, 'model1_acc': 90.23, 'geometric_acc': 84.95, 'prediction_acc': 91.14, 'naive_acc': 14.95, 'geometric_gain': -5.280000000000001, 'geometric_gain_%': -5.85171229081237, 'prediction_gain': 0.9099999999999966, 'prediction_gain_%': 1.0085337470907643, 'relative_loss_wrt_prediction': 6.860246037903134, 'geometric_time': 3.450525116175413, 'retrain_geometric_best': 10.000000670552254, 'retrain_naive_best': 82.28000402450562, 'retrain_model0_best': 86.84000492095947, 'retrain_model1_best': 87.25000500679016, 'retrain_epochs': 30}\n",
      "FYI: the parameters were: \n",
      " Namespace(act_bug=False, act_num_samples=100, activation_histograms=False, activation_mode=None, activation_seed=21, activation_time=9.231269359588623e-06, alpha=0.7, baseroot='/root/autodl-tmp/OTfusion_pruning/otfusion', batch_size_test=1000, batch_size_train=256, center_acts=False, choice='0 2 4 6 8', cifar_style_data=False, ckpt_type='best', clip_gm=False, clip_max=5, clip_min=0, config={'dataset': 'Cifar10', 'model': 'vgg11_nobias', 'optimizer': 'SGD', 'optimizer_decay_at_epochs': [30, 60, 90, 120, 150, 180, 210, 240, 270], 'optimizer_decay_with_factor': 2.0, 'optimizer_learning_rate': 0.05, 'optimizer_momentum': 0.9, 'optimizer_weight_decay': 0.0005, 'batch_size': 128, 'num_epochs': 30, 'seed': 42, 'nick': 'naive_averaging', 'start_acc': 14.95}, config_dir='/root/autodl-tmp/OTfusion_pruning/otfusion/exp_sample/configurations', config_file=None, correction=True, csv_dir='/root/autodl-tmp/OTfusion_pruning/otfusion/exp_sample/csv', dataset='Cifar10', debug=False, deprecated=None, deterministic=False, diff_init=False, disable_bias=True, dist_epochs=60, dist_normalize=False, dump_final_models=False, dump_model=False, enable_dropout=False, ensemble_step=0.5, eval_aligned=False, exact=True, exp_name='exp_2023-09-17_11-50-46_510369', experiment_dir='experiment3', geom_ensemble_type='wts', geometric_time=3.450525116175413, gpu_id=0, gromov=False, gromov_loss='square_loss', ground_metric='euclidean', ground_metric_eff=True, ground_metric_normalize='none', handle_skips=False, importance=None, learning_rate=0.0001, load_model_dir='./cifar_models', load_models='', log_interval=100, model_name='vgg11_nobias', momentum=0.5, n_epochs=30, no_random_trainloaders=False, normalize_acts=False, normalize_wts=False, not_squared=True, num_hidden_nodes=400, num_hidden_nodes1=400, num_hidden_nodes2=200, num_hidden_nodes3=100, num_hidden_nodes4=50, num_models=2, options_type='generic', params_geometric=9222848, params_model_0=9222848, params_model_1=9222848, partial_reshape=False, partition_dataloader=-1, partition_type='labels', past_correction=True, personal_class_idx=9, personal_split_frac=0.1, pool_acts=False, pool_relu=False, prediction_wts=False, prelu_acts=True, print_distances=False, proper_marginals=False, prunint_rate=0.4, recheck_acc=False, recheck_cifar=True, reg=0.01, reg_m=0.001, reinit_trainloaders=False, result_dir='/root/autodl-tmp/OTfusion_pruning/otfusion/exp_sample/results', retrain=30, retrain_avg_only=False, retrain_geometric_best=0.10000000670552253, retrain_geometric_only=False, retrain_lr_decay=-1, retrain_lr_decay_epochs=None, retrain_lr_decay_factor=None, retrain_model0_best=0.8684000492095947, retrain_model1_best=0.8725000500679017, retrain_naive_best=0.8228000402450562, retrain_seed=-1, rootdir='/root/autodl-tmp/OTfusion_pruning/otfusion/exp_sample', run_mode='finetune', same_model=-1, save_result_file='sample.csv', second_config={'dataset': 'Cifar10', 'model': 'vgg11_nobias', 'optimizer': 'SGD', 'optimizer_decay_at_epochs': [30, 60, 90, 120, 150, 180, 210, 240, 270], 'optimizer_decay_with_factor': 2.0, 'optimizer_learning_rate': 0.05, 'optimizer_momentum': 0.9, 'optimizer_weight_decay': 0.0005, 'batch_size': 128, 'num_epochs': 30, 'seed': 42, 'nick': 'naive_averaging', 'start_acc': 14.95}, second_model_name=None, sinkhorn_type='normal', skip_last_layer=False, skip_last_layer_type='average', skip_personal_idx=False, skip_retrain=-1, softmax_temperature=1, standardize_acts=False, sweep_id=90, sweep_name='exp_sample', temperature=20, tensorboard=False, tensorboard_root='./tensorboard', timestamp='2023-09-17_11-50-46_510369', tmap_stats=False, to_download=True, transform_acts=False, unbalanced=False, update_acts=False, weight_stats=True, width_ratio=1, **{'trace_sum_ratio_classifier.weight': 1.0, 'trace_sum_ratio_features.0.weight': 0.046875, 'trace_sum_ratio_features.11.weight': 0.001953125, 'trace_sum_ratio_features.13.weight': 0.00390625, 'trace_sum_ratio_features.16.weight': 0.005859375, 'trace_sum_ratio_features.18.weight': 0.001953125, 'trace_sum_ratio_features.3.weight': 0.0, 'trace_sum_ratio_features.6.weight': 0.0, 'trace_sum_ratio_features.8.weight': 0.0078125})\n"
     ]
    }
   ],
   "source": [
    "!python main.py --gpu-id 0 --model-name vgg11_nobias --n-epochs 30 --save-result-file sample.csv \\\n",
    "--sweep-name exp_sample --correction --ground-metric euclidean --weight-stats \\\n",
    "--geom-ensemble-type wts --ground-metric-normalize none --sweep-id 90 \\\n",
    "--ckpt-type best --dataset Cifar10 --ground-metric-eff --recheck-cifar --activation-seed {activation_seed} \\\n",
    "--prelu-acts --past-correction --not-squared --exact\\\n",
    "--learning-rate 0.0001\\\n",
    "--momentum 0.5\\\n",
    "--batch-size-train 256 --experiment-dir {experiment_dir}\\\n",
    "--prunint-rate {prunint_rate}\\\n",
    "--run-mode {run_mode}\\\n",
    "--to-download\\\n",
    "--load-model-dir {load_model_dir}\\\n",
    "--retrain 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b254c91c-5642-4acd-8742-0e75c864bf1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#3:prune times without finetune\n",
    "activation_seed=21\n",
    "experiment_dir=\"experiment3\"\n",
    "prunint_rate=0.4\n",
    "run_mode='prune_times'\n",
    "load_model_dir='./cifar_models'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "895f6565-50a2-4ce3-b0ac-7cafb30ce751",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------- Setting up parameters -------\n",
      "dumping parameters at  /root/autodl-tmp/OTfusion_pruning/otfusion/exp_sample/configurations\n",
      "The parameters are: \n",
      " Namespace(act_bug=False, act_num_samples=100, activation_histograms=False, activation_mode=None, activation_seed=21, alpha=0.7, baseroot='/root/autodl-tmp/OTfusion_pruning/otfusion', batch_size_test=1000, batch_size_train=256, center_acts=False, choice='0 2 4 6 8', cifar_style_data=False, ckpt_type='best', clip_gm=False, clip_max=5, clip_min=0, config_dir='/root/autodl-tmp/OTfusion_pruning/otfusion/exp_sample/configurations', config_file=None, correction=True, csv_dir='/root/autodl-tmp/OTfusion_pruning/otfusion/exp_sample/csv', dataset='Cifar10', debug=False, deprecated=None, deterministic=False, diff_init=False, disable_bias=True, dist_epochs=60, dist_normalize=False, dump_final_models=False, dump_model=False, enable_dropout=False, ensemble_step=0.5, eval_aligned=False, exact=True, exp_name='exp_2023-09-17_12-49-42_642492', experiment_dir='experiment3', geom_ensemble_type='wts', gpu_id=0, gromov=False, gromov_loss='square_loss', ground_metric='euclidean', ground_metric_eff=True, ground_metric_normalize='none', handle_skips=False, importance=None, learning_rate=0.0001, load_model_dir='./cifar_models', load_models='', log_interval=100, model_name='vgg11_nobias', momentum=0.5, n_epochs=10, no_random_trainloaders=False, normalize_acts=False, normalize_wts=False, not_squared=True, num_hidden_nodes=400, num_hidden_nodes1=400, num_hidden_nodes2=200, num_hidden_nodes3=100, num_hidden_nodes4=50, num_models=2, options_type='generic', partial_reshape=False, partition_dataloader=-1, partition_type='labels', past_correction=True, personal_class_idx=9, personal_split_frac=0.1, pool_acts=False, pool_relu=False, prediction_wts=False, prelu_acts=True, print_distances=False, proper_marginals=False, prune_times=3, prunint_rate=0.4, recheck_acc=False, recheck_cifar=True, reg=0.01, reg_m=0.001, reinit_trainloaders=False, result_dir='/root/autodl-tmp/OTfusion_pruning/otfusion/exp_sample/results', retrain=30, retrain_avg_only=False, retrain_geometric_only=False, retrain_lr_decay=-1, retrain_lr_decay_epochs=None, retrain_lr_decay_factor=None, retrain_seed=-1, rootdir='/root/autodl-tmp/OTfusion_pruning/otfusion/exp_sample', run_mode='prune_times', same_model=-1, save_result_file='sample.csv', second_model_name=None, sinkhorn_type='normal', skip_last_layer=False, skip_last_layer_type='average', skip_personal_idx=False, skip_retrain=-1, softmax_temperature=1, standardize_acts=False, sweep_id=90, sweep_name='exp_sample', temperature=20, tensorboard=False, tensorboard_root='./tensorboard', timestamp='2023-09-17_12-49-42_642492', tmap_stats=False, to_download=True, transform_acts=False, unbalanced=False, update_acts=False, weight_stats=True, width_ratio=1)\n",
      "refactored get_config\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "in _make_layers [Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), AvgPool2d(kernel_size=1, stride=1, padding=0)]\n",
      "Relu Inplace is  False\n",
      "model parameters are \n",
      " [torch.Size([64, 3, 3, 3]), torch.Size([128, 64, 3, 3]), torch.Size([256, 128, 3, 3]), torch.Size([256, 256, 3, 3]), torch.Size([512, 256, 3, 3]), torch.Size([512, 512, 3, 3]), torch.Size([512, 512, 3, 3]), torch.Size([512, 512, 3, 3]), torch.Size([10, 512])]\n",
      "in _make_layers [Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), AvgPool2d(kernel_size=1, stride=1, padding=0)]\n",
      "Relu Inplace is  False\n",
      "model parameters are \n",
      " [torch.Size([64, 3, 3, 3]), torch.Size([128, 64, 3, 3]), torch.Size([256, 128, 3, 3]), torch.Size([256, 256, 3, 3]), torch.Size([512, 256, 3, 3]), torch.Size([512, 512, 3, 3]), torch.Size([512, 512, 3, 3]), torch.Size([512, 512, 3, 3]), torch.Size([10, 512])]\n",
      "in _make_layers [Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), AvgPool2d(kernel_size=1, stride=1, padding=0)]\n",
      "Relu Inplace is  False\n",
      "model parameters are \n",
      " [torch.Size([64, 3, 3, 3]), torch.Size([128, 64, 3, 3]), torch.Size([256, 128, 3, 3]), torch.Size([256, 256, 3, 3]), torch.Size([512, 256, 3, 3]), torch.Size([512, 512, 3, 3]), torch.Size([512, 512, 3, 3]), torch.Size([512, 512, 3, 3]), torch.Size([10, 512])]\n",
      "in _make_layers [Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), AvgPool2d(kernel_size=1, stride=1, padding=0)]\n",
      "Relu Inplace is  False\n",
      "model parameters are \n",
      " [torch.Size([64, 3, 3, 3]), torch.Size([128, 64, 3, 3]), torch.Size([256, 128, 3, 3]), torch.Size([256, 256, 3, 3]), torch.Size([512, 256, 3, 3]), torch.Size([512, 512, 3, 3]), torch.Size([512, 512, 3, 3]), torch.Size([512, 512, 3, 3]), torch.Size([10, 512])]\n",
      "enter prune_times, the times is 3,ratio:0.13333333333333333\n",
      "Train Epoch: 1 [0/50000 (0%)]\tLoss: 0.101390\n",
      "Train Epoch: 1 [25600/50000 (51%)]\tLoss: 0.094847\n",
      "\n",
      "--------- Testing in global mode ---------\n",
      "size of test_loader dataset:  10000\n",
      "\n",
      "Test set: Avg. loss: 0.0003, Accuracy: 9038/10000 (90%)\n",
      "\n",
      "Train Epoch: 2 [0/50000 (0%)]\tLoss: 0.073905\n",
      "Train Epoch: 2 [25600/50000 (51%)]\tLoss: 0.089815\n",
      "\n",
      "--------- Testing in global mode ---------\n",
      "size of test_loader dataset:  10000\n",
      "\n",
      "Test set: Avg. loss: 0.0003, Accuracy: 9045/10000 (90%)\n",
      "\n",
      "Train Epoch: 3 [0/50000 (0%)]\tLoss: 0.065545\n",
      "Train Epoch: 3 [25600/50000 (51%)]\tLoss: 0.059600\n",
      "\n",
      "--------- Testing in global mode ---------\n",
      "size of test_loader dataset:  10000\n",
      "\n",
      "Test set: Avg. loss: 0.0003, Accuracy: 9046/10000 (90%)\n",
      "\n",
      "Train Epoch: 4 [0/50000 (0%)]\tLoss: 0.070259\n",
      "Train Epoch: 4 [25600/50000 (51%)]\tLoss: 0.055962\n",
      "\n",
      "--------- Testing in global mode ---------\n",
      "size of test_loader dataset:  10000\n",
      "\n",
      "Test set: Avg. loss: 0.0003, Accuracy: 9041/10000 (90%)\n",
      "\n",
      "Train Epoch: 5 [0/50000 (0%)]\tLoss: 0.067846\n",
      "Train Epoch: 5 [25600/50000 (51%)]\tLoss: 0.069253\n",
      "\n",
      "--------- Testing in global mode ---------\n",
      "size of test_loader dataset:  10000\n",
      "\n",
      "Test set: Avg. loss: 0.0003, Accuracy: 9044/10000 (90%)\n",
      "\n",
      "Train Epoch: 6 [0/50000 (0%)]\tLoss: 0.063304\n",
      "Train Epoch: 6 [25600/50000 (51%)]\tLoss: 0.062643\n",
      "\n",
      "--------- Testing in global mode ---------\n",
      "size of test_loader dataset:  10000\n",
      "\n",
      "Test set: Avg. loss: 0.0003, Accuracy: 9041/10000 (90%)\n",
      "\n",
      "Train Epoch: 7 [0/50000 (0%)]\tLoss: 0.070086\n",
      "Train Epoch: 7 [25600/50000 (51%)]\tLoss: 0.075303\n",
      "\n",
      "--------- Testing in global mode ---------\n",
      "size of test_loader dataset:  10000\n",
      "\n",
      "Test set: Avg. loss: 0.0003, Accuracy: 9041/10000 (90%)\n",
      "\n",
      "Train Epoch: 8 [0/50000 (0%)]\tLoss: 0.055575\n",
      "Train Epoch: 8 [25600/50000 (51%)]\tLoss: 0.072508\n",
      "\n",
      "--------- Testing in global mode ---------\n",
      "size of test_loader dataset:  10000\n",
      "\n",
      "Test set: Avg. loss: 0.0003, Accuracy: 9046/10000 (90%)\n",
      "\n",
      "Train Epoch: 9 [0/50000 (0%)]\tLoss: 0.057988\n",
      "Train Epoch: 9 [25600/50000 (51%)]\tLoss: 0.065345\n",
      "\n",
      "--------- Testing in global mode ---------\n",
      "size of test_loader dataset:  10000\n",
      "\n",
      "Test set: Avg. loss: 0.0003, Accuracy: 9046/10000 (90%)\n",
      "\n",
      "Train Epoch: 10 [0/50000 (0%)]\tLoss: 0.061518\n",
      "Train Epoch: 10 [25600/50000 (51%)]\tLoss: 0.061647\n",
      "\n",
      "--------- Testing in global mode ---------\n",
      "size of test_loader dataset:  10000\n",
      "\n",
      "Test set: Avg. loss: 0.0003, Accuracy: 9045/10000 (90%)\n",
      "\n",
      "Train Epoch: 1 [0/50000 (0%)]\tLoss: 0.099334\n",
      "Train Epoch: 1 [25600/50000 (51%)]\tLoss: 0.078876\n",
      "\n",
      "--------- Testing in global mode ---------\n",
      "size of test_loader dataset:  10000\n",
      "\n",
      "Test set: Avg. loss: 0.0003, Accuracy: 9058/10000 (91%)\n",
      "\n",
      "Train Epoch: 2 [0/50000 (0%)]\tLoss: 0.070143\n",
      "Train Epoch: 2 [25600/50000 (51%)]\tLoss: 0.094725\n",
      "\n",
      "--------- Testing in global mode ---------\n",
      "size of test_loader dataset:  10000\n",
      "\n",
      "Test set: Avg. loss: 0.0003, Accuracy: 9070/10000 (91%)\n",
      "\n",
      "Train Epoch: 3 [0/50000 (0%)]\tLoss: 0.073057\n",
      "Train Epoch: 3 [25600/50000 (51%)]\tLoss: 0.074628\n",
      "\n",
      "--------- Testing in global mode ---------\n",
      "size of test_loader dataset:  10000\n",
      "\n",
      "Test set: Avg. loss: 0.0003, Accuracy: 9066/10000 (91%)\n",
      "\n",
      "Train Epoch: 4 [0/50000 (0%)]\tLoss: 0.086730\n",
      "Train Epoch: 4 [25600/50000 (51%)]\tLoss: 0.067090\n",
      "\n",
      "--------- Testing in global mode ---------\n",
      "size of test_loader dataset:  10000\n",
      "\n",
      "Test set: Avg. loss: 0.0003, Accuracy: 9063/10000 (91%)\n",
      "\n",
      "Train Epoch: 5 [0/50000 (0%)]\tLoss: 0.065105\n",
      "Train Epoch: 5 [25600/50000 (51%)]\tLoss: 0.091160\n",
      "\n",
      "--------- Testing in global mode ---------\n",
      "size of test_loader dataset:  10000\n",
      "\n",
      "Test set: Avg. loss: 0.0003, Accuracy: 9063/10000 (91%)\n",
      "\n",
      "Train Epoch: 6 [0/50000 (0%)]\tLoss: 0.063593\n",
      "Train Epoch: 6 [25600/50000 (51%)]\tLoss: 0.061089\n",
      "\n",
      "--------- Testing in global mode ---------\n",
      "size of test_loader dataset:  10000\n",
      "\n",
      "Test set: Avg. loss: 0.0003, Accuracy: 9060/10000 (91%)\n",
      "\n",
      "Train Epoch: 7 [0/50000 (0%)]\tLoss: 0.065845\n",
      "Train Epoch: 7 [25600/50000 (51%)]\tLoss: 0.053192\n",
      "\n",
      "--------- Testing in global mode ---------\n",
      "size of test_loader dataset:  10000\n",
      "\n",
      "Test set: Avg. loss: 0.0003, Accuracy: 9065/10000 (91%)\n",
      "\n",
      "Train Epoch: 8 [0/50000 (0%)]\tLoss: 0.052597\n",
      "Train Epoch: 8 [25600/50000 (51%)]\tLoss: 0.063790\n",
      "\n",
      "--------- Testing in global mode ---------\n",
      "size of test_loader dataset:  10000\n",
      "\n",
      "Test set: Avg. loss: 0.0003, Accuracy: 9068/10000 (91%)\n",
      "\n",
      "Train Epoch: 9 [0/50000 (0%)]\tLoss: 0.059498\n",
      "Train Epoch: 9 [25600/50000 (51%)]\tLoss: 0.061890\n",
      "\n",
      "--------- Testing in global mode ---------\n",
      "size of test_loader dataset:  10000\n",
      "\n",
      "Test set: Avg. loss: 0.0003, Accuracy: 9066/10000 (91%)\n",
      "\n",
      "Train Epoch: 10 [0/50000 (0%)]\tLoss: 0.056503\n",
      "Train Epoch: 10 [25600/50000 (51%)]\tLoss: 0.053735\n",
      "\n",
      "--------- Testing in global mode ---------\n",
      "size of test_loader dataset:  10000\n",
      "\n",
      "Test set: Avg. loss: 0.0003, Accuracy: 9062/10000 (91%)\n",
      "\n",
      "in times 0, now cumm_ratio 0.13333333333333333,accuracies:[90.45, 90.62],losses:[0.0002996106028556824, 0.00029617898762226107]\n",
      "Train Epoch: 1 [0/50000 (0%)]\tLoss: 0.046391\n",
      "Train Epoch: 1 [25600/50000 (51%)]\tLoss: 0.063309\n",
      "\n",
      "--------- Testing in global mode ---------\n",
      "size of test_loader dataset:  10000\n",
      "\n",
      "Test set: Avg. loss: 0.0003, Accuracy: 9029/10000 (90%)\n",
      "\n",
      "Train Epoch: 2 [0/50000 (0%)]\tLoss: 0.074412\n",
      "Train Epoch: 2 [25600/50000 (51%)]\tLoss: 0.053176\n",
      "\n",
      "--------- Testing in global mode ---------\n",
      "size of test_loader dataset:  10000\n",
      "\n",
      "Test set: Avg. loss: 0.0003, Accuracy: 9032/10000 (90%)\n",
      "\n",
      "Train Epoch: 3 [0/50000 (0%)]\tLoss: 0.052397\n",
      "Train Epoch: 3 [25600/50000 (51%)]\tLoss: 0.054971\n",
      "\n",
      "--------- Testing in global mode ---------\n",
      "size of test_loader dataset:  10000\n",
      "\n",
      "Test set: Avg. loss: 0.0003, Accuracy: 9030/10000 (90%)\n",
      "\n",
      "Train Epoch: 4 [0/50000 (0%)]\tLoss: 0.044266\n",
      "Train Epoch: 4 [25600/50000 (51%)]\tLoss: 0.054542\n",
      "\n",
      "--------- Testing in global mode ---------\n",
      "size of test_loader dataset:  10000\n",
      "\n",
      "Test set: Avg. loss: 0.0003, Accuracy: 9033/10000 (90%)\n",
      "\n",
      "Train Epoch: 5 [0/50000 (0%)]\tLoss: 0.044172\n",
      "Train Epoch: 5 [25600/50000 (51%)]\tLoss: 0.047735\n",
      "\n",
      "--------- Testing in global mode ---------\n",
      "size of test_loader dataset:  10000\n",
      "\n",
      "Test set: Avg. loss: 0.0003, Accuracy: 9033/10000 (90%)\n",
      "\n",
      "Train Epoch: 6 [0/50000 (0%)]\tLoss: 0.056094\n",
      "Train Epoch: 6 [25600/50000 (51%)]\tLoss: 0.041934\n",
      "\n",
      "--------- Testing in global mode ---------\n",
      "size of test_loader dataset:  10000\n",
      "\n",
      "Test set: Avg. loss: 0.0003, Accuracy: 9031/10000 (90%)\n",
      "\n",
      "Train Epoch: 7 [0/50000 (0%)]\tLoss: 0.036625\n",
      "Train Epoch: 7 [25600/50000 (51%)]\tLoss: 0.058822\n",
      "\n",
      "--------- Testing in global mode ---------\n",
      "size of test_loader dataset:  10000\n",
      "\n",
      "Test set: Avg. loss: 0.0003, Accuracy: 9027/10000 (90%)\n",
      "\n",
      "Train Epoch: 8 [0/50000 (0%)]\tLoss: 0.051968\n",
      "Train Epoch: 8 [25600/50000 (51%)]\tLoss: 0.044903\n",
      "\n",
      "--------- Testing in global mode ---------\n",
      "size of test_loader dataset:  10000\n",
      "\n",
      "Test set: Avg. loss: 0.0003, Accuracy: 9028/10000 (90%)\n",
      "\n",
      "Train Epoch: 9 [0/50000 (0%)]\tLoss: 0.048571\n",
      "Train Epoch: 9 [25600/50000 (51%)]\tLoss: 0.046579\n",
      "\n",
      "--------- Testing in global mode ---------\n",
      "size of test_loader dataset:  10000\n",
      "\n",
      "Test set: Avg. loss: 0.0003, Accuracy: 9030/10000 (90%)\n",
      "\n",
      "Train Epoch: 10 [0/50000 (0%)]\tLoss: 0.057078\n",
      "Train Epoch: 10 [25600/50000 (51%)]\tLoss: 0.046758\n",
      "\n",
      "--------- Testing in global mode ---------\n",
      "size of test_loader dataset:  10000\n",
      "\n",
      "Test set: Avg. loss: 0.0003, Accuracy: 9033/10000 (90%)\n",
      "\n",
      "Train Epoch: 1 [0/50000 (0%)]\tLoss: 0.054643\n",
      "Train Epoch: 1 [25600/50000 (51%)]\tLoss: 0.053391\n",
      "\n",
      "--------- Testing in global mode ---------\n",
      "size of test_loader dataset:  10000\n",
      "\n",
      "Test set: Avg. loss: 0.0003, Accuracy: 9046/10000 (90%)\n",
      "\n",
      "Train Epoch: 2 [0/50000 (0%)]\tLoss: 0.056186\n",
      "Train Epoch: 2 [25600/50000 (51%)]\tLoss: 0.049419\n",
      "\n",
      "--------- Testing in global mode ---------\n",
      "size of test_loader dataset:  10000\n",
      "\n",
      "Test set: Avg. loss: 0.0003, Accuracy: 9054/10000 (91%)\n",
      "\n",
      "Train Epoch: 3 [0/50000 (0%)]\tLoss: 0.058067\n",
      "Train Epoch: 3 [25600/50000 (51%)]\tLoss: 0.051919\n",
      "\n",
      "--------- Testing in global mode ---------\n",
      "size of test_loader dataset:  10000\n",
      "\n",
      "Test set: Avg. loss: 0.0003, Accuracy: 9053/10000 (91%)\n",
      "\n",
      "Train Epoch: 4 [0/50000 (0%)]\tLoss: 0.058036\n",
      "Train Epoch: 4 [25600/50000 (51%)]\tLoss: 0.051271\n",
      "\n",
      "--------- Testing in global mode ---------\n",
      "size of test_loader dataset:  10000\n",
      "\n",
      "Test set: Avg. loss: 0.0003, Accuracy: 9054/10000 (91%)\n",
      "\n",
      "Train Epoch: 5 [0/50000 (0%)]\tLoss: 0.050172\n",
      "Train Epoch: 5 [25600/50000 (51%)]\tLoss: 0.061991\n",
      "\n",
      "--------- Testing in global mode ---------\n",
      "size of test_loader dataset:  10000\n",
      "\n",
      "Test set: Avg. loss: 0.0003, Accuracy: 9050/10000 (90%)\n",
      "\n",
      "Train Epoch: 6 [0/50000 (0%)]\tLoss: 0.053839\n",
      "Train Epoch: 6 [25600/50000 (51%)]\tLoss: 0.045203\n",
      "\n",
      "--------- Testing in global mode ---------\n",
      "size of test_loader dataset:  10000\n",
      "\n",
      "Test set: Avg. loss: 0.0003, Accuracy: 9042/10000 (90%)\n",
      "\n",
      "Train Epoch: 7 [0/50000 (0%)]\tLoss: 0.039227\n",
      "Train Epoch: 7 [25600/50000 (51%)]\tLoss: 0.043792\n",
      "\n",
      "--------- Testing in global mode ---------\n",
      "size of test_loader dataset:  10000\n",
      "\n",
      "Test set: Avg. loss: 0.0003, Accuracy: 9039/10000 (90%)\n",
      "\n",
      "Train Epoch: 8 [0/50000 (0%)]\tLoss: 0.052356\n",
      "Train Epoch: 8 [25600/50000 (51%)]\tLoss: 0.056358\n",
      "\n",
      "--------- Testing in global mode ---------\n",
      "size of test_loader dataset:  10000\n",
      "\n",
      "Test set: Avg. loss: 0.0003, Accuracy: 9038/10000 (90%)\n",
      "\n",
      "Train Epoch: 9 [0/50000 (0%)]\tLoss: 0.059934\n",
      "Train Epoch: 9 [25600/50000 (51%)]\tLoss: 0.047491\n",
      "\n",
      "--------- Testing in global mode ---------\n",
      "size of test_loader dataset:  10000\n",
      "\n",
      "Test set: Avg. loss: 0.0003, Accuracy: 9039/10000 (90%)\n",
      "\n",
      "Train Epoch: 10 [0/50000 (0%)]\tLoss: 0.046190\n",
      "Train Epoch: 10 [25600/50000 (51%)]\tLoss: 0.060985\n",
      "\n",
      "--------- Testing in global mode ---------\n",
      "size of test_loader dataset:  10000\n",
      "\n",
      "Test set: Avg. loss: 0.0003, Accuracy: 9035/10000 (90%)\n",
      "\n",
      "in times 1, now cumm_ratio 0.26666666666666666,accuracies:[90.33, 90.35],losses:[0.00030376456677913663, 0.00029912107586860655]\n",
      "Train Epoch: 1 [0/50000 (0%)]\tLoss: 0.073415\n",
      "Train Epoch: 1 [25600/50000 (51%)]\tLoss: 0.087932\n",
      "\n",
      "--------- Testing in global mode ---------\n",
      "size of test_loader dataset:  10000\n",
      "\n",
      "Test set: Avg. loss: 0.0003, Accuracy: 8893/10000 (89%)\n",
      "\n",
      "Train Epoch: 2 [0/50000 (0%)]\tLoss: 0.085898\n",
      "Train Epoch: 2 [25600/50000 (51%)]\tLoss: 0.078772\n",
      "\n",
      "--------- Testing in global mode ---------\n",
      "size of test_loader dataset:  10000\n",
      "\n",
      "Test set: Avg. loss: 0.0003, Accuracy: 8923/10000 (89%)\n",
      "\n",
      "Train Epoch: 3 [0/50000 (0%)]\tLoss: 0.096749\n",
      "Train Epoch: 3 [25600/50000 (51%)]\tLoss: 0.078501\n",
      "\n",
      "--------- Testing in global mode ---------\n",
      "size of test_loader dataset:  10000\n",
      "\n",
      "Test set: Avg. loss: 0.0003, Accuracy: 8937/10000 (89%)\n",
      "\n",
      "Train Epoch: 4 [0/50000 (0%)]\tLoss: 0.070018\n",
      "Train Epoch: 4 [25600/50000 (51%)]\tLoss: 0.070201\n",
      "\n",
      "--------- Testing in global mode ---------\n",
      "size of test_loader dataset:  10000\n",
      "\n",
      "Test set: Avg. loss: 0.0003, Accuracy: 8955/10000 (90%)\n",
      "\n",
      "Train Epoch: 5 [0/50000 (0%)]\tLoss: 0.085427\n",
      "Train Epoch: 5 [25600/50000 (51%)]\tLoss: 0.060058\n",
      "\n",
      "--------- Testing in global mode ---------\n",
      "size of test_loader dataset:  10000\n",
      "\n",
      "Test set: Avg. loss: 0.0003, Accuracy: 8967/10000 (90%)\n",
      "\n",
      "Train Epoch: 6 [0/50000 (0%)]\tLoss: 0.075998\n",
      "Train Epoch: 6 [25600/50000 (51%)]\tLoss: 0.065892\n",
      "\n",
      "--------- Testing in global mode ---------\n",
      "size of test_loader dataset:  10000\n",
      "\n",
      "Test set: Avg. loss: 0.0003, Accuracy: 8981/10000 (90%)\n",
      "\n",
      "Train Epoch: 7 [0/50000 (0%)]\tLoss: 0.046508\n",
      "Train Epoch: 7 [25600/50000 (51%)]\tLoss: 0.064537\n",
      "\n",
      "--------- Testing in global mode ---------\n",
      "size of test_loader dataset:  10000\n",
      "\n",
      "Test set: Avg. loss: 0.0003, Accuracy: 8989/10000 (90%)\n",
      "\n",
      "Train Epoch: 8 [0/50000 (0%)]\tLoss: 0.073105\n",
      "Train Epoch: 8 [25600/50000 (51%)]\tLoss: 0.057310\n",
      "\n",
      "--------- Testing in global mode ---------\n",
      "size of test_loader dataset:  10000\n",
      "\n",
      "Test set: Avg. loss: 0.0003, Accuracy: 8993/10000 (90%)\n",
      "\n",
      "Train Epoch: 9 [0/50000 (0%)]\tLoss: 0.045147\n",
      "Train Epoch: 9 [25600/50000 (51%)]\tLoss: 0.056896\n",
      "\n",
      "--------- Testing in global mode ---------\n",
      "size of test_loader dataset:  10000\n",
      "\n",
      "Test set: Avg. loss: 0.0003, Accuracy: 8994/10000 (90%)\n",
      "\n",
      "Train Epoch: 10 [0/50000 (0%)]\tLoss: 0.061856\n",
      "Train Epoch: 10 [25600/50000 (51%)]\tLoss: 0.065492\n",
      "\n",
      "--------- Testing in global mode ---------\n",
      "size of test_loader dataset:  10000\n",
      "\n",
      "Test set: Avg. loss: 0.0003, Accuracy: 8996/10000 (90%)\n",
      "\n",
      "Train Epoch: 1 [0/50000 (0%)]\tLoss: 0.078049\n",
      "Train Epoch: 1 [25600/50000 (51%)]\tLoss: 0.078638\n",
      "\n",
      "--------- Testing in global mode ---------\n",
      "size of test_loader dataset:  10000\n",
      "\n",
      "Test set: Avg. loss: 0.0003, Accuracy: 8996/10000 (90%)\n",
      "\n",
      "Train Epoch: 2 [0/50000 (0%)]\tLoss: 0.056674\n",
      "Train Epoch: 2 [25600/50000 (51%)]\tLoss: 0.076249\n",
      "\n",
      "--------- Testing in global mode ---------\n",
      "size of test_loader dataset:  10000\n",
      "\n",
      "Test set: Avg. loss: 0.0003, Accuracy: 9002/10000 (90%)\n",
      "\n",
      "Train Epoch: 3 [0/50000 (0%)]\tLoss: 0.086056\n",
      "Train Epoch: 3 [25600/50000 (51%)]\tLoss: 0.058595\n",
      "\n",
      "--------- Testing in global mode ---------\n",
      "size of test_loader dataset:  10000\n",
      "\n",
      "Test set: Avg. loss: 0.0003, Accuracy: 9007/10000 (90%)\n",
      "\n",
      "Train Epoch: 4 [0/50000 (0%)]\tLoss: 0.052330\n",
      "Train Epoch: 4 [25600/50000 (51%)]\tLoss: 0.070661\n",
      "\n",
      "--------- Testing in global mode ---------\n",
      "size of test_loader dataset:  10000\n",
      "\n",
      "Test set: Avg. loss: 0.0003, Accuracy: 9012/10000 (90%)\n",
      "\n",
      "Train Epoch: 5 [0/50000 (0%)]\tLoss: 0.066515\n",
      "Train Epoch: 5 [25600/50000 (51%)]\tLoss: 0.055772\n",
      "\n",
      "--------- Testing in global mode ---------\n",
      "size of test_loader dataset:  10000\n",
      "\n",
      "Test set: Avg. loss: 0.0003, Accuracy: 9009/10000 (90%)\n",
      "\n",
      "Train Epoch: 6 [0/50000 (0%)]\tLoss: 0.054848\n",
      "Train Epoch: 6 [25600/50000 (51%)]\tLoss: 0.054296\n",
      "\n",
      "--------- Testing in global mode ---------\n",
      "size of test_loader dataset:  10000\n",
      "\n",
      "Test set: Avg. loss: 0.0003, Accuracy: 9014/10000 (90%)\n",
      "\n",
      "Train Epoch: 7 [0/50000 (0%)]\tLoss: 0.067562\n",
      "Train Epoch: 7 [25600/50000 (51%)]\tLoss: 0.064403\n",
      "\n",
      "--------- Testing in global mode ---------\n",
      "size of test_loader dataset:  10000\n",
      "\n",
      "Test set: Avg. loss: 0.0003, Accuracy: 9013/10000 (90%)\n",
      "\n",
      "Train Epoch: 8 [0/50000 (0%)]\tLoss: 0.070233\n",
      "Train Epoch: 8 [25600/50000 (51%)]\tLoss: 0.055418\n",
      "\n",
      "--------- Testing in global mode ---------\n",
      "size of test_loader dataset:  10000\n",
      "\n",
      "Test set: Avg. loss: 0.0003, Accuracy: 9014/10000 (90%)\n",
      "\n",
      "Train Epoch: 9 [0/50000 (0%)]\tLoss: 0.053874\n",
      "Train Epoch: 9 [25600/50000 (51%)]\tLoss: 0.065037\n",
      "\n",
      "--------- Testing in global mode ---------\n",
      "size of test_loader dataset:  10000\n",
      "\n",
      "Test set: Avg. loss: 0.0003, Accuracy: 9020/10000 (90%)\n",
      "\n",
      "Train Epoch: 10 [0/50000 (0%)]\tLoss: 0.048608\n",
      "Train Epoch: 10 [25600/50000 (51%)]\tLoss: 0.062633\n",
      "\n",
      "--------- Testing in global mode ---------\n",
      "size of test_loader dataset:  10000\n",
      "\n",
      "Test set: Avg. loss: 0.0003, Accuracy: 9024/10000 (90%)\n",
      "\n",
      "in times 2, now cumm_ratio 0.4,accuracies:[89.96, 90.24],losses:[0.0003167308211326599, 0.0003101238518953323]\n",
      "===============================================\n",
      "===============================================\n",
      "before Ensemble,acc: [89.96, 90.24] loss [0.0003167308211326599, 0.0003101238518953323]\n",
      "===============================================\n",
      "===============================================\n",
      "check zero rate for model 0\n",
      "odict_keys(['features.0.weight', 'features.3.weight', 'features.6.weight', 'features.8.weight', 'features.11.weight', 'features.13.weight', 'features.16.weight', 'features.18.weight', 'classifier.weight'])\n",
      "the ratio of zero for features.0.weight\n",
      "tensor(0.3999, device='cuda:0')\n",
      "the ratio of zero for features.3.weight\n",
      "tensor(0.4000, device='cuda:0')\n",
      "the ratio of zero for features.6.weight\n",
      "tensor(0.4000, device='cuda:0')\n",
      "the ratio of zero for features.8.weight\n",
      "tensor(0.4000, device='cuda:0')\n",
      "the ratio of zero for features.11.weight\n",
      "tensor(0.4000, device='cuda:0')\n",
      "the ratio of zero for features.13.weight\n",
      "tensor(0.4000, device='cuda:0')\n",
      "the ratio of zero for features.16.weight\n",
      "tensor(0.4000, device='cuda:0')\n",
      "the ratio of zero for features.18.weight\n",
      "tensor(0.4000, device='cuda:0')\n",
      "the ratio of zero for classifier.weight\n",
      "tensor(0.4000, device='cuda:0')\n",
      "check zero rate for model 1\n",
      "odict_keys(['features.0.weight', 'features.3.weight', 'features.6.weight', 'features.8.weight', 'features.11.weight', 'features.13.weight', 'features.16.weight', 'features.18.weight', 'classifier.weight'])\n",
      "the ratio of zero for features.0.weight\n",
      "tensor(0.3999, device='cuda:0')\n",
      "the ratio of zero for features.3.weight\n",
      "tensor(0.4000, device='cuda:0')\n",
      "the ratio of zero for features.6.weight\n",
      "tensor(0.4000, device='cuda:0')\n",
      "the ratio of zero for features.8.weight\n",
      "tensor(0.4000, device='cuda:0')\n",
      "the ratio of zero for features.11.weight\n",
      "tensor(0.4000, device='cuda:0')\n",
      "the ratio of zero for features.13.weight\n",
      "tensor(0.4000, device='cuda:0')\n",
      "the ratio of zero for features.16.weight\n",
      "tensor(0.4000, device='cuda:0')\n",
      "the ratio of zero for features.18.weight\n",
      "tensor(0.4000, device='cuda:0')\n",
      "the ratio of zero for classifier.weight\n",
      "tensor(0.4000, device='cuda:0')\n",
      "layer features.0.weight has #params  1728\n",
      "layer features.3.weight has #params  73728\n",
      "layer features.6.weight has #params  294912\n",
      "layer features.8.weight has #params  589824\n",
      "layer features.11.weight has #params  1179648\n",
      "layer features.13.weight has #params  2359296\n",
      "layer features.16.weight has #params  2359296\n",
      "layer features.18.weight has #params  2359296\n",
      "layer classifier.weight has #params  5120\n",
      "Activation Timer start\n",
      "Activation Timer ends\n",
      "------- Geometric Ensembling -------\n",
      "Timer start\n",
      "odict_keys(['features.0.weight', 'features.3.weight', 'features.6.weight', 'features.8.weight', 'features.11.weight', 'features.13.weight', 'features.16.weight', 'features.18.weight', 'classifier.weight'])\n",
      "the ratio of zero for features.0.weight\n",
      "tensor(0.3999, device='cuda:0')\n",
      "the ratio of zero for features.3.weight\n",
      "tensor(0.4000, device='cuda:0')\n",
      "the ratio of zero for features.6.weight\n",
      "tensor(0.4000, device='cuda:0')\n",
      "the ratio of zero for features.8.weight\n",
      "tensor(0.4000, device='cuda:0')\n",
      "the ratio of zero for features.11.weight\n",
      "tensor(0.4000, device='cuda:0')\n",
      "the ratio of zero for features.13.weight\n",
      "tensor(0.4000, device='cuda:0')\n",
      "the ratio of zero for features.16.weight\n",
      "tensor(0.4000, device='cuda:0')\n",
      "the ratio of zero for features.18.weight\n",
      "tensor(0.4000, device='cuda:0')\n",
      "the ratio of zero for classifier.weight\n",
      "tensor(0.4000, device='cuda:0')\n",
      "Previous layer shape is  None\n",
      "Processing the coordinates to form ground_metric\n",
      "dont leave off the squaring of the ground metric\n",
      "returns a uniform measure of cardinality:  64\n",
      "returns a uniform measure of cardinality:  64\n",
      "the transport map is  tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')\n",
      "Ratio of trace to the matrix sum:  tensor(0.0312, device='cuda:0')\n",
      "Here, trace is 2.0 and matrix sum is 64.0 \n",
      "this is past correction for weight mode\n",
      "Shape of aligned wt is  torch.Size([64, 3, 9])\n",
      "Shape of fc_layer0_weight_data is  torch.Size([64, 3, 9])\n",
      "Previous layer shape is  torch.Size([64, 3, 3, 3])\n",
      "shape of layer: model 0 torch.Size([128, 64, 9])\n",
      "shape of layer: model 1 torch.Size([128, 64, 9])\n",
      "shape of previous transport map torch.Size([64, 64])\n",
      "Processing the coordinates to form ground_metric\n",
      "dont leave off the squaring of the ground metric\n",
      "returns a uniform measure of cardinality:  128\n",
      "returns a uniform measure of cardinality:  128\n",
      "the transport map is  tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        ...,\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0078],\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],\n",
      "       device='cuda:0')\n",
      "Ratio of trace to the matrix sum:  tensor(0.0078, device='cuda:0')\n",
      "Here, trace is 1.0 and matrix sum is 128.0 \n",
      "this is past correction for weight mode\n",
      "Shape of aligned wt is  torch.Size([128, 64, 9])\n",
      "Shape of fc_layer0_weight_data is  torch.Size([128, 64, 9])\n",
      "Previous layer shape is  torch.Size([128, 64, 3, 3])\n",
      "shape of layer: model 0 torch.Size([256, 128, 9])\n",
      "shape of layer: model 1 torch.Size([256, 128, 9])\n",
      "shape of previous transport map torch.Size([128, 128])\n",
      "Processing the coordinates to form ground_metric\n",
      "dont leave off the squaring of the ground metric\n",
      "returns a uniform measure of cardinality:  256\n",
      "returns a uniform measure of cardinality:  256\n",
      "the transport map is  tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')\n",
      "Ratio of trace to the matrix sum:  tensor(0., device='cuda:0')\n",
      "Here, trace is 0.0 and matrix sum is 256.0 \n",
      "this is past correction for weight mode\n",
      "Shape of aligned wt is  torch.Size([256, 128, 9])\n",
      "Shape of fc_layer0_weight_data is  torch.Size([256, 128, 9])\n",
      "Previous layer shape is  torch.Size([256, 128, 3, 3])\n",
      "shape of layer: model 0 torch.Size([256, 256, 9])\n",
      "shape of layer: model 1 torch.Size([256, 256, 9])\n",
      "shape of previous transport map torch.Size([256, 256])\n",
      "Processing the coordinates to form ground_metric\n",
      "dont leave off the squaring of the ground metric\n",
      "returns a uniform measure of cardinality:  256\n",
      "returns a uniform measure of cardinality:  256\n",
      "the transport map is  tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        ...,\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.0039, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],\n",
      "       device='cuda:0')\n",
      "Ratio of trace to the matrix sum:  tensor(0.0039, device='cuda:0')\n",
      "Here, trace is 1.0 and matrix sum is 256.0 \n",
      "this is past correction for weight mode\n",
      "Shape of aligned wt is  torch.Size([256, 256, 9])\n",
      "Shape of fc_layer0_weight_data is  torch.Size([256, 256, 9])\n",
      "Previous layer shape is  torch.Size([256, 256, 3, 3])\n",
      "shape of layer: model 0 torch.Size([512, 256, 9])\n",
      "shape of layer: model 1 torch.Size([512, 256, 9])\n",
      "shape of previous transport map torch.Size([256, 256])\n",
      "Processing the coordinates to form ground_metric\n",
      "dont leave off the squaring of the ground metric\n",
      "returns a uniform measure of cardinality:  512\n",
      "returns a uniform measure of cardinality:  512\n",
      "the transport map is  tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')\n",
      "Ratio of trace to the matrix sum:  tensor(0.0020, device='cuda:0')\n",
      "Here, trace is 1.0 and matrix sum is 512.0 \n",
      "this is past correction for weight mode\n",
      "Shape of aligned wt is  torch.Size([512, 256, 9])\n",
      "Shape of fc_layer0_weight_data is  torch.Size([512, 256, 9])\n",
      "Previous layer shape is  torch.Size([512, 256, 3, 3])\n",
      "shape of layer: model 0 torch.Size([512, 512, 9])\n",
      "shape of layer: model 1 torch.Size([512, 512, 9])\n",
      "shape of previous transport map torch.Size([512, 512])\n",
      "Processing the coordinates to form ground_metric\n",
      "dont leave off the squaring of the ground metric\n",
      "returns a uniform measure of cardinality:  512\n",
      "returns a uniform measure of cardinality:  512\n",
      "the transport map is  tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')\n",
      "Ratio of trace to the matrix sum:  tensor(0.0020, device='cuda:0')\n",
      "Here, trace is 1.0 and matrix sum is 512.0 \n",
      "this is past correction for weight mode\n",
      "Shape of aligned wt is  torch.Size([512, 512, 9])\n",
      "Shape of fc_layer0_weight_data is  torch.Size([512, 512, 9])\n",
      "Previous layer shape is  torch.Size([512, 512, 3, 3])\n",
      "shape of layer: model 0 torch.Size([512, 512, 9])\n",
      "shape of layer: model 1 torch.Size([512, 512, 9])\n",
      "shape of previous transport map torch.Size([512, 512])\n",
      "Processing the coordinates to form ground_metric\n",
      "dont leave off the squaring of the ground metric\n",
      "returns a uniform measure of cardinality:  512\n",
      "returns a uniform measure of cardinality:  512\n",
      "the transport map is  tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')\n",
      "Ratio of trace to the matrix sum:  tensor(0.0039, device='cuda:0')\n",
      "Here, trace is 2.0 and matrix sum is 512.0 \n",
      "this is past correction for weight mode\n",
      "Shape of aligned wt is  torch.Size([512, 512, 9])\n",
      "Shape of fc_layer0_weight_data is  torch.Size([512, 512, 9])\n",
      "Previous layer shape is  torch.Size([512, 512, 3, 3])\n",
      "shape of layer: model 0 torch.Size([512, 512, 9])\n",
      "shape of layer: model 1 torch.Size([512, 512, 9])\n",
      "shape of previous transport map torch.Size([512, 512])\n",
      "Processing the coordinates to form ground_metric\n",
      "dont leave off the squaring of the ground metric\n",
      "returns a uniform measure of cardinality:  512\n",
      "returns a uniform measure of cardinality:  512\n",
      "the transport map is  tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        [0.0020, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        ...,\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],\n",
      "       device='cuda:0')\n",
      "Ratio of trace to the matrix sum:  tensor(0., device='cuda:0')\n",
      "Here, trace is 0.0 and matrix sum is 512.0 \n",
      "this is past correction for weight mode\n",
      "Shape of aligned wt is  torch.Size([512, 512, 9])\n",
      "Shape of fc_layer0_weight_data is  torch.Size([512, 512, 9])\n",
      "Previous layer shape is  torch.Size([512, 512, 3, 3])\n",
      "shape of layer: model 0 torch.Size([10, 512])\n",
      "shape of layer: model 1 torch.Size([10, 512])\n",
      "shape of previous transport map torch.Size([512, 512])\n",
      "Processing the coordinates to form ground_metric\n",
      "dont leave off the squaring of the ground metric\n",
      "ground metric is  tensor([[1.4515, 2.5032, 2.7813, 2.8595, 2.6963, 2.8556, 2.7084, 2.6356, 2.5819,\n",
      "         2.5831],\n",
      "        [2.4811, 1.3371, 2.6736, 2.5539, 2.7487, 2.4933, 2.4141, 2.5387, 2.2911,\n",
      "         2.4121],\n",
      "        [2.7556, 2.6423, 1.3763, 2.9615, 2.8870, 2.8693, 2.7265, 2.8250, 2.7485,\n",
      "         2.7411],\n",
      "        [2.8142, 2.6160, 2.9150, 1.6076, 2.8861, 2.8993, 2.7213, 2.7913, 2.8028,\n",
      "         2.6503],\n",
      "        [2.7380, 2.5466, 2.8493, 2.8355, 1.5465, 2.7624, 2.7643, 2.6727, 2.6952,\n",
      "         2.7334],\n",
      "        [2.9511, 2.5569, 2.9651, 2.8494, 2.7186, 1.5300, 2.7458, 2.6881, 2.7743,\n",
      "         2.6639],\n",
      "        [2.6466, 2.4110, 2.6816, 2.9041, 2.6316, 2.8519, 1.4458, 2.7159, 2.4807,\n",
      "         2.5223],\n",
      "        [2.7309, 2.5550, 2.7360, 2.7759, 2.6992, 2.7220, 2.6621, 1.4075, 2.5304,\n",
      "         2.5009],\n",
      "        [2.5759, 2.3913, 2.7526, 2.7197, 2.7023, 2.8165, 2.5054, 2.6043, 1.2794,\n",
      "         2.3579],\n",
      "        [2.5590, 2.2673, 2.7875, 2.7136, 2.6469, 2.7023, 2.5379, 2.5220, 2.5219,\n",
      "         1.3202]], device='cuda:0', grad_fn=<PowBackward0>)\n",
      "returns a uniform measure of cardinality:  10\n",
      "returns a uniform measure of cardinality:  10\n",
      "the transport map is  tensor([[0.1000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0000, 0.1000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0000, 0.0000, 0.1000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.1000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.1000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.1000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.1000, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.1000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.1000,\n",
      "         0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.1000]], device='cuda:0')\n",
      "Ratio of trace to the matrix sum:  tensor(1., device='cuda:0')\n",
      "Here, trace is 9.99755859375 and matrix sum is 9.99755859375 \n",
      "this is past correction for weight mode\n",
      "Shape of aligned wt is  torch.Size([10, 512])\n",
      "Shape of fc_layer0_weight_data is  torch.Size([10, 512])\n",
      "using independent method\n",
      "in _make_layers [Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), AvgPool2d(kernel_size=1, stride=1, padding=0)]\n",
      "Relu Inplace is  False\n",
      "model parameters are \n",
      " [torch.Size([64, 3, 3, 3]), torch.Size([128, 64, 3, 3]), torch.Size([256, 128, 3, 3]), torch.Size([256, 256, 3, 3]), torch.Size([512, 256, 3, 3]), torch.Size([512, 512, 3, 3]), torch.Size([512, 512, 3, 3]), torch.Size([512, 512, 3, 3]), torch.Size([10, 512])]\n",
      "\n",
      "--------- Testing in global mode ---------\n",
      "size of test_loader dataset:  10000\n",
      "\n",
      "Test set: Avg. loss: 0.0023, Accuracy: 1000/10000 (10%)\n",
      "\n",
      "len of model parameters and avg aligned layers is  9 9\n",
      "len of model_state_dict is  9\n",
      "len of param_list is  9\n",
      "\n",
      "--------- Testing in global mode ---------\n",
      "size of test_loader dataset:  10000\n",
      "\n",
      "Test set: Avg. loss: 0.0013, Accuracy: 8509/10000 (85%)\n",
      "\n",
      "===============================================\n",
      "===============================================\n",
      "After Ensemble,acc: 85.09 loss 0.0013217577457427978\n",
      "===============================================\n",
      "===============================================\n",
      "Timer ends\n",
      "Time taken for geometric ensembling is 3.3407420106232166 seconds\n",
      "------- Prediction based ensembling -------\n",
      "\n",
      "Test set: Avg. loss: 0.0003, Accuracy: 9091/10000 (91%)\n",
      "\n",
      "------- Naive ensembling of weights -------\n",
      "[torch.Size([64, 3, 3, 3]), torch.Size([64, 3, 3, 3])]\n",
      "torch.Size([64, 3, 3, 3])\n",
      "[torch.Size([128, 64, 3, 3]), torch.Size([128, 64, 3, 3])]\n",
      "torch.Size([128, 64, 3, 3])\n",
      "[torch.Size([256, 128, 3, 3]), torch.Size([256, 128, 3, 3])]\n",
      "torch.Size([256, 128, 3, 3])\n",
      "[torch.Size([256, 256, 3, 3]), torch.Size([256, 256, 3, 3])]\n",
      "torch.Size([256, 256, 3, 3])\n",
      "[torch.Size([512, 256, 3, 3]), torch.Size([512, 256, 3, 3])]\n",
      "torch.Size([512, 256, 3, 3])\n",
      "[torch.Size([512, 512, 3, 3]), torch.Size([512, 512, 3, 3])]\n",
      "torch.Size([512, 512, 3, 3])\n",
      "[torch.Size([512, 512, 3, 3]), torch.Size([512, 512, 3, 3])]\n",
      "torch.Size([512, 512, 3, 3])\n",
      "[torch.Size([512, 512, 3, 3]), torch.Size([512, 512, 3, 3])]\n",
      "torch.Size([512, 512, 3, 3])\n",
      "[torch.Size([10, 512]), torch.Size([10, 512])]\n",
      "torch.Size([10, 512])\n",
      "in _make_layers [Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), AvgPool2d(kernel_size=1, stride=1, padding=0)]\n",
      "Relu Inplace is  False\n",
      "model parameters are \n",
      " [torch.Size([64, 3, 3, 3]), torch.Size([128, 64, 3, 3]), torch.Size([256, 128, 3, 3]), torch.Size([256, 256, 3, 3]), torch.Size([512, 256, 3, 3]), torch.Size([512, 512, 3, 3]), torch.Size([512, 512, 3, 3]), torch.Size([512, 512, 3, 3]), torch.Size([10, 512])]\n",
      "\n",
      "--------- Testing in global mode ---------\n",
      "size of test_loader dataset:  10000\n",
      "\n",
      "Test set: Avg. loss: 0.0023, Accuracy: 904/10000 (9%)\n",
      "\n",
      "\n",
      "--------- Testing in global mode ---------\n",
      "size of test_loader dataset:  10000\n",
      "\n",
      "Test set: Avg. loss: 0.0023, Accuracy: 1449/10000 (14%)\n",
      "\n",
      "-------- Retraining the models ---------\n",
      "Retraining model :  model_0\n",
      "num_epochs---------------+++))))))+++ 300\n",
      "num_epochs---------------+++))))))+++ 300\n",
      "lr is  0.05\n",
      "number of epochs would be  30\n",
      "num_epochs-------------- 30\n",
      "Epoch 000\n",
      "/root/miniconda3/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:129: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\"Detected call of `lr_scheduler.step()` before `optimizer.step()`. \"\n",
      "/root/miniconda3/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:154: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "accuracy: {'epoch': 0, 'value': 0.9048599999809264} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 0, 'value': 0.28228466499805466} ({'split': 'train'})\n",
      "accuracy: {'epoch': 0, 'value': 0.848300039768219} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 0, 'value': 0.4811398953199386} ({'split': 'test'})\n",
      "Epoch 001\n",
      "accuracy: {'epoch': 1, 'value': 0.9294799999999999} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 1, 'value': 0.20904540875911712} ({'split': 'train'})\n",
      "accuracy: {'epoch': 1, 'value': 0.8347000360488891} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 1, 'value': 0.5281963795423508} ({'split': 'test'})\n",
      "Epoch 002\n",
      "accuracy: {'epoch': 2, 'value': 0.9483000000190733} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 2, 'value': 0.1535868446445465} ({'split': 'train'})\n",
      "accuracy: {'epoch': 2, 'value': 0.8607000410556794} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 2, 'value': 0.47121756672859194} ({'split': 'test'})\n",
      "Epoch 003\n",
      "accuracy: {'epoch': 3, 'value': 0.9642999999809263} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 3, 'value': 0.11013494791269307} ({'split': 'train'})\n",
      "accuracy: {'epoch': 3, 'value': 0.8619000315666199} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 3, 'value': 0.4889505326747894} ({'split': 'test'})\n",
      "Epoch 004\n",
      "accuracy: {'epoch': 4, 'value': 0.9711800000190731} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 4, 'value': 0.08731912880182259} ({'split': 'train'})\n",
      "accuracy: {'epoch': 4, 'value': 0.8548000395298004} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 4, 'value': 0.5664981186389924} ({'split': 'test'})\n",
      "Epoch 005\n",
      "accuracy: {'epoch': 5, 'value': 0.9754400000572204} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 5, 'value': 0.07574884919404988} ({'split': 'train'})\n",
      "accuracy: {'epoch': 5, 'value': 0.8402000427246094} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 5, 'value': 0.6581050217151642} ({'split': 'test'})\n",
      "Epoch 006\n",
      "accuracy: {'epoch': 6, 'value': 0.9762000000190737} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 6, 'value': 0.07266219260931014} ({'split': 'train'})\n",
      "accuracy: {'epoch': 6, 'value': 0.8469000458717346} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 6, 'value': 0.6421196699142456} ({'split': 'test'})\n",
      "Epoch 007\n",
      "accuracy: {'epoch': 7, 'value': 0.9812000000572209} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 7, 'value': 0.05775524254083635} ({'split': 'train'})\n",
      "accuracy: {'epoch': 7, 'value': 0.854300045967102} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 7, 'value': 0.5618448674678803} ({'split': 'test'})\n",
      "Epoch 008\n",
      "accuracy: {'epoch': 8, 'value': 0.9825200000381471} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 8, 'value': 0.054026804313659686} ({'split': 'train'})\n",
      "accuracy: {'epoch': 8, 'value': 0.8544000446796417} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 8, 'value': 0.5681492447853088} ({'split': 'test'})\n",
      "Epoch 009\n",
      "accuracy: {'epoch': 9, 'value': 0.9830200000190735} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 9, 'value': 0.05181902339577675} ({'split': 'train'})\n",
      "accuracy: {'epoch': 9, 'value': 0.8710000336170197} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 9, 'value': 0.5444774866104125} ({'split': 'test'})\n",
      "Epoch 010\n",
      "accuracy: {'epoch': 10, 'value': 0.9853800000572206} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 10, 'value': 0.045210842492580394} ({'split': 'train'})\n",
      "accuracy: {'epoch': 10, 'value': 0.8569000422954559} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 10, 'value': 0.5505182296037674} ({'split': 'test'})\n",
      "Epoch 011\n",
      "accuracy: {'epoch': 11, 'value': 0.9844800000572204} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 11, 'value': 0.04816190872669219} ({'split': 'train'})\n",
      "accuracy: {'epoch': 11, 'value': 0.849500036239624} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 11, 'value': 0.6495538353919983} ({'split': 'test'})\n",
      "Epoch 012\n",
      "accuracy: {'epoch': 12, 'value': 0.9837599999999999} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 12, 'value': 0.0524350780880451} ({'split': 'train'})\n",
      "accuracy: {'epoch': 12, 'value': 0.8526000380516052} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 12, 'value': 0.6499205827713013} ({'split': 'test'})\n",
      "Epoch 013\n",
      "accuracy: {'epoch': 13, 'value': 0.986} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 13, 'value': 0.044367329350113886} ({'split': 'train'})\n",
      "accuracy: {'epoch': 13, 'value': 0.851900053024292} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 13, 'value': 0.6178136289119721} ({'split': 'test'})\n",
      "Epoch 014\n",
      "accuracy: {'epoch': 14, 'value': 0.9898200000190733} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 14, 'value': 0.03334506731688977} ({'split': 'train'})\n",
      "accuracy: {'epoch': 14, 'value': 0.8555000483989715} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 14, 'value': 0.5886612355709075} ({'split': 'test'})\n",
      "Epoch 015\n",
      "accuracy: {'epoch': 15, 'value': 0.9888800000190733} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 15, 'value': 0.03633824720501899} ({'split': 'train'})\n",
      "accuracy: {'epoch': 15, 'value': 0.8681000351905823} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 15, 'value': 0.5552268624305725} ({'split': 'test'})\n",
      "Epoch 016\n",
      "accuracy: {'epoch': 16, 'value': 0.9910000000190734} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 16, 'value': 0.029442228795289967} ({'split': 'train'})\n",
      "accuracy: {'epoch': 16, 'value': 0.860200035572052} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 16, 'value': 0.6035563766956329} ({'split': 'test'})\n",
      "Epoch 017\n",
      "accuracy: {'epoch': 17, 'value': 0.9889200000190733} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 17, 'value': 0.038089092228412634} ({'split': 'train'})\n",
      "accuracy: {'epoch': 17, 'value': 0.8570000469684601} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 17, 'value': 0.651814180612564} ({'split': 'test'})\n",
      "Epoch 018\n",
      "accuracy: {'epoch': 18, 'value': 0.989200000038147} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 18, 'value': 0.03397289373755455} ({'split': 'train'})\n",
      "accuracy: {'epoch': 18, 'value': 0.8636000394821167} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 18, 'value': 0.5849474459886551} ({'split': 'test'})\n",
      "Epoch 019\n",
      "accuracy: {'epoch': 19, 'value': 0.9899400000381469} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 19, 'value': 0.03315983539581299} ({'split': 'train'})\n",
      "accuracy: {'epoch': 19, 'value': 0.8606000483036041} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 19, 'value': 0.566795563697815} ({'split': 'test'})\n",
      "Epoch 020\n",
      "accuracy: {'epoch': 20, 'value': 0.9889199999999997} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 20, 'value': 0.03493055349320175} ({'split': 'train'})\n",
      "accuracy: {'epoch': 20, 'value': 0.8545000433921813} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 20, 'value': 0.6018692135810851} ({'split': 'test'})\n",
      "Epoch 021\n",
      "accuracy: {'epoch': 21, 'value': 0.9872000000000002} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 21, 'value': 0.042517819133698946} ({'split': 'train'})\n",
      "accuracy: {'epoch': 21, 'value': 0.8611000418663025} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 21, 'value': 0.6006911277770995} ({'split': 'test'})\n",
      "Epoch 022\n",
      "accuracy: {'epoch': 22, 'value': 0.9875200000000003} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 22, 'value': 0.03965821515262125} ({'split': 'train'})\n",
      "accuracy: {'epoch': 22, 'value': 0.8531000435352326} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 22, 'value': 0.6281635105609894} ({'split': 'test'})\n",
      "Epoch 023\n",
      "accuracy: {'epoch': 23, 'value': 0.98966} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 23, 'value': 0.034904311580657967} ({'split': 'train'})\n",
      "accuracy: {'epoch': 23, 'value': 0.860000056028366} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 23, 'value': 0.5772126376628877} ({'split': 'test'})\n",
      "Epoch 024\n",
      "accuracy: {'epoch': 24, 'value': 0.9866800000381468} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 24, 'value': 0.042694747941494} ({'split': 'train'})\n",
      "accuracy: {'epoch': 24, 'value': 0.848600035905838} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 24, 'value': 0.665454351902008} ({'split': 'test'})\n",
      "Epoch 025\n",
      "accuracy: {'epoch': 25, 'value': 0.9853999999999998} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 25, 'value': 0.046872925963997836} ({'split': 'train'})\n",
      "accuracy: {'epoch': 25, 'value': 0.851900029182434} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 25, 'value': 0.6385241568088532} ({'split': 'test'})\n",
      "Epoch 026\n",
      "accuracy: {'epoch': 26, 'value': 0.9878600000572209} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 26, 'value': 0.04041390852451323} ({'split': 'train'})\n",
      "accuracy: {'epoch': 26, 'value': 0.8549000382423402} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 26, 'value': 0.6172730922698975} ({'split': 'test'})\n",
      "Epoch 027\n",
      "accuracy: {'epoch': 27, 'value': 0.9850000000190734} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 27, 'value': 0.047637141909599306} ({'split': 'train'})\n",
      "accuracy: {'epoch': 27, 'value': 0.8512000441551208} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 27, 'value': 0.6047444880008698} ({'split': 'test'})\n",
      "Epoch 028\n",
      "accuracy: {'epoch': 28, 'value': 0.9889800000190733} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 28, 'value': 0.03384549186944961} ({'split': 'train'})\n",
      "accuracy: {'epoch': 28, 'value': 0.8482000350952148} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 28, 'value': 0.6182754218578339} ({'split': 'test'})\n",
      "Epoch 029\n",
      "accuracy: {'epoch': 29, 'value': 0.9912600000190736} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 29, 'value': 0.029521149569749838} ({'split': 'train'})\n",
      "accuracy: {'epoch': 29, 'value': 0.8413000404834747} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 29, 'value': 0.6607108533382416} ({'split': 'test'})\n",
      "mean_test_accuracy <cifar_utils.accumulators.Mean object at 0x7f781fe6aee0>\n",
      "QWERTY: Enter Cifar 2, acc 0.8710000336170197\n",
      "Retraining model :  model_1\n",
      "num_epochs---------------+++))))))+++ 30\n",
      "num_epochs---------------+++))))))+++ 30\n",
      "lr is  0.05\n",
      "number of epochs would be  30\n",
      "num_epochs-------------- 30\n",
      "Epoch 000\n",
      "accuracy: {'epoch': 0, 'value': 0.8918199999809263} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 0, 'value': 0.32592260821342456} ({'split': 'train'})\n",
      "accuracy: {'epoch': 0, 'value': 0.8342000365257263} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 0, 'value': 0.5010293275117874} ({'split': 'test'})\n",
      "Epoch 001\n",
      "accuracy: {'epoch': 1, 'value': 0.9264000000572206} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 1, 'value': 0.21645115309715274} ({'split': 'train'})\n",
      "accuracy: {'epoch': 1, 'value': 0.8270000338554382} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 1, 'value': 0.5805166482925415} ({'split': 'test'})\n",
      "Epoch 002\n",
      "accuracy: {'epoch': 2, 'value': 0.9488} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 2, 'value': 0.151396317949295} ({'split': 'train'})\n",
      "accuracy: {'epoch': 2, 'value': 0.85040003657341} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 2, 'value': 0.5335677772760391} ({'split': 'test'})\n",
      "Epoch 003\n",
      "accuracy: {'epoch': 3, 'value': 0.963160000019074} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 3, 'value': 0.11013595155000684} ({'split': 'train'})\n",
      "accuracy: {'epoch': 3, 'value': 0.8577000439167024} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 3, 'value': 0.49640526175498967} ({'split': 'test'})\n",
      "Epoch 004\n",
      "accuracy: {'epoch': 4, 'value': 0.9698000000381471} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 4, 'value': 0.08949917506456372} ({'split': 'train'})\n",
      "accuracy: {'epoch': 4, 'value': 0.8539000391960144} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 4, 'value': 0.5190329879522324} ({'split': 'test'})\n",
      "Epoch 005\n",
      "accuracy: {'epoch': 5, 'value': 0.9754199999809263} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 5, 'value': 0.07621120319366453} ({'split': 'train'})\n",
      "accuracy: {'epoch': 5, 'value': 0.8657000482082366} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 5, 'value': 0.5176772475242615} ({'split': 'test'})\n",
      "Epoch 006\n",
      "accuracy: {'epoch': 6, 'value': 0.9770799999999997} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 6, 'value': 0.06955886370241636} ({'split': 'train'})\n",
      "accuracy: {'epoch': 6, 'value': 0.8346000432968139} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 6, 'value': 0.6516801595687867} ({'split': 'test'})\n",
      "Epoch 007\n",
      "accuracy: {'epoch': 7, 'value': 0.978980000019073} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 7, 'value': 0.0648666816759109} ({'split': 'train'})\n",
      "accuracy: {'epoch': 7, 'value': 0.8694000422954561} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 7, 'value': 0.4985185205936432} ({'split': 'test'})\n",
      "Epoch 008\n",
      "accuracy: {'epoch': 8, 'value': 0.9856400000572204} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 8, 'value': 0.04542727702140808} ({'split': 'train'})\n",
      "accuracy: {'epoch': 8, 'value': 0.8531000435352325} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 8, 'value': 0.5910034596920014} ({'split': 'test'})\n",
      "Epoch 009\n",
      "accuracy: {'epoch': 9, 'value': 0.981380000038147} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 9, 'value': 0.05850293894529342} ({'split': 'train'})\n",
      "accuracy: {'epoch': 9, 'value': 0.8571000337600709} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 9, 'value': 0.583672970533371} ({'split': 'test'})\n",
      "Epoch 010\n",
      "accuracy: {'epoch': 10, 'value': 0.983880000038147} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 10, 'value': 0.049535866131782534} ({'split': 'train'})\n",
      "accuracy: {'epoch': 10, 'value': 0.8645000457763672} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 10, 'value': 0.5618795156478882} ({'split': 'test'})\n",
      "Epoch 011\n",
      "accuracy: {'epoch': 11, 'value': 0.9859800000381465} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 11, 'value': 0.04441754033327102} ({'split': 'train'})\n",
      "accuracy: {'epoch': 11, 'value': 0.8614000439643861} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 11, 'value': 0.5524378299713134} ({'split': 'test'})\n",
      "Epoch 012\n",
      "accuracy: {'epoch': 12, 'value': 0.9881800000190738} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 12, 'value': 0.04002756243169308} ({'split': 'train'})\n",
      "accuracy: {'epoch': 12, 'value': 0.8586000442504883} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 12, 'value': 0.6142895102500915} ({'split': 'test'})\n",
      "Epoch 013\n",
      "accuracy: {'epoch': 13, 'value': 0.9892399999999999} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 13, 'value': 0.03470947456657885} ({'split': 'train'})\n",
      "accuracy: {'epoch': 13, 'value': 0.8667000353336334} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 13, 'value': 0.5948651492595672} ({'split': 'test'})\n",
      "Epoch 014\n",
      "accuracy: {'epoch': 14, 'value': 0.9904799999999998} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 14, 'value': 0.03211536277115343} ({'split': 'train'})\n",
      "accuracy: {'epoch': 14, 'value': 0.8685000538825989} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 14, 'value': 0.5354840159416199} ({'split': 'test'})\n",
      "Epoch 015\n",
      "accuracy: {'epoch': 15, 'value': 0.9914600000190736} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 15, 'value': 0.029331676981449124} ({'split': 'train'})\n",
      "accuracy: {'epoch': 15, 'value': 0.8661000430583954} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 15, 'value': 0.5473445266485214} ({'split': 'test'})\n",
      "Epoch 016\n",
      "accuracy: {'epoch': 16, 'value': 0.9873599999999999} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 16, 'value': 0.041817036986947054} ({'split': 'train'})\n",
      "accuracy: {'epoch': 16, 'value': 0.8669000446796418} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 16, 'value': 0.5589381992816925} ({'split': 'test'})\n",
      "Epoch 017\n",
      "accuracy: {'epoch': 17, 'value': 0.9896000000190734} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 17, 'value': 0.03537251672685146} ({'split': 'train'})\n",
      "accuracy: {'epoch': 17, 'value': 0.8539000391960144} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 17, 'value': 0.5952471107244491} ({'split': 'test'})\n",
      "Epoch 018\n",
      "accuracy: {'epoch': 18, 'value': 0.9907400000190737} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 18, 'value': 0.03168227812409403} ({'split': 'train'})\n",
      "accuracy: {'epoch': 18, 'value': 0.8587000370025635} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 18, 'value': 0.591354787349701} ({'split': 'test'})\n",
      "Epoch 019\n",
      "accuracy: {'epoch': 19, 'value': 0.9884000000190736} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 19, 'value': 0.038933824691772465} ({'split': 'train'})\n",
      "accuracy: {'epoch': 19, 'value': 0.8594000458717346} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 19, 'value': 0.5511402368545532} ({'split': 'test'})\n",
      "Epoch 020\n",
      "accuracy: {'epoch': 20, 'value': 0.9867600000190735} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 20, 'value': 0.0419210163974762} ({'split': 'train'})\n",
      "accuracy: {'epoch': 20, 'value': 0.8557000398635864} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 20, 'value': 0.5608546376228333} ({'split': 'test'})\n",
      "Epoch 021\n",
      "accuracy: {'epoch': 21, 'value': 0.9878399999999996} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 21, 'value': 0.038777527461051946} ({'split': 'train'})\n",
      "accuracy: {'epoch': 21, 'value': 0.8536000430583954} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 21, 'value': 0.6360077798366547} ({'split': 'test'})\n",
      "Epoch 022\n",
      "accuracy: {'epoch': 22, 'value': 0.9871200000000001} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 22, 'value': 0.04186088892459872} ({'split': 'train'})\n",
      "accuracy: {'epoch': 22, 'value': 0.859000039100647} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 22, 'value': 0.6098061680793763} ({'split': 'test'})\n",
      "Epoch 023\n",
      "accuracy: {'epoch': 23, 'value': 0.9882800000190732} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 23, 'value': 0.03966971435427663} ({'split': 'train'})\n",
      "accuracy: {'epoch': 23, 'value': 0.8484000325202943} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 23, 'value': 0.5933841168880463} ({'split': 'test'})\n",
      "Epoch 024\n",
      "accuracy: {'epoch': 24, 'value': 0.9886800000190736} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 24, 'value': 0.03761729367852213} ({'split': 'train'})\n",
      "accuracy: {'epoch': 24, 'value': 0.8533000349998474} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 24, 'value': 0.6283060729503631} ({'split': 'test'})\n",
      "Epoch 025\n",
      "accuracy: {'epoch': 25, 'value': 0.9886} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 25, 'value': 0.035979698671102504} ({'split': 'train'})\n",
      "accuracy: {'epoch': 25, 'value': 0.8541000425815581} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 25, 'value': 0.5931896626949311} ({'split': 'test'})\n",
      "Epoch 026\n",
      "accuracy: {'epoch': 26, 'value': 0.9886000000000005} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 26, 'value': 0.0359857161128521} ({'split': 'train'})\n",
      "accuracy: {'epoch': 26, 'value': 0.8616000413894653} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 26, 'value': 0.5755826294422149} ({'split': 'test'})\n",
      "Epoch 027\n",
      "accuracy: {'epoch': 27, 'value': 0.9905600000190736} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 27, 'value': 0.030094163792133324} ({'split': 'train'})\n",
      "accuracy: {'epoch': 27, 'value': 0.8601000428199768} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 27, 'value': 0.6180042386054992} ({'split': 'test'})\n",
      "Epoch 028\n",
      "accuracy: {'epoch': 28, 'value': 0.9851200000190741} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 28, 'value': 0.046659929162859916} ({'split': 'train'})\n",
      "accuracy: {'epoch': 28, 'value': 0.8493000328540802} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 28, 'value': 0.6008539527654649} ({'split': 'test'})\n",
      "Epoch 029\n",
      "accuracy: {'epoch': 29, 'value': 0.9869000000190733} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 29, 'value': 0.04288564828872681} ({'split': 'train'})\n",
      "accuracy: {'epoch': 29, 'value': 0.8562000453472138} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 29, 'value': 0.5751944065093993} ({'split': 'test'})\n",
      "mean_test_accuracy <cifar_utils.accumulators.Mean object at 0x7f781fe6a160>\n",
      "QWERTY: Enter Cifar 2, acc 0.8694000422954561\n",
      "Retraining model :  geometric\n",
      "num_epochs---------------+++))))))+++ 30\n",
      "num_epochs---------------+++))))))+++ 30\n",
      "lr is  0.05\n",
      "number of epochs would be  30\n",
      "num_epochs-------------- 30\n",
      "Epoch 000\n",
      "accuracy: {'epoch': 0, 'value': 0.13674000001430509} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 0, 'value': 2.5030365495300293} ({'split': 'train'})\n",
      "accuracy: {'epoch': 0, 'value': 0.13790000677108766} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 0, 'value': 2.2849407672882083} ({'split': 'test'})\n",
      "Epoch 001\n",
      "accuracy: {'epoch': 1, 'value': 0.15472000000476835} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 1, 'value': 2.225644811935424} ({'split': 'train'})\n",
      "accuracy: {'epoch': 1, 'value': 0.17520000785589218} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 1, 'value': 2.1764817476272587} ({'split': 'test'})\n",
      "Epoch 002\n",
      "accuracy: {'epoch': 2, 'value': 0.17878} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 2, 'value': 2.177207101211548} ({'split': 'train'})\n",
      "accuracy: {'epoch': 2, 'value': 0.14520000517368317} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 2, 'value': 2.2242746591567992} ({'split': 'test'})\n",
      "Epoch 003\n",
      "accuracy: {'epoch': 3, 'value': 0.18819999999999995} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 3, 'value': 2.157733343200685} ({'split': 'train'})\n",
      "accuracy: {'epoch': 3, 'value': 0.19510000795125962} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 3, 'value': 2.1311973571777347} ({'split': 'test'})\n",
      "Epoch 004\n",
      "accuracy: {'epoch': 4, 'value': 0.19988000001430514} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 4, 'value': 2.1335762017822266} ({'split': 'train'})\n",
      "accuracy: {'epoch': 4, 'value': 0.23710000663995742} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 4, 'value': 2.1020966291427614} ({'split': 'test'})\n",
      "Epoch 005\n",
      "accuracy: {'epoch': 5, 'value': 0.26843999999046314} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 5, 'value': 1.9527225225448601} ({'split': 'train'})\n",
      "accuracy: {'epoch': 5, 'value': 0.2962000131607056} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 5, 'value': 1.8772193312644958} ({'split': 'test'})\n",
      "Epoch 006\n",
      "accuracy: {'epoch': 6, 'value': 0.34600000002861026} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 6, 'value': 1.7726261877059932} ({'split': 'train'})\n",
      "accuracy: {'epoch': 6, 'value': 0.35850001871585846} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 6, 'value': 1.7468609094619751} ({'split': 'test'})\n",
      "Epoch 007\n",
      "accuracy: {'epoch': 7, 'value': 0.41312000001907334} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 7, 'value': 1.6203371538162235} ({'split': 'train'})\n",
      "accuracy: {'epoch': 7, 'value': 0.4130000174045563} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 7, 'value': 1.5844701528549194} ({'split': 'test'})\n",
      "Epoch 008\n",
      "accuracy: {'epoch': 8, 'value': 0.4617000000190735} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 8, 'value': 1.4817156115341175} ({'split': 'train'})\n",
      "accuracy: {'epoch': 8, 'value': 0.4564000219106674} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 8, 'value': 1.543138313293457} ({'split': 'test'})\n",
      "Epoch 009\n",
      "accuracy: {'epoch': 9, 'value': 0.50442} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 9, 'value': 1.377653155975342} ({'split': 'train'})\n",
      "accuracy: {'epoch': 9, 'value': 0.4794000267982483} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 9, 'value': 1.4491366624832152} ({'split': 'test'})\n",
      "Epoch 010\n",
      "accuracy: {'epoch': 10, 'value': 0.5445000000190737} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 10, 'value': 1.2736862883758537} ({'split': 'train'})\n",
      "accuracy: {'epoch': 10, 'value': 0.5296000242233277} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 10, 'value': 1.3029135465621948} ({'split': 'test'})\n",
      "Epoch 011\n",
      "accuracy: {'epoch': 11, 'value': 0.5940199999999998} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 11, 'value': 1.1452225461578371} ({'split': 'train'})\n",
      "accuracy: {'epoch': 11, 'value': 0.5837000250816344} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 11, 'value': 1.216512131690979} ({'split': 'test'})\n",
      "Epoch 012\n",
      "accuracy: {'epoch': 12, 'value': 0.6297999999999999} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 12, 'value': 1.0548535918998714} ({'split': 'train'})\n",
      "accuracy: {'epoch': 12, 'value': 0.6068000197410583} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 12, 'value': 1.1613952517509458} ({'split': 'test'})\n",
      "Epoch 013\n",
      "accuracy: {'epoch': 13, 'value': 0.6576799999999997} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 13, 'value': 0.9842030314254758} ({'split': 'train'})\n",
      "accuracy: {'epoch': 13, 'value': 0.6260000348091125} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 13, 'value': 1.0968897819519041} ({'split': 'test'})\n",
      "Epoch 014\n",
      "accuracy: {'epoch': 14, 'value': 0.6903200000381474} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 14, 'value': 0.8961407113075256} ({'split': 'train'})\n",
      "accuracy: {'epoch': 14, 'value': 0.6571000277996063} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 14, 'value': 1.0244101405143737} ({'split': 'test'})\n",
      "Epoch 015\n",
      "accuracy: {'epoch': 15, 'value': 0.710719999980926} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 15, 'value': 0.8309532616806034} ({'split': 'train'})\n",
      "accuracy: {'epoch': 15, 'value': 0.665900033712387} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 15, 'value': 0.9811078488826751} ({'split': 'test'})\n",
      "Epoch 016\n",
      "accuracy: {'epoch': 16, 'value': 0.7320799999809264} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 16, 'value': 0.7646875545883184} ({'split': 'train'})\n",
      "accuracy: {'epoch': 16, 'value': 0.6543000280857086} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 16, 'value': 1.0306976079940797} ({'split': 'test'})\n",
      "Epoch 017\n",
      "accuracy: {'epoch': 17, 'value': 0.7517400000572203} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 17, 'value': 0.7088768863296508} ({'split': 'train'})\n",
      "accuracy: {'epoch': 17, 'value': 0.6695000231266022} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 17, 'value': 1.0004787981510164} ({'split': 'test'})\n",
      "Epoch 018\n",
      "accuracy: {'epoch': 18, 'value': 0.7723400000190734} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 18, 'value': 0.6538137104415896} ({'split': 'train'})\n",
      "accuracy: {'epoch': 18, 'value': 0.6580000221729279} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 18, 'value': 1.0518869996070863} ({'split': 'test'})\n",
      "Epoch 019\n",
      "accuracy: {'epoch': 19, 'value': 0.7901400000572204} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 19, 'value': 0.6057251623916626} ({'split': 'train'})\n",
      "accuracy: {'epoch': 19, 'value': 0.6558000326156616} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 19, 'value': 1.0919278502464298} ({'split': 'test'})\n",
      "Epoch 020\n",
      "accuracy: {'epoch': 20, 'value': 0.8012200000190737} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 20, 'value': 0.5725121937179563} ({'split': 'train'})\n",
      "accuracy: {'epoch': 20, 'value': 0.6644000291824341} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 20, 'value': 1.097878086566925} ({'split': 'test'})\n",
      "Epoch 021\n",
      "accuracy: {'epoch': 21, 'value': 0.8162799999809264} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 21, 'value': 0.527584839038849} ({'split': 'train'})\n",
      "accuracy: {'epoch': 21, 'value': 0.6658000230789185} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 21, 'value': 1.1904912233352662} ({'split': 'test'})\n",
      "Epoch 022\n",
      "accuracy: {'epoch': 22, 'value': 0.8357199999809264} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 22, 'value': 0.4721909183692932} ({'split': 'train'})\n",
      "accuracy: {'epoch': 22, 'value': 0.6585000336170197} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 22, 'value': 1.1434064388275145} ({'split': 'test'})\n",
      "Epoch 023\n",
      "accuracy: {'epoch': 23, 'value': 0.8496199999809266} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 23, 'value': 0.43400502355575554} ({'split': 'train'})\n",
      "accuracy: {'epoch': 23, 'value': 0.6797000288963319} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 23, 'value': 1.1730770826339723} ({'split': 'test'})\n",
      "Epoch 024\n",
      "accuracy: {'epoch': 24, 'value': 0.8599600000572204} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 24, 'value': 0.39560793472290023} ({'split': 'train'})\n",
      "accuracy: {'epoch': 24, 'value': 0.6778000354766845} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 24, 'value': 1.2455737113952636} ({'split': 'test'})\n",
      "Epoch 025\n",
      "accuracy: {'epoch': 25, 'value': 0.8753999999809265} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 25, 'value': 0.3590661035919189} ({'split': 'train'})\n",
      "accuracy: {'epoch': 25, 'value': 0.6677000284194946} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 25, 'value': 1.161982250213623} ({'split': 'test'})\n",
      "Epoch 026\n",
      "accuracy: {'epoch': 26, 'value': 0.8746799999809267} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 26, 'value': 0.36110509237289423} ({'split': 'train'})\n",
      "accuracy: {'epoch': 26, 'value': 0.6655000388622284} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 26, 'value': 1.1476434350013736} ({'split': 'test'})\n",
      "Epoch 027\n",
      "accuracy: {'epoch': 27, 'value': 0.8879600000572204} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 27, 'value': 0.3260940931701657} ({'split': 'train'})\n",
      "accuracy: {'epoch': 27, 'value': 0.679000037908554} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 27, 'value': 1.265192902088165} ({'split': 'test'})\n",
      "Epoch 028\n",
      "accuracy: {'epoch': 28, 'value': 0.897420000038147} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 28, 'value': 0.30308445357322683} ({'split': 'train'})\n",
      "accuracy: {'epoch': 28, 'value': 0.6674000322818756} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 28, 'value': 1.3089237213134766} ({'split': 'test'})\n",
      "Epoch 029\n",
      "accuracy: {'epoch': 29, 'value': 0.900320000019074} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 29, 'value': 0.2890660067176817} ({'split': 'train'})\n",
      "accuracy: {'epoch': 29, 'value': 0.6696000397205353} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 29, 'value': 1.4018916964530947} ({'split': 'test'})\n",
      "mean_test_accuracy <cifar_utils.accumulators.Mean object at 0x7f781fe6ad90>\n",
      "QWERTY: Enter Cifar 2, acc 0.6797000288963319\n",
      "Retraining model :  naive_averaging\n",
      "num_epochs---------------+++))))))+++ 30\n",
      "num_epochs---------------+++))))))+++ 30\n",
      "lr is  0.05\n",
      "number of epochs would be  30\n",
      "num_epochs-------------- 30\n",
      "Epoch 000\n",
      "accuracy: {'epoch': 0, 'value': 0.3859799999809266} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 0, 'value': 1.7087645770645141} ({'split': 'train'})\n",
      "accuracy: {'epoch': 0, 'value': 0.5757000386714936} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 0, 'value': 1.2509217739105223} ({'split': 'test'})\n",
      "Epoch 001\n",
      "accuracy: {'epoch': 1, 'value': 0.7078600000190739} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 1, 'value': 0.8525390306663515} ({'split': 'train'})\n",
      "accuracy: {'epoch': 1, 'value': 0.7482000350952147} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 1, 'value': 0.7408135652542114} ({'split': 'test'})\n",
      "Epoch 002\n",
      "accuracy: {'epoch': 2, 'value': 0.8189800000000003} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 2, 'value': 0.543474534282685} ({'split': 'train'})\n",
      "accuracy: {'epoch': 2, 'value': 0.8107000410556793} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 2, 'value': 0.582606890797615} ({'split': 'test'})\n",
      "Epoch 003\n",
      "accuracy: {'epoch': 3, 'value': 0.8654600000572211} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 3, 'value': 0.4013099864673614} ({'split': 'train'})\n",
      "accuracy: {'epoch': 3, 'value': 0.826900041103363} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 3, 'value': 0.52923564016819} ({'split': 'test'})\n",
      "Epoch 004\n",
      "accuracy: {'epoch': 4, 'value': 0.8994800000381472} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 4, 'value': 0.29321992636680605} ({'split': 'train'})\n",
      "accuracy: {'epoch': 4, 'value': 0.8298000514507293} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 4, 'value': 0.5317889302968979} ({'split': 'test'})\n",
      "Epoch 005\n",
      "accuracy: {'epoch': 5, 'value': 0.9215999999809265} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 5, 'value': 0.23203376175880433} ({'split': 'train'})\n",
      "accuracy: {'epoch': 5, 'value': 0.8365000307559967} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 5, 'value': 0.541719850897789} ({'split': 'test'})\n",
      "Epoch 006\n",
      "accuracy: {'epoch': 6, 'value': 0.9322599999809266} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 6, 'value': 0.19764319697380064} ({'split': 'train'})\n",
      "accuracy: {'epoch': 6, 'value': 0.8265000343322754} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 6, 'value': 0.6193189680576324} ({'split': 'test'})\n",
      "Epoch 007\n",
      "accuracy: {'epoch': 7, 'value': 0.9550599999999996} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 7, 'value': 0.13433430566787716} ({'split': 'train'})\n",
      "accuracy: {'epoch': 7, 'value': 0.8320000350475312} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 7, 'value': 0.5915901482105255} ({'split': 'test'})\n",
      "Epoch 008\n",
      "accuracy: {'epoch': 8, 'value': 0.9600000000572202} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 8, 'value': 0.11816625933647161} ({'split': 'train'})\n",
      "accuracy: {'epoch': 8, 'value': 0.8433000385761261} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 8, 'value': 0.5868480741977691} ({'split': 'test'})\n",
      "Epoch 009\n",
      "accuracy: {'epoch': 9, 'value': 0.9653999999999996} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 9, 'value': 0.10385810595512396} ({'split': 'train'})\n",
      "accuracy: {'epoch': 9, 'value': 0.8445000410079956} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 9, 'value': 0.5868588626384734} ({'split': 'test'})\n",
      "Epoch 010\n",
      "accuracy: {'epoch': 10, 'value': 0.9674800000381474} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 10, 'value': 0.097879739626646} ({'split': 'train'})\n",
      "accuracy: {'epoch': 10, 'value': 0.8418000340461731} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 10, 'value': 0.6489918291568756} ({'split': 'test'})\n",
      "Epoch 011\n",
      "accuracy: {'epoch': 11, 'value': 0.9725400000572203} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 11, 'value': 0.0809720240783691} ({'split': 'train'})\n",
      "accuracy: {'epoch': 11, 'value': 0.8372000396251679} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 11, 'value': 0.6809798181056976} ({'split': 'test'})\n",
      "Epoch 012\n",
      "accuracy: {'epoch': 12, 'value': 0.9752000000190734} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 12, 'value': 0.07387118735909463} ({'split': 'train'})\n",
      "accuracy: {'epoch': 12, 'value': 0.8496000409126281} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 12, 'value': 0.6322039127349853} ({'split': 'test'})\n",
      "Epoch 013\n",
      "accuracy: {'epoch': 13, 'value': 0.9798399999809264} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 13, 'value': 0.05906145050525665} ({'split': 'train'})\n",
      "accuracy: {'epoch': 13, 'value': 0.837900048494339} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 13, 'value': 0.7128178238868713} ({'split': 'test'})\n",
      "Epoch 014\n",
      "accuracy: {'epoch': 14, 'value': 0.9796600000381469} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 14, 'value': 0.0624589782476425} ({'split': 'train'})\n",
      "accuracy: {'epoch': 14, 'value': 0.828000044822693} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 14, 'value': 0.787116003036499} ({'split': 'test'})\n",
      "Epoch 015\n",
      "accuracy: {'epoch': 15, 'value': 0.9798599999809267} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 15, 'value': 0.06098442039966585} ({'split': 'train'})\n",
      "accuracy: {'epoch': 15, 'value': 0.8263000369071961} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 15, 'value': 0.7093778312206268} ({'split': 'test'})\n",
      "Epoch 016\n",
      "accuracy: {'epoch': 16, 'value': 0.9827200000381473} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 16, 'value': 0.05384228311300278} ({'split': 'train'})\n",
      "accuracy: {'epoch': 16, 'value': 0.8431000351905822} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 16, 'value': 0.7085062265396117} ({'split': 'test'})\n",
      "Epoch 017\n",
      "accuracy: {'epoch': 17, 'value': 0.9826400000381471} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 17, 'value': 0.052008678718805296} ({'split': 'train'})\n",
      "accuracy: {'epoch': 17, 'value': 0.8406000435352325} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 17, 'value': 0.6932110726833344} ({'split': 'test'})\n",
      "Epoch 018\n",
      "accuracy: {'epoch': 18, 'value': 0.9855800000000003} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 18, 'value': 0.04383429485440254} ({'split': 'train'})\n",
      "accuracy: {'epoch': 18, 'value': 0.8456000447273254} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 18, 'value': 0.6871641993522644} ({'split': 'test'})\n",
      "Epoch 019\n",
      "accuracy: {'epoch': 19, 'value': 0.9845999999999999} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 19, 'value': 0.0493294049140811} ({'split': 'train'})\n",
      "accuracy: {'epoch': 19, 'value': 0.8468000411987303} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 19, 'value': 0.7289603650569916} ({'split': 'test'})\n",
      "Epoch 020\n",
      "accuracy: {'epoch': 20, 'value': 0.9898600000190735} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 20, 'value': 0.03115334043860436} ({'split': 'train'})\n",
      "accuracy: {'epoch': 20, 'value': 0.8397000312805175} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 20, 'value': 0.6973673045635224} ({'split': 'test'})\n",
      "Epoch 021\n",
      "accuracy: {'epoch': 21, 'value': 0.9892400000190736} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 21, 'value': 0.03490633457779888} ({'split': 'train'})\n",
      "accuracy: {'epoch': 21, 'value': 0.8447000503540039} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 21, 'value': 0.6827536880970001} ({'split': 'test'})\n",
      "Epoch 022\n",
      "accuracy: {'epoch': 22, 'value': 0.99} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 22, 'value': 0.032444743515253054} ({'split': 'train'})\n",
      "accuracy: {'epoch': 22, 'value': 0.8378000497817992} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 22, 'value': 0.7291625499725342} ({'split': 'test'})\n",
      "Epoch 023\n",
      "accuracy: {'epoch': 23, 'value': 0.9886000000000001} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 23, 'value': 0.03552789309263229} ({'split': 'train'})\n",
      "accuracy: {'epoch': 23, 'value': 0.8346000432968139} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 23, 'value': 0.7106355786323548} ({'split': 'test'})\n",
      "Epoch 024\n",
      "accuracy: {'epoch': 24, 'value': 0.98048} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 24, 'value': 0.05909871094346047} ({'split': 'train'})\n",
      "accuracy: {'epoch': 24, 'value': 0.8408000469207764} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 24, 'value': 0.7369950473308563} ({'split': 'test'})\n",
      "Epoch 025\n",
      "accuracy: {'epoch': 25, 'value': 0.9842400000190733} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 25, 'value': 0.04877843269586564} ({'split': 'train'})\n",
      "accuracy: {'epoch': 25, 'value': 0.8491000354290009} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 25, 'value': 0.6410262107849121} ({'split': 'test'})\n",
      "Epoch 026\n",
      "accuracy: {'epoch': 26, 'value': 0.9864600000190735} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 26, 'value': 0.04299305166244505} ({'split': 'train'})\n",
      "accuracy: {'epoch': 26, 'value': 0.8272000432014466} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 26, 'value': 0.7184829592704773} ({'split': 'test'})\n",
      "Epoch 027\n",
      "accuracy: {'epoch': 27, 'value': 0.9827000000190732} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 27, 'value': 0.05425914278984072} ({'split': 'train'})\n",
      "accuracy: {'epoch': 27, 'value': 0.8521000444889069} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 27, 'value': 0.6618095636367798} ({'split': 'test'})\n",
      "Epoch 028\n",
      "accuracy: {'epoch': 28, 'value': 0.9850800000381467} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 28, 'value': 0.0480750241494179} ({'split': 'train'})\n",
      "accuracy: {'epoch': 28, 'value': 0.8474000453948973} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 28, 'value': 0.6146647751331329} ({'split': 'test'})\n",
      "Epoch 029\n",
      "accuracy: {'epoch': 29, 'value': 0.9844000000190736} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 29, 'value': 0.04752551974177358} ({'split': 'train'})\n",
      "accuracy: {'epoch': 29, 'value': 0.8510000348091126} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 29, 'value': 0.6406845986843109} ({'split': 'test'})\n",
      "mean_test_accuracy <cifar_utils.accumulators.Mean object at 0x7f781fe33e80>\n",
      "QWERTY: Enter Cifar 2, acc 0.8521000444889069\n",
      "----- Saved results at sample.csv ------\n",
      "{'exp_name': 'exp_2023-09-17_12-49-42_642492', 'model0_acc': 89.96, 'model1_acc': 90.24, 'geometric_acc': 85.09, 'prediction_acc': 90.91, 'naive_acc': 14.49, 'geometric_gain': -5.1499999999999915, 'geometric_gain_%': -5.707003546099281, 'prediction_gain': 0.6700000000000017, 'prediction_gain_%': 0.7424645390070941, 'relative_loss_wrt_prediction': 6.449468085106375, 'geometric_time': 3.3407420106232166, 'retrain_geometric_best': 67.97000288963318, 'retrain_naive_best': 85.21000444889069, 'retrain_model0_best': 87.10000336170197, 'retrain_model1_best': 86.94000422954561, 'retrain_epochs': 30}\n",
      "FYI: the parameters were: \n",
      " Namespace(act_bug=False, act_num_samples=100, activation_histograms=False, activation_mode=None, activation_seed=21, activation_time=8.832663297653198e-06, alpha=0.7, baseroot='/root/autodl-tmp/OTfusion_pruning/otfusion', batch_size_test=1000, batch_size_train=256, center_acts=False, choice='0 2 4 6 8', cifar_style_data=False, ckpt_type='best', clip_gm=False, clip_max=5, clip_min=0, config={'dataset': 'Cifar10', 'model': 'vgg11_nobias', 'optimizer': 'SGD', 'optimizer_decay_at_epochs': [30, 60, 90, 120, 150, 180, 210, 240, 270], 'optimizer_decay_with_factor': 2.0, 'optimizer_learning_rate': 0.05, 'optimizer_momentum': 0.9, 'optimizer_weight_decay': 0.0005, 'batch_size': 128, 'num_epochs': 30, 'seed': 42, 'nick': 'naive_averaging', 'start_acc': 14.49}, config_dir='/root/autodl-tmp/OTfusion_pruning/otfusion/exp_sample/configurations', config_file=None, correction=True, csv_dir='/root/autodl-tmp/OTfusion_pruning/otfusion/exp_sample/csv', dataset='Cifar10', debug=False, deprecated=None, deterministic=False, diff_init=False, disable_bias=True, dist_epochs=60, dist_normalize=False, dump_final_models=False, dump_model=False, enable_dropout=False, ensemble_step=0.5, eval_aligned=False, exact=True, exp_name='exp_2023-09-17_12-49-42_642492', experiment_dir='experiment3', geom_ensemble_type='wts', geometric_time=3.3407420106232166, gpu_id=0, gromov=False, gromov_loss='square_loss', ground_metric='euclidean', ground_metric_eff=True, ground_metric_normalize='none', handle_skips=False, importance=None, learning_rate=0.0001, load_model_dir='./cifar_models', load_models='', log_interval=100, model_name='vgg11_nobias', momentum=0.5, n_epochs=10, no_random_trainloaders=False, normalize_acts=False, normalize_wts=False, not_squared=True, num_hidden_nodes=400, num_hidden_nodes1=400, num_hidden_nodes2=200, num_hidden_nodes3=100, num_hidden_nodes4=50, num_models=2, options_type='generic', params_geometric=9222848, params_model_0=9222848, params_model_1=9222848, partial_reshape=False, partition_dataloader=-1, partition_type='labels', past_correction=True, personal_class_idx=9, personal_split_frac=0.1, pool_acts=False, pool_relu=False, prediction_wts=False, prelu_acts=True, print_distances=False, proper_marginals=False, prune_times=3, prunint_rate=0.4, recheck_acc=False, recheck_cifar=True, reg=0.01, reg_m=0.001, reinit_trainloaders=False, result_dir='/root/autodl-tmp/OTfusion_pruning/otfusion/exp_sample/results', retrain=30, retrain_avg_only=False, retrain_geometric_best=0.6797000288963319, retrain_geometric_only=False, retrain_lr_decay=-1, retrain_lr_decay_epochs=None, retrain_lr_decay_factor=None, retrain_model0_best=0.8710000336170197, retrain_model1_best=0.8694000422954561, retrain_naive_best=0.8521000444889069, retrain_seed=-1, rootdir='/root/autodl-tmp/OTfusion_pruning/otfusion/exp_sample', run_mode='prune_times', same_model=-1, save_result_file='sample.csv', second_config={'dataset': 'Cifar10', 'model': 'vgg11_nobias', 'optimizer': 'SGD', 'optimizer_decay_at_epochs': [30, 60, 90, 120, 150, 180, 210, 240, 270], 'optimizer_decay_with_factor': 2.0, 'optimizer_learning_rate': 0.05, 'optimizer_momentum': 0.9, 'optimizer_weight_decay': 0.0005, 'batch_size': 128, 'num_epochs': 30, 'seed': 42, 'nick': 'naive_averaging', 'start_acc': 14.49}, second_model_name=None, sinkhorn_type='normal', skip_last_layer=False, skip_last_layer_type='average', skip_personal_idx=False, skip_retrain=-1, softmax_temperature=1, standardize_acts=False, sweep_id=90, sweep_name='exp_sample', temperature=20, tensorboard=False, tensorboard_root='./tensorboard', timestamp='2023-09-17_12-49-42_642492', tmap_stats=False, to_download=True, transform_acts=False, unbalanced=False, update_acts=False, weight_stats=True, width_ratio=1, **{'trace_sum_ratio_classifier.weight': 1.0, 'trace_sum_ratio_features.0.weight': 0.03125, 'trace_sum_ratio_features.11.weight': 0.001953125, 'trace_sum_ratio_features.13.weight': 0.001953125, 'trace_sum_ratio_features.16.weight': 0.00390625, 'trace_sum_ratio_features.18.weight': 0.0, 'trace_sum_ratio_features.3.weight': 0.0078125, 'trace_sum_ratio_features.6.weight': 0.0, 'trace_sum_ratio_features.8.weight': 0.00390625})\n"
     ]
    }
   ],
   "source": [
    "!python main.py --gpu-id 0 --model-name vgg11_nobias --n-epochs 10 --save-result-file sample.csv \\\n",
    "--sweep-name exp_sample --correction --ground-metric euclidean --weight-stats \\\n",
    "--geom-ensemble-type wts --ground-metric-normalize none --sweep-id 90 \\\n",
    "--ckpt-type best --dataset Cifar10 --ground-metric-eff --recheck-cifar --activation-seed {activation_seed} \\\n",
    "--prelu-acts --past-correction --not-squared --exact\\\n",
    "--learning-rate 0.0001\\\n",
    "--momentum 0.5\\\n",
    "--batch-size-train 256 --experiment-dir {experiment_dir}\\\n",
    "--prunint-rate {prunint_rate}\\\n",
    "--run-mode {run_mode}\\\n",
    "--to-download\\\n",
    "--load-model-dir {load_model_dir}\\\n",
    "--retrain 30\\\n",
    "--prune_times 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8744cf0e-060c-44e7-9fcc-e507860d22f6",
   "metadata": {},
   "source": [
    "# finetuen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6dd3f2b3-a28e-48de-9734-3c48c3d6514d",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------- Setting up parameters -------\n",
      "dumping parameters at  /root/autodl-tmp/OTfusion_pruning/otfusion/exp_sample/configurations\n",
      "The parameters are: \n",
      " Namespace(act_bug=False, act_num_samples=100, activation_histograms=False, activation_mode=None, activation_seed=21, alpha=0.7, baseroot='/root/autodl-tmp/OTfusion_pruning/otfusion', batch_size_test=1000, batch_size_train=256, center_acts=False, choice='0 2 4 6 8', cifar_style_data=False, ckpt_type='best', clip_gm=False, clip_max=5, clip_min=0, config_dir='/root/autodl-tmp/OTfusion_pruning/otfusion/exp_sample/configurations', config_file=None, correction=True, csv_dir='/root/autodl-tmp/OTfusion_pruning/otfusion/exp_sample/csv', dataset='Cifar10', debug=False, deprecated=None, deterministic=False, diff_init=False, disable_bias=True, dist_epochs=60, dist_normalize=False, dump_final_models=False, dump_model=False, enable_dropout=False, ensemble_step=0.5, eval_aligned=False, exact=True, exp_name='exp_2023-09-16_17-12-35_115737', experiment_dir='experiment2', geom_ensemble_type='wts', gpu_id=0, gromov=False, gromov_loss='square_loss', ground_metric='euclidean', ground_metric_eff=True, ground_metric_normalize='none', handle_skips=False, importance=None, learning_rate=0.0001, load_model_dir='./cifar_models', load_models='', log_interval=100, model_name='vgg11_nobias', momentum=0.5, n_epochs=10, no_random_trainloaders=False, normalize_acts=False, normalize_wts=False, not_squared=True, num_hidden_nodes=400, num_hidden_nodes1=400, num_hidden_nodes2=200, num_hidden_nodes3=100, num_hidden_nodes4=50, num_models=2, options_type='generic', partial_reshape=False, partition_dataloader=-1, partition_type='labels', past_correction=True, personal_class_idx=9, personal_split_frac=0.1, pool_acts=False, pool_relu=False, prediction_wts=False, prelu_acts=True, print_distances=False, proper_marginals=False, prunint_rate=0.2, recheck_acc=False, recheck_cifar=True, reg=0.01, reg_m=0.001, reinit_trainloaders=False, result_dir='/root/autodl-tmp/OTfusion_pruning/otfusion/exp_sample/results', retrain=30, retrain_avg_only=False, retrain_geometric_only=False, retrain_lr_decay=-1, retrain_lr_decay_epochs=None, retrain_lr_decay_factor=None, retrain_seed=-1, rootdir='/root/autodl-tmp/OTfusion_pruning/otfusion/exp_sample', run_mode='finetune', same_model=-1, save_result_file='sample.csv', second_model_name=None, sinkhorn_type='normal', skip_last_layer=False, skip_last_layer_type='average', skip_personal_idx=False, skip_retrain=-1, softmax_temperature=1, standardize_acts=False, sweep_id=90, sweep_name='exp_sample', temperature=20, tensorboard=False, tensorboard_root='./tensorboard', timestamp='2023-09-16_17-12-35_115737', tmap_stats=False, to_download=True, transform_acts=False, unbalanced=False, update_acts=False, weight_stats=True, width_ratio=1)\n",
      "refactored get_config\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "in _make_layers [Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), AvgPool2d(kernel_size=1, stride=1, padding=0)]\n",
      "Relu Inplace is  False\n",
      "model parameters are \n",
      " [torch.Size([64, 3, 3, 3]), torch.Size([128, 64, 3, 3]), torch.Size([256, 128, 3, 3]), torch.Size([256, 256, 3, 3]), torch.Size([512, 256, 3, 3]), torch.Size([512, 512, 3, 3]), torch.Size([512, 512, 3, 3]), torch.Size([512, 512, 3, 3]), torch.Size([10, 512])]\n",
      "in _make_layers [Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), AvgPool2d(kernel_size=1, stride=1, padding=0)]\n",
      "Relu Inplace is  False\n",
      "model parameters are \n",
      " [torch.Size([64, 3, 3, 3]), torch.Size([128, 64, 3, 3]), torch.Size([256, 128, 3, 3]), torch.Size([256, 256, 3, 3]), torch.Size([512, 256, 3, 3]), torch.Size([512, 512, 3, 3]), torch.Size([512, 512, 3, 3]), torch.Size([512, 512, 3, 3]), torch.Size([10, 512])]\n",
      "in _make_layers [Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), AvgPool2d(kernel_size=1, stride=1, padding=0)]\n",
      "Relu Inplace is  False\n",
      "model parameters are \n",
      " [torch.Size([64, 3, 3, 3]), torch.Size([128, 64, 3, 3]), torch.Size([256, 128, 3, 3]), torch.Size([256, 256, 3, 3]), torch.Size([512, 256, 3, 3]), torch.Size([512, 512, 3, 3]), torch.Size([512, 512, 3, 3]), torch.Size([512, 512, 3, 3]), torch.Size([10, 512])]\n",
      "in _make_layers [Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), AvgPool2d(kernel_size=1, stride=1, padding=0)]\n",
      "Relu Inplace is  False\n",
      "model parameters are \n",
      " [torch.Size([64, 3, 3, 3]), torch.Size([128, 64, 3, 3]), torch.Size([256, 128, 3, 3]), torch.Size([256, 256, 3, 3]), torch.Size([512, 256, 3, 3]), torch.Size([512, 512, 3, 3]), torch.Size([512, 512, 3, 3]), torch.Size([512, 512, 3, 3]), torch.Size([10, 512])]\n",
      "Train Epoch: 1 [0/50000 (0%)]\tLoss: 0.104261\n",
      "Train Epoch: 1 [25600/50000 (51%)]\tLoss: 0.096386\n",
      "\n",
      "--------- Testing in global mode ---------\n",
      "size of test_loader dataset:  10000\n",
      "\n",
      "Test set: Avg. loss: 0.0003, Accuracy: 9038/10000 (90%)\n",
      "\n",
      "Train Epoch: 2 [0/50000 (0%)]\tLoss: 0.073829\n",
      "Train Epoch: 2 [25600/50000 (51%)]\tLoss: 0.090221\n",
      "\n",
      "--------- Testing in global mode ---------\n",
      "size of test_loader dataset:  10000\n",
      "\n",
      "Test set: Avg. loss: 0.0003, Accuracy: 9043/10000 (90%)\n",
      "\n",
      "Train Epoch: 3 [0/50000 (0%)]\tLoss: 0.064929\n",
      "Train Epoch: 3 [25600/50000 (51%)]\tLoss: 0.059550\n",
      "\n",
      "--------- Testing in global mode ---------\n",
      "size of test_loader dataset:  10000\n",
      "\n",
      "Test set: Avg. loss: 0.0003, Accuracy: 9050/10000 (90%)\n",
      "\n",
      "Train Epoch: 4 [0/50000 (0%)]\tLoss: 0.071164\n",
      "Train Epoch: 4 [25600/50000 (51%)]\tLoss: 0.056062\n",
      "\n",
      "--------- Testing in global mode ---------\n",
      "size of test_loader dataset:  10000\n",
      "\n",
      "Test set: Avg. loss: 0.0003, Accuracy: 9048/10000 (90%)\n",
      "\n",
      "Train Epoch: 5 [0/50000 (0%)]\tLoss: 0.069778\n",
      "Train Epoch: 5 [25600/50000 (51%)]\tLoss: 0.068953\n",
      "\n",
      "--------- Testing in global mode ---------\n",
      "size of test_loader dataset:  10000\n",
      "\n",
      "Test set: Avg. loss: 0.0003, Accuracy: 9040/10000 (90%)\n",
      "\n",
      "Train Epoch: 6 [0/50000 (0%)]\tLoss: 0.064667\n",
      "Train Epoch: 6 [25600/50000 (51%)]\tLoss: 0.064324\n",
      "\n",
      "--------- Testing in global mode ---------\n",
      "size of test_loader dataset:  10000\n",
      "\n",
      "Test set: Avg. loss: 0.0003, Accuracy: 9040/10000 (90%)\n",
      "\n",
      "Train Epoch: 7 [0/50000 (0%)]\tLoss: 0.070104\n",
      "Train Epoch: 7 [25600/50000 (51%)]\tLoss: 0.074657\n",
      "\n",
      "--------- Testing in global mode ---------\n",
      "size of test_loader dataset:  10000\n",
      "\n",
      "Test set: Avg. loss: 0.0003, Accuracy: 9044/10000 (90%)\n",
      "\n",
      "Train Epoch: 8 [0/50000 (0%)]\tLoss: 0.056472\n",
      "Train Epoch: 8 [25600/50000 (51%)]\tLoss: 0.074212\n",
      "\n",
      "--------- Testing in global mode ---------\n",
      "size of test_loader dataset:  10000\n",
      "\n",
      "Test set: Avg. loss: 0.0003, Accuracy: 9047/10000 (90%)\n",
      "\n",
      "Train Epoch: 9 [0/50000 (0%)]\tLoss: 0.058916\n",
      "Train Epoch: 9 [25600/50000 (51%)]\tLoss: 0.066633\n",
      "\n",
      "--------- Testing in global mode ---------\n",
      "size of test_loader dataset:  10000\n",
      "\n",
      "Test set: Avg. loss: 0.0003, Accuracy: 9046/10000 (90%)\n",
      "\n",
      "Train Epoch: 10 [0/50000 (0%)]\tLoss: 0.062924\n",
      "Train Epoch: 10 [25600/50000 (51%)]\tLoss: 0.063471\n",
      "\n",
      "--------- Testing in global mode ---------\n",
      "size of test_loader dataset:  10000\n",
      "\n",
      "Test set: Avg. loss: 0.0003, Accuracy: 9044/10000 (90%)\n",
      "\n",
      "Train Epoch: 1 [0/50000 (0%)]\tLoss: 0.097751\n",
      "Train Epoch: 1 [25600/50000 (51%)]\tLoss: 0.080212\n",
      "\n",
      "--------- Testing in global mode ---------\n",
      "size of test_loader dataset:  10000\n",
      "\n",
      "Test set: Avg. loss: 0.0003, Accuracy: 9056/10000 (91%)\n",
      "\n",
      "Train Epoch: 2 [0/50000 (0%)]\tLoss: 0.072035\n",
      "Train Epoch: 2 [25600/50000 (51%)]\tLoss: 0.098448\n",
      "\n",
      "--------- Testing in global mode ---------\n",
      "size of test_loader dataset:  10000\n",
      "\n",
      "Test set: Avg. loss: 0.0003, Accuracy: 9065/10000 (91%)\n",
      "\n",
      "Train Epoch: 3 [0/50000 (0%)]\tLoss: 0.075295\n",
      "Train Epoch: 3 [25600/50000 (51%)]\tLoss: 0.076242\n",
      "\n",
      "--------- Testing in global mode ---------\n",
      "size of test_loader dataset:  10000\n",
      "\n",
      "Test set: Avg. loss: 0.0003, Accuracy: 9066/10000 (91%)\n",
      "\n",
      "Train Epoch: 4 [0/50000 (0%)]\tLoss: 0.089911\n",
      "Train Epoch: 4 [25600/50000 (51%)]\tLoss: 0.069176\n",
      "\n",
      "--------- Testing in global mode ---------\n",
      "size of test_loader dataset:  10000\n",
      "\n",
      "Test set: Avg. loss: 0.0003, Accuracy: 9068/10000 (91%)\n",
      "\n",
      "Train Epoch: 5 [0/50000 (0%)]\tLoss: 0.066074\n",
      "Train Epoch: 5 [25600/50000 (51%)]\tLoss: 0.093553\n",
      "\n",
      "--------- Testing in global mode ---------\n",
      "size of test_loader dataset:  10000\n",
      "\n",
      "Test set: Avg. loss: 0.0003, Accuracy: 9067/10000 (91%)\n",
      "\n",
      "Train Epoch: 6 [0/50000 (0%)]\tLoss: 0.065930\n",
      "Train Epoch: 6 [25600/50000 (51%)]\tLoss: 0.063445\n",
      "\n",
      "--------- Testing in global mode ---------\n",
      "size of test_loader dataset:  10000\n",
      "\n",
      "Test set: Avg. loss: 0.0003, Accuracy: 9068/10000 (91%)\n",
      "\n",
      "Train Epoch: 7 [0/50000 (0%)]\tLoss: 0.067796\n",
      "Train Epoch: 7 [25600/50000 (51%)]\tLoss: 0.056102\n",
      "\n",
      "--------- Testing in global mode ---------\n",
      "size of test_loader dataset:  10000\n",
      "\n",
      "Test set: Avg. loss: 0.0003, Accuracy: 9064/10000 (91%)\n",
      "\n",
      "Train Epoch: 8 [0/50000 (0%)]\tLoss: 0.053087\n",
      "Train Epoch: 8 [25600/50000 (51%)]\tLoss: 0.066485\n",
      "\n",
      "--------- Testing in global mode ---------\n",
      "size of test_loader dataset:  10000\n",
      "\n",
      "Test set: Avg. loss: 0.0003, Accuracy: 9067/10000 (91%)\n",
      "\n",
      "Train Epoch: 9 [0/50000 (0%)]\tLoss: 0.061531\n",
      "Train Epoch: 9 [25600/50000 (51%)]\tLoss: 0.062868\n",
      "\n",
      "--------- Testing in global mode ---------\n",
      "size of test_loader dataset:  10000\n",
      "\n",
      "Test set: Avg. loss: 0.0003, Accuracy: 9067/10000 (91%)\n",
      "\n",
      "Train Epoch: 10 [0/50000 (0%)]\tLoss: 0.057892\n",
      "Train Epoch: 10 [25600/50000 (51%)]\tLoss: 0.056442\n",
      "\n",
      "--------- Testing in global mode ---------\n",
      "size of test_loader dataset:  10000\n",
      "\n",
      "Test set: Avg. loss: 0.0003, Accuracy: 9058/10000 (91%)\n",
      "\n",
      "========================================================\n",
      "accuracies [90.44, 90.58]\n",
      "=========================================================\n",
      "layer features.0.weight_orig has #params  1728\n",
      "layer features.3.weight_orig has #params  73728\n",
      "layer features.6.weight_orig has #params  294912\n",
      "layer features.8.weight_orig has #params  589824\n",
      "layer features.11.weight_orig has #params  1179648\n",
      "layer features.13.weight_orig has #params  2359296\n",
      "layer features.16.weight_orig has #params  2359296\n",
      "layer features.18.weight_orig has #params  2359296\n",
      "layer classifier.weight has #params  5120\n",
      "Activation Timer start\n",
      "Activation Timer ends\n",
      "------- Geometric Ensembling -------\n",
      "Timer start\n",
      "Previous layer shape is  None\n",
      "Processing the coordinates to form ground_metric\n",
      "dont leave off the squaring of the ground metric\n",
      "returns a uniform measure of cardinality:  64\n",
      "returns a uniform measure of cardinality:  64\n",
      "the transport map is  tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')\n",
      "Ratio of trace to the matrix sum:  tensor(0.0469, device='cuda:0')\n",
      "Here, trace is 3.0 and matrix sum is 64.0 \n",
      "this is past correction for weight mode\n",
      "Shape of aligned wt is  torch.Size([64, 3, 9])\n",
      "Shape of fc_layer0_weight_data is  torch.Size([64, 3, 9])\n",
      "Previous layer shape is  torch.Size([64, 3, 3, 3])\n",
      "shape of layer: model 0 torch.Size([128, 64, 9])\n",
      "shape of layer: model 1 torch.Size([128, 64, 9])\n",
      "shape of previous transport map torch.Size([64, 64])\n",
      "Processing the coordinates to form ground_metric\n",
      "dont leave off the squaring of the ground metric\n",
      "returns a uniform measure of cardinality:  128\n",
      "returns a uniform measure of cardinality:  128\n",
      "the transport map is  tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        ...,\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0078],\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],\n",
      "       device='cuda:0')\n",
      "Ratio of trace to the matrix sum:  tensor(0.0078, device='cuda:0')\n",
      "Here, trace is 1.0 and matrix sum is 128.0 \n",
      "this is past correction for weight mode\n",
      "Shape of aligned wt is  torch.Size([128, 64, 9])\n",
      "Shape of fc_layer0_weight_data is  torch.Size([128, 64, 9])\n",
      "Previous layer shape is  torch.Size([128, 64, 3, 3])\n",
      "shape of layer: model 0 torch.Size([256, 128, 9])\n",
      "shape of layer: model 1 torch.Size([256, 128, 9])\n",
      "shape of previous transport map torch.Size([128, 128])\n",
      "Processing the coordinates to form ground_metric\n",
      "dont leave off the squaring of the ground metric\n",
      "returns a uniform measure of cardinality:  256\n",
      "returns a uniform measure of cardinality:  256\n",
      "the transport map is  tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')\n",
      "Ratio of trace to the matrix sum:  tensor(0., device='cuda:0')\n",
      "Here, trace is 0.0 and matrix sum is 256.0 \n",
      "this is past correction for weight mode\n",
      "Shape of aligned wt is  torch.Size([256, 128, 9])\n",
      "Shape of fc_layer0_weight_data is  torch.Size([256, 128, 9])\n",
      "Previous layer shape is  torch.Size([256, 128, 3, 3])\n",
      "shape of layer: model 0 torch.Size([256, 256, 9])\n",
      "shape of layer: model 1 torch.Size([256, 256, 9])\n",
      "shape of previous transport map torch.Size([256, 256])\n",
      "Processing the coordinates to form ground_metric\n",
      "dont leave off the squaring of the ground metric\n",
      "returns a uniform measure of cardinality:  256\n",
      "returns a uniform measure of cardinality:  256\n",
      "the transport map is  tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        ...,\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.0039, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],\n",
      "       device='cuda:0')\n",
      "Ratio of trace to the matrix sum:  tensor(0.0078, device='cuda:0')\n",
      "Here, trace is 2.0 and matrix sum is 256.0 \n",
      "this is past correction for weight mode\n",
      "Shape of aligned wt is  torch.Size([256, 256, 9])\n",
      "Shape of fc_layer0_weight_data is  torch.Size([256, 256, 9])\n",
      "Previous layer shape is  torch.Size([256, 256, 3, 3])\n",
      "shape of layer: model 0 torch.Size([512, 256, 9])\n",
      "shape of layer: model 1 torch.Size([512, 256, 9])\n",
      "shape of previous transport map torch.Size([256, 256])\n",
      "Processing the coordinates to form ground_metric\n",
      "dont leave off the squaring of the ground metric\n",
      "returns a uniform measure of cardinality:  512\n",
      "returns a uniform measure of cardinality:  512\n",
      "the transport map is  tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')\n",
      "Ratio of trace to the matrix sum:  tensor(0.0039, device='cuda:0')\n",
      "Here, trace is 2.0 and matrix sum is 512.0 \n",
      "this is past correction for weight mode\n",
      "Shape of aligned wt is  torch.Size([512, 256, 9])\n",
      "Shape of fc_layer0_weight_data is  torch.Size([512, 256, 9])\n",
      "Previous layer shape is  torch.Size([512, 256, 3, 3])\n",
      "shape of layer: model 0 torch.Size([512, 512, 9])\n",
      "shape of layer: model 1 torch.Size([512, 512, 9])\n",
      "shape of previous transport map torch.Size([512, 512])\n",
      "Processing the coordinates to form ground_metric\n",
      "dont leave off the squaring of the ground metric\n",
      "returns a uniform measure of cardinality:  512\n",
      "returns a uniform measure of cardinality:  512\n",
      "the transport map is  tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')\n",
      "Ratio of trace to the matrix sum:  tensor(0.0020, device='cuda:0')\n",
      "Here, trace is 1.0 and matrix sum is 512.0 \n",
      "this is past correction for weight mode\n",
      "Shape of aligned wt is  torch.Size([512, 512, 9])\n",
      "Shape of fc_layer0_weight_data is  torch.Size([512, 512, 9])\n",
      "Previous layer shape is  torch.Size([512, 512, 3, 3])\n",
      "shape of layer: model 0 torch.Size([512, 512, 9])\n",
      "shape of layer: model 1 torch.Size([512, 512, 9])\n",
      "shape of previous transport map torch.Size([512, 512])\n",
      "Processing the coordinates to form ground_metric\n",
      "dont leave off the squaring of the ground metric\n",
      "returns a uniform measure of cardinality:  512\n",
      "returns a uniform measure of cardinality:  512\n",
      "the transport map is  tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')\n",
      "Ratio of trace to the matrix sum:  tensor(0.0020, device='cuda:0')\n",
      "Here, trace is 1.0 and matrix sum is 512.0 \n",
      "this is past correction for weight mode\n",
      "Shape of aligned wt is  torch.Size([512, 512, 9])\n",
      "Shape of fc_layer0_weight_data is  torch.Size([512, 512, 9])\n",
      "Previous layer shape is  torch.Size([512, 512, 3, 3])\n",
      "shape of layer: model 0 torch.Size([512, 512, 9])\n",
      "shape of layer: model 1 torch.Size([512, 512, 9])\n",
      "shape of previous transport map torch.Size([512, 512])\n",
      "Processing the coordinates to form ground_metric\n",
      "dont leave off the squaring of the ground metric\n",
      "returns a uniform measure of cardinality:  512\n",
      "returns a uniform measure of cardinality:  512\n",
      "the transport map is  tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        [0.0020, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        ...,\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],\n",
      "       device='cuda:0')\n",
      "Ratio of trace to the matrix sum:  tensor(0., device='cuda:0')\n",
      "Here, trace is 0.0 and matrix sum is 512.0 \n",
      "this is past correction for weight mode\n",
      "Shape of aligned wt is  torch.Size([512, 512, 9])\n",
      "Shape of fc_layer0_weight_data is  torch.Size([512, 512, 9])\n",
      "Previous layer shape is  torch.Size([512, 512, 3, 3])\n",
      "shape of layer: model 0 torch.Size([10, 512])\n",
      "shape of layer: model 1 torch.Size([10, 512])\n",
      "shape of previous transport map torch.Size([512, 512])\n",
      "Processing the coordinates to form ground_metric\n",
      "dont leave off the squaring of the ground metric\n",
      "ground metric is  tensor([[1.4271, 2.4927, 2.7867, 2.8786, 2.7436, 2.8868, 2.7326, 2.6399, 2.5700,\n",
      "         2.5976],\n",
      "        [2.4939, 1.3272, 2.6784, 2.5964, 2.7632, 2.5546, 2.4204, 2.5255, 2.3095,\n",
      "         2.4138],\n",
      "        [2.7656, 2.6762, 1.3861, 2.9526, 2.9123, 2.8761, 2.7322, 2.8437, 2.7544,\n",
      "         2.7917],\n",
      "        [2.8411, 2.6845, 2.8984, 1.5462, 2.8664, 2.9223, 2.7624, 2.8035, 2.8226,\n",
      "         2.6405],\n",
      "        [2.7609, 2.6010, 2.8692, 2.8510, 1.5072, 2.7504, 2.7899, 2.6860, 2.7091,\n",
      "         2.7345],\n",
      "        [2.9250, 2.5443, 2.9528, 2.8477, 2.7327, 1.5533, 2.7827, 2.7352, 2.8038,\n",
      "         2.6747],\n",
      "        [2.6826, 2.4488, 2.7238, 2.9161, 2.6529, 2.8572, 1.3947, 2.6929, 2.5024,\n",
      "         2.5595],\n",
      "        [2.7443, 2.5535, 2.7852, 2.7956, 2.7201, 2.6639, 2.7226, 1.3946, 2.5633,\n",
      "         2.5158],\n",
      "        [2.6031, 2.3702, 2.7753, 2.6925, 2.7078, 2.8341, 2.5361, 2.6413, 1.3028,\n",
      "         2.4083],\n",
      "        [2.5967, 2.2764, 2.7867, 2.7625, 2.6741, 2.7321, 2.5512, 2.5326, 2.5255,\n",
      "         1.2897]], device='cuda:0', grad_fn=<PowBackward0>)\n",
      "returns a uniform measure of cardinality:  10\n",
      "returns a uniform measure of cardinality:  10\n",
      "the transport map is  tensor([[0.1000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0000, 0.1000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0000, 0.0000, 0.1000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.1000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.1000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.1000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.1000, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.1000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.1000,\n",
      "         0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.1000]], device='cuda:0')\n",
      "Ratio of trace to the matrix sum:  tensor(1., device='cuda:0')\n",
      "Here, trace is 9.99755859375 and matrix sum is 9.99755859375 \n",
      "this is past correction for weight mode\n",
      "Shape of aligned wt is  torch.Size([10, 512])\n",
      "Shape of fc_layer0_weight_data is  torch.Size([10, 512])\n",
      "using independent method\n",
      "in _make_layers [Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), AvgPool2d(kernel_size=1, stride=1, padding=0)]\n",
      "Relu Inplace is  False\n",
      "model parameters are \n",
      " [torch.Size([64, 3, 3, 3]), torch.Size([128, 64, 3, 3]), torch.Size([256, 128, 3, 3]), torch.Size([256, 256, 3, 3]), torch.Size([512, 256, 3, 3]), torch.Size([512, 512, 3, 3]), torch.Size([512, 512, 3, 3]), torch.Size([512, 512, 3, 3]), torch.Size([10, 512])]\n",
      "\n",
      "--------- Testing in global mode ---------\n",
      "size of test_loader dataset:  10000\n",
      "\n",
      "Test set: Avg. loss: 0.0023, Accuracy: 776/10000 (8%)\n",
      "\n",
      "len of model parameters and avg aligned layers is  9 9\n",
      "len of model_state_dict is  9\n",
      "len of param_list is  9\n",
      "\n",
      "--------- Testing in global mode ---------\n",
      "size of test_loader dataset:  10000\n",
      "\n",
      "Test set: Avg. loss: 0.0014, Accuracy: 8693/10000 (87%)\n",
      "\n",
      "===============================================\n",
      "===============================================\n",
      "After Ensemble: 86.93\n",
      "===============================================\n",
      "===============================================\n",
      "Timer ends\n",
      "Time taken for geometric ensembling is 4.122280936688185 seconds\n",
      "------- Prediction based ensembling -------\n",
      "\n",
      "Test set: Avg. loss: 0.0003, Accuracy: 9125/10000 (91%)\n",
      "\n",
      "------- Naive ensembling of weights -------\n",
      "[torch.Size([64, 3, 3, 3]), torch.Size([64, 3, 3, 3])]\n",
      "torch.Size([64, 3, 3, 3])\n",
      "[torch.Size([128, 64, 3, 3]), torch.Size([128, 64, 3, 3])]\n",
      "torch.Size([128, 64, 3, 3])\n",
      "[torch.Size([256, 128, 3, 3]), torch.Size([256, 128, 3, 3])]\n",
      "torch.Size([256, 128, 3, 3])\n",
      "[torch.Size([256, 256, 3, 3]), torch.Size([256, 256, 3, 3])]\n",
      "torch.Size([256, 256, 3, 3])\n",
      "[torch.Size([512, 256, 3, 3]), torch.Size([512, 256, 3, 3])]\n",
      "torch.Size([512, 256, 3, 3])\n",
      "[torch.Size([512, 512, 3, 3]), torch.Size([512, 512, 3, 3])]\n",
      "torch.Size([512, 512, 3, 3])\n",
      "[torch.Size([512, 512, 3, 3]), torch.Size([512, 512, 3, 3])]\n",
      "torch.Size([512, 512, 3, 3])\n",
      "[torch.Size([512, 512, 3, 3]), torch.Size([512, 512, 3, 3])]\n",
      "torch.Size([512, 512, 3, 3])\n",
      "[torch.Size([10, 512]), torch.Size([10, 512])]\n",
      "torch.Size([10, 512])\n",
      "in _make_layers [Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), AvgPool2d(kernel_size=1, stride=1, padding=0)]\n",
      "Relu Inplace is  False\n",
      "model parameters are \n",
      " [torch.Size([64, 3, 3, 3]), torch.Size([128, 64, 3, 3]), torch.Size([256, 128, 3, 3]), torch.Size([256, 256, 3, 3]), torch.Size([512, 256, 3, 3]), torch.Size([512, 512, 3, 3]), torch.Size([512, 512, 3, 3]), torch.Size([512, 512, 3, 3]), torch.Size([10, 512])]\n",
      "\n",
      "--------- Testing in global mode ---------\n",
      "size of test_loader dataset:  10000\n",
      "\n",
      "Test set: Avg. loss: 0.0023, Accuracy: 1066/10000 (11%)\n",
      "\n",
      "\n",
      "--------- Testing in global mode ---------\n",
      "size of test_loader dataset:  10000\n",
      "\n",
      "Test set: Avg. loss: 0.0023, Accuracy: 1596/10000 (16%)\n",
      "\n",
      "-------- Retraining the models ---------\n",
      "Retraining model :  model_0\n",
      "num_epochs---------------+++))))))+++ 300\n",
      "num_epochs---------------+++))))))+++ 300\n",
      "lr is  0.05\n",
      "number of epochs would be  30\n",
      "num_epochs-------------- 30\n",
      "Epoch 000\n",
      "/root/miniconda3/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:129: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\"Detected call of `lr_scheduler.step()` before `optimizer.step()`. \"\n",
      "/root/miniconda3/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:154: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "accuracy: {'epoch': 0, 'value': 0.9122000000190741} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 0, 'value': 0.2636602558898925} ({'split': 'train'})\n",
      "accuracy: {'epoch': 0, 'value': 0.8344000399112701} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 0, 'value': 0.5101864248514175} ({'split': 'test'})\n",
      "Epoch 001\n",
      "accuracy: {'epoch': 1, 'value': 0.9403200000190738} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 1, 'value': 0.17580926229715352} ({'split': 'train'})\n",
      "accuracy: {'epoch': 1, 'value': 0.8468000352382661} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 1, 'value': 0.5193602144718171} ({'split': 'test'})\n",
      "Epoch 002\n",
      "accuracy: {'epoch': 2, 'value': 0.9563600000381468} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 2, 'value': 0.13225128568172462} ({'split': 'train'})\n",
      "accuracy: {'epoch': 2, 'value': 0.8551000356674194} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 2, 'value': 0.5112047284841538} ({'split': 'test'})\n",
      "Epoch 003\n",
      "accuracy: {'epoch': 3, 'value': 0.9671600000572209} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 3, 'value': 0.10211080060482026} ({'split': 'train'})\n",
      "accuracy: {'epoch': 3, 'value': 0.8414000451564789} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 3, 'value': 0.5935918271541595} ({'split': 'test'})\n",
      "Epoch 004\n",
      "accuracy: {'epoch': 4, 'value': 0.9735000000572205} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 4, 'value': 0.07740002972126014} ({'split': 'train'})\n",
      "accuracy: {'epoch': 4, 'value': 0.8538000345230102} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 4, 'value': 0.576252418756485} ({'split': 'test'})\n",
      "Epoch 005\n",
      "accuracy: {'epoch': 5, 'value': 0.9776800000572204} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 5, 'value': 0.06936297762870791} ({'split': 'train'})\n",
      "accuracy: {'epoch': 5, 'value': 0.8592000424861909} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 5, 'value': 0.5628400206565856} ({'split': 'test'})\n",
      "Epoch 006\n",
      "accuracy: {'epoch': 6, 'value': 0.9816400000190737} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 6, 'value': 0.05853538508176803} ({'split': 'train'})\n",
      "accuracy: {'epoch': 6, 'value': 0.866800045967102} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 6, 'value': 0.5465849697589873} ({'split': 'test'})\n",
      "Epoch 007\n",
      "accuracy: {'epoch': 7, 'value': 0.984740000019074} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 7, 'value': 0.04710702549099921} ({'split': 'train'})\n",
      "accuracy: {'epoch': 7, 'value': 0.8397000372409821} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 7, 'value': 0.6637592494487763} ({'split': 'test'})\n",
      "Epoch 008\n",
      "accuracy: {'epoch': 8, 'value': 0.9859000000572207} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 8, 'value': 0.043933117284774746} ({'split': 'train'})\n",
      "accuracy: {'epoch': 8, 'value': 0.8613000571727752} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 8, 'value': 0.5364737719297409} ({'split': 'test'})\n",
      "Epoch 009\n",
      "accuracy: {'epoch': 9, 'value': 0.985540000038147} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 9, 'value': 0.04429035544872284} ({'split': 'train'})\n",
      "accuracy: {'epoch': 9, 'value': 0.84560005068779} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 9, 'value': 0.6203998327255248} ({'split': 'test'})\n",
      "Epoch 010\n",
      "accuracy: {'epoch': 10, 'value': 0.980520000019073} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 10, 'value': 0.062353112208843216} ({'split': 'train'})\n",
      "accuracy: {'epoch': 10, 'value': 0.8550000369548798} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 10, 'value': 0.5902862787246704} ({'split': 'test'})\n",
      "Epoch 011\n",
      "accuracy: {'epoch': 11, 'value': 0.9872200000381468} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 11, 'value': 0.042148954392671585} ({'split': 'train'})\n",
      "accuracy: {'epoch': 11, 'value': 0.8621000409126282} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 11, 'value': 0.6437871098518372} ({'split': 'test'})\n",
      "Epoch 012\n",
      "accuracy: {'epoch': 12, 'value': 0.9894400000190738} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 12, 'value': 0.03598721664547921} ({'split': 'train'})\n",
      "accuracy: {'epoch': 12, 'value': 0.8698000431060791} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 12, 'value': 0.5256512731313705} ({'split': 'test'})\n",
      "Epoch 013\n",
      "accuracy: {'epoch': 13, 'value': 0.990660000038147} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 13, 'value': 0.030652406332492835} ({'split': 'train'})\n",
      "accuracy: {'epoch': 13, 'value': 0.8585000395774841} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 13, 'value': 0.5822527706623077} ({'split': 'test'})\n",
      "Epoch 014\n",
      "accuracy: {'epoch': 14, 'value': 0.9904200000000004} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 14, 'value': 0.031495134933590886} ({'split': 'train'})\n",
      "accuracy: {'epoch': 14, 'value': 0.8659000396728516} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 14, 'value': 0.5479746639728547} ({'split': 'test'})\n",
      "Epoch 015\n",
      "accuracy: {'epoch': 15, 'value': 0.9911200000381467} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 15, 'value': 0.02920728697061539} ({'split': 'train'})\n",
      "accuracy: {'epoch': 15, 'value': 0.8625000357627868} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 15, 'value': 0.5630833506584167} ({'split': 'test'})\n",
      "Epoch 016\n",
      "accuracy: {'epoch': 16, 'value': 0.9894400000190737} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 16, 'value': 0.0346684301567078} ({'split': 'train'})\n",
      "accuracy: {'epoch': 16, 'value': 0.8657000362873077} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 16, 'value': 0.5487367331981658} ({'split': 'test'})\n",
      "Epoch 017\n",
      "accuracy: {'epoch': 17, 'value': 0.9887399999999998} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 17, 'value': 0.03685286352962254} ({'split': 'train'})\n",
      "accuracy: {'epoch': 17, 'value': 0.8545000314712525} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 17, 'value': 0.6119182616472244} ({'split': 'test'})\n",
      "Epoch 018\n",
      "accuracy: {'epoch': 18, 'value': 0.9890800000190734} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 18, 'value': 0.035313584553003305} ({'split': 'train'})\n",
      "accuracy: {'epoch': 18, 'value': 0.8621000349521637} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 18, 'value': 0.555291822552681} ({'split': 'test'})\n",
      "Epoch 019\n",
      "accuracy: {'epoch': 19, 'value': 0.9899600000190744} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 19, 'value': 0.03316129726171496} ({'split': 'train'})\n",
      "accuracy: {'epoch': 19, 'value': 0.8682000398635864} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 19, 'value': 0.5559415608644485} ({'split': 'test'})\n",
      "Epoch 020\n",
      "accuracy: {'epoch': 20, 'value': 0.9898600000190733} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 20, 'value': 0.03351216415524483} ({'split': 'train'})\n",
      "accuracy: {'epoch': 20, 'value': 0.8601000487804413} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 20, 'value': 0.5782888352870942} ({'split': 'test'})\n",
      "Epoch 021\n",
      "accuracy: {'epoch': 21, 'value': 0.9883600000381472} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 21, 'value': 0.03587706937551499} ({'split': 'train'})\n",
      "accuracy: {'epoch': 21, 'value': 0.8587000370025635} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 21, 'value': 0.6062435984611511} ({'split': 'test'})\n",
      "Epoch 022\n",
      "accuracy: {'epoch': 22, 'value': 0.9858000000381465} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 22, 'value': 0.04532035360813142} ({'split': 'train'})\n",
      "accuracy: {'epoch': 22, 'value': 0.8450000405311585} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 22, 'value': 0.6278650104999542} ({'split': 'test'})\n",
      "Epoch 023\n",
      "accuracy: {'epoch': 23, 'value': 0.9867000000190733} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 23, 'value': 0.04363761785984038} ({'split': 'train'})\n",
      "accuracy: {'epoch': 23, 'value': 0.847100043296814} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 23, 'value': 0.6292963147163391} ({'split': 'test'})\n",
      "Epoch 024\n",
      "accuracy: {'epoch': 24, 'value': 0.9867600000190734} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 24, 'value': 0.042320175387263334} ({'split': 'train'})\n",
      "accuracy: {'epoch': 24, 'value': 0.8553000450134276} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 24, 'value': 0.630251544713974} ({'split': 'test'})\n",
      "Epoch 025\n",
      "^C\n",
      "Traceback (most recent call last):\n",
      "  File \"main.py\", line 351, in <module>\n",
      "    _, best_retrain_acc = routines.retrain_models(args, [*original_models, geometric_model, naive_model], retrain_loader, test_loader, config, tensorboard_obj=tensorboard_obj, initial_acc=initial_acc, nicks=nicks)\n",
      "  File \"/root/autodl-tmp/OTfusion_pruning/otfusion/routines.py\", line 394, in retrain_models\n",
      "    retrained_network, acc = cifar_train.get_retrained_model(args, retrain_loader, test_loader, old_networks[i], config, output_root_dir, tensorboard_obj=tensorboard_obj, nick=nick, start_acc=initial_acc[i])\n",
      "  File \"/root/autodl-tmp/OTfusion_pruning/otfusion/./cifar/train.py\", line 333, in get_retrained_model\n",
      "    best_acc,model = main(config, output_dir, args.gpu_id, pretrained_model=old_network, pretrained_dataset=(train_loader, test_loader), tensorboard_obj=tensorboard_obj,return_model=True)\n",
      "  File \"/root/autodl-tmp/OTfusion_pruning/otfusion/./cifar/train.py\", line 67, in main\n",
      "    for batch_x, batch_y in training_loader:\n",
      "  File \"/root/miniconda3/lib/python3.8/site-packages/torch/utils/data/dataloader.py\", line 521, in __next__\n",
      "    data = self._next_data()\n",
      "  File \"/root/miniconda3/lib/python3.8/site-packages/torch/utils/data/dataloader.py\", line 561, in _next_data\n",
      "    data = self._dataset_fetcher.fetch(index)  # may raise StopIteration\n",
      "  File \"/root/miniconda3/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py\", line 49, in fetch\n",
      "    data = [self.dataset[idx] for idx in possibly_batched_index]\n",
      "  File \"/root/miniconda3/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py\", line 49, in <listcomp>\n",
      "    data = [self.dataset[idx] for idx in possibly_batched_index]\n",
      "  File \"/root/miniconda3/lib/python3.8/site-packages/torchvision/datasets/cifar.py\", line 121, in __getitem__\n",
      "    img = self.transform(img)\n",
      "  File \"/root/miniconda3/lib/python3.8/site-packages/torchvision/transforms/transforms.py\", line 61, in __call__\n",
      "    img = t(img)\n",
      "  File \"/root/miniconda3/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1102, in _call_impl\n",
      "    return forward_call(*input, **kwargs)\n",
      "  File \"/root/miniconda3/lib/python3.8/site-packages/torchvision/transforms/transforms.py\", line 226, in forward\n",
      "    return F.normalize(tensor, self.mean, self.std, self.inplace)\n",
      "  File \"/root/miniconda3/lib/python3.8/site-packages/torchvision/transforms/functional.py\", line 351, in normalize\n",
      "    tensor.sub_(mean).div_(std)\n",
      "KeyboardInterrupt\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m~/miniconda3/lib/python3.8/site-packages/IPython/utils/_process_posix.py\u001b[0m in \u001b[0;36msystem\u001b[0;34m(self, cmd)\u001b[0m\n\u001b[1;32m    161\u001b[0m                 \u001b[0;31m# know whether we've finished (if we matched EOF) or not\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 162\u001b[0;31m                 \u001b[0mres_idx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mchild\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexpect_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpatterns\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_timeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    163\u001b[0m                 \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchild\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbefore\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mout_size\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'replace'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m''\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.8/site-packages/pexpect/spawnbase.py\u001b[0m in \u001b[0;36mexpect_list\u001b[0;34m(self, pattern_list, timeout, searchwindowsize, async_, **kw)\u001b[0m\n\u001b[1;32m    371\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 372\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mexp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexpect_loop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    373\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.8/site-packages/pexpect/expect.py\u001b[0m in \u001b[0;36mexpect_loop\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    168\u001b[0m                 \u001b[0;31m# Still have time left, so read more data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 169\u001b[0;31m                 \u001b[0mincoming\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mspawn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_nonblocking\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mspawn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmaxread\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    170\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mspawn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdelayafterread\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.8/site-packages/pexpect/pty_spawn.py\u001b[0m in \u001b[0;36mread_nonblocking\u001b[0;34m(self, size, timeout)\u001b[0m\n\u001b[1;32m    499\u001b[0m         \u001b[0;31m# (possibly timeout=None), we call select() with a timeout.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 500\u001b[0;31m         \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mselect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    501\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mspawn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_nonblocking\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.8/site-packages/pexpect/pty_spawn.py\u001b[0m in \u001b[0;36mselect\u001b[0;34m(timeout)\u001b[0m\n\u001b[1;32m    449\u001b[0m             \u001b[0;32mdef\u001b[0m \u001b[0mselect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 450\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mselect_ignore_interrupts\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchild_fd\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    451\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.8/site-packages/pexpect/utils.py\u001b[0m in \u001b[0;36mselect_ignore_interrupts\u001b[0;34m(iwtd, owtd, ewtd, timeout)\u001b[0m\n\u001b[1;32m    142\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 143\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mselect\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miwtd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mowtd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mewtd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    144\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mInterruptedError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: ",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_9919/1363029856.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msystem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'python main.py --gpu-id 0 --model-name vgg11_nobias --n-epochs 10 --save-result-file sample.csv  --sweep-name exp_sample --correction --ground-metric euclidean --weight-stats  --geom-ensemble-type wts --ground-metric-normalize none --sweep-id 90  --ckpt-type best --dataset Cifar10 --ground-metric-eff --recheck-cifar --activation-seed {activation_seed}  --prelu-acts --past-correction --not-squared --exact --learning-rate 0.0001 --momentum 0.5 --batch-size-train 256 --experiment-dir {experiment_dir} --prunint-rate {prunint_rate} --run-mode {run_mode} --to-download --load-model-dir {load_model_dir} --retrain 30'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/miniconda3/lib/python3.8/site-packages/ipykernel/zmqshell.py\u001b[0m in \u001b[0;36msystem_piped\u001b[0;34m(self, cmd)\u001b[0m\n\u001b[1;32m    635\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muser_ns\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'_exit_code'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msystem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcmd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    636\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 637\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muser_ns\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'_exit_code'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msystem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvar_expand\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcmd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdepth\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    638\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    639\u001b[0m     \u001b[0;31m# Ensure new system_piped implementation is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.8/site-packages/IPython/utils/_process_posix.py\u001b[0m in \u001b[0;36msystem\u001b[0;34m(self, cmd)\u001b[0m\n\u001b[1;32m    184\u001b[0m             \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    185\u001b[0m                 \u001b[0;31m# Ensure the subprocess really is terminated\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 186\u001b[0;31m                 \u001b[0mchild\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mterminate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mforce\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    187\u001b[0m         \u001b[0;31m# add isalive check, to ensure exitstatus is set:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    188\u001b[0m         \u001b[0mchild\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misalive\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.8/site-packages/pexpect/pty_spawn.py\u001b[0m in \u001b[0;36mterminate\u001b[0;34m(self, force)\u001b[0m\n\u001b[1;32m    653\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mforce\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    654\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkill\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msignal\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSIGKILL\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 655\u001b[0;31m                 \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdelayafterterminate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    656\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misalive\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    657\u001b[0m                     \u001b[0;32mreturn\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "!python main.py --gpu-id 0 --model-name vgg11_nobias --n-epochs 10 --save-result-file sample.csv \\\n",
    "--sweep-name exp_sample --correction --ground-metric euclidean --weight-stats \\\n",
    "--geom-ensemble-type wts --ground-metric-normalize none --sweep-id 90 \\\n",
    "--ckpt-type best --dataset Cifar10 --ground-metric-eff --recheck-cifar --activation-seed {activation_seed} \\\n",
    "--prelu-acts --past-correction --not-squared --exact\\\n",
    "--learning-rate 0.0001\\\n",
    "--momentum 0.5\\\n",
    "--batch-size-train 256 --experiment-dir {experiment_dir}\\\n",
    "--prunint-rate {prunint_rate}\\\n",
    "--run-mode {run_mode}\\\n",
    "--to-download\\\n",
    "--load-model-dir {load_model_dir}\\\n",
    "--retrain 30\n",
    "# After Ensemble: 86.93"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e1701397-8c63-4095-be56-b3464267c277",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------- Setting up parameters -------\n",
      "dumping parameters at  /root/autodl-tmp/OTfusion_pruning/otfusion/exp_sample/configurations\n",
      "The parameters are: \n",
      " Namespace(act_bug=False, act_num_samples=100, activation_histograms=False, activation_mode=None, activation_seed=21, alpha=0.7, baseroot='/root/autodl-tmp/OTfusion_pruning/otfusion', batch_size_test=1000, batch_size_train=64, center_acts=False, choice='0 2 4 6 8', cifar_style_data=False, ckpt_type='best', clip_gm=False, clip_max=5, clip_min=0, config_dir='/root/autodl-tmp/OTfusion_pruning/otfusion/exp_sample/configurations', config_file=None, correction=True, csv_dir='/root/autodl-tmp/OTfusion_pruning/otfusion/exp_sample/csv', dataset='Cifar10', debug=False, deprecated=None, deterministic=False, diff_init=False, disable_bias=True, dist_epochs=60, dist_normalize=False, dump_final_models=False, dump_model=False, enable_dropout=False, ensemble_step=0.5, eval_aligned=False, exact=True, exp_name='exp_2023-09-16_17-01-31_730310', experiment_dir='experiment2', geom_ensemble_type='wts', gpu_id=0, gromov=False, gromov_loss='square_loss', ground_metric='euclidean', ground_metric_eff=True, ground_metric_normalize='none', handle_skips=False, importance=None, learning_rate=0.0001, load_model_dir='./cifar_models', load_models='', log_interval=100, model_name='vgg11_nobias', momentum=0.5, n_epochs=10, no_random_trainloaders=False, normalize_acts=False, normalize_wts=False, not_squared=True, num_hidden_nodes=400, num_hidden_nodes1=400, num_hidden_nodes2=200, num_hidden_nodes3=100, num_hidden_nodes4=50, num_models=2, options_type='generic', partial_reshape=False, partition_dataloader=-1, partition_type='labels', past_correction=True, personal_class_idx=9, personal_split_frac=0.1, pool_acts=False, pool_relu=False, prediction_wts=False, prelu_acts=True, print_distances=False, proper_marginals=False, prunint_rate=0.2, recheck_acc=False, recheck_cifar=True, reg=0.01, reg_m=0.001, reinit_trainloaders=False, result_dir='/root/autodl-tmp/OTfusion_pruning/otfusion/exp_sample/results', retrain=30, retrain_avg_only=False, retrain_geometric_only=False, retrain_lr_decay=-1, retrain_lr_decay_epochs=None, retrain_lr_decay_factor=None, retrain_seed=-1, rootdir='/root/autodl-tmp/OTfusion_pruning/otfusion/exp_sample', run_mode='finetune', same_model=-1, save_result_file='sample.csv', second_model_name=None, sinkhorn_type='normal', skip_last_layer=False, skip_last_layer_type='average', skip_personal_idx=False, skip_retrain=-1, softmax_temperature=1, standardize_acts=False, sweep_id=90, sweep_name='exp_sample', temperature=20, tensorboard=False, tensorboard_root='./tensorboard', timestamp='2023-09-16_17-01-31_730310', tmap_stats=False, to_download=True, transform_acts=False, unbalanced=False, update_acts=False, weight_stats=True, width_ratio=1)\n",
      "refactored get_config\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "in _make_layers [Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), AvgPool2d(kernel_size=1, stride=1, padding=0)]\n",
      "Relu Inplace is  False\n",
      "model parameters are \n",
      " [torch.Size([64, 3, 3, 3]), torch.Size([128, 64, 3, 3]), torch.Size([256, 128, 3, 3]), torch.Size([256, 256, 3, 3]), torch.Size([512, 256, 3, 3]), torch.Size([512, 512, 3, 3]), torch.Size([512, 512, 3, 3]), torch.Size([512, 512, 3, 3]), torch.Size([10, 512])]\n",
      "in _make_layers [Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), AvgPool2d(kernel_size=1, stride=1, padding=0)]\n",
      "Relu Inplace is  False\n",
      "model parameters are \n",
      " [torch.Size([64, 3, 3, 3]), torch.Size([128, 64, 3, 3]), torch.Size([256, 128, 3, 3]), torch.Size([256, 256, 3, 3]), torch.Size([512, 256, 3, 3]), torch.Size([512, 512, 3, 3]), torch.Size([512, 512, 3, 3]), torch.Size([512, 512, 3, 3]), torch.Size([10, 512])]\n",
      "in _make_layers [Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), AvgPool2d(kernel_size=1, stride=1, padding=0)]\n",
      "Relu Inplace is  False\n",
      "model parameters are \n",
      " [torch.Size([64, 3, 3, 3]), torch.Size([128, 64, 3, 3]), torch.Size([256, 128, 3, 3]), torch.Size([256, 256, 3, 3]), torch.Size([512, 256, 3, 3]), torch.Size([512, 512, 3, 3]), torch.Size([512, 512, 3, 3]), torch.Size([512, 512, 3, 3]), torch.Size([10, 512])]\n",
      "in _make_layers [Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), AvgPool2d(kernel_size=1, stride=1, padding=0)]\n",
      "Relu Inplace is  False\n",
      "model parameters are \n",
      " [torch.Size([64, 3, 3, 3]), torch.Size([128, 64, 3, 3]), torch.Size([256, 128, 3, 3]), torch.Size([256, 256, 3, 3]), torch.Size([512, 256, 3, 3]), torch.Size([512, 512, 3, 3]), torch.Size([512, 512, 3, 3]), torch.Size([512, 512, 3, 3]), torch.Size([10, 512])]\n",
      "Train Epoch: 1 [0/50000 (0%)]\tLoss: 0.116993\n",
      "Train Epoch: 1 [6400/50000 (13%)]\tLoss: 0.069534\n",
      "Train Epoch: 1 [12800/50000 (26%)]\tLoss: 0.115613\n",
      "Train Epoch: 1 [19200/50000 (38%)]\tLoss: 0.062993\n",
      "Train Epoch: 1 [25600/50000 (51%)]\tLoss: 0.083842\n",
      "Train Epoch: 1 [32000/50000 (64%)]\tLoss: 0.060105\n",
      "Train Epoch: 1 [38400/50000 (77%)]\tLoss: 0.070053\n",
      "Train Epoch: 1 [44800/50000 (90%)]\tLoss: 0.066773\n",
      "\n",
      "--------- Testing in global mode ---------\n",
      "size of test_loader dataset:  10000\n",
      "\n",
      "Test set: Avg. loss: 0.0003, Accuracy: 9049/10000 (90%)\n",
      "\n",
      "Train Epoch: 2 [0/50000 (0%)]\tLoss: 0.059949\n",
      "Train Epoch: 2 [6400/50000 (13%)]\tLoss: 0.060339\n",
      "Train Epoch: 2 [12800/50000 (26%)]\tLoss: 0.044316\n",
      "Train Epoch: 2 [19200/50000 (38%)]\tLoss: 0.052728\n",
      "Train Epoch: 2 [25600/50000 (51%)]\tLoss: 0.062013\n",
      "Train Epoch: 2 [32000/50000 (64%)]\tLoss: 0.036824\n",
      "Train Epoch: 2 [38400/50000 (77%)]\tLoss: 0.046963\n",
      "Train Epoch: 2 [44800/50000 (90%)]\tLoss: 0.037231\n",
      "\n",
      "--------- Testing in global mode ---------\n",
      "size of test_loader dataset:  10000\n",
      "\n",
      "Test set: Avg. loss: 0.0003, Accuracy: 9044/10000 (90%)\n",
      "\n",
      "Train Epoch: 3 [0/50000 (0%)]\tLoss: 0.050803\n",
      "Train Epoch: 3 [6400/50000 (13%)]\tLoss: 0.039283\n",
      "Train Epoch: 3 [12800/50000 (26%)]\tLoss: 0.063139\n",
      "Train Epoch: 3 [19200/50000 (38%)]\tLoss: 0.077094\n",
      "Train Epoch: 3 [25600/50000 (51%)]\tLoss: 0.052441\n",
      "Train Epoch: 3 [32000/50000 (64%)]\tLoss: 0.038409\n",
      "Train Epoch: 3 [38400/50000 (77%)]\tLoss: 0.046758\n",
      "Train Epoch: 3 [44800/50000 (90%)]\tLoss: 0.033687\n",
      "\n",
      "--------- Testing in global mode ---------\n",
      "size of test_loader dataset:  10000\n",
      "\n",
      "Test set: Avg. loss: 0.0003, Accuracy: 9044/10000 (90%)\n",
      "\n",
      "Train Epoch: 4 [0/50000 (0%)]\tLoss: 0.052528\n",
      "Train Epoch: 4 [6400/50000 (13%)]\tLoss: 0.055730\n",
      "Train Epoch: 4 [12800/50000 (26%)]\tLoss: 0.040544\n",
      "Train Epoch: 4 [19200/50000 (38%)]\tLoss: 0.040515\n",
      "Train Epoch: 4 [25600/50000 (51%)]\tLoss: 0.040505\n",
      "Train Epoch: 4 [32000/50000 (64%)]\tLoss: 0.044889\n",
      "Train Epoch: 4 [38400/50000 (77%)]\tLoss: 0.032337\n",
      "Train Epoch: 4 [44800/50000 (90%)]\tLoss: 0.031939\n",
      "\n",
      "--------- Testing in global mode ---------\n",
      "size of test_loader dataset:  10000\n",
      "\n",
      "Test set: Avg. loss: 0.0003, Accuracy: 9049/10000 (90%)\n",
      "\n",
      "Train Epoch: 5 [0/50000 (0%)]\tLoss: 0.054305\n",
      "Train Epoch: 5 [6400/50000 (13%)]\tLoss: 0.024743\n",
      "Train Epoch: 5 [12800/50000 (26%)]\tLoss: 0.029276\n",
      "Train Epoch: 5 [19200/50000 (38%)]\tLoss: 0.028308\n",
      "Train Epoch: 5 [25600/50000 (51%)]\tLoss: 0.063834\n",
      "Train Epoch: 5 [32000/50000 (64%)]\tLoss: 0.038146\n",
      "Train Epoch: 5 [38400/50000 (77%)]\tLoss: 0.038030\n",
      "Train Epoch: 5 [44800/50000 (90%)]\tLoss: 0.045305\n",
      "\n",
      "--------- Testing in global mode ---------\n",
      "size of test_loader dataset:  10000\n",
      "\n",
      "Test set: Avg. loss: 0.0003, Accuracy: 9047/10000 (90%)\n",
      "\n",
      "Train Epoch: 6 [0/50000 (0%)]\tLoss: 0.043455\n",
      "Train Epoch: 6 [6400/50000 (13%)]\tLoss: 0.025087\n",
      "Train Epoch: 6 [12800/50000 (26%)]\tLoss: 0.029782\n",
      "Train Epoch: 6 [19200/50000 (38%)]\tLoss: 0.026195\n",
      "Train Epoch: 6 [25600/50000 (51%)]\tLoss: 0.041571\n",
      "Train Epoch: 6 [32000/50000 (64%)]\tLoss: 0.041046\n",
      "Train Epoch: 6 [38400/50000 (77%)]\tLoss: 0.033723\n",
      "Train Epoch: 6 [44800/50000 (90%)]\tLoss: 0.025436\n",
      "\n",
      "--------- Testing in global mode ---------\n",
      "size of test_loader dataset:  10000\n",
      "\n",
      "Test set: Avg. loss: 0.0003, Accuracy: 9043/10000 (90%)\n",
      "\n",
      "Train Epoch: 7 [0/50000 (0%)]\tLoss: 0.038252\n",
      "Train Epoch: 7 [6400/50000 (13%)]\tLoss: 0.047840\n",
      "Train Epoch: 7 [12800/50000 (26%)]\tLoss: 0.070959\n",
      "Train Epoch: 7 [19200/50000 (38%)]\tLoss: 0.033207\n",
      "Train Epoch: 7 [25600/50000 (51%)]\tLoss: 0.050586\n",
      "Train Epoch: 7 [32000/50000 (64%)]\tLoss: 0.029546\n",
      "Train Epoch: 7 [38400/50000 (77%)]\tLoss: 0.061426\n",
      "Train Epoch: 7 [44800/50000 (90%)]\tLoss: 0.057670\n",
      "\n",
      "--------- Testing in global mode ---------\n",
      "size of test_loader dataset:  10000\n",
      "\n",
      "Test set: Avg. loss: 0.0003, Accuracy: 9041/10000 (90%)\n",
      "\n",
      "Train Epoch: 8 [0/50000 (0%)]\tLoss: 0.032923\n",
      "Train Epoch: 8 [6400/50000 (13%)]\tLoss: 0.028961\n",
      "Train Epoch: 8 [12800/50000 (26%)]\tLoss: 0.033977\n",
      "Train Epoch: 8 [19200/50000 (38%)]\tLoss: 0.038018\n",
      "Train Epoch: 8 [25600/50000 (51%)]\tLoss: 0.065654\n",
      "Train Epoch: 8 [32000/50000 (64%)]\tLoss: 0.035545\n",
      "Train Epoch: 8 [38400/50000 (77%)]\tLoss: 0.021207\n",
      "Train Epoch: 8 [44800/50000 (90%)]\tLoss: 0.038753\n",
      "\n",
      "--------- Testing in global mode ---------\n",
      "size of test_loader dataset:  10000\n",
      "\n",
      "Test set: Avg. loss: 0.0003, Accuracy: 9035/10000 (90%)\n",
      "\n",
      "Train Epoch: 9 [0/50000 (0%)]\tLoss: 0.026012\n",
      "Train Epoch: 9 [6400/50000 (13%)]\tLoss: 0.019942\n",
      "Train Epoch: 9 [12800/50000 (26%)]\tLoss: 0.039452\n",
      "Train Epoch: 9 [19200/50000 (38%)]\tLoss: 0.027666\n",
      "Train Epoch: 9 [25600/50000 (51%)]\tLoss: 0.038081\n",
      "Train Epoch: 9 [32000/50000 (64%)]\tLoss: 0.053346\n",
      "Train Epoch: 9 [38400/50000 (77%)]\tLoss: 0.049406\n",
      "Train Epoch: 9 [44800/50000 (90%)]\tLoss: 0.057711\n",
      "\n",
      "--------- Testing in global mode ---------\n",
      "size of test_loader dataset:  10000\n",
      "\n",
      "Test set: Avg. loss: 0.0003, Accuracy: 9031/10000 (90%)\n",
      "\n",
      "Train Epoch: 10 [0/50000 (0%)]\tLoss: 0.052621\n",
      "Train Epoch: 10 [6400/50000 (13%)]\tLoss: 0.036410\n",
      "Train Epoch: 10 [12800/50000 (26%)]\tLoss: 0.038551\n",
      "Train Epoch: 10 [19200/50000 (38%)]\tLoss: 0.024619\n",
      "Train Epoch: 10 [25600/50000 (51%)]\tLoss: 0.056248\n",
      "Train Epoch: 10 [32000/50000 (64%)]\tLoss: 0.025337\n",
      "Train Epoch: 10 [38400/50000 (77%)]\tLoss: 0.039602\n",
      "Train Epoch: 10 [44800/50000 (90%)]\tLoss: 0.037355\n",
      "\n",
      "--------- Testing in global mode ---------\n",
      "size of test_loader dataset:  10000\n",
      "\n",
      "Test set: Avg. loss: 0.0003, Accuracy: 9032/10000 (90%)\n",
      "\n",
      "Train Epoch: 1 [0/50000 (0%)]\tLoss: 0.142689\n",
      "Train Epoch: 1 [6400/50000 (13%)]\tLoss: 0.052603\n",
      "Train Epoch: 1 [12800/50000 (26%)]\tLoss: 0.093420\n",
      "Train Epoch: 1 [19200/50000 (38%)]\tLoss: 0.100826\n",
      "Train Epoch: 1 [25600/50000 (51%)]\tLoss: 0.070845\n",
      "Train Epoch: 1 [32000/50000 (64%)]\tLoss: 0.096255\n",
      "Train Epoch: 1 [38400/50000 (77%)]\tLoss: 0.041809\n",
      "Train Epoch: 1 [44800/50000 (90%)]\tLoss: 0.077177\n",
      "\n",
      "--------- Testing in global mode ---------\n",
      "size of test_loader dataset:  10000\n",
      "\n",
      "Test set: Avg. loss: 0.0003, Accuracy: 9069/10000 (91%)\n",
      "\n",
      "Train Epoch: 2 [0/50000 (0%)]\tLoss: 0.084418\n",
      "Train Epoch: 2 [6400/50000 (13%)]\tLoss: 0.082418\n",
      "Train Epoch: 2 [12800/50000 (26%)]\tLoss: 0.057694\n",
      "Train Epoch: 2 [19200/50000 (38%)]\tLoss: 0.075087\n",
      "Train Epoch: 2 [25600/50000 (51%)]\tLoss: 0.113169\n",
      "Train Epoch: 2 [32000/50000 (64%)]\tLoss: 0.061594\n",
      "Train Epoch: 2 [38400/50000 (77%)]\tLoss: 0.083725\n",
      "Train Epoch: 2 [44800/50000 (90%)]\tLoss: 0.055700\n",
      "\n",
      "--------- Testing in global mode ---------\n",
      "size of test_loader dataset:  10000\n",
      "\n",
      "Test set: Avg. loss: 0.0003, Accuracy: 9069/10000 (91%)\n",
      "\n",
      "Train Epoch: 3 [0/50000 (0%)]\tLoss: 0.065325\n",
      "Train Epoch: 3 [6400/50000 (13%)]\tLoss: 0.036698\n",
      "Train Epoch: 3 [12800/50000 (26%)]\tLoss: 0.071318\n",
      "Train Epoch: 3 [19200/50000 (38%)]\tLoss: 0.054485\n",
      "Train Epoch: 3 [25600/50000 (51%)]\tLoss: 0.058405\n",
      "Train Epoch: 3 [32000/50000 (64%)]\tLoss: 0.041059\n",
      "Train Epoch: 3 [38400/50000 (77%)]\tLoss: 0.044897\n",
      "Train Epoch: 3 [44800/50000 (90%)]\tLoss: 0.085841\n",
      "\n",
      "--------- Testing in global mode ---------\n",
      "size of test_loader dataset:  10000\n",
      "\n",
      "Test set: Avg. loss: 0.0003, Accuracy: 9060/10000 (91%)\n",
      "\n",
      "Train Epoch: 4 [0/50000 (0%)]\tLoss: 0.088576\n",
      "Train Epoch: 4 [6400/50000 (13%)]\tLoss: 0.050531\n",
      "Train Epoch: 4 [12800/50000 (26%)]\tLoss: 0.083464\n",
      "Train Epoch: 4 [19200/50000 (38%)]\tLoss: 0.039458\n",
      "Train Epoch: 4 [25600/50000 (51%)]\tLoss: 0.040206\n",
      "Train Epoch: 4 [32000/50000 (64%)]\tLoss: 0.070399\n",
      "Train Epoch: 4 [38400/50000 (77%)]\tLoss: 0.072445\n",
      "Train Epoch: 4 [44800/50000 (90%)]\tLoss: 0.071258\n",
      "\n",
      "--------- Testing in global mode ---------\n",
      "size of test_loader dataset:  10000\n",
      "\n",
      "Test set: Avg. loss: 0.0003, Accuracy: 9050/10000 (90%)\n",
      "\n",
      "Train Epoch: 5 [0/50000 (0%)]\tLoss: 0.052611\n",
      "Train Epoch: 5 [6400/50000 (13%)]\tLoss: 0.061676\n",
      "Train Epoch: 5 [12800/50000 (26%)]\tLoss: 0.049378\n",
      "Train Epoch: 5 [19200/50000 (38%)]\tLoss: 0.048139\n",
      "Train Epoch: 5 [25600/50000 (51%)]\tLoss: 0.042785\n",
      "Train Epoch: 5 [32000/50000 (64%)]\tLoss: 0.037133\n",
      "Train Epoch: 5 [38400/50000 (77%)]\tLoss: 0.076163\n",
      "Train Epoch: 5 [44800/50000 (90%)]\tLoss: 0.027200\n",
      "\n",
      "--------- Testing in global mode ---------\n",
      "size of test_loader dataset:  10000\n",
      "\n",
      "Test set: Avg. loss: 0.0003, Accuracy: 9043/10000 (90%)\n",
      "\n",
      "Train Epoch: 6 [0/50000 (0%)]\tLoss: 0.032286\n",
      "Train Epoch: 6 [6400/50000 (13%)]\tLoss: 0.055140\n",
      "Train Epoch: 6 [12800/50000 (26%)]\tLoss: 0.044526\n",
      "Train Epoch: 6 [19200/50000 (38%)]\tLoss: 0.038840\n",
      "Train Epoch: 6 [25600/50000 (51%)]\tLoss: 0.022438\n",
      "Train Epoch: 6 [32000/50000 (64%)]\tLoss: 0.055642\n",
      "Train Epoch: 6 [38400/50000 (77%)]\tLoss: 0.045887\n",
      "Train Epoch: 6 [44800/50000 (90%)]\tLoss: 0.034372\n",
      "\n",
      "--------- Testing in global mode ---------\n",
      "size of test_loader dataset:  10000\n",
      "\n",
      "Test set: Avg. loss: 0.0003, Accuracy: 9037/10000 (90%)\n",
      "\n",
      "Train Epoch: 7 [0/50000 (0%)]\tLoss: 0.086913\n",
      "Train Epoch: 7 [6400/50000 (13%)]\tLoss: 0.054592\n",
      "Train Epoch: 7 [12800/50000 (26%)]\tLoss: 0.040316\n",
      "Train Epoch: 7 [19200/50000 (38%)]\tLoss: 0.038977\n",
      "Train Epoch: 7 [25600/50000 (51%)]\tLoss: 0.042053\n",
      "Train Epoch: 7 [32000/50000 (64%)]\tLoss: 0.047258\n",
      "Train Epoch: 7 [38400/50000 (77%)]\tLoss: 0.033528\n",
      "Train Epoch: 7 [44800/50000 (90%)]\tLoss: 0.064025\n",
      "\n",
      "--------- Testing in global mode ---------\n",
      "size of test_loader dataset:  10000\n",
      "\n",
      "Test set: Avg. loss: 0.0003, Accuracy: 9036/10000 (90%)\n",
      "\n",
      "Train Epoch: 8 [0/50000 (0%)]\tLoss: 0.038836\n",
      "Train Epoch: 8 [6400/50000 (13%)]\tLoss: 0.025743\n",
      "Train Epoch: 8 [12800/50000 (26%)]\tLoss: 0.024755\n",
      "Train Epoch: 8 [19200/50000 (38%)]\tLoss: 0.039706\n",
      "Train Epoch: 8 [25600/50000 (51%)]\tLoss: 0.056052\n",
      "Train Epoch: 8 [32000/50000 (64%)]\tLoss: 0.033363\n",
      "Train Epoch: 8 [38400/50000 (77%)]\tLoss: 0.028756\n",
      "Train Epoch: 8 [44800/50000 (90%)]\tLoss: 0.029979\n",
      "\n",
      "--------- Testing in global mode ---------\n",
      "size of test_loader dataset:  10000\n",
      "\n",
      "Test set: Avg. loss: 0.0003, Accuracy: 9030/10000 (90%)\n",
      "\n",
      "Train Epoch: 9 [0/50000 (0%)]\tLoss: 0.037950\n",
      "Train Epoch: 9 [6400/50000 (13%)]\tLoss: 0.018366\n",
      "Train Epoch: 9 [12800/50000 (26%)]\tLoss: 0.028059\n",
      "Train Epoch: 9 [19200/50000 (38%)]\tLoss: 0.046587\n",
      "Train Epoch: 9 [25600/50000 (51%)]\tLoss: 0.012991\n",
      "Train Epoch: 9 [32000/50000 (64%)]\tLoss: 0.055657\n",
      "Train Epoch: 9 [38400/50000 (77%)]\tLoss: 0.052003\n",
      "Train Epoch: 9 [44800/50000 (90%)]\tLoss: 0.021347\n",
      "\n",
      "--------- Testing in global mode ---------\n",
      "size of test_loader dataset:  10000\n",
      "\n",
      "Test set: Avg. loss: 0.0003, Accuracy: 9029/10000 (90%)\n",
      "\n",
      "Train Epoch: 10 [0/50000 (0%)]\tLoss: 0.032026\n",
      "Train Epoch: 10 [6400/50000 (13%)]\tLoss: 0.039842\n",
      "Train Epoch: 10 [12800/50000 (26%)]\tLoss: 0.050016\n",
      "Train Epoch: 10 [19200/50000 (38%)]\tLoss: 0.027208\n",
      "Train Epoch: 10 [25600/50000 (51%)]\tLoss: 0.032963\n",
      "Train Epoch: 10 [32000/50000 (64%)]\tLoss: 0.055067\n",
      "Train Epoch: 10 [38400/50000 (77%)]\tLoss: 0.031083\n",
      "Train Epoch: 10 [44800/50000 (90%)]\tLoss: 0.043165\n",
      "\n",
      "--------- Testing in global mode ---------\n",
      "size of test_loader dataset:  10000\n",
      "\n",
      "Test set: Avg. loss: 0.0003, Accuracy: 9027/10000 (90%)\n",
      "\n",
      "========================================================\n",
      "accuracies [90.32, 90.27]\n",
      "=========================================================\n",
      "layer features.0.weight_orig has #params  1728\n",
      "layer features.3.weight_orig has #params  73728\n",
      "layer features.6.weight_orig has #params  294912\n",
      "layer features.8.weight_orig has #params  589824\n",
      "layer features.11.weight_orig has #params  1179648\n",
      "layer features.13.weight_orig has #params  2359296\n",
      "layer features.16.weight_orig has #params  2359296\n",
      "layer features.18.weight_orig has #params  2359296\n",
      "layer classifier.weight has #params  5120\n",
      "Activation Timer start\n",
      "Activation Timer ends\n",
      "------- Geometric Ensembling -------\n",
      "Timer start\n",
      "Previous layer shape is  None\n",
      "Processing the coordinates to form ground_metric\n",
      "dont leave off the squaring of the ground metric\n",
      "returns a uniform measure of cardinality:  64\n",
      "returns a uniform measure of cardinality:  64\n",
      "the transport map is  tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')\n",
      "Ratio of trace to the matrix sum:  tensor(0.0469, device='cuda:0')\n",
      "Here, trace is 3.0 and matrix sum is 64.0 \n",
      "this is past correction for weight mode\n",
      "Shape of aligned wt is  torch.Size([64, 3, 9])\n",
      "Shape of fc_layer0_weight_data is  torch.Size([64, 3, 9])\n",
      "Previous layer shape is  torch.Size([64, 3, 3, 3])\n",
      "shape of layer: model 0 torch.Size([128, 64, 9])\n",
      "shape of layer: model 1 torch.Size([128, 64, 9])\n",
      "shape of previous transport map torch.Size([64, 64])\n",
      "Processing the coordinates to form ground_metric\n",
      "dont leave off the squaring of the ground metric\n",
      "returns a uniform measure of cardinality:  128\n",
      "returns a uniform measure of cardinality:  128\n",
      "the transport map is  tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        ...,\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0078],\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],\n",
      "       device='cuda:0')\n",
      "Ratio of trace to the matrix sum:  tensor(0.0078, device='cuda:0')\n",
      "Here, trace is 1.0 and matrix sum is 128.0 \n",
      "this is past correction for weight mode\n",
      "Shape of aligned wt is  torch.Size([128, 64, 9])\n",
      "Shape of fc_layer0_weight_data is  torch.Size([128, 64, 9])\n",
      "Previous layer shape is  torch.Size([128, 64, 3, 3])\n",
      "shape of layer: model 0 torch.Size([256, 128, 9])\n",
      "shape of layer: model 1 torch.Size([256, 128, 9])\n",
      "shape of previous transport map torch.Size([128, 128])\n",
      "Processing the coordinates to form ground_metric\n",
      "dont leave off the squaring of the ground metric\n",
      "returns a uniform measure of cardinality:  256\n",
      "returns a uniform measure of cardinality:  256\n",
      "the transport map is  tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')\n",
      "Ratio of trace to the matrix sum:  tensor(0., device='cuda:0')\n",
      "Here, trace is 0.0 and matrix sum is 256.0 \n",
      "this is past correction for weight mode\n",
      "Shape of aligned wt is  torch.Size([256, 128, 9])\n",
      "Shape of fc_layer0_weight_data is  torch.Size([256, 128, 9])\n",
      "Previous layer shape is  torch.Size([256, 128, 3, 3])\n",
      "shape of layer: model 0 torch.Size([256, 256, 9])\n",
      "shape of layer: model 1 torch.Size([256, 256, 9])\n",
      "shape of previous transport map torch.Size([256, 256])\n",
      "Processing the coordinates to form ground_metric\n",
      "dont leave off the squaring of the ground metric\n",
      "returns a uniform measure of cardinality:  256\n",
      "returns a uniform measure of cardinality:  256\n",
      "the transport map is  tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        ...,\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.0039, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],\n",
      "       device='cuda:0')\n",
      "Ratio of trace to the matrix sum:  tensor(0.0117, device='cuda:0')\n",
      "Here, trace is 3.0 and matrix sum is 256.0 \n",
      "this is past correction for weight mode\n",
      "Shape of aligned wt is  torch.Size([256, 256, 9])\n",
      "Shape of fc_layer0_weight_data is  torch.Size([256, 256, 9])\n",
      "Previous layer shape is  torch.Size([256, 256, 3, 3])\n",
      "shape of layer: model 0 torch.Size([512, 256, 9])\n",
      "shape of layer: model 1 torch.Size([512, 256, 9])\n",
      "shape of previous transport map torch.Size([256, 256])\n",
      "Processing the coordinates to form ground_metric\n",
      "dont leave off the squaring of the ground metric\n",
      "returns a uniform measure of cardinality:  512\n",
      "returns a uniform measure of cardinality:  512\n",
      "the transport map is  tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')\n",
      "Ratio of trace to the matrix sum:  tensor(0.0039, device='cuda:0')\n",
      "Here, trace is 2.0 and matrix sum is 512.0 \n",
      "this is past correction for weight mode\n",
      "Shape of aligned wt is  torch.Size([512, 256, 9])\n",
      "Shape of fc_layer0_weight_data is  torch.Size([512, 256, 9])\n",
      "Previous layer shape is  torch.Size([512, 256, 3, 3])\n",
      "shape of layer: model 0 torch.Size([512, 512, 9])\n",
      "shape of layer: model 1 torch.Size([512, 512, 9])\n",
      "shape of previous transport map torch.Size([512, 512])\n",
      "Processing the coordinates to form ground_metric\n",
      "dont leave off the squaring of the ground metric\n",
      "returns a uniform measure of cardinality:  512\n",
      "returns a uniform measure of cardinality:  512\n",
      "the transport map is  tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')\n",
      "Ratio of trace to the matrix sum:  tensor(0.0020, device='cuda:0')\n",
      "Here, trace is 1.0 and matrix sum is 512.0 \n",
      "this is past correction for weight mode\n",
      "Shape of aligned wt is  torch.Size([512, 512, 9])\n",
      "Shape of fc_layer0_weight_data is  torch.Size([512, 512, 9])\n",
      "Previous layer shape is  torch.Size([512, 512, 3, 3])\n",
      "shape of layer: model 0 torch.Size([512, 512, 9])\n",
      "shape of layer: model 1 torch.Size([512, 512, 9])\n",
      "shape of previous transport map torch.Size([512, 512])\n",
      "Processing the coordinates to form ground_metric\n",
      "dont leave off the squaring of the ground metric\n",
      "returns a uniform measure of cardinality:  512\n",
      "returns a uniform measure of cardinality:  512\n",
      "the transport map is  tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')\n",
      "Ratio of trace to the matrix sum:  tensor(0.0020, device='cuda:0')\n",
      "Here, trace is 1.0 and matrix sum is 512.0 \n",
      "this is past correction for weight mode\n",
      "Shape of aligned wt is  torch.Size([512, 512, 9])\n",
      "Shape of fc_layer0_weight_data is  torch.Size([512, 512, 9])\n",
      "Previous layer shape is  torch.Size([512, 512, 3, 3])\n",
      "shape of layer: model 0 torch.Size([512, 512, 9])\n",
      "shape of layer: model 1 torch.Size([512, 512, 9])\n",
      "shape of previous transport map torch.Size([512, 512])\n",
      "Processing the coordinates to form ground_metric\n",
      "dont leave off the squaring of the ground metric\n",
      "returns a uniform measure of cardinality:  512\n",
      "returns a uniform measure of cardinality:  512\n",
      "the transport map is  tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        [0.0020, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        ...,\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],\n",
      "       device='cuda:0')\n",
      "Ratio of trace to the matrix sum:  tensor(0., device='cuda:0')\n",
      "Here, trace is 0.0 and matrix sum is 512.0 \n",
      "this is past correction for weight mode\n",
      "Shape of aligned wt is  torch.Size([512, 512, 9])\n",
      "Shape of fc_layer0_weight_data is  torch.Size([512, 512, 9])\n",
      "Previous layer shape is  torch.Size([512, 512, 3, 3])\n",
      "shape of layer: model 0 torch.Size([10, 512])\n",
      "shape of layer: model 1 torch.Size([10, 512])\n",
      "shape of previous transport map torch.Size([512, 512])\n",
      "Processing the coordinates to form ground_metric\n",
      "dont leave off the squaring of the ground metric\n",
      "ground metric is  tensor([[1.4325, 2.4911, 2.8079, 2.8856, 2.7468, 2.8870, 2.7353, 2.6645, 2.5804,\n",
      "         2.6195],\n",
      "        [2.5208, 1.3107, 2.7097, 2.5832, 2.7581, 2.5509, 2.4414, 2.5393, 2.3284,\n",
      "         2.4193],\n",
      "        [2.7692, 2.6807, 1.3577, 2.9864, 2.9482, 2.8599, 2.7595, 2.8709, 2.7691,\n",
      "         2.7716],\n",
      "        [2.8418, 2.6804, 2.9077, 1.5495, 2.8531, 2.9723, 2.7705, 2.8076, 2.8298,\n",
      "         2.6796],\n",
      "        [2.8005, 2.5947, 2.8385, 2.9068, 1.5305, 2.7492, 2.8050, 2.6826, 2.6967,\n",
      "         2.7531],\n",
      "        [2.9442, 2.5952, 2.9950, 2.8124, 2.7448, 1.5445, 2.7571, 2.7091, 2.8377,\n",
      "         2.7059],\n",
      "        [2.6948, 2.4699, 2.7141, 2.9192, 2.6649, 2.8837, 1.4110, 2.7135, 2.4951,\n",
      "         2.5593],\n",
      "        [2.7428, 2.5757, 2.7854, 2.8189, 2.7297, 2.7164, 2.7106, 1.3886, 2.5619,\n",
      "         2.5139],\n",
      "        [2.5881, 2.3878, 2.7745, 2.7406, 2.7236, 2.8404, 2.5578, 2.6629, 1.3044,\n",
      "         2.3747],\n",
      "        [2.5971, 2.2644, 2.8401, 2.7362, 2.6806, 2.7179, 2.5752, 2.5408, 2.5460,\n",
      "         1.3207]], device='cuda:0', grad_fn=<PowBackward0>)\n",
      "returns a uniform measure of cardinality:  10\n",
      "returns a uniform measure of cardinality:  10\n",
      "the transport map is  tensor([[0.1000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0000, 0.1000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0000, 0.0000, 0.1000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.1000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.1000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.1000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.1000, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.1000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.1000,\n",
      "         0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.1000]], device='cuda:0')\n",
      "Ratio of trace to the matrix sum:  tensor(1., device='cuda:0')\n",
      "Here, trace is 9.99755859375 and matrix sum is 9.99755859375 \n",
      "this is past correction for weight mode\n",
      "Shape of aligned wt is  torch.Size([10, 512])\n",
      "Shape of fc_layer0_weight_data is  torch.Size([10, 512])\n",
      "using independent method\n",
      "in _make_layers [Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), AvgPool2d(kernel_size=1, stride=1, padding=0)]\n",
      "Relu Inplace is  False\n",
      "model parameters are \n",
      " [torch.Size([64, 3, 3, 3]), torch.Size([128, 64, 3, 3]), torch.Size([256, 128, 3, 3]), torch.Size([256, 256, 3, 3]), torch.Size([512, 256, 3, 3]), torch.Size([512, 512, 3, 3]), torch.Size([512, 512, 3, 3]), torch.Size([512, 512, 3, 3]), torch.Size([10, 512])]\n",
      "\n",
      "--------- Testing in global mode ---------\n",
      "size of test_loader dataset:  10000\n",
      "\n",
      "Test set: Avg. loss: 0.0023, Accuracy: 776/10000 (8%)\n",
      "\n",
      "len of model parameters and avg aligned layers is  9 9\n",
      "len of model_state_dict is  9\n",
      "len of param_list is  9\n",
      "\n",
      "--------- Testing in global mode ---------\n",
      "size of test_loader dataset:  10000\n",
      "\n",
      "Test set: Avg. loss: 0.0012, Accuracy: 8607/10000 (86%)\n",
      "\n",
      "===============================================\n",
      "===============================================\n",
      "After Ensemble: 86.07\n",
      "===============================================\n",
      "===============================================\n",
      "Timer ends\n",
      "Time taken for geometric ensembling is 3.543765887618065 seconds\n",
      "------- Prediction based ensembling -------\n",
      "\n",
      "Test set: Avg. loss: 0.0003, Accuracy: 9120/10000 (91%)\n",
      "\n",
      "------- Naive ensembling of weights -------\n",
      "[torch.Size([64, 3, 3, 3]), torch.Size([64, 3, 3, 3])]\n",
      "torch.Size([64, 3, 3, 3])\n",
      "[torch.Size([128, 64, 3, 3]), torch.Size([128, 64, 3, 3])]\n",
      "torch.Size([128, 64, 3, 3])\n",
      "[torch.Size([256, 128, 3, 3]), torch.Size([256, 128, 3, 3])]\n",
      "torch.Size([256, 128, 3, 3])\n",
      "[torch.Size([256, 256, 3, 3]), torch.Size([256, 256, 3, 3])]\n",
      "torch.Size([256, 256, 3, 3])\n",
      "[torch.Size([512, 256, 3, 3]), torch.Size([512, 256, 3, 3])]\n",
      "torch.Size([512, 256, 3, 3])\n",
      "[torch.Size([512, 512, 3, 3]), torch.Size([512, 512, 3, 3])]\n",
      "torch.Size([512, 512, 3, 3])\n",
      "[torch.Size([512, 512, 3, 3]), torch.Size([512, 512, 3, 3])]\n",
      "torch.Size([512, 512, 3, 3])\n",
      "[torch.Size([512, 512, 3, 3]), torch.Size([512, 512, 3, 3])]\n",
      "torch.Size([512, 512, 3, 3])\n",
      "[torch.Size([10, 512]), torch.Size([10, 512])]\n",
      "torch.Size([10, 512])\n",
      "in _make_layers [Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), AvgPool2d(kernel_size=1, stride=1, padding=0)]\n",
      "Relu Inplace is  False\n",
      "model parameters are \n",
      " [torch.Size([64, 3, 3, 3]), torch.Size([128, 64, 3, 3]), torch.Size([256, 128, 3, 3]), torch.Size([256, 256, 3, 3]), torch.Size([512, 256, 3, 3]), torch.Size([512, 512, 3, 3]), torch.Size([512, 512, 3, 3]), torch.Size([512, 512, 3, 3]), torch.Size([10, 512])]\n",
      "\n",
      "--------- Testing in global mode ---------\n",
      "size of test_loader dataset:  10000\n",
      "\n",
      "Test set: Avg. loss: 0.0023, Accuracy: 1066/10000 (11%)\n",
      "\n",
      "\n",
      "--------- Testing in global mode ---------\n",
      "size of test_loader dataset:  10000\n",
      "\n",
      "Test set: Avg. loss: 0.0023, Accuracy: 1430/10000 (14%)\n",
      "\n",
      "-------- Retraining the models ---------\n",
      "Retraining model :  model_0\n",
      "num_epochs---------------+++))))))+++ 300\n",
      "num_epochs---------------+++))))))+++ 300\n",
      "lr is  0.05\n",
      "number of epochs would be  30\n",
      "num_epochs-------------- 30\n",
      "Epoch 000\n",
      "/root/miniconda3/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:129: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\"Detected call of `lr_scheduler.step()` before `optimizer.step()`. \"\n",
      "/root/miniconda3/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:154: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "accuracy: {'epoch': 0, 'value': 0.46289999999999987} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 0, 'value': 1.4781726442313183} ({'split': 'train'})\n",
      "accuracy: {'epoch': 0, 'value': 0.6501000344753266} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 0, 'value': 1.1087595582008363} ({'split': 'test'})\n",
      "Epoch 001\n",
      "accuracy: {'epoch': 1, 'value': 0.7291599999999998} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 1, 'value': 0.826286238250733} ({'split': 'train'})\n",
      "accuracy: {'epoch': 1, 'value': 0.7320000410079956} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 1, 'value': 0.8250816404819489} ({'split': 'test'})\n",
      "Epoch 002\n",
      "accuracy: {'epoch': 2, 'value': 0.7880999999999994} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 2, 'value': 0.6528088059806817} ({'split': 'train'})\n",
      "accuracy: {'epoch': 2, 'value': 0.7668000400066376} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 2, 'value': 0.730934739112854} ({'split': 'test'})\n",
      "Epoch 003\n",
      "accuracy: {'epoch': 3, 'value': 0.8120399999999987} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 3, 'value': 0.5733953863716125} ({'split': 'train'})\n",
      "accuracy: {'epoch': 3, 'value': 0.7851000368595124} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 3, 'value': 0.6722977876663209} ({'split': 'test'})\n",
      "Epoch 004\n",
      "accuracy: {'epoch': 4, 'value': 0.8281799999999998} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 4, 'value': 0.522081580562592} ({'split': 'train'})\n",
      "accuracy: {'epoch': 4, 'value': 0.742600041627884} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 4, 'value': 0.7877674579620361} ({'split': 'test'})\n",
      "Epoch 005\n",
      "accuracy: {'epoch': 5, 'value': 0.8413599999999998} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 5, 'value': 0.4835167716407773} ({'split': 'train'})\n",
      "accuracy: {'epoch': 5, 'value': 0.7761000335216522} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 5, 'value': 0.6785974979400635} ({'split': 'test'})\n",
      "Epoch 006\n",
      "accuracy: {'epoch': 6, 'value': 0.8492999999999998} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 6, 'value': 0.45949079609870924} ({'split': 'train'})\n",
      "accuracy: {'epoch': 6, 'value': 0.7769000351428985} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 6, 'value': 0.7717680156230928} ({'split': 'test'})\n",
      "Epoch 007\n",
      "accuracy: {'epoch': 7, 'value': 0.8543799999999994} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 7, 'value': 0.44271955781936656} ({'split': 'train'})\n",
      "accuracy: {'epoch': 7, 'value': 0.7910000324249268} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 7, 'value': 0.6960731267929077} ({'split': 'test'})\n",
      "Epoch 008\n",
      "accuracy: {'epoch': 8, 'value': 0.8610800000000001} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 8, 'value': 0.42570056202888484} ({'split': 'train'})\n",
      "accuracy: {'epoch': 8, 'value': 0.7890000283718108} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 8, 'value': 0.6952049136161804} ({'split': 'test'})\n",
      "Epoch 009\n",
      "accuracy: {'epoch': 9, 'value': 0.8681000000000014} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 9, 'value': 0.4033397150611873} ({'split': 'train'})\n",
      "accuracy: {'epoch': 9, 'value': 0.7845000445842742} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 9, 'value': 0.7178995668888092} ({'split': 'test'})\n",
      "Epoch 010\n",
      "accuracy: {'epoch': 10, 'value': 0.8682400000000006} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 10, 'value': 0.4090823437023163} ({'split': 'train'})\n",
      "accuracy: {'epoch': 10, 'value': 0.7613000392913818} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 10, 'value': 0.767329216003418} ({'split': 'test'})\n",
      "Epoch 011\n",
      "accuracy: {'epoch': 11, 'value': 0.8749000000000005} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 11, 'value': 0.38218344170808766} ({'split': 'train'})\n",
      "accuracy: {'epoch': 11, 'value': 0.789300036430359} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 11, 'value': 0.7229805827140808} ({'split': 'test'})\n",
      "Epoch 012\n",
      "accuracy: {'epoch': 12, 'value': 0.8726600000000002} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 12, 'value': 0.3849793776178355} ({'split': 'train'})\n",
      "accuracy: {'epoch': 12, 'value': 0.7916000425815582} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 12, 'value': 0.6692538261413574} ({'split': 'test'})\n",
      "Epoch 013\n",
      "accuracy: {'epoch': 13, 'value': 0.8781799999999991} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 13, 'value': 0.3750991787767409} ({'split': 'train'})\n",
      "accuracy: {'epoch': 13, 'value': 0.7958000361919403} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 13, 'value': 0.643027800321579} ({'split': 'test'})\n",
      "Epoch 014\n",
      "accuracy: {'epoch': 14, 'value': 0.8838600000000002} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 14, 'value': 0.3563724224233628} ({'split': 'train'})\n",
      "accuracy: {'epoch': 14, 'value': 0.7645000338554382} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 14, 'value': 0.8039357185363771} ({'split': 'test'})\n",
      "Epoch 015\n",
      "accuracy: {'epoch': 15, 'value': 0.8795399999999995} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 15, 'value': 0.3723861938476566} ({'split': 'train'})\n",
      "accuracy: {'epoch': 15, 'value': 0.8037000477313996} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 15, 'value': 0.6595152020454408} ({'split': 'test'})\n",
      "Epoch 016\n",
      "accuracy: {'epoch': 16, 'value': 0.8880200000000007} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 16, 'value': 0.349146549606323} ({'split': 'train'})\n",
      "accuracy: {'epoch': 16, 'value': 0.7815000355243683} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 16, 'value': 0.7728685617446899} ({'split': 'test'})\n",
      "Epoch 017\n",
      "accuracy: {'epoch': 17, 'value': 0.8816200000000005} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 17, 'value': 0.3663059703636166} ({'split': 'train'})\n",
      "accuracy: {'epoch': 17, 'value': 0.810200035572052} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 17, 'value': 0.648629641532898} ({'split': 'test'})\n",
      "Epoch 018\n",
      "accuracy: {'epoch': 18, 'value': 0.8779999999999991} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 18, 'value': 0.37434383252620684} ({'split': 'train'})\n",
      "accuracy: {'epoch': 18, 'value': 0.7528000354766846} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 18, 'value': 0.8281574130058289} ({'split': 'test'})\n",
      "Epoch 019\n",
      "^C\n",
      "Traceback (most recent call last):\n",
      "  File \"main.py\", line 351, in <module>\n",
      "    _, best_retrain_acc = routines.retrain_models(args, [*original_models, geometric_model, naive_model], retrain_loader, test_loader, config, tensorboard_obj=tensorboard_obj, initial_acc=initial_acc, nicks=nicks)\n",
      "  File \"/root/autodl-tmp/OTfusion_pruning/otfusion/routines.py\", line 394, in retrain_models\n",
      "    retrained_network, acc = cifar_train.get_retrained_model(args, retrain_loader, test_loader, old_networks[i], config, output_root_dir, tensorboard_obj=tensorboard_obj, nick=nick, start_acc=initial_acc[i])\n",
      "  File \"/root/autodl-tmp/OTfusion_pruning/otfusion/./cifar/train.py\", line 333, in get_retrained_model\n",
      "    best_acc,model = main(config, output_dir, args.gpu_id, pretrained_model=old_network, pretrained_dataset=(train_loader, test_loader), tensorboard_obj=tensorboard_obj,return_model=True)\n",
      "  File \"/root/autodl-tmp/OTfusion_pruning/otfusion/./cifar/train.py\", line 67, in main\n",
      "    for batch_x, batch_y in training_loader:\n",
      "  File \"/root/miniconda3/lib/python3.8/site-packages/torch/utils/data/dataloader.py\", line 521, in __next__\n",
      "    data = self._next_data()\n",
      "  File \"/root/miniconda3/lib/python3.8/site-packages/torch/utils/data/dataloader.py\", line 561, in _next_data\n",
      "    data = self._dataset_fetcher.fetch(index)  # may raise StopIteration\n",
      "  File \"/root/miniconda3/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py\", line 49, in fetch\n",
      "    data = [self.dataset[idx] for idx in possibly_batched_index]\n",
      "  File \"/root/miniconda3/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py\", line 49, in <listcomp>\n",
      "    data = [self.dataset[idx] for idx in possibly_batched_index]\n",
      "  File \"/root/miniconda3/lib/python3.8/site-packages/torchvision/datasets/cifar.py\", line 121, in __getitem__\n",
      "    img = self.transform(img)\n",
      "  File \"/root/miniconda3/lib/python3.8/site-packages/torchvision/transforms/transforms.py\", line 61, in __call__\n",
      "    img = t(img)\n",
      "  File \"/root/miniconda3/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1102, in _call_impl\n",
      "    return forward_call(*input, **kwargs)\n",
      "  File \"/root/miniconda3/lib/python3.8/site-packages/torchvision/transforms/transforms.py\", line 226, in forward\n",
      "    return F.normalize(tensor, self.mean, self.std, self.inplace)\n",
      "  File \"/root/miniconda3/lib/python3.8/site-packages/torchvision/transforms/functional.py\", line 345, in normalize\n",
      "    if (std == 0).any():\n",
      "KeyboardInterrupt\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m~/miniconda3/lib/python3.8/site-packages/IPython/utils/_process_posix.py\u001b[0m in \u001b[0;36msystem\u001b[0;34m(self, cmd)\u001b[0m\n\u001b[1;32m    161\u001b[0m                 \u001b[0;31m# know whether we've finished (if we matched EOF) or not\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 162\u001b[0;31m                 \u001b[0mres_idx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mchild\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexpect_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpatterns\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_timeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    163\u001b[0m                 \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchild\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbefore\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mout_size\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'replace'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m''\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.8/site-packages/pexpect/spawnbase.py\u001b[0m in \u001b[0;36mexpect_list\u001b[0;34m(self, pattern_list, timeout, searchwindowsize, async_, **kw)\u001b[0m\n\u001b[1;32m    371\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 372\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mexp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexpect_loop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    373\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.8/site-packages/pexpect/expect.py\u001b[0m in \u001b[0;36mexpect_loop\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    168\u001b[0m                 \u001b[0;31m# Still have time left, so read more data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 169\u001b[0;31m                 \u001b[0mincoming\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mspawn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_nonblocking\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mspawn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmaxread\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    170\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mspawn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdelayafterread\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.8/site-packages/pexpect/pty_spawn.py\u001b[0m in \u001b[0;36mread_nonblocking\u001b[0;34m(self, size, timeout)\u001b[0m\n\u001b[1;32m    499\u001b[0m         \u001b[0;31m# (possibly timeout=None), we call select() with a timeout.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 500\u001b[0;31m         \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mselect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    501\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mspawn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_nonblocking\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.8/site-packages/pexpect/pty_spawn.py\u001b[0m in \u001b[0;36mselect\u001b[0;34m(timeout)\u001b[0m\n\u001b[1;32m    449\u001b[0m             \u001b[0;32mdef\u001b[0m \u001b[0mselect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 450\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mselect_ignore_interrupts\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchild_fd\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    451\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.8/site-packages/pexpect/utils.py\u001b[0m in \u001b[0;36mselect_ignore_interrupts\u001b[0;34m(iwtd, owtd, ewtd, timeout)\u001b[0m\n\u001b[1;32m    142\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 143\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mselect\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miwtd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mowtd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mewtd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    144\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mInterruptedError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: ",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_9719/719175872.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msystem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'python main.py --gpu-id 0 --model-name vgg11_nobias --n-epochs 10 --save-result-file sample.csv  --sweep-name exp_sample --correction --ground-metric euclidean --weight-stats  --geom-ensemble-type wts --ground-metric-normalize none --sweep-id 90  --ckpt-type best --dataset Cifar10 --ground-metric-eff --recheck-cifar --activation-seed {activation_seed}  --prelu-acts --past-correction --not-squared --exact --learning-rate 0.0001 --momentum 0.5 --batch-size-train 64 --experiment-dir {experiment_dir} --prunint-rate {prunint_rate} --run-mode {run_mode} --to-download --load-model-dir {load_model_dir} --retrain 30'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/miniconda3/lib/python3.8/site-packages/ipykernel/zmqshell.py\u001b[0m in \u001b[0;36msystem_piped\u001b[0;34m(self, cmd)\u001b[0m\n\u001b[1;32m    635\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muser_ns\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'_exit_code'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msystem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcmd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    636\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 637\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muser_ns\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'_exit_code'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msystem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvar_expand\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcmd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdepth\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    638\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    639\u001b[0m     \u001b[0;31m# Ensure new system_piped implementation is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.8/site-packages/IPython/utils/_process_posix.py\u001b[0m in \u001b[0;36msystem\u001b[0;34m(self, cmd)\u001b[0m\n\u001b[1;32m    184\u001b[0m             \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    185\u001b[0m                 \u001b[0;31m# Ensure the subprocess really is terminated\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 186\u001b[0;31m                 \u001b[0mchild\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mterminate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mforce\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    187\u001b[0m         \u001b[0;31m# add isalive check, to ensure exitstatus is set:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    188\u001b[0m         \u001b[0mchild\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misalive\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.8/site-packages/pexpect/pty_spawn.py\u001b[0m in \u001b[0;36mterminate\u001b[0;34m(self, force)\u001b[0m\n\u001b[1;32m    653\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mforce\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    654\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkill\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msignal\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSIGKILL\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 655\u001b[0;31m                 \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdelayafterterminate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    656\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misalive\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    657\u001b[0m                     \u001b[0;32mreturn\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "!python main.py --gpu-id 0 --model-name vgg11_nobias --n-epochs 10 --save-result-file sample.csv \\\n",
    "--sweep-name exp_sample --correction --ground-metric euclidean --weight-stats \\\n",
    "--geom-ensemble-type wts --ground-metric-normalize none --sweep-id 90 \\\n",
    "--ckpt-type best --dataset Cifar10 --ground-metric-eff --recheck-cifar --activation-seed {activation_seed} \\\n",
    "--prelu-acts --past-correction --not-squared --exact\\\n",
    "--learning-rate 0.0001\\\n",
    "--momentum 0.5\\\n",
    "--batch-size-train 64 --experiment-dir {experiment_dir}\\\n",
    "--prunint-rate {prunint_rate}\\\n",
    "--run-mode {run_mode}\\\n",
    "--to-download\\\n",
    "--load-model-dir {load_model_dir}\\\n",
    "--retrain 30\n",
    "# After Ensemble: 86.07"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a78f1c0-85a7-4629-ba27-fcbf65e981c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "activation_seed=21\n",
    "experiment_dir=\"experiment1\"\n",
    "prunint_rate=0.2\n",
    "run_mode='benchmark'\n",
    "load_model_dir='./cifar_models'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c5b102d6-1a16-4de0-88d5-d56ced2e8d4a",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------- Setting up parameters -------\n",
      "dumping parameters at  /root/autodl-tmp/OTfusion_pruning/otfusion/exp_sample/configurations\n",
      "The parameters are: \n",
      " Namespace(act_bug=False, act_num_samples=100, activation_histograms=False, activation_mode=None, activation_seed=21, alpha=0.7, baseroot='/root/autodl-tmp/OTfusion_pruning/otfusion', batch_size_test=1000, batch_size_train=64, center_acts=False, choice='0 2 4 6 8', cifar_style_data=False, ckpt_type='best', clip_gm=False, clip_max=5, clip_min=0, config_dir='/root/autodl-tmp/OTfusion_pruning/otfusion/exp_sample/configurations', config_file=None, correction=True, csv_dir='/root/autodl-tmp/OTfusion_pruning/otfusion/exp_sample/csv', dataset='Cifar10', debug=False, deprecated=None, deterministic=False, diff_init=False, disable_bias=True, dist_epochs=60, dist_normalize=False, dump_final_models=False, dump_model=False, enable_dropout=False, ensemble_step=0.5, eval_aligned=True, exact=True, exp_name='exp_2023-09-16_10-25-05_499081', experiment_dir='experiment1', geom_ensemble_type='wts', gpu_id=0, gromov=False, gromov_loss='square_loss', ground_metric='euclidean', ground_metric_eff=True, ground_metric_normalize='none', handle_skips=False, importance=None, learning_rate=0.004, load_model_dir='./cifar_models', load_models='', log_interval=100, model_name='vgg11_nobias', momentum=0.5, n_epochs=0, no_random_trainloaders=False, normalize_acts=False, normalize_wts=False, not_squared=True, num_hidden_nodes=400, num_hidden_nodes1=400, num_hidden_nodes2=200, num_hidden_nodes3=100, num_hidden_nodes4=50, num_models=2, options_type='generic', partial_reshape=False, partition_dataloader=-1, partition_type='labels', past_correction=True, personal_class_idx=9, personal_split_frac=0.1, pool_acts=False, pool_relu=False, prediction_wts=False, prelu_acts=True, print_distances=False, proper_marginals=False, prunint_rate=0.2, recheck_acc=False, recheck_cifar=True, reg=0.01, reg_m=0.001, reinit_trainloaders=False, result_dir='/root/autodl-tmp/OTfusion_pruning/otfusion/exp_sample/results', retrain=30, retrain_avg_only=False, retrain_geometric_only=False, retrain_lr_decay=-1, retrain_lr_decay_epochs=None, retrain_lr_decay_factor=None, retrain_seed=-1, rootdir='/root/autodl-tmp/OTfusion_pruning/otfusion/exp_sample', run_mode='benchmark', same_model=-1, save_result_file='sample.csv', second_model_name=None, sinkhorn_type='normal', skip_last_layer=False, skip_last_layer_type='average', skip_personal_idx=False, skip_retrain=-1, softmax_temperature=1, standardize_acts=False, sweep_id=90, sweep_name='exp_sample', temperature=20, tensorboard=False, tensorboard_root='./tensorboard', timestamp='2023-09-16_10-25-05_499081', tmap_stats=False, to_download=True, transform_acts=False, unbalanced=False, update_acts=False, weight_stats=True, width_ratio=1)\n",
      "refactored get_config\n",
      "------- Obtain dataloaders -------\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "------- Training independent models -------\n",
      "QWERTY: Enter Cifar 1\n",
      "number of epochs would b>>>>>>>e  300\n",
      "=======================================================\n",
      "=======================================================\n",
      "=======================================================\n",
      "Print the config {'dataset': 'Cifar10', 'model': 'vgg11_nobias', 'optimizer': 'SGD', 'optimizer_decay_at_epochs': [30, 60, 90, 120, 150, 180, 210, 240, 270], 'optimizer_decay_with_factor': 2.0, 'optimizer_learning_rate': 0.05, 'optimizer_momentum': 0.9, 'optimizer_weight_decay': 0.0005, 'batch_size': 128, 'num_epochs': 300, 'seed': 42}\n",
      "in _make_layers [Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), AvgPool2d(kernel_size=1, stride=1, padding=0)]\n",
      "Relu Inplace is  False\n",
      "model parameters are \n",
      " [torch.Size([64, 3, 3, 3]), torch.Size([128, 64, 3, 3]), torch.Size([256, 128, 3, 3]), torch.Size([256, 256, 3, 3]), torch.Size([512, 256, 3, 3]), torch.Size([512, 512, 3, 3]), torch.Size([512, 512, 3, 3]), torch.Size([512, 512, 3, 3]), torch.Size([10, 512])]\n",
      "Loading model at path ./cifar_models/model_0/best.checkpoint which had accuracy 0.9030999821424489 and at epoch 127\n",
      "QWERTY: Enter Cifar 1\n",
      "number of epochs would b>>>>>>>e  300\n",
      "=======================================================\n",
      "=======================================================\n",
      "=======================================================\n",
      "Print the config {'dataset': 'Cifar10', 'model': 'vgg11_nobias', 'optimizer': 'SGD', 'optimizer_decay_at_epochs': [30, 60, 90, 120, 150, 180, 210, 240, 270], 'optimizer_decay_with_factor': 2.0, 'optimizer_learning_rate': 0.05, 'optimizer_momentum': 0.9, 'optimizer_weight_decay': 0.0005, 'batch_size': 128, 'num_epochs': 300, 'seed': 42}\n",
      "in _make_layers [Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), AvgPool2d(kernel_size=1, stride=1, padding=0)]\n",
      "Relu Inplace is  False\n",
      "model parameters are \n",
      " [torch.Size([64, 3, 3, 3]), torch.Size([128, 64, 3, 3]), torch.Size([256, 128, 3, 3]), torch.Size([256, 256, 3, 3]), torch.Size([512, 256, 3, 3]), torch.Size([512, 512, 3, 3]), torch.Size([512, 512, 3, 3]), torch.Size([512, 512, 3, 3]), torch.Size([10, 512])]\n",
      "Loading model at path ./cifar_models/model_1/best.checkpoint which had accuracy 0.9049999803304669 and at epoch 134\n",
      "num_epochs---------------++++++ 300\n",
      "========================================\n",
      "========================================\n",
      "TEST:args.learning_rate,args.momentum 0.004 0.5\n",
      "========================================\n",
      "========================================\n",
      "========================================\n",
      "\n",
      "--------- Testing in global mode ---------\n",
      "size of test_loader dataset:  10000\n",
      "\n",
      "Test set: Avg. loss: 0.0003, Accuracy: 9019/10000 (90%)\n",
      "\n",
      "========================================\n",
      "========================================\n",
      "TEST:args.learning_rate,args.momentum 0.004 0.5\n",
      "========================================\n",
      "========================================\n",
      "========================================\n",
      "\n",
      "--------- Testing in global mode ---------\n",
      "size of test_loader dataset:  10000\n",
      "\n",
      "Test set: Avg. loss: 0.0003, Accuracy: 9046/10000 (90%)\n",
      "\n",
      "models [VGG(\n",
      "  (features): Sequential(\n",
      "    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    (1): ReLU()\n",
      "    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (3): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    (4): ReLU()\n",
      "    (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (6): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    (7): ReLU()\n",
      "    (8): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    (9): ReLU()\n",
      "    (10): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (11): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    (12): ReLU()\n",
      "    (13): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    (14): ReLU()\n",
      "    (15): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (16): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    (17): ReLU()\n",
      "    (18): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    (19): ReLU()\n",
      "    (20): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (21): AvgPool2d(kernel_size=1, stride=1, padding=0)\n",
      "  )\n",
      "  (classifier): Linear(in_features=512, out_features=10, bias=False)\n",
      "), VGG(\n",
      "  (features): Sequential(\n",
      "    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    (1): ReLU()\n",
      "    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (3): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    (4): ReLU()\n",
      "    (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (6): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    (7): ReLU()\n",
      "    (8): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    (9): ReLU()\n",
      "    (10): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (11): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    (12): ReLU()\n",
      "    (13): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    (14): ReLU()\n",
      "    (15): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (16): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    (17): ReLU()\n",
      "    (18): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    (19): ReLU()\n",
      "    (20): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (21): AvgPool2d(kernel_size=1, stride=1, padding=0)\n",
      "  )\n",
      "  (classifier): Linear(in_features=512, out_features=10, bias=False)\n",
      ")]\n",
      "Apply Unstructured L1 Pruning Globally (all conv layers)\n",
      "Apply Unstructured L1 Pruning Globally (all conv layers)\n",
      "layer features.0.weight has #params  1728\n",
      "layer features.3.weight has #params  73728\n",
      "layer features.6.weight has #params  294912\n",
      "layer features.8.weight has #params  589824\n",
      "layer features.11.weight has #params  1179648\n",
      "layer features.13.weight has #params  2359296\n",
      "layer features.16.weight has #params  2359296\n",
      "layer features.18.weight has #params  2359296\n",
      "layer classifier.weight has #params  5120\n",
      "Activation Timer start\n",
      "Activation Timer ends\n",
      "------- Geometric Ensembling -------\n",
      "Timer start\n",
      "Previous layer shape is  None\n",
      "Processing the coordinates to form ground_metric\n",
      "dont leave off the squaring of the ground metric\n",
      "returns a uniform measure of cardinality:  64\n",
      "returns a uniform measure of cardinality:  64\n",
      "the transport map is  tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')\n",
      "Ratio of trace to the matrix sum:  tensor(0.0469, device='cuda:0')\n",
      "Here, trace is 3.0 and matrix sum is 64.0 \n",
      "this is past correction for weight mode\n",
      "Shape of aligned wt is  torch.Size([64, 3, 9])\n",
      "Shape of fc_layer0_weight_data is  torch.Size([64, 3, 9])\n",
      "in _make_layers [Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), AvgPool2d(kernel_size=1, stride=1, padding=0)]\n",
      "Relu Inplace is  False\n",
      "model parameters are \n",
      " [torch.Size([64, 3, 3, 3]), torch.Size([128, 64, 3, 3]), torch.Size([256, 128, 3, 3]), torch.Size([256, 256, 3, 3]), torch.Size([512, 256, 3, 3]), torch.Size([512, 512, 3, 3]), torch.Size([512, 512, 3, 3]), torch.Size([512, 512, 3, 3]), torch.Size([10, 512])]\n",
      "len of model_state_dict is  9\n",
      "len of new_params is  1\n",
      "updated parameters for layer  features.0.weight\n",
      "\n",
      "--------- Testing in global mode ---------\n",
      "size of test_loader dataset:  10000\n",
      "\n",
      "Test set: Avg. loss: 0.0027, Accuracy: 1249/10000 (12%)\n",
      "\n",
      "accuracy after update is  12.49\n",
      "For layer idx 0, accuracy of the updated model is 12.49\n",
      "Previous layer shape is  torch.Size([64, 3, 3, 3])\n",
      "shape of layer: model 0 torch.Size([128, 64, 9])\n",
      "shape of layer: model 1 torch.Size([128, 64, 9])\n",
      "shape of previous transport map torch.Size([64, 64])\n",
      "Processing the coordinates to form ground_metric\n",
      "dont leave off the squaring of the ground metric\n",
      "returns a uniform measure of cardinality:  128\n",
      "returns a uniform measure of cardinality:  128\n",
      "the transport map is  tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        ...,\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0078],\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],\n",
      "       device='cuda:0')\n",
      "Ratio of trace to the matrix sum:  tensor(0.0078, device='cuda:0')\n",
      "Here, trace is 1.0 and matrix sum is 128.0 \n",
      "this is past correction for weight mode\n",
      "Shape of aligned wt is  torch.Size([128, 64, 9])\n",
      "Shape of fc_layer0_weight_data is  torch.Size([128, 64, 9])\n",
      "in _make_layers [Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), AvgPool2d(kernel_size=1, stride=1, padding=0)]\n",
      "Relu Inplace is  False\n",
      "model parameters are \n",
      " [torch.Size([64, 3, 3, 3]), torch.Size([128, 64, 3, 3]), torch.Size([256, 128, 3, 3]), torch.Size([256, 256, 3, 3]), torch.Size([512, 256, 3, 3]), torch.Size([512, 512, 3, 3]), torch.Size([512, 512, 3, 3]), torch.Size([512, 512, 3, 3]), torch.Size([10, 512])]\n",
      "len of model_state_dict is  9\n",
      "len of new_params is  2\n",
      "updated parameters for layer  features.0.weight\n",
      "updated parameters for layer  features.3.weight\n",
      "\n",
      "--------- Testing in global mode ---------\n",
      "size of test_loader dataset:  10000\n",
      "\n",
      "Test set: Avg. loss: 0.0024, Accuracy: 1036/10000 (10%)\n",
      "\n",
      "accuracy after update is  10.36\n",
      "For layer idx 1, accuracy of the updated model is 10.36\n",
      "Previous layer shape is  torch.Size([128, 64, 3, 3])\n",
      "shape of layer: model 0 torch.Size([256, 128, 9])\n",
      "shape of layer: model 1 torch.Size([256, 128, 9])\n",
      "shape of previous transport map torch.Size([128, 128])\n",
      "Processing the coordinates to form ground_metric\n",
      "dont leave off the squaring of the ground metric\n",
      "returns a uniform measure of cardinality:  256\n",
      "returns a uniform measure of cardinality:  256\n",
      "the transport map is  tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')\n",
      "Ratio of trace to the matrix sum:  tensor(0., device='cuda:0')\n",
      "Here, trace is 0.0 and matrix sum is 256.0 \n",
      "this is past correction for weight mode\n",
      "Shape of aligned wt is  torch.Size([256, 128, 9])\n",
      "Shape of fc_layer0_weight_data is  torch.Size([256, 128, 9])\n",
      "in _make_layers [Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), AvgPool2d(kernel_size=1, stride=1, padding=0)]\n",
      "Relu Inplace is  False\n",
      "model parameters are \n",
      " [torch.Size([64, 3, 3, 3]), torch.Size([128, 64, 3, 3]), torch.Size([256, 128, 3, 3]), torch.Size([256, 256, 3, 3]), torch.Size([512, 256, 3, 3]), torch.Size([512, 512, 3, 3]), torch.Size([512, 512, 3, 3]), torch.Size([512, 512, 3, 3]), torch.Size([10, 512])]\n",
      "len of model_state_dict is  9\n",
      "len of new_params is  3\n",
      "updated parameters for layer  features.0.weight\n",
      "updated parameters for layer  features.3.weight\n",
      "updated parameters for layer  features.6.weight\n",
      "\n",
      "--------- Testing in global mode ---------\n",
      "size of test_loader dataset:  10000\n",
      "\n",
      "Test set: Avg. loss: 0.0023, Accuracy: 1146/10000 (11%)\n",
      "\n",
      "accuracy after update is  11.46\n",
      "For layer idx 2, accuracy of the updated model is 11.46\n",
      "Previous layer shape is  torch.Size([256, 128, 3, 3])\n",
      "shape of layer: model 0 torch.Size([256, 256, 9])\n",
      "shape of layer: model 1 torch.Size([256, 256, 9])\n",
      "shape of previous transport map torch.Size([256, 256])\n",
      "Processing the coordinates to form ground_metric\n",
      "dont leave off the squaring of the ground metric\n",
      "returns a uniform measure of cardinality:  256\n",
      "returns a uniform measure of cardinality:  256\n",
      "the transport map is  tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        ...,\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.0039, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],\n",
      "       device='cuda:0')\n",
      "Ratio of trace to the matrix sum:  tensor(0.0078, device='cuda:0')\n",
      "Here, trace is 2.0 and matrix sum is 256.0 \n",
      "this is past correction for weight mode\n",
      "Shape of aligned wt is  torch.Size([256, 256, 9])\n",
      "Shape of fc_layer0_weight_data is  torch.Size([256, 256, 9])\n",
      "in _make_layers [Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), AvgPool2d(kernel_size=1, stride=1, padding=0)]\n",
      "Relu Inplace is  False\n",
      "model parameters are \n",
      " [torch.Size([64, 3, 3, 3]), torch.Size([128, 64, 3, 3]), torch.Size([256, 128, 3, 3]), torch.Size([256, 256, 3, 3]), torch.Size([512, 256, 3, 3]), torch.Size([512, 512, 3, 3]), torch.Size([512, 512, 3, 3]), torch.Size([512, 512, 3, 3]), torch.Size([10, 512])]\n",
      "len of model_state_dict is  9\n",
      "len of new_params is  4\n",
      "updated parameters for layer  features.0.weight\n",
      "updated parameters for layer  features.3.weight\n",
      "updated parameters for layer  features.6.weight\n",
      "updated parameters for layer  features.8.weight\n",
      "\n",
      "--------- Testing in global mode ---------\n",
      "size of test_loader dataset:  10000\n",
      "\n",
      "Test set: Avg. loss: 0.0024, Accuracy: 1249/10000 (12%)\n",
      "\n",
      "accuracy after update is  12.49\n",
      "For layer idx 3, accuracy of the updated model is 12.49\n",
      "Previous layer shape is  torch.Size([256, 256, 3, 3])\n",
      "shape of layer: model 0 torch.Size([512, 256, 9])\n",
      "shape of layer: model 1 torch.Size([512, 256, 9])\n",
      "shape of previous transport map torch.Size([256, 256])\n",
      "Processing the coordinates to form ground_metric\n",
      "dont leave off the squaring of the ground metric\n",
      "returns a uniform measure of cardinality:  512\n",
      "returns a uniform measure of cardinality:  512\n",
      "the transport map is  tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')\n",
      "Ratio of trace to the matrix sum:  tensor(0.0039, device='cuda:0')\n",
      "Here, trace is 2.0 and matrix sum is 512.0 \n",
      "this is past correction for weight mode\n",
      "Shape of aligned wt is  torch.Size([512, 256, 9])\n",
      "Shape of fc_layer0_weight_data is  torch.Size([512, 256, 9])\n",
      "in _make_layers [Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), AvgPool2d(kernel_size=1, stride=1, padding=0)]\n",
      "Relu Inplace is  False\n",
      "model parameters are \n",
      " [torch.Size([64, 3, 3, 3]), torch.Size([128, 64, 3, 3]), torch.Size([256, 128, 3, 3]), torch.Size([256, 256, 3, 3]), torch.Size([512, 256, 3, 3]), torch.Size([512, 512, 3, 3]), torch.Size([512, 512, 3, 3]), torch.Size([512, 512, 3, 3]), torch.Size([10, 512])]\n",
      "len of model_state_dict is  9\n",
      "len of new_params is  5\n",
      "updated parameters for layer  features.0.weight\n",
      "updated parameters for layer  features.3.weight\n",
      "updated parameters for layer  features.6.weight\n",
      "updated parameters for layer  features.8.weight\n",
      "updated parameters for layer  features.11.weight\n",
      "\n",
      "--------- Testing in global mode ---------\n",
      "size of test_loader dataset:  10000\n",
      "\n",
      "Test set: Avg. loss: 0.0027, Accuracy: 903/10000 (9%)\n",
      "\n",
      "accuracy after update is  9.03\n",
      "For layer idx 4, accuracy of the updated model is 9.03\n",
      "Previous layer shape is  torch.Size([512, 256, 3, 3])\n",
      "shape of layer: model 0 torch.Size([512, 512, 9])\n",
      "shape of layer: model 1 torch.Size([512, 512, 9])\n",
      "shape of previous transport map torch.Size([512, 512])\n",
      "Processing the coordinates to form ground_metric\n",
      "dont leave off the squaring of the ground metric\n",
      "returns a uniform measure of cardinality:  512\n",
      "returns a uniform measure of cardinality:  512\n",
      "the transport map is  tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')\n",
      "Ratio of trace to the matrix sum:  tensor(0.0020, device='cuda:0')\n",
      "Here, trace is 1.0 and matrix sum is 512.0 \n",
      "this is past correction for weight mode\n",
      "Shape of aligned wt is  torch.Size([512, 512, 9])\n",
      "Shape of fc_layer0_weight_data is  torch.Size([512, 512, 9])\n",
      "in _make_layers [Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), AvgPool2d(kernel_size=1, stride=1, padding=0)]\n",
      "Relu Inplace is  False\n",
      "model parameters are \n",
      " [torch.Size([64, 3, 3, 3]), torch.Size([128, 64, 3, 3]), torch.Size([256, 128, 3, 3]), torch.Size([256, 256, 3, 3]), torch.Size([512, 256, 3, 3]), torch.Size([512, 512, 3, 3]), torch.Size([512, 512, 3, 3]), torch.Size([512, 512, 3, 3]), torch.Size([10, 512])]\n",
      "len of model_state_dict is  9\n",
      "len of new_params is  6\n",
      "updated parameters for layer  features.0.weight\n",
      "updated parameters for layer  features.3.weight\n",
      "updated parameters for layer  features.6.weight\n",
      "updated parameters for layer  features.8.weight\n",
      "updated parameters for layer  features.11.weight\n",
      "updated parameters for layer  features.13.weight\n",
      "\n",
      "--------- Testing in global mode ---------\n",
      "size of test_loader dataset:  10000\n",
      "\n",
      "Test set: Avg. loss: 0.0024, Accuracy: 1410/10000 (14%)\n",
      "\n",
      "accuracy after update is  14.1\n",
      "For layer idx 5, accuracy of the updated model is 14.1\n",
      "Previous layer shape is  torch.Size([512, 512, 3, 3])\n",
      "shape of layer: model 0 torch.Size([512, 512, 9])\n",
      "shape of layer: model 1 torch.Size([512, 512, 9])\n",
      "shape of previous transport map torch.Size([512, 512])\n",
      "Processing the coordinates to form ground_metric\n",
      "dont leave off the squaring of the ground metric\n",
      "returns a uniform measure of cardinality:  512\n",
      "returns a uniform measure of cardinality:  512\n",
      "the transport map is  tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')\n",
      "Ratio of trace to the matrix sum:  tensor(0.0020, device='cuda:0')\n",
      "Here, trace is 1.0 and matrix sum is 512.0 \n",
      "this is past correction for weight mode\n",
      "Shape of aligned wt is  torch.Size([512, 512, 9])\n",
      "Shape of fc_layer0_weight_data is  torch.Size([512, 512, 9])\n",
      "in _make_layers [Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), AvgPool2d(kernel_size=1, stride=1, padding=0)]\n",
      "Relu Inplace is  False\n",
      "model parameters are \n",
      " [torch.Size([64, 3, 3, 3]), torch.Size([128, 64, 3, 3]), torch.Size([256, 128, 3, 3]), torch.Size([256, 256, 3, 3]), torch.Size([512, 256, 3, 3]), torch.Size([512, 512, 3, 3]), torch.Size([512, 512, 3, 3]), torch.Size([512, 512, 3, 3]), torch.Size([10, 512])]\n",
      "len of model_state_dict is  9\n",
      "len of new_params is  7\n",
      "updated parameters for layer  features.0.weight\n",
      "updated parameters for layer  features.3.weight\n",
      "updated parameters for layer  features.6.weight\n",
      "updated parameters for layer  features.8.weight\n",
      "updated parameters for layer  features.11.weight\n",
      "updated parameters for layer  features.13.weight\n",
      "updated parameters for layer  features.16.weight\n",
      "\n",
      "--------- Testing in global mode ---------\n",
      "size of test_loader dataset:  10000\n",
      "\n",
      "Test set: Avg. loss: 0.0022, Accuracy: 2217/10000 (22%)\n",
      "\n",
      "accuracy after update is  22.17\n",
      "For layer idx 6, accuracy of the updated model is 22.17\n",
      "Previous layer shape is  torch.Size([512, 512, 3, 3])\n",
      "shape of layer: model 0 torch.Size([512, 512, 9])\n",
      "shape of layer: model 1 torch.Size([512, 512, 9])\n",
      "shape of previous transport map torch.Size([512, 512])\n",
      "Processing the coordinates to form ground_metric\n",
      "dont leave off the squaring of the ground metric\n",
      "returns a uniform measure of cardinality:  512\n",
      "returns a uniform measure of cardinality:  512\n",
      "the transport map is  tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        [0.0020, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        ...,\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],\n",
      "       device='cuda:0')\n",
      "Ratio of trace to the matrix sum:  tensor(0., device='cuda:0')\n",
      "Here, trace is 0.0 and matrix sum is 512.0 \n",
      "this is past correction for weight mode\n",
      "Shape of aligned wt is  torch.Size([512, 512, 9])\n",
      "Shape of fc_layer0_weight_data is  torch.Size([512, 512, 9])\n",
      "in _make_layers [Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), AvgPool2d(kernel_size=1, stride=1, padding=0)]\n",
      "Relu Inplace is  False\n",
      "model parameters are \n",
      " [torch.Size([64, 3, 3, 3]), torch.Size([128, 64, 3, 3]), torch.Size([256, 128, 3, 3]), torch.Size([256, 256, 3, 3]), torch.Size([512, 256, 3, 3]), torch.Size([512, 512, 3, 3]), torch.Size([512, 512, 3, 3]), torch.Size([512, 512, 3, 3]), torch.Size([10, 512])]\n",
      "len of model_state_dict is  9\n",
      "len of new_params is  8\n",
      "updated parameters for layer  features.0.weight\n",
      "updated parameters for layer  features.3.weight\n",
      "updated parameters for layer  features.6.weight\n",
      "updated parameters for layer  features.8.weight\n",
      "updated parameters for layer  features.11.weight\n",
      "updated parameters for layer  features.13.weight\n",
      "updated parameters for layer  features.16.weight\n",
      "updated parameters for layer  features.18.weight\n",
      "\n",
      "--------- Testing in global mode ---------\n",
      "size of test_loader dataset:  10000\n",
      "\n",
      "Test set: Avg. loss: 0.0024, Accuracy: 582/10000 (6%)\n",
      "\n",
      "accuracy after update is  5.82\n",
      "For layer idx 7, accuracy of the updated model is 5.82\n",
      "Previous layer shape is  torch.Size([512, 512, 3, 3])\n",
      "shape of layer: model 0 torch.Size([10, 512])\n",
      "shape of layer: model 1 torch.Size([10, 512])\n",
      "shape of previous transport map torch.Size([512, 512])\n",
      "Processing the coordinates to form ground_metric\n",
      "dont leave off the squaring of the ground metric\n",
      "ground metric is  tensor([[1.4241, 2.4872, 2.7804, 2.8719, 2.7374, 2.8799, 2.7276, 2.6342, 2.5641,\n",
      "         2.5922],\n",
      "        [2.4889, 1.3253, 2.6732, 2.5914, 2.7584, 2.5501, 2.4174, 2.5215, 2.3048,\n",
      "         2.4101],\n",
      "        [2.7600, 2.6711, 1.3832, 2.9465, 2.9065, 2.8701, 2.7276, 2.8385, 2.7485,\n",
      "         2.7866],\n",
      "        [2.8352, 2.6791, 2.8919, 1.5430, 2.8605, 2.9159, 2.7574, 2.7980, 2.8164,\n",
      "         2.6354],\n",
      "        [2.7551, 2.5960, 2.8635, 2.8450, 1.5042, 2.7447, 2.7853, 2.6811, 2.7031,\n",
      "         2.7294],\n",
      "        [2.9190, 2.5396, 2.9469, 2.8416, 2.7272, 1.5502, 2.7781, 2.7303, 2.7977,\n",
      "         2.6697],\n",
      "        [2.6778, 2.4457, 2.7190, 2.9111, 2.6488, 2.8526, 1.3931, 2.6890, 2.4981,\n",
      "         2.5560],\n",
      "        [2.7394, 2.5496, 2.7799, 2.7903, 2.7151, 2.6590, 2.7190, 1.3924, 2.5583,\n",
      "         2.5121],\n",
      "        [2.5972, 2.3660, 2.7693, 2.6872, 2.7026, 2.8280, 2.5325, 2.6366, 1.2999,\n",
      "         2.4041],\n",
      "        [2.5918, 2.2730, 2.7816, 2.7576, 2.6697, 2.7273, 2.5482, 2.5290, 2.5207,\n",
      "         1.2878]], device='cuda:0', grad_fn=<PowBackward0>)\n",
      "returns a uniform measure of cardinality:  10\n",
      "returns a uniform measure of cardinality:  10\n",
      "the transport map is  tensor([[0.1000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0000, 0.1000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0000, 0.0000, 0.1000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.1000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.1000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.1000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.1000, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.1000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.1000,\n",
      "         0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.1000]], device='cuda:0')\n",
      "Ratio of trace to the matrix sum:  tensor(1., device='cuda:0')\n",
      "Here, trace is 9.99755859375 and matrix sum is 9.99755859375 \n",
      "this is past correction for weight mode\n",
      "Shape of aligned wt is  torch.Size([10, 512])\n",
      "Shape of fc_layer0_weight_data is  torch.Size([10, 512])\n",
      "in _make_layers [Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), AvgPool2d(kernel_size=1, stride=1, padding=0)]\n",
      "Relu Inplace is  False\n",
      "model parameters are \n",
      " [torch.Size([64, 3, 3, 3]), torch.Size([128, 64, 3, 3]), torch.Size([256, 128, 3, 3]), torch.Size([256, 256, 3, 3]), torch.Size([512, 256, 3, 3]), torch.Size([512, 512, 3, 3]), torch.Size([512, 512, 3, 3]), torch.Size([512, 512, 3, 3]), torch.Size([10, 512])]\n",
      "len of model_state_dict is  9\n",
      "len of new_params is  9\n",
      "updated parameters for layer  features.0.weight\n",
      "updated parameters for layer  features.3.weight\n",
      "updated parameters for layer  features.6.weight\n",
      "updated parameters for layer  features.8.weight\n",
      "updated parameters for layer  features.11.weight\n",
      "updated parameters for layer  features.13.weight\n",
      "updated parameters for layer  features.16.weight\n",
      "updated parameters for layer  features.18.weight\n",
      "updated parameters for layer  classifier.weight\n",
      "\n",
      "--------- Testing in global mode ---------\n",
      "size of test_loader dataset:  10000\n",
      "\n",
      "Test set: Avg. loss: 0.0003, Accuracy: 9019/10000 (90%)\n",
      "\n",
      "accuracy after update is  90.19\n",
      "For layer idx 8, accuracy of the updated model is 90.19\n",
      "using independent method\n",
      "in _make_layers [Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), AvgPool2d(kernel_size=1, stride=1, padding=0)]\n",
      "Relu Inplace is  False\n",
      "model parameters are \n",
      " [torch.Size([64, 3, 3, 3]), torch.Size([128, 64, 3, 3]), torch.Size([256, 128, 3, 3]), torch.Size([256, 256, 3, 3]), torch.Size([512, 256, 3, 3]), torch.Size([512, 512, 3, 3]), torch.Size([512, 512, 3, 3]), torch.Size([512, 512, 3, 3]), torch.Size([10, 512])]\n",
      "\n",
      "--------- Testing in global mode ---------\n",
      "size of test_loader dataset:  10000\n",
      "\n",
      "Test set: Avg. loss: 0.0023, Accuracy: 1091/10000 (11%)\n",
      "\n",
      "len of model parameters and avg aligned layers is  9 9\n",
      "len of model_state_dict is  9\n",
      "len of param_list is  9\n",
      "\n",
      "--------- Testing in global mode ---------\n",
      "size of test_loader dataset:  10000\n",
      "\n",
      "Test set: Avg. loss: 0.0015, Accuracy: 8544/10000 (85%)\n",
      "\n",
      "===============================================\n",
      "===============================================\n",
      "After Ensemble: 85.44\n",
      "===============================================\n",
      "===============================================\n",
      "Timer ends\n",
      "Time taken for geometric ensembling is 35.47849554568529 seconds\n",
      "------- Prediction based ensembling -------\n",
      "\n",
      "Test set: Avg. loss: 0.0003, Accuracy: 9133/10000 (91%)\n",
      "\n",
      "------- Naive ensembling of weights -------\n",
      "[torch.Size([64, 3, 3, 3]), torch.Size([64, 3, 3, 3])]\n",
      "torch.Size([64, 3, 3, 3])\n",
      "[torch.Size([128, 64, 3, 3]), torch.Size([128, 64, 3, 3])]\n",
      "torch.Size([128, 64, 3, 3])\n",
      "[torch.Size([256, 128, 3, 3]), torch.Size([256, 128, 3, 3])]\n",
      "torch.Size([256, 128, 3, 3])\n",
      "[torch.Size([256, 256, 3, 3]), torch.Size([256, 256, 3, 3])]\n",
      "torch.Size([256, 256, 3, 3])\n",
      "[torch.Size([512, 256, 3, 3]), torch.Size([512, 256, 3, 3])]\n",
      "torch.Size([512, 256, 3, 3])\n",
      "[torch.Size([512, 512, 3, 3]), torch.Size([512, 512, 3, 3])]\n",
      "torch.Size([512, 512, 3, 3])\n",
      "[torch.Size([512, 512, 3, 3]), torch.Size([512, 512, 3, 3])]\n",
      "torch.Size([512, 512, 3, 3])\n",
      "[torch.Size([512, 512, 3, 3]), torch.Size([512, 512, 3, 3])]\n",
      "torch.Size([512, 512, 3, 3])\n",
      "[torch.Size([10, 512]), torch.Size([10, 512])]\n",
      "torch.Size([10, 512])\n",
      "in _make_layers [Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), AvgPool2d(kernel_size=1, stride=1, padding=0)]\n",
      "Relu Inplace is  False\n",
      "model parameters are \n",
      " [torch.Size([64, 3, 3, 3]), torch.Size([128, 64, 3, 3]), torch.Size([256, 128, 3, 3]), torch.Size([256, 256, 3, 3]), torch.Size([512, 256, 3, 3]), torch.Size([512, 512, 3, 3]), torch.Size([512, 512, 3, 3]), torch.Size([512, 512, 3, 3]), torch.Size([10, 512])]\n",
      "\n",
      "--------- Testing in global mode ---------\n",
      "size of test_loader dataset:  10000\n",
      "\n",
      "Test set: Avg. loss: 0.0023, Accuracy: 1003/10000 (10%)\n",
      "\n",
      "\n",
      "--------- Testing in global mode ---------\n",
      "size of test_loader dataset:  10000\n",
      "\n",
      "Test set: Avg. loss: 0.0023, Accuracy: 1689/10000 (17%)\n",
      "\n",
      "-------- Retraining the models ---------\n",
      "Retraining model :  model_0\n",
      "num_epochs---------------+++))))))+++ 300\n",
      "num_epochs---------------+++))))))+++ 300\n",
      "lr is  0.05\n",
      "number of epochs would be  30\n",
      "num_epochs-------------- 30\n",
      "Epoch 000\n",
      "/root/miniconda3/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:129: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\"Detected call of `lr_scheduler.step()` before `optimizer.step()`. \"\n",
      "/root/miniconda3/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:154: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "accuracy: {'epoch': 0, 'value': 0.4336000000000002} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 0, 'value': 1.5597181462955472} ({'split': 'train'})\n",
      "accuracy: {'epoch': 0, 'value': 0.6713000297546385} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 0, 'value': 0.9801202356815338} ({'split': 'test'})\n",
      "Epoch 001\n",
      "accuracy: {'epoch': 1, 'value': 0.7297600000000003} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 1, 'value': 0.8241118642234808} ({'split': 'train'})\n",
      "accuracy: {'epoch': 1, 'value': 0.7458000421524047} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 1, 'value': 0.7581176578998565} ({'split': 'test'})\n",
      "Epoch 002\n",
      "accuracy: {'epoch': 2, 'value': 0.7813599999999998} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 2, 'value': 0.6696514800357815} ({'split': 'train'})\n",
      "accuracy: {'epoch': 2, 'value': 0.7658000349998475} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 2, 'value': 0.7249000668525696} ({'split': 'test'})\n",
      "Epoch 003\n",
      "accuracy: {'epoch': 3, 'value': 0.8145399999999998} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 3, 'value': 0.564621856822967} ({'split': 'train'})\n",
      "accuracy: {'epoch': 3, 'value': 0.7531000256538392} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 3, 'value': 0.7701649069786072} ({'split': 'test'})\n",
      "Epoch 004\n",
      "accuracy: {'epoch': 4, 'value': 0.8280599999999995} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 4, 'value': 0.5265779199981689} ({'split': 'train'})\n",
      "accuracy: {'epoch': 4, 'value': 0.7901000320911407} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 4, 'value': 0.6598710715770723} ({'split': 'test'})\n",
      "Epoch 005\n",
      "accuracy: {'epoch': 5, 'value': 0.8370799999999992} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 5, 'value': 0.49813019565582267} ({'split': 'train'})\n",
      "accuracy: {'epoch': 5, 'value': 0.7936000287532806} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 5, 'value': 0.6691171824932098} ({'split': 'test'})\n",
      "Epoch 006\n",
      "accuracy: {'epoch': 6, 'value': 0.84734} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 6, 'value': 0.46337060040473904} ({'split': 'train'})\n",
      "accuracy: {'epoch': 6, 'value': 0.7907000362873078} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 6, 'value': 0.6734202086925507} ({'split': 'test'})\n",
      "Epoch 007\n",
      "accuracy: {'epoch': 7, 'value': 0.8553199999999999} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 7, 'value': 0.441009681873322} ({'split': 'train'})\n",
      "accuracy: {'epoch': 7, 'value': 0.7926000356674195} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 7, 'value': 0.7215842723846435} ({'split': 'test'})\n",
      "Epoch 008\n",
      "accuracy: {'epoch': 8, 'value': 0.8580200000000007} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 8, 'value': 0.43672133791446677} ({'split': 'train'})\n",
      "accuracy: {'epoch': 8, 'value': 0.7659000396728515} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 8, 'value': 0.7722597718238831} ({'split': 'test'})\n",
      "Epoch 009\n",
      "accuracy: {'epoch': 9, 'value': 0.8680199999999998} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 9, 'value': 0.4041635980033878} ({'split': 'train'})\n",
      "accuracy: {'epoch': 9, 'value': 0.787600040435791} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 9, 'value': 0.6870992124080658} ({'split': 'test'})\n",
      "Epoch 010\n",
      "accuracy: {'epoch': 10, 'value': 0.8661799999999993} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 10, 'value': 0.4053042185401919} ({'split': 'train'})\n",
      "accuracy: {'epoch': 10, 'value': 0.7779000461101532} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 10, 'value': 0.7810189187526704} ({'split': 'test'})\n",
      "Epoch 011\n",
      "accuracy: {'epoch': 11, 'value': 0.8683600000000004} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 11, 'value': 0.4106298659515381} ({'split': 'train'})\n",
      "accuracy: {'epoch': 11, 'value': 0.78340003490448} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 11, 'value': 0.7214223980903626} ({'split': 'test'})\n",
      "Epoch 012\n",
      "accuracy: {'epoch': 12, 'value': 0.86858} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 12, 'value': 0.4060838037776946} ({'split': 'train'})\n",
      "accuracy: {'epoch': 12, 'value': 0.7826000392436981} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 12, 'value': 0.7811884343624116} ({'split': 'test'})\n",
      "Epoch 013\n",
      "accuracy: {'epoch': 13, 'value': 0.8745399999999994} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 13, 'value': 0.38643022662639603} ({'split': 'train'})\n",
      "accuracy: {'epoch': 13, 'value': 0.7889000356197357} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 13, 'value': 0.7206459224224089} ({'split': 'test'})\n",
      "Epoch 014\n",
      "accuracy: {'epoch': 14, 'value': 0.8773999999999998} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 14, 'value': 0.378996676464081} ({'split': 'train'})\n",
      "accuracy: {'epoch': 14, 'value': 0.7685000360012055} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 14, 'value': 0.8538445293903351} ({'split': 'test'})\n",
      "Epoch 015\n",
      "accuracy: {'epoch': 15, 'value': 0.8744800000000001} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 15, 'value': 0.38972430214881854} ({'split': 'train'})\n",
      "accuracy: {'epoch': 15, 'value': 0.7941000342369079} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 15, 'value': 0.6818698287010193} ({'split': 'test'})\n",
      "Epoch 016\n",
      "accuracy: {'epoch': 16, 'value': 0.885179999999999} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 16, 'value': 0.3622493941354754} ({'split': 'train'})\n",
      "accuracy: {'epoch': 16, 'value': 0.796500039100647} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 16, 'value': 0.781831818819046} ({'split': 'test'})\n",
      "Epoch 017\n",
      "accuracy: {'epoch': 17, 'value': 0.8827199999999997} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 17, 'value': 0.36708273889780063} ({'split': 'train'})\n",
      "accuracy: {'epoch': 17, 'value': 0.7708000421524047} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 17, 'value': 0.7427750289440155} ({'split': 'test'})\n",
      "Epoch 018\n",
      "accuracy: {'epoch': 18, 'value': 0.8781000000000007} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 18, 'value': 0.3761559832382206} ({'split': 'train'})\n",
      "accuracy: {'epoch': 18, 'value': 0.7759000360965729} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 18, 'value': 0.7987822651863099} ({'split': 'test'})\n",
      "Epoch 019\n",
      "accuracy: {'epoch': 19, 'value': 0.8807000000000008} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 19, 'value': 0.37083543067932184} ({'split': 'train'})\n",
      "accuracy: {'epoch': 19, 'value': 0.7736000418663025} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 19, 'value': 0.7551369607448578} ({'split': 'test'})\n",
      "Epoch 020\n",
      "accuracy: {'epoch': 20, 'value': 0.8820399999999996} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 20, 'value': 0.361744907941818} ({'split': 'train'})\n",
      "accuracy: {'epoch': 20, 'value': 0.7601000308990478} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 20, 'value': 0.8091262161731719} ({'split': 'test'})\n",
      "Epoch 021\n",
      "accuracy: {'epoch': 21, 'value': 0.8845800000000003} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 21, 'value': 0.35935356058120765} ({'split': 'train'})\n",
      "accuracy: {'epoch': 21, 'value': 0.7866000413894653} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 21, 'value': 0.7499563694000244} ({'split': 'test'})\n",
      "Epoch 022\n",
      "accuracy: {'epoch': 22, 'value': 0.8894200000000007} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 22, 'value': 0.3403456424617768} ({'split': 'train'})\n",
      "accuracy: {'epoch': 22, 'value': 0.7830000340938569} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 22, 'value': 0.7549613058567046} ({'split': 'test'})\n",
      "Epoch 023\n",
      "accuracy: {'epoch': 25, 'value': 0.8904999999999996} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 25, 'value': 0.34193298570632985} ({'split': 'train'})\n",
      "accuracy: {'epoch': 25, 'value': 0.7815000414848328} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 25, 'value': 0.7715516805648803} ({'split': 'test'})\n",
      "Epoch 026\n",
      "accuracy: {'epoch': 26, 'value': 0.8886600000000013} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 26, 'value': 0.3446659360599519} ({'split': 'train'})\n",
      "accuracy: {'epoch': 26, 'value': 0.7733000338077545} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 26, 'value': 0.7918766796588899} ({'split': 'test'})\n",
      "Epoch 027\n",
      "accuracy: {'epoch': 27, 'value': 0.8841799999999997} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 27, 'value': 0.35458068172454826} ({'split': 'train'})\n",
      "accuracy: {'epoch': 27, 'value': 0.8018000304698943} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 27, 'value': 0.6729396283626556} ({'split': 'test'})\n",
      "Epoch 028\n",
      "accuracy: {'epoch': 28, 'value': 0.8891599999999995} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 28, 'value': 0.3437848457622529} ({'split': 'train'})\n",
      "accuracy: {'epoch': 28, 'value': 0.7695000410079956} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 28, 'value': 0.8653207838535308} ({'split': 'test'})\n",
      "Epoch 029\n",
      "accuracy: {'epoch': 29, 'value': 0.8875800000000001} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 29, 'value': 0.35262730140209186} ({'split': 'train'})\n",
      "accuracy: {'epoch': 29, 'value': 0.7907000243663789} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 29, 'value': 0.7512350857257842} ({'split': 'test'})\n",
      "mean_test_accuracy <cifar_utils.accumulators.Mean object at 0x7f2aad59b3d0>\n",
      "QWERTY: Enter Cifar 2, acc 0.8018000304698943\n",
      "Retraining model :  model_1\n",
      "num_epochs---------------+++))))))+++ 30\n",
      "num_epochs---------------+++))))))+++ 30\n",
      "lr is  0.05\n",
      "number of epochs would be  30\n",
      "num_epochs-------------- 30\n",
      "Epoch 000\n",
      "accuracy: {'epoch': 0, 'value': 0.4562399999999997} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 0, 'value': 1.496698154916764} ({'split': 'train'})\n",
      "accuracy: {'epoch': 0, 'value': 0.7115000307559967} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 0, 'value': 0.8756510436534881} ({'split': 'test'})\n",
      "Epoch 001\n",
      "accuracy: {'epoch': 1, 'value': 0.7396399999999997} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 1, 'value': 0.8022125393104566} ({'split': 'train'})\n",
      "accuracy: {'epoch': 1, 'value': 0.761500036716461} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 1, 'value': 0.7302771985530853} ({'split': 'test'})\n",
      "Epoch 002\n",
      "accuracy: {'epoch': 2, 'value': 0.7912800000000003} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 2, 'value': 0.6435718333625792} ({'split': 'train'})\n",
      "accuracy: {'epoch': 2, 'value': 0.7750000417232513} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 2, 'value': 0.7061725318431854} ({'split': 'test'})\n",
      "Epoch 003\n",
      "accuracy: {'epoch': 3, 'value': 0.8162999999999998} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 3, 'value': 0.5576388818359374} ({'split': 'train'})\n",
      "accuracy: {'epoch': 3, 'value': 0.7696000397205353} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 3, 'value': 0.7479754745960236} ({'split': 'test'})\n",
      "Epoch 004\n",
      "accuracy: {'epoch': 4, 'value': 0.8327800000000001} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 4, 'value': 0.5099289708137514} ({'split': 'train'})\n",
      "accuracy: {'epoch': 4, 'value': 0.7567000329494477} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 4, 'value': 0.7603942573070526} ({'split': 'test'})\n",
      "Epoch 005\n",
      "accuracy: {'epoch': 5, 'value': 0.8438000000000007} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 5, 'value': 0.4749817955398563} ({'split': 'train'})\n",
      "accuracy: {'epoch': 5, 'value': 0.7863000333309174} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 5, 'value': 0.6923391103744505} ({'split': 'test'})\n",
      "Epoch 006\n",
      "accuracy: {'epoch': 6, 'value': 0.8524400000000002} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 6, 'value': 0.45001534381866426} ({'split': 'train'})\n",
      "accuracy: {'epoch': 6, 'value': 0.7624000370502473} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 6, 'value': 0.821131807565689} ({'split': 'test'})\n",
      "Epoch 007\n",
      "accuracy: {'epoch': 7, 'value': 0.8577799999999999} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 7, 'value': 0.4336183850860595} ({'split': 'train'})\n",
      "accuracy: {'epoch': 7, 'value': 0.8062000453472138} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 7, 'value': 0.6474069118499756} ({'split': 'test'})\n",
      "Epoch 008\n",
      "accuracy: {'epoch': 8, 'value': 0.8606999999999989} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 8, 'value': 0.4213998666000364} ({'split': 'train'})\n",
      "accuracy: {'epoch': 8, 'value': 0.8039000332355499} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 8, 'value': 0.6299012899398804} ({'split': 'test'})\n",
      "Epoch 009\n",
      "accuracy: {'epoch': 9, 'value': 0.8705800000000001} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 9, 'value': 0.39901255009412734} ({'split': 'train'})\n",
      "accuracy: {'epoch': 9, 'value': 0.7828000247478485} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 9, 'value': 0.7423131942749024} ({'split': 'test'})\n",
      "Epoch 010\n",
      "accuracy: {'epoch': 10, 'value': 0.8684999999999999} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 10, 'value': 0.40051394992291944} ({'split': 'train'})\n",
      "accuracy: {'epoch': 10, 'value': 0.78790003657341} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 10, 'value': 0.7405853033065797} ({'split': 'test'})\n",
      "Epoch 011\n",
      "accuracy: {'epoch': 11, 'value': 0.8790399999999987} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 11, 'value': 0.37526750086784366} ({'split': 'train'})\n",
      "accuracy: {'epoch': 11, 'value': 0.7529000401496887} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 11, 'value': 0.8844749212265015} ({'split': 'test'})\n",
      "Epoch 012\n",
      "accuracy: {'epoch': 12, 'value': 0.8727799999999999} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 12, 'value': 0.3940035326480871} ({'split': 'train'})\n",
      "accuracy: {'epoch': 12, 'value': 0.7744000375270844} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 12, 'value': 0.7863071262836456} ({'split': 'test'})\n",
      "Epoch 013\n",
      "accuracy: {'epoch': 13, 'value': 0.8805999999999999} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 13, 'value': 0.3762404103851319} ({'split': 'train'})\n",
      "accuracy: {'epoch': 13, 'value': 0.7987000286579132} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 13, 'value': 0.6668142974376678} ({'split': 'test'})\n",
      "Epoch 014\n",
      "accuracy: {'epoch': 14, 'value': 0.8776199999999988} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 14, 'value': 0.3790101517868039} ({'split': 'train'})\n",
      "accuracy: {'epoch': 14, 'value': 0.7832000434398652} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 14, 'value': 0.7833970129489899} ({'split': 'test'})\n",
      "Epoch 015\n",
      "accuracy: {'epoch': 15, 'value': 0.8863400000000005} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 15, 'value': 0.3520174220085142} ({'split': 'train'})\n",
      "accuracy: {'epoch': 15, 'value': 0.8015000402927398} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 15, 'value': 0.6719992339611054} ({'split': 'test'})\n",
      "Epoch 016\n",
      "accuracy: {'epoch': 16, 'value': 0.8879400000000001} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 16, 'value': 0.3468151648044589} ({'split': 'train'})\n",
      "accuracy: {'epoch': 16, 'value': 0.8029000401496886} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 16, 'value': 0.7086418151855468} ({'split': 'test'})\n",
      "Epoch 017\n",
      "accuracy: {'epoch': 17, 'value': 0.8865199999999999} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 17, 'value': 0.3528199974060061} ({'split': 'train'})\n",
      "accuracy: {'epoch': 17, 'value': 0.7965000391006469} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 17, 'value': 0.6700470209121704} ({'split': 'test'})\n",
      "Epoch 018\n",
      "accuracy: {'epoch': 18, 'value': 0.8836800000000002} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 18, 'value': 0.36027796088695474} ({'split': 'train'})\n",
      "accuracy: {'epoch': 18, 'value': 0.7464000344276428} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 18, 'value': 0.9345724999904633} ({'split': 'test'})\n",
      "Epoch 019\n",
      "accuracy: {'epoch': 19, 'value': 0.8822200000000003} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 19, 'value': 0.3631704073953623} ({'split': 'train'})\n",
      "accuracy: {'epoch': 19, 'value': 0.7701000392436981} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 19, 'value': 0.8051477551460267} ({'split': 'test'})\n",
      "Epoch 020\n",
      "accuracy: {'epoch': 20, 'value': 0.8887999999999988} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 20, 'value': 0.3430306144428252} ({'split': 'train'})\n",
      "accuracy: {'epoch': 20, 'value': 0.7840000450611114} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 20, 'value': 0.727930200099945} ({'split': 'test'})\n",
      "Epoch 021\n",
      "accuracy: {'epoch': 21, 'value': 0.8862800000000005} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 21, 'value': 0.35263646491050715} ({'split': 'train'})\n",
      "accuracy: {'epoch': 21, 'value': 0.7920000433921814} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 21, 'value': 0.7039123952388763} ({'split': 'test'})\n",
      "Epoch 022\n",
      "accuracy: {'epoch': 22, 'value': 0.8949599999999996} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 22, 'value': 0.32281504945755} ({'split': 'train'})\n",
      "accuracy: {'epoch': 22, 'value': 0.7661000311374664} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 22, 'value': 0.736232316493988} ({'split': 'test'})\n",
      "Epoch 023\n",
      "accuracy: {'epoch': 23, 'value': 0.8886800000000004} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 23, 'value': 0.34536652940750107} ({'split': 'train'})\n",
      "accuracy: {'epoch': 23, 'value': 0.7937000453472137} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 23, 'value': 0.7506037533283233} ({'split': 'test'})\n",
      "Epoch 024\n",
      "accuracy: {'epoch': 24, 'value': 0.8912799999999999} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 24, 'value': 0.33646515978813146} ({'split': 'train'})\n",
      "accuracy: {'epoch': 24, 'value': 0.7760000348091126} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 24, 'value': 0.800137460231781} ({'split': 'test'})\n",
      "Epoch 025\n",
      "accuracy: {'epoch': 25, 'value': 0.891459999999999} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 25, 'value': 0.34031870148658755} ({'split': 'train'})\n",
      "accuracy: {'epoch': 25, 'value': 0.7586000263690948} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 25, 'value': 0.8300036609172821} ({'split': 'test'})\n",
      "Epoch 026\n",
      "accuracy: {'epoch': 26, 'value': 0.8915000000000001} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 26, 'value': 0.33868253534316983} ({'split': 'train'})\n",
      "accuracy: {'epoch': 26, 'value': 0.7952000439167023} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 26, 'value': 0.7199432194232941} ({'split': 'test'})\n",
      "Epoch 027\n",
      "accuracy: {'epoch': 27, 'value': 0.8945200000000003} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 27, 'value': 0.32929059752464246} ({'split': 'train'})\n",
      "accuracy: {'epoch': 27, 'value': 0.7878000378608704} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 27, 'value': 0.7372381269931794} ({'split': 'test'})\n",
      "Epoch 028\n",
      "accuracy: {'epoch': 28, 'value': 0.8939799999999997} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 28, 'value': 0.3328656266784667} ({'split': 'train'})\n",
      "accuracy: {'epoch': 28, 'value': 0.7779000461101531} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 28, 'value': 0.7272949159145355} ({'split': 'test'})\n",
      "Epoch 029\n",
      "accuracy: {'epoch': 29, 'value': 0.8935199999999992} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 29, 'value': 0.33136307847022994} ({'split': 'train'})\n",
      "accuracy: {'epoch': 29, 'value': 0.7899000406265259} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 29, 'value': 0.8140536367893219} ({'split': 'test'})\n",
      "mean_test_accuracy <cifar_utils.accumulators.Mean object at 0x7f2aad59b490>\n",
      "QWERTY: Enter Cifar 2, acc 0.8062000453472138\n",
      "Retraining model :  geometric\n",
      "num_epochs---------------+++))))))+++ 30\n",
      "num_epochs---------------+++))))))+++ 30\n",
      "lr is  0.05\n",
      "number of epochs would be  30\n",
      "num_epochs-------------- 30\n",
      "Epoch 000\n",
      "accuracy: {'epoch': 0, 'value': 0.10548000000000002} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 0, 'value': 2.3047519077301026} ({'split': 'train'})\n",
      "accuracy: {'epoch': 0, 'value': 0.10000000447034836} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 0, 'value': 2.3025848865509033} ({'split': 'test'})\n",
      "Epoch 001\n",
      "accuracy: {'epoch': 1, 'value': 0.10002000000000001} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 1, 'value': 2.302584886703491} ({'split': 'train'})\n",
      "accuracy: {'epoch': 1, 'value': 0.10000000521540642} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 1, 'value': 2.3025848865509033} ({'split': 'test'})\n",
      "Epoch 002\n",
      "accuracy: {'epoch': 2, 'value': 0.09999999999999995} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 2, 'value': 2.302584886703491} ({'split': 'train'})\n",
      "accuracy: {'epoch': 2, 'value': 0.10000000447034835} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 2, 'value': 2.3025848865509033} ({'split': 'test'})\n",
      "Epoch 003\n",
      "accuracy: {'epoch': 3, 'value': 0.09999999999999995} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 3, 'value': 2.302584886703491} ({'split': 'train'})\n",
      "accuracy: {'epoch': 3, 'value': 0.1000000037252903} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 3, 'value': 2.3025848865509033} ({'split': 'test'})\n",
      "Epoch 004\n",
      "accuracy: {'epoch': 4, 'value': 0.09999999999999994} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 4, 'value': 2.302584886703491} ({'split': 'train'})\n",
      "accuracy: {'epoch': 4, 'value': 0.1000000037252903} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 4, 'value': 2.3025848865509033} ({'split': 'test'})\n",
      "Epoch 005\n",
      "accuracy: {'epoch': 5, 'value': 0.10000000000000009} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 5, 'value': 2.302584886703491} ({'split': 'train'})\n",
      "accuracy: {'epoch': 5, 'value': 0.10000000447034836} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 5, 'value': 2.3025848865509033} ({'split': 'test'})\n",
      "Epoch 006\n",
      "accuracy: {'epoch': 6, 'value': 0.09999999999999999} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 6, 'value': 2.302584886703491} ({'split': 'train'})\n",
      "accuracy: {'epoch': 6, 'value': 0.10000000447034835} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 6, 'value': 2.3025848865509033} ({'split': 'test'})\n",
      "Epoch 007\n",
      "accuracy: {'epoch': 7, 'value': 0.09999999999999991} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 7, 'value': 2.302584886703491} ({'split': 'train'})\n",
      "accuracy: {'epoch': 7, 'value': 0.1000000037252903} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 7, 'value': 2.3025848865509033} ({'split': 'test'})\n",
      "Epoch 008\n",
      "accuracy: {'epoch': 8, 'value': 0.09999999999999995} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 8, 'value': 2.302584886703491} ({'split': 'train'})\n",
      "accuracy: {'epoch': 8, 'value': 0.1000000037252903} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 8, 'value': 2.3025848865509033} ({'split': 'test'})\n",
      "Epoch 009\n",
      "accuracy: {'epoch': 9, 'value': 0.09999999999999996} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 9, 'value': 2.302584886703491} ({'split': 'train'})\n",
      "accuracy: {'epoch': 9, 'value': 0.10000000298023225} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 9, 'value': 2.3025848865509033} ({'split': 'test'})\n",
      "Epoch 010\n",
      "accuracy: {'epoch': 10, 'value': 0.09999999999999995} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 10, 'value': 2.302584886703491} ({'split': 'train'})\n",
      "accuracy: {'epoch': 10, 'value': 0.10000000447034836} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 10, 'value': 2.3025848865509033} ({'split': 'test'})\n",
      "Epoch 011\n",
      "accuracy: {'epoch': 11, 'value': 0.10000000000000003} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 11, 'value': 2.302584886703491} ({'split': 'train'})\n",
      "accuracy: {'epoch': 11, 'value': 0.10000000596046449} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 11, 'value': 2.3025848865509033} ({'split': 'test'})\n",
      "Epoch 012\n",
      "accuracy: {'epoch': 12, 'value': 0.10000000000000006} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 12, 'value': 2.302584886703491} ({'split': 'train'})\n",
      "accuracy: {'epoch': 12, 'value': 0.10000000521540643} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 12, 'value': 2.3025848865509033} ({'split': 'test'})\n",
      "Epoch 013\n",
      "accuracy: {'epoch': 13, 'value': 0.09999999999999999} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 13, 'value': 2.302584886703491} ({'split': 'train'})\n",
      "accuracy: {'epoch': 13, 'value': 0.10000000447034837} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 13, 'value': 2.3025848865509033} ({'split': 'test'})\n",
      "Epoch 014\n",
      "accuracy: {'epoch': 14, 'value': 0.09999999999999998} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 14, 'value': 2.302584886703491} ({'split': 'train'})\n",
      "accuracy: {'epoch': 14, 'value': 0.10000000447034836} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 14, 'value': 2.3025848865509033} ({'split': 'test'})\n",
      "Epoch 015\n",
      "accuracy: {'epoch': 15, 'value': 0.09999999999999994} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 15, 'value': 2.302584886703491} ({'split': 'train'})\n",
      "accuracy: {'epoch': 15, 'value': 0.10000000670552253} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 15, 'value': 2.3025848865509033} ({'split': 'test'})\n",
      "Epoch 016\n",
      "accuracy: {'epoch': 16, 'value': 0.10000000000000009} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 16, 'value': 2.302584886703491} ({'split': 'train'})\n",
      "accuracy: {'epoch': 16, 'value': 0.10000000447034836} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 16, 'value': 2.3025848865509033} ({'split': 'test'})\n",
      "Epoch 017\n",
      "accuracy: {'epoch': 17, 'value': 0.10000000000000002} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 17, 'value': 2.302584886703491} ({'split': 'train'})\n",
      "accuracy: {'epoch': 17, 'value': 0.10000000521540642} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 17, 'value': 2.3025848865509033} ({'split': 'test'})\n",
      "Epoch 018\n",
      "accuracy: {'epoch': 18, 'value': 0.09999999999999996} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 18, 'value': 2.302584886703491} ({'split': 'train'})\n",
      "accuracy: {'epoch': 18, 'value': 0.10000000521540642} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 18, 'value': 2.3025848865509033} ({'split': 'test'})\n",
      "Epoch 019\n",
      "accuracy: {'epoch': 19, 'value': 0.1000000000000001} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 19, 'value': 2.302584886703491} ({'split': 'train'})\n",
      "accuracy: {'epoch': 19, 'value': 0.10000000447034836} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 19, 'value': 2.3025848865509033} ({'split': 'test'})\n",
      "Epoch 020\n",
      "accuracy: {'epoch': 20, 'value': 0.1} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 20, 'value': 2.302584886703491} ({'split': 'train'})\n",
      "accuracy: {'epoch': 20, 'value': 0.10000000447034836} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 20, 'value': 2.3025848865509033} ({'split': 'test'})\n",
      "Epoch 021\n",
      "accuracy: {'epoch': 21, 'value': 0.10000000000000002} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 21, 'value': 2.302584886703491} ({'split': 'train'})\n",
      "accuracy: {'epoch': 21, 'value': 0.10000000596046447} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 21, 'value': 2.3025848865509033} ({'split': 'test'})\n",
      "Epoch 022\n",
      "accuracy: {'epoch': 22, 'value': 0.09999999999999998} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 22, 'value': 2.302584886703491} ({'split': 'train'})\n",
      "accuracy: {'epoch': 22, 'value': 0.10000000447034836} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 22, 'value': 2.3025848865509033} ({'split': 'test'})\n",
      "Epoch 023\n",
      "accuracy: {'epoch': 23, 'value': 0.10000000000000013} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 23, 'value': 2.302584886703491} ({'split': 'train'})\n",
      "accuracy: {'epoch': 23, 'value': 0.10000000521540642} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 23, 'value': 2.3025848865509033} ({'split': 'test'})\n",
      "Epoch 024\n",
      "accuracy: {'epoch': 24, 'value': 0.10000000000000003} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 24, 'value': 2.302584886703491} ({'split': 'train'})\n",
      "accuracy: {'epoch': 24, 'value': 0.10000000521540642} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 24, 'value': 2.3025848865509033} ({'split': 'test'})\n",
      "Epoch 025\n",
      "accuracy: {'epoch': 25, 'value': 0.10000000000000005} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 25, 'value': 2.302584886703491} ({'split': 'train'})\n",
      "accuracy: {'epoch': 25, 'value': 0.10000000521540642} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 25, 'value': 2.3025848865509033} ({'split': 'test'})\n",
      "Epoch 026\n",
      "accuracy: {'epoch': 26, 'value': 0.10000000000000002} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 26, 'value': 2.302584886703491} ({'split': 'train'})\n",
      "accuracy: {'epoch': 26, 'value': 0.1000000037252903} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 26, 'value': 2.3025848865509033} ({'split': 'test'})\n",
      "Epoch 027\n",
      "accuracy: {'epoch': 27, 'value': 0.09999999999999995} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 27, 'value': 2.302584886703491} ({'split': 'train'})\n",
      "accuracy: {'epoch': 27, 'value': 0.10000000447034836} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 27, 'value': 2.3025848865509033} ({'split': 'test'})\n",
      "Epoch 028\n",
      "accuracy: {'epoch': 28, 'value': 0.1} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 28, 'value': 2.302584886703491} ({'split': 'train'})\n",
      "accuracy: {'epoch': 28, 'value': 0.10000000447034836} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 28, 'value': 2.3025848865509033} ({'split': 'test'})\n",
      "Epoch 029\n",
      "accuracy: {'epoch': 29, 'value': 0.10000000000000012} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 29, 'value': 2.302584886703491} ({'split': 'train'})\n",
      "accuracy: {'epoch': 29, 'value': 0.10000000596046449} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 29, 'value': 2.3025848865509033} ({'split': 'test'})\n",
      "mean_test_accuracy <cifar_utils.accumulators.Mean object at 0x7f2aad58bd60>\n",
      "QWERTY: Enter Cifar 2, acc 0.10000000670552253\n",
      "Retraining model :  naive_averaging\n",
      "num_epochs---------------+++))))))+++ 30\n",
      "num_epochs---------------+++))))))+++ 30\n",
      "lr is  0.05\n",
      "number of epochs would be  30\n",
      "num_epochs-------------- 30\n",
      "Epoch 000\n",
      "accuracy: {'epoch': 0, 'value': 0.1818000000000001} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 0, 'value': 2.1084258403396605} ({'split': 'train'})\n",
      "accuracy: {'epoch': 0, 'value': 0.29000001251697544} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 0, 'value': 2.00372257232666} ({'split': 'test'})\n",
      "Epoch 001\n",
      "accuracy: {'epoch': 1, 'value': 0.41307999999999967} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 1, 'value': 1.5593513153457639} ({'split': 'train'})\n",
      "accuracy: {'epoch': 1, 'value': 0.4804000228643417} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 1, 'value': 1.436147403717041} ({'split': 'test'})\n",
      "Epoch 002\n",
      "accuracy: {'epoch': 2, 'value': 0.5633599999999995} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 2, 'value': 1.2410577666091918} ({'split': 'train'})\n",
      "accuracy: {'epoch': 2, 'value': 0.6057000219821931} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 2, 'value': 1.1310455203056335} ({'split': 'test'})\n",
      "Epoch 003\n",
      "accuracy: {'epoch': 3, 'value': 0.6448999999999999} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 3, 'value': 1.0355456029510495} ({'split': 'train'})\n",
      "accuracy: {'epoch': 3, 'value': 0.6595000207424163} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 3, 'value': 1.0092097103595734} ({'split': 'test'})\n",
      "Epoch 004\n",
      "accuracy: {'epoch': 4, 'value': 0.6929599999999994} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 4, 'value': 0.9104726698303223} ({'split': 'train'})\n",
      "accuracy: {'epoch': 4, 'value': 0.6862000286579132} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 4, 'value': 0.9560130357742309} ({'split': 'test'})\n",
      "Epoch 005\n",
      "accuracy: {'epoch': 5, 'value': 0.7254400000000005} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 5, 'value': 0.8230020975494388} ({'split': 'train'})\n",
      "accuracy: {'epoch': 5, 'value': 0.7016000390052795} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 5, 'value': 0.931545615196228} ({'split': 'test'})\n",
      "Epoch 006\n",
      "accuracy: {'epoch': 6, 'value': 0.7560800000000003} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 6, 'value': 0.7380233353424076} ({'split': 'train'})\n",
      "accuracy: {'epoch': 6, 'value': 0.7365000367164611} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 6, 'value': 0.7930045664310454} ({'split': 'test'})\n",
      "Epoch 007\n",
      "accuracy: {'epoch': 7, 'value': 0.7677000000000005} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 7, 'value': 0.7053903690338134} ({'split': 'train'})\n",
      "accuracy: {'epoch': 7, 'value': 0.7321000277996064} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 7, 'value': 0.8166541755199432} ({'split': 'test'})\n",
      "Epoch 008\n",
      "accuracy: {'epoch': 8, 'value': 0.7808999999999993} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 8, 'value': 0.670510867290497} ({'split': 'train'})\n",
      "accuracy: {'epoch': 8, 'value': 0.7476000308990479} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 8, 'value': 0.7851340174674988} ({'split': 'test'})\n",
      "Epoch 009\n",
      "accuracy: {'epoch': 9, 'value': 0.7931599999999998} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 9, 'value': 0.6288634703350067} ({'split': 'train'})\n",
      "accuracy: {'epoch': 9, 'value': 0.714900028705597} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 9, 'value': 0.8830189943313599} ({'split': 'test'})\n",
      "Epoch 010\n",
      "accuracy: {'epoch': 10, 'value': 0.8060800000000004} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 10, 'value': 0.5915442943668356} ({'split': 'train'})\n",
      "accuracy: {'epoch': 10, 'value': 0.7544000327587128} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 10, 'value': 0.7752367436885833} ({'split': 'test'})\n",
      "Epoch 011\n",
      "accuracy: {'epoch': 11, 'value': 0.8147400000000006} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 11, 'value': 0.5666357011795046} ({'split': 'train'})\n",
      "accuracy: {'epoch': 11, 'value': 0.7673000276088714} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 11, 'value': 0.7452292919158935} ({'split': 'test'})\n",
      "Epoch 012\n",
      "accuracy: {'epoch': 12, 'value': 0.8215399999999993} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 12, 'value': 0.5385940451908102} ({'split': 'train'})\n",
      "accuracy: {'epoch': 12, 'value': 0.7758000373840332} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 12, 'value': 0.7684906601905823} ({'split': 'test'})\n",
      "Epoch 013\n",
      "accuracy: {'epoch': 13, 'value': 0.8263999999999997} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 13, 'value': 0.5294094088745124} ({'split': 'train'})\n",
      "accuracy: {'epoch': 13, 'value': 0.7405000329017639} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 13, 'value': 0.8507984519004823} ({'split': 'test'})\n",
      "Epoch 014\n",
      "accuracy: {'epoch': 14, 'value': 0.8327999999999992} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 14, 'value': 0.5077233056163789} ({'split': 'train'})\n",
      "accuracy: {'epoch': 14, 'value': 0.7593000471591949} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 14, 'value': 0.8864212810993195} ({'split': 'test'})\n",
      "Epoch 015\n",
      "accuracy: {'epoch': 15, 'value': 0.8390800000000007} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 15, 'value': 0.49282721412658687} ({'split': 'train'})\n",
      "accuracy: {'epoch': 15, 'value': 0.7870000302791595} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 15, 'value': 0.6905732035636902} ({'split': 'test'})\n",
      "Epoch 016\n",
      "accuracy: {'epoch': 16, 'value': 0.8408599999999995} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 16, 'value': 0.48485248830795274} ({'split': 'train'})\n",
      "accuracy: {'epoch': 16, 'value': 0.7647000312805176} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 16, 'value': 0.7677149832248688} ({'split': 'test'})\n",
      "Epoch 017\n",
      "accuracy: {'epoch': 17, 'value': 0.8466000000000005} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 17, 'value': 0.46630572265625014} ({'split': 'train'})\n",
      "accuracy: {'epoch': 17, 'value': 0.7615000307559967} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 17, 'value': 0.774495267868042} ({'split': 'test'})\n",
      "Epoch 018\n",
      "accuracy: {'epoch': 18, 'value': 0.8548000000000004} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 18, 'value': 0.44244194842338586} ({'split': 'train'})\n",
      "accuracy: {'epoch': 18, 'value': 0.7836000323295593} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 18, 'value': 0.7346784889698028} ({'split': 'test'})\n",
      "Epoch 019\n",
      "accuracy: {'epoch': 19, 'value': 0.8513599999999995} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 19, 'value': 0.4528655918788907} ({'split': 'train'})\n",
      "accuracy: {'epoch': 19, 'value': 0.7727000296115876} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 19, 'value': 0.7576559960842133} ({'split': 'test'})\n",
      "Epoch 020\n",
      "accuracy: {'epoch': 20, 'value': 0.8535000000000004} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 20, 'value': 0.4416997969341278} ({'split': 'train'})\n",
      "accuracy: {'epoch': 20, 'value': 0.7704000413417816} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 20, 'value': 0.7616524457931518} ({'split': 'test'})\n",
      "Epoch 021\n",
      "accuracy: {'epoch': 21, 'value': 0.8582400000000001} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 21, 'value': 0.43410323319435107} ({'split': 'train'})\n",
      "accuracy: {'epoch': 21, 'value': 0.7851000368595124} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 21, 'value': 0.7218056201934814} ({'split': 'test'})\n",
      "Epoch 022\n",
      "accuracy: {'epoch': 22, 'value': 0.8590800000000013} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 22, 'value': 0.4304526620197294} ({'split': 'train'})\n",
      "accuracy: {'epoch': 22, 'value': 0.7629000306129455} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 22, 'value': 0.7964264631271363} ({'split': 'test'})\n",
      "Epoch 023\n",
      "accuracy: {'epoch': 23, 'value': 0.8570999999999996} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 23, 'value': 0.43874537193298346} ({'split': 'train'})\n",
      "accuracy: {'epoch': 23, 'value': 0.7834000408649444} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 23, 'value': 0.7295995056629181} ({'split': 'test'})\n",
      "Epoch 024\n",
      "accuracy: {'epoch': 24, 'value': 0.8617} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 24, 'value': 0.4195485930061342} ({'split': 'train'})\n",
      "accuracy: {'epoch': 24, 'value': 0.7871000409126282} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 24, 'value': 0.7368532538414002} ({'split': 'test'})\n",
      "Epoch 025\n",
      "accuracy: {'epoch': 25, 'value': 0.8661000000000009} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 25, 'value': 0.41145368385314945} ({'split': 'train'})\n",
      "accuracy: {'epoch': 25, 'value': 0.7763000369071961} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 25, 'value': 0.7468551576137542} ({'split': 'test'})\n",
      "Epoch 026\n",
      "accuracy: {'epoch': 26, 'value': 0.8684400000000007} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 26, 'value': 0.4110174777984616} ({'split': 'train'})\n",
      "accuracy: {'epoch': 26, 'value': 0.7855000376701355} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 26, 'value': 0.7419838845729827} ({'split': 'test'})\n",
      "Epoch 027\n",
      "accuracy: {'epoch': 27, 'value': 0.8673} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 27, 'value': 0.4019548893642424} ({'split': 'train'})\n",
      "accuracy: {'epoch': 27, 'value': 0.7728000342845918} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 27, 'value': 0.8169683396816254} ({'split': 'test'})\n",
      "Epoch 028\n",
      "accuracy: {'epoch': 28, 'value': 0.8700799999999992} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 28, 'value': 0.40044418381691016} ({'split': 'train'})\n",
      "accuracy: {'epoch': 28, 'value': 0.7898000478744507} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 28, 'value': 0.7550821602344513} ({'split': 'test'})\n",
      "Epoch 029\n",
      "accuracy: {'epoch': 29, 'value': 0.8686200000000005} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 29, 'value': 0.4080447007179266} ({'split': 'train'})\n",
      "accuracy: {'epoch': 29, 'value': 0.7714000344276428} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 29, 'value': 0.8107502162456512} ({'split': 'test'})\n",
      "mean_test_accuracy <cifar_utils.accumulators.Mean object at 0x7f2aad5a0820>\n",
      "QWERTY: Enter Cifar 2, acc 0.7898000478744507\n",
      "----- Saved results at sample.csv ------\n",
      "{'exp_name': 'exp_2023-09-16_10-25-05_499081', 'model0_acc': 90.30999821424489, 'model1_acc': 90.4999980330467, 'geometric_acc': 85.44, 'prediction_acc': 91.33, 'naive_acc': 16.89, 'geometric_gain': -5.059998033046696, 'geometric_gain_%': -5.591158169085267, 'prediction_gain': 0.8300019669533043, 'prediction_gain_%': 0.917129265185424, 'relative_loss_wrt_prediction': 6.508287434270691, 'model0_aligned': 90.19, 'geometric_time': 35.47849554568529, 'retrain_geometric_best': 10.000000670552254, 'retrain_naive_best': 78.98000478744507, 'retrain_model0_best': 80.18000304698944, 'retrain_model1_best': 80.62000453472137, 'retrain_epochs': 30}\n",
      "FYI: the parameters were: \n",
      " Namespace(act_bug=False, act_num_samples=100, activation_histograms=False, activation_mode=None, activation_seed=21, activation_time=2.627447247505188e-05, alpha=0.7, baseroot='/root/autodl-tmp/OTfusion_pruning/otfusion', batch_size_test=1000, batch_size_train=64, center_acts=False, choice='0 2 4 6 8', cifar_style_data=False, ckpt_type='best', clip_gm=False, clip_max=5, clip_min=0, config={'dataset': 'Cifar10', 'model': 'vgg11_nobias', 'optimizer': 'SGD', 'optimizer_decay_at_epochs': [30, 60, 90, 120, 150, 180, 210, 240, 270], 'optimizer_decay_with_factor': 2.0, 'optimizer_learning_rate': 0.05, 'optimizer_momentum': 0.9, 'optimizer_weight_decay': 0.0005, 'batch_size': 128, 'num_epochs': 30, 'seed': 42, 'nick': 'naive_averaging', 'start_acc': 16.89}, config_dir='/root/autodl-tmp/OTfusion_pruning/otfusion/exp_sample/configurations', config_file=None, correction=True, csv_dir='/root/autodl-tmp/OTfusion_pruning/otfusion/exp_sample/csv', dataset='Cifar10', debug=False, deprecated=None, deterministic=False, diff_init=False, disable_bias=True, dist_epochs=60, dist_normalize=False, dump_final_models=False, dump_model=False, enable_dropout=False, ensemble_step=0.5, eval_aligned=True, exact=True, exp_name='exp_2023-09-16_10-25-05_499081', experiment_dir='experiment1', geom_ensemble_type='wts', geometric_time=35.47849554568529, gpu_id=0, gromov=False, gromov_loss='square_loss', ground_metric='euclidean', ground_metric_eff=True, ground_metric_normalize='none', handle_skips=False, importance=None, learning_rate=0.004, load_model_dir='./cifar_models', load_models='', log_interval=100, model0_aligned_acc=90.19, model0_aligned_acc_layer_0=12.49, model0_aligned_acc_layer_1=10.36, model0_aligned_acc_layer_2=11.46, model0_aligned_acc_layer_3=12.49, model0_aligned_acc_layer_4=9.03, model0_aligned_acc_layer_5=14.1, model0_aligned_acc_layer_6=22.17, model0_aligned_acc_layer_7=5.82, model0_aligned_acc_layer_8=90.19, model_name='vgg11_nobias', momentum=0.5, n_epochs=0, no_random_trainloaders=False, normalize_acts=False, normalize_wts=False, not_squared=True, num_hidden_nodes=400, num_hidden_nodes1=400, num_hidden_nodes2=200, num_hidden_nodes3=100, num_hidden_nodes4=50, num_models=2, options_type='generic', params_geometric=9222848, params_model_0=9222848, params_model_1=9222848, partial_reshape=False, partition_dataloader=-1, partition_type='labels', past_correction=True, personal_class_idx=9, personal_split_frac=0.1, pool_acts=False, pool_relu=False, prediction_wts=False, prelu_acts=True, print_distances=False, proper_marginals=False, prunint_rate=0.2, recheck_acc=False, recheck_cifar=True, reg=0.01, reg_m=0.001, reinit_trainloaders=False, result_dir='/root/autodl-tmp/OTfusion_pruning/otfusion/exp_sample/results', retrain=30, retrain_avg_only=False, retrain_geometric_best=0.10000000670552253, retrain_geometric_only=False, retrain_lr_decay=-1, retrain_lr_decay_epochs=None, retrain_lr_decay_factor=None, retrain_model0_best=0.8018000304698943, retrain_model1_best=0.8062000453472138, retrain_naive_best=0.7898000478744507, retrain_seed=-1, rootdir='/root/autodl-tmp/OTfusion_pruning/otfusion/exp_sample', run_mode='benchmark', same_model=-1, save_result_file='sample.csv', second_config={'dataset': 'Cifar10', 'model': 'vgg11_nobias', 'optimizer': 'SGD', 'optimizer_decay_at_epochs': [30, 60, 90, 120, 150, 180, 210, 240, 270], 'optimizer_decay_with_factor': 2.0, 'optimizer_learning_rate': 0.05, 'optimizer_momentum': 0.9, 'optimizer_weight_decay': 0.0005, 'batch_size': 128, 'num_epochs': 30, 'seed': 42, 'nick': 'naive_averaging', 'start_acc': 16.89}, second_model_name=None, sinkhorn_type='normal', skip_last_layer=False, skip_last_layer_type='average', skip_personal_idx=False, skip_retrain=-1, softmax_temperature=1, standardize_acts=False, sweep_id=90, sweep_name='exp_sample', temperature=20, tensorboard=False, tensorboard_root='./tensorboard', timestamp='2023-09-16_10-25-05_499081', tmap_stats=False, to_download=True, transform_acts=False, unbalanced=False, update_acts=False, weight_stats=True, width_ratio=1, **{'trace_sum_ratio_classifier.weight': 1.0, 'trace_sum_ratio_features.0.weight': 0.046875, 'trace_sum_ratio_features.11.weight': 0.00390625, 'trace_sum_ratio_features.13.weight': 0.001953125, 'trace_sum_ratio_features.16.weight': 0.001953125, 'trace_sum_ratio_features.18.weight': 0.0, 'trace_sum_ratio_features.3.weight': 0.0078125, 'trace_sum_ratio_features.6.weight': 0.0, 'trace_sum_ratio_features.8.weight': 0.0078125})\n"
     ]
    }
   ],
   "source": [
    "!python main.py --gpu-id 0 --model-name vgg11_nobias --n-epochs 0 --save-result-file sample.csv \\\n",
    "--sweep-name exp_sample --correction --ground-metric euclidean --weight-stats \\\n",
    "--geom-ensemble-type wts --ground-metric-normalize none --sweep-id 90 \\\n",
    "--ckpt-type best --dataset Cifar10 --ground-metric-eff --recheck-cifar --activation-seed {activation_seed} \\\n",
    "--prelu-acts --past-correction --not-squared --exact\\\n",
    "--learning-rate 0.004\\\n",
    "--momentum 0.5\\\n",
    "--batch-size-train 64 --experiment-dir {experiment_dir}\\\n",
    "--prunint-rate {prunint_rate}\\\n",
    "--run-mode {run_mode}\\\n",
    "--to-download\\\n",
    "--load-model-dir {load_model_dir}\\\n",
    "--retrain 30\\\n",
    "--eval-aligned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c4db3b58-04ae-4bdf-b853-68b86e28644a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#1:prune without finetune\n",
    "activation_seed=21\n",
    "experiment_dir=\"experiment1\"\n",
    "prunint_rate=0.2\n",
    "run_mode='prune'\n",
    "load_model_dir='./cifar_models'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "acf10f3e-1c4a-40b3-85af-c6fc57512703",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------- Setting up parameters -------\n",
      "dumping parameters at  /root/autodl-tmp/OTfusion_pruning/otfusion/exp_sample/configurations\n",
      "The parameters are: \n",
      " Namespace(act_bug=False, act_num_samples=100, activation_histograms=False, activation_mode=None, activation_seed=21, alpha=0.7, baseroot='/root/autodl-tmp/OTfusion_pruning/otfusion', batch_size_test=1000, batch_size_train=64, center_acts=False, choice='0 2 4 6 8', cifar_style_data=False, ckpt_type='best', clip_gm=False, clip_max=5, clip_min=0, config_dir='/root/autodl-tmp/OTfusion_pruning/otfusion/exp_sample/configurations', config_file=None, correction=True, csv_dir='/root/autodl-tmp/OTfusion_pruning/otfusion/exp_sample/csv', dataset='Cifar10', debug=False, deprecated=None, deterministic=False, diff_init=False, disable_bias=True, dist_epochs=60, dist_normalize=False, dump_final_models=False, dump_model=False, enable_dropout=False, ensemble_step=0.5, eval_aligned=False, exact=True, exp_name='exp_2023-09-16_14-53-20_717630', experiment_dir='experiment1', geom_ensemble_type='wts', gpu_id=0, gromov=False, gromov_loss='square_loss', ground_metric='euclidean', ground_metric_eff=True, ground_metric_normalize='none', handle_skips=False, importance=None, learning_rate=0.08, load_model_dir='./cifar_models', load_models='', log_interval=100, model_name='vgg11_nobias', momentum=0.5, n_epochs=90, no_random_trainloaders=False, normalize_acts=False, normalize_wts=False, not_squared=True, num_hidden_nodes=400, num_hidden_nodes1=400, num_hidden_nodes2=200, num_hidden_nodes3=100, num_hidden_nodes4=50, num_models=2, options_type='generic', partial_reshape=False, partition_dataloader=-1, partition_type='labels', past_correction=True, personal_class_idx=9, personal_split_frac=0.1, pool_acts=False, pool_relu=False, prediction_wts=False, prelu_acts=True, print_distances=False, proper_marginals=False, prunint_rate=0.2, recheck_acc=False, recheck_cifar=True, reg=0.01, reg_m=0.001, reinit_trainloaders=False, result_dir='/root/autodl-tmp/OTfusion_pruning/otfusion/exp_sample/results', retrain=30, retrain_avg_only=False, retrain_geometric_only=False, retrain_lr_decay=-1, retrain_lr_decay_epochs=None, retrain_lr_decay_factor=None, retrain_seed=-1, rootdir='/root/autodl-tmp/OTfusion_pruning/otfusion/exp_sample', run_mode='prune', same_model=-1, save_result_file='sample.csv', second_model_name=None, sinkhorn_type='normal', skip_last_layer=False, skip_last_layer_type='average', skip_personal_idx=False, skip_retrain=-1, softmax_temperature=1, standardize_acts=False, sweep_id=90, sweep_name='exp_sample', temperature=20, tensorboard=False, tensorboard_root='./tensorboard', timestamp='2023-09-16_14-53-20_717630', tmap_stats=False, to_download=True, transform_acts=False, unbalanced=False, update_acts=False, weight_stats=True, width_ratio=1)\n",
      "refactored get_config\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "in _make_layers [Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), AvgPool2d(kernel_size=1, stride=1, padding=0)]\n",
      "Relu Inplace is  False\n",
      "model parameters are \n",
      " [torch.Size([64, 3, 3, 3]), torch.Size([128, 64, 3, 3]), torch.Size([256, 128, 3, 3]), torch.Size([256, 256, 3, 3]), torch.Size([512, 256, 3, 3]), torch.Size([512, 512, 3, 3]), torch.Size([512, 512, 3, 3]), torch.Size([512, 512, 3, 3]), torch.Size([10, 512])]\n",
      "in _make_layers [Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), AvgPool2d(kernel_size=1, stride=1, padding=0)]\n",
      "Relu Inplace is  False\n",
      "model parameters are \n",
      " [torch.Size([64, 3, 3, 3]), torch.Size([128, 64, 3, 3]), torch.Size([256, 128, 3, 3]), torch.Size([256, 256, 3, 3]), torch.Size([512, 256, 3, 3]), torch.Size([512, 512, 3, 3]), torch.Size([512, 512, 3, 3]), torch.Size([512, 512, 3, 3]), torch.Size([10, 512])]\n",
      "in _make_layers [Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), AvgPool2d(kernel_size=1, stride=1, padding=0)]\n",
      "Relu Inplace is  False\n",
      "model parameters are \n",
      " [torch.Size([64, 3, 3, 3]), torch.Size([128, 64, 3, 3]), torch.Size([256, 128, 3, 3]), torch.Size([256, 256, 3, 3]), torch.Size([512, 256, 3, 3]), torch.Size([512, 512, 3, 3]), torch.Size([512, 512, 3, 3]), torch.Size([512, 512, 3, 3]), torch.Size([10, 512])]\n",
      "in _make_layers [Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), AvgPool2d(kernel_size=1, stride=1, padding=0)]\n",
      "Relu Inplace is  False\n",
      "model parameters are \n",
      " [torch.Size([64, 3, 3, 3]), torch.Size([128, 64, 3, 3]), torch.Size([256, 128, 3, 3]), torch.Size([256, 256, 3, 3]), torch.Size([512, 256, 3, 3]), torch.Size([512, 512, 3, 3]), torch.Size([512, 512, 3, 3]), torch.Size([512, 512, 3, 3]), torch.Size([10, 512])]\n",
      "Pruning with custom mask (all conv layers)\n",
      "Can not find [features.0.weight] in mask_dict\n",
      "Can not find [features.3.weight] in mask_dict\n",
      "Can not find [features.6.weight] in mask_dict\n",
      "Can not find [features.8.weight] in mask_dict\n",
      "Can not find [features.11.weight] in mask_dict\n",
      "Can not find [features.13.weight] in mask_dict\n",
      "Can not find [features.16.weight] in mask_dict\n",
      "Can not find [features.18.weight] in mask_dict\n",
      "\n",
      "--------- Testing in global mode ---------\n",
      "size of test_loader dataset:  10000\n",
      "\n",
      "Test set: Avg. loss: 0.0003, Accuracy: 9019/10000 (90%)\n",
      "\n",
      "Pruning with custom mask (all conv layers)\n",
      "Can not find [features.0.weight] in mask_dict\n",
      "Can not find [features.3.weight] in mask_dict\n",
      "Can not find [features.6.weight] in mask_dict\n",
      "Can not find [features.8.weight] in mask_dict\n",
      "Can not find [features.11.weight] in mask_dict\n",
      "Can not find [features.13.weight] in mask_dict\n",
      "Can not find [features.16.weight] in mask_dict\n",
      "Can not find [features.18.weight] in mask_dict\n",
      "\n",
      "--------- Testing in global mode ---------\n",
      "size of test_loader dataset:  10000\n",
      "\n",
      "Test set: Avg. loss: 0.0003, Accuracy: 9046/10000 (90%)\n",
      "\n",
      "layer features.0.weight has #params  1728\n",
      "layer features.3.weight has #params  73728\n",
      "layer features.6.weight has #params  294912\n",
      "layer features.8.weight has #params  589824\n",
      "layer features.11.weight has #params  1179648\n",
      "layer features.13.weight has #params  2359296\n",
      "layer features.16.weight has #params  2359296\n",
      "layer features.18.weight has #params  2359296\n",
      "layer classifier.weight has #params  5120\n",
      "Activation Timer start\n",
      "Activation Timer ends\n",
      "------- Geometric Ensembling -------\n",
      "Timer start\n",
      "Previous layer shape is  None\n",
      "Processing the coordinates to form ground_metric\n",
      "dont leave off the squaring of the ground metric\n",
      "returns a uniform measure of cardinality:  64\n",
      "returns a uniform measure of cardinality:  64\n",
      "the transport map is  tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')\n",
      "Ratio of trace to the matrix sum:  tensor(0.0469, device='cuda:0')\n",
      "Here, trace is 3.0 and matrix sum is 64.0 \n",
      "this is past correction for weight mode\n",
      "Shape of aligned wt is  torch.Size([64, 3, 9])\n",
      "Shape of fc_layer0_weight_data is  torch.Size([64, 3, 9])\n",
      "Previous layer shape is  torch.Size([64, 3, 3, 3])\n",
      "shape of layer: model 0 torch.Size([128, 64, 9])\n",
      "shape of layer: model 1 torch.Size([128, 64, 9])\n",
      "shape of previous transport map torch.Size([64, 64])\n",
      "Processing the coordinates to form ground_metric\n",
      "dont leave off the squaring of the ground metric\n",
      "returns a uniform measure of cardinality:  128\n",
      "returns a uniform measure of cardinality:  128\n",
      "the transport map is  tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        ...,\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0078],\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],\n",
      "       device='cuda:0')\n",
      "Ratio of trace to the matrix sum:  tensor(0.0078, device='cuda:0')\n",
      "Here, trace is 1.0 and matrix sum is 128.0 \n",
      "this is past correction for weight mode\n",
      "Shape of aligned wt is  torch.Size([128, 64, 9])\n",
      "Shape of fc_layer0_weight_data is  torch.Size([128, 64, 9])\n",
      "Previous layer shape is  torch.Size([128, 64, 3, 3])\n",
      "shape of layer: model 0 torch.Size([256, 128, 9])\n",
      "shape of layer: model 1 torch.Size([256, 128, 9])\n",
      "shape of previous transport map torch.Size([128, 128])\n",
      "Processing the coordinates to form ground_metric\n",
      "dont leave off the squaring of the ground metric\n",
      "returns a uniform measure of cardinality:  256\n",
      "returns a uniform measure of cardinality:  256\n",
      "the transport map is  tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')\n",
      "Ratio of trace to the matrix sum:  tensor(0., device='cuda:0')\n",
      "Here, trace is 0.0 and matrix sum is 256.0 \n",
      "this is past correction for weight mode\n",
      "Shape of aligned wt is  torch.Size([256, 128, 9])\n",
      "Shape of fc_layer0_weight_data is  torch.Size([256, 128, 9])\n",
      "Previous layer shape is  torch.Size([256, 128, 3, 3])\n",
      "shape of layer: model 0 torch.Size([256, 256, 9])\n",
      "shape of layer: model 1 torch.Size([256, 256, 9])\n",
      "shape of previous transport map torch.Size([256, 256])\n",
      "Processing the coordinates to form ground_metric\n",
      "dont leave off the squaring of the ground metric\n",
      "returns a uniform measure of cardinality:  256\n",
      "returns a uniform measure of cardinality:  256\n",
      "the transport map is  tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        ...,\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.0039, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],\n",
      "       device='cuda:0')\n",
      "Ratio of trace to the matrix sum:  tensor(0.0078, device='cuda:0')\n",
      "Here, trace is 2.0 and matrix sum is 256.0 \n",
      "this is past correction for weight mode\n",
      "Shape of aligned wt is  torch.Size([256, 256, 9])\n",
      "Shape of fc_layer0_weight_data is  torch.Size([256, 256, 9])\n",
      "Previous layer shape is  torch.Size([256, 256, 3, 3])\n",
      "shape of layer: model 0 torch.Size([512, 256, 9])\n",
      "shape of layer: model 1 torch.Size([512, 256, 9])\n",
      "shape of previous transport map torch.Size([256, 256])\n",
      "Processing the coordinates to form ground_metric\n",
      "dont leave off the squaring of the ground metric\n",
      "returns a uniform measure of cardinality:  512\n",
      "returns a uniform measure of cardinality:  512\n",
      "the transport map is  tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')\n",
      "Ratio of trace to the matrix sum:  tensor(0.0039, device='cuda:0')\n",
      "Here, trace is 2.0 and matrix sum is 512.0 \n",
      "this is past correction for weight mode\n",
      "Shape of aligned wt is  torch.Size([512, 256, 9])\n",
      "Shape of fc_layer0_weight_data is  torch.Size([512, 256, 9])\n",
      "Previous layer shape is  torch.Size([512, 256, 3, 3])\n",
      "shape of layer: model 0 torch.Size([512, 512, 9])\n",
      "shape of layer: model 1 torch.Size([512, 512, 9])\n",
      "shape of previous transport map torch.Size([512, 512])\n",
      "Processing the coordinates to form ground_metric\n",
      "dont leave off the squaring of the ground metric\n",
      "returns a uniform measure of cardinality:  512\n",
      "returns a uniform measure of cardinality:  512\n",
      "the transport map is  tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')\n",
      "Ratio of trace to the matrix sum:  tensor(0.0020, device='cuda:0')\n",
      "Here, trace is 1.0 and matrix sum is 512.0 \n",
      "this is past correction for weight mode\n",
      "Shape of aligned wt is  torch.Size([512, 512, 9])\n",
      "Shape of fc_layer0_weight_data is  torch.Size([512, 512, 9])\n",
      "Previous layer shape is  torch.Size([512, 512, 3, 3])\n",
      "shape of layer: model 0 torch.Size([512, 512, 9])\n",
      "shape of layer: model 1 torch.Size([512, 512, 9])\n",
      "shape of previous transport map torch.Size([512, 512])\n",
      "Processing the coordinates to form ground_metric\n",
      "dont leave off the squaring of the ground metric\n",
      "returns a uniform measure of cardinality:  512\n",
      "returns a uniform measure of cardinality:  512\n",
      "the transport map is  tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')\n",
      "Ratio of trace to the matrix sum:  tensor(0.0020, device='cuda:0')\n",
      "Here, trace is 1.0 and matrix sum is 512.0 \n",
      "this is past correction for weight mode\n",
      "Shape of aligned wt is  torch.Size([512, 512, 9])\n",
      "Shape of fc_layer0_weight_data is  torch.Size([512, 512, 9])\n",
      "Previous layer shape is  torch.Size([512, 512, 3, 3])\n",
      "shape of layer: model 0 torch.Size([512, 512, 9])\n",
      "shape of layer: model 1 torch.Size([512, 512, 9])\n",
      "shape of previous transport map torch.Size([512, 512])\n",
      "Processing the coordinates to form ground_metric\n",
      "dont leave off the squaring of the ground metric\n",
      "returns a uniform measure of cardinality:  512\n",
      "returns a uniform measure of cardinality:  512\n",
      "the transport map is  tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        [0.0020, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        ...,\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],\n",
      "       device='cuda:0')\n",
      "Ratio of trace to the matrix sum:  tensor(0., device='cuda:0')\n",
      "Here, trace is 0.0 and matrix sum is 512.0 \n",
      "this is past correction for weight mode\n",
      "Shape of aligned wt is  torch.Size([512, 512, 9])\n",
      "Shape of fc_layer0_weight_data is  torch.Size([512, 512, 9])\n",
      "Previous layer shape is  torch.Size([512, 512, 3, 3])\n",
      "shape of layer: model 0 torch.Size([10, 512])\n",
      "shape of layer: model 1 torch.Size([10, 512])\n",
      "shape of previous transport map torch.Size([512, 512])\n",
      "Processing the coordinates to form ground_metric\n",
      "dont leave off the squaring of the ground metric\n",
      "ground metric is  tensor([[1.4241, 2.4872, 2.7804, 2.8719, 2.7374, 2.8799, 2.7276, 2.6342, 2.5641,\n",
      "         2.5922],\n",
      "        [2.4889, 1.3253, 2.6732, 2.5914, 2.7584, 2.5501, 2.4174, 2.5215, 2.3048,\n",
      "         2.4101],\n",
      "        [2.7600, 2.6711, 1.3832, 2.9465, 2.9065, 2.8701, 2.7276, 2.8385, 2.7485,\n",
      "         2.7866],\n",
      "        [2.8352, 2.6791, 2.8919, 1.5430, 2.8605, 2.9159, 2.7574, 2.7980, 2.8164,\n",
      "         2.6354],\n",
      "        [2.7551, 2.5960, 2.8635, 2.8450, 1.5042, 2.7447, 2.7853, 2.6811, 2.7031,\n",
      "         2.7294],\n",
      "        [2.9190, 2.5396, 2.9469, 2.8416, 2.7272, 1.5502, 2.7781, 2.7303, 2.7977,\n",
      "         2.6697],\n",
      "        [2.6778, 2.4457, 2.7190, 2.9111, 2.6488, 2.8526, 1.3931, 2.6890, 2.4981,\n",
      "         2.5560],\n",
      "        [2.7394, 2.5496, 2.7799, 2.7903, 2.7151, 2.6590, 2.7190, 1.3924, 2.5583,\n",
      "         2.5121],\n",
      "        [2.5972, 2.3660, 2.7693, 2.6872, 2.7026, 2.8280, 2.5325, 2.6366, 1.2999,\n",
      "         2.4041],\n",
      "        [2.5918, 2.2730, 2.7816, 2.7576, 2.6697, 2.7273, 2.5482, 2.5290, 2.5207,\n",
      "         1.2878]], device='cuda:0', grad_fn=<PowBackward0>)\n",
      "returns a uniform measure of cardinality:  10\n",
      "returns a uniform measure of cardinality:  10\n",
      "the transport map is  tensor([[0.1000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0000, 0.1000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0000, 0.0000, 0.1000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.1000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.1000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.1000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.1000, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.1000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.1000,\n",
      "         0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.1000]], device='cuda:0')\n",
      "Ratio of trace to the matrix sum:  tensor(1., device='cuda:0')\n",
      "Here, trace is 9.99755859375 and matrix sum is 9.99755859375 \n",
      "this is past correction for weight mode\n",
      "Shape of aligned wt is  torch.Size([10, 512])\n",
      "Shape of fc_layer0_weight_data is  torch.Size([10, 512])\n",
      "using independent method\n",
      "in _make_layers [Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), AvgPool2d(kernel_size=1, stride=1, padding=0)]\n",
      "Relu Inplace is  False\n",
      "model parameters are \n",
      " [torch.Size([64, 3, 3, 3]), torch.Size([128, 64, 3, 3]), torch.Size([256, 128, 3, 3]), torch.Size([256, 256, 3, 3]), torch.Size([512, 256, 3, 3]), torch.Size([512, 512, 3, 3]), torch.Size([512, 512, 3, 3]), torch.Size([512, 512, 3, 3]), torch.Size([10, 512])]\n",
      "\n",
      "--------- Testing in global mode ---------\n",
      "size of test_loader dataset:  10000\n",
      "\n",
      "Test set: Avg. loss: 0.0023, Accuracy: 1019/10000 (10%)\n",
      "\n",
      "len of model parameters and avg aligned layers is  9 9\n",
      "len of model_state_dict is  9\n",
      "len of param_list is  9\n",
      "\n",
      "--------- Testing in global mode ---------\n",
      "size of test_loader dataset:  10000\n",
      "\n",
      "Test set: Avg. loss: 0.0015, Accuracy: 8545/10000 (85%)\n",
      "\n",
      "===============================================\n",
      "===============================================\n",
      "After Ensemble: 85.45\n",
      "===============================================\n",
      "===============================================\n",
      "Timer ends\n",
      "Time taken for geometric ensembling is 3.620155781507492 seconds\n",
      "------- Prediction based ensembling -------\n",
      "\n",
      "Test set: Avg. loss: 0.0003, Accuracy: 9133/10000 (91%)\n",
      "\n",
      "------- Naive ensembling of weights -------\n",
      "[torch.Size([64, 3, 3, 3]), torch.Size([64, 3, 3, 3])]\n",
      "torch.Size([64, 3, 3, 3])\n",
      "[torch.Size([128, 64, 3, 3]), torch.Size([128, 64, 3, 3])]\n",
      "torch.Size([128, 64, 3, 3])\n",
      "[torch.Size([256, 128, 3, 3]), torch.Size([256, 128, 3, 3])]\n",
      "torch.Size([256, 128, 3, 3])\n",
      "[torch.Size([256, 256, 3, 3]), torch.Size([256, 256, 3, 3])]\n",
      "torch.Size([256, 256, 3, 3])\n",
      "[torch.Size([512, 256, 3, 3]), torch.Size([512, 256, 3, 3])]\n",
      "torch.Size([512, 256, 3, 3])\n",
      "[torch.Size([512, 512, 3, 3]), torch.Size([512, 512, 3, 3])]\n",
      "torch.Size([512, 512, 3, 3])\n",
      "[torch.Size([512, 512, 3, 3]), torch.Size([512, 512, 3, 3])]\n",
      "torch.Size([512, 512, 3, 3])\n",
      "[torch.Size([512, 512, 3, 3]), torch.Size([512, 512, 3, 3])]\n",
      "torch.Size([512, 512, 3, 3])\n",
      "[torch.Size([10, 512]), torch.Size([10, 512])]\n",
      "torch.Size([10, 512])\n",
      "in _make_layers [Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False), ReLU(), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), AvgPool2d(kernel_size=1, stride=1, padding=0)]\n",
      "Relu Inplace is  False\n",
      "model parameters are \n",
      " [torch.Size([64, 3, 3, 3]), torch.Size([128, 64, 3, 3]), torch.Size([256, 128, 3, 3]), torch.Size([256, 256, 3, 3]), torch.Size([512, 256, 3, 3]), torch.Size([512, 512, 3, 3]), torch.Size([512, 512, 3, 3]), torch.Size([512, 512, 3, 3]), torch.Size([10, 512])]\n",
      "\n",
      "--------- Testing in global mode ---------\n",
      "size of test_loader dataset:  10000\n",
      "\n",
      "Test set: Avg. loss: 0.0023, Accuracy: 1143/10000 (11%)\n",
      "\n",
      "\n",
      "--------- Testing in global mode ---------\n",
      "size of test_loader dataset:  10000\n",
      "\n",
      "Test set: Avg. loss: 0.0023, Accuracy: 1689/10000 (17%)\n",
      "\n",
      "-------- Retraining the models ---------\n",
      "Retraining model :  model_0\n",
      "num_epochs---------------+++))))))+++ 300\n",
      "num_epochs---------------+++))))))+++ 300\n",
      "lr is  0.05\n",
      "number of epochs would be  30\n",
      "num_epochs-------------- 30\n",
      "Epoch 000\n",
      "/root/miniconda3/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:129: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\"Detected call of `lr_scheduler.step()` before `optimizer.step()`. \"\n",
      "/root/miniconda3/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:154: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "accuracy: {'epoch': 0, 'value': 0.5033000000000002} ({'split': 'train'})\n",
      "cross_entropy: {'epoch': 0, 'value': 1.4018298149490362} ({'split': 'train'})\n",
      "accuracy: {'epoch': 0, 'value': 0.7034000277519227} ({'split': 'test'})\n",
      "cross_entropy: {'epoch': 0, 'value': 0.9205841004848481} ({'split': 'test'})\n",
      "Epoch 001\n",
      "^C\n",
      "Traceback (most recent call last):\n",
      "  File \"main.py\", line 338, in <module>\n",
      "    _, best_retrain_acc = routines.retrain_models(args, [*original_models, geometric_model, naive_model], retrain_loader, test_loader, config, tensorboard_obj=tensorboard_obj, initial_acc=initial_acc, nicks=nicks)\n",
      "  File \"/root/autodl-tmp/OTfusion_pruning/otfusion/routines.py\", line 392, in retrain_models\n",
      "    retrained_network, acc = cifar_train.get_retrained_model(args, retrain_loader, test_loader, old_networks[i], config, output_root_dir, tensorboard_obj=tensorboard_obj, nick=nick, start_acc=initial_acc[i])\n",
      "  File \"/root/autodl-tmp/OTfusion_pruning/otfusion/./cifar/train.py\", line 333, in get_retrained_model\n",
      "    best_acc,model = main(config, output_dir, args.gpu_id, pretrained_model=old_network, pretrained_dataset=(train_loader, test_loader), tensorboard_obj=tensorboard_obj,return_model=True)\n",
      "  File \"/root/autodl-tmp/OTfusion_pruning/otfusion/./cifar/train.py\", line 75, in main\n",
      "    loss.backward()\n",
      "  File \"/root/miniconda3/lib/python3.8/site-packages/torch/_tensor.py\", line 307, in backward\n",
      "    torch.autograd.backward(self, gradient, retain_graph, create_graph, inputs=inputs)\n",
      "  File \"/root/miniconda3/lib/python3.8/site-packages/torch/autograd/__init__.py\", line 154, in backward\n",
      "    Variable._execution_engine.run_backward(\n",
      "KeyboardInterrupt\n"
     ]
    }
   ],
   "source": [
    "!python main.py --gpu-id 0 --model-name vgg11_nobias --n-epochs 90 --save-result-file sample.csv \\\n",
    "--sweep-name exp_sample --correction --ground-metric euclidean --weight-stats \\\n",
    "--geom-ensemble-type wts --ground-metric-normalize none --sweep-id 90 \\\n",
    "--ckpt-type best --dataset Cifar10 --ground-metric-eff --recheck-cifar --activation-seed {activation_seed} \\\n",
    "--prelu-acts --past-correction --not-squared --exact\\\n",
    "--learning-rate 0.08\\\n",
    "--momentum 0.5\\\n",
    "--batch-size-train 64 --experiment-dir {experiment_dir}\\\n",
    "--prunint-rate {prunint_rate}\\\n",
    "--run-mode {run_mode}\\\n",
    "--to-download\\\n",
    "--load-model-dir {load_model_dir}\\\n",
    "--retrain 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f467a3e4-8ba5-45f2-b0e9-8c80dd5fe3e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#2:prune finetune\n",
    "activation_seed=21\n",
    "experiment_dir=\"experiment1\"\n",
    "prunint_rate=0.2\n",
    "run_mode='finetune'\n",
    "load_model_dir='./cifar_models'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6e7e97b-c2e3-453d-8c13-88d3c93fad93",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python main.py --gpu-id 0 --model-name vgg11_nobias --n-epochs 90 --save-result-file sample.csv \\\n",
    "--sweep-name exp_sample --correction --ground-metric euclidean --weight-stats \\\n",
    "--geom-ensemble-type wts --ground-metric-normalize none --sweep-id 90 \\\n",
    "--ckpt-type best --dataset Cifar10 --ground-metric-eff --recheck-cifar --activation-seed {activation_seed} \\\n",
    "--prelu-acts --past-correction --not-squared --exact\\\n",
    "--learning-rate 0.08\\\n",
    "--momentum 0.5\\\n",
    "--batch-size-train 64 --experiment-dir {experiment_dir}\\\n",
    "--prunint-rate {prunint_rate}\\\n",
    "--run-mode {run_mode}\\\n",
    "--to-download\\\n",
    "--load-model-dir {load_model_dir}\\\n",
    "--retrain 30"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
